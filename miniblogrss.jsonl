{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura Data Delivery Network Alpha Description: We are excited to unveil the alpha version of Hasura Data Delivery Network (DDN) and share key features of our new platform building on the advancements introduced in Hasura version 3.  Author: Rahul Agarwal"}, {"role": "assistant", "content": "We are excited to announce the alpha version of Hasura Data Delivery Network (DDN) and sharing key features of our new platform, building on the advancements introduced in Hasura version 3. In speaking with developers building APIs and microservices, we realize the community is faced with significant challenges. Hasura DDN is being built to address these problems and simplify the lives of software developers and architects everywhere. Here\u2019s what we heard most frequently in our conversations:1. Developers want to create APIs in their preferred programming languageAdopting a new language involves a steep learning curve and requires adapting to different tools. As a result, in creating a new microservice or API, developers often opt for the programming language they\u2019re most comfortable with \u2013 a preference driven by concerns about performance, reliability, backward compatibility, and integration, especially for crucial applications. Also, a new language and framework usually means more training, adding overhead to the business.Not just a black box \u2013 Hasura DDN is a low-code API development platform, fully language agnostic, so developers are free to augment Hasura's capabilities and compose business logic in the programming languages they\u2019ve already invested in.2. Developers want to create complex and feature-rich APIs \u2013 fast.Creating an API server that aligns with business needs and user expectations involves addressing various requirements \u2013 \u00a0supporting different API styles (such as Relay, GraphQL queries, subscriptions, pagination, and sorting) and formats like REST and gRPC. Backend developers are navigating complexity, designing for scalability, defining user experiences, testing and monitoring, and maintaining the codebase \u2013 and it\u2019s exceptionally time-consuming.Hasura DDN enables developers to create complex API features in minutes, rather than the days or months it would have taken to build manually. The capabilities for creating feature-rich APIs extend across all of our data connectors. Our platform is also extensible \u2013 our GraphQL capabilities extend to other API protocols, like gRPC and REST, with our open metadata specification. 3. Developers want the ability to quickly test and iterate.Building a feature-rich microservice that optimally utilizes the data source demands significant expertise and effort to make inferences from the available information. Another challenge is implementing continuous integration (CI) to handle frequent code changes and database schema adjustments. This process involves writing extensive boilerplate code, managing dependencies, and coordinating with other developers.Hasura DDN offers a free and intuitive developer environment with robust tooling, seamlessly combining local and cloud development for efficient API design, testing, iteration, and maintenance. The platform also provides code generation and reusable components for rapid development, allowing full flexibility for customization.Hasura DDN Alpha FeaturesOn the path to achieving the vision of Hasura being a \"CDN\"-like layer for real-time (transactional), analytical, and streaming data, the following are features found in the Hasura DDN alpha release.New console geared for rapid onboarding and troubleshootingWe have introduced major changes to the console in Hasura DDN. We have added features that help in onboarding developers in your team on your data domain. That way, you\u2019re able to collaborate and go to market faster. As the number of models and relationships increases, so does the complexity of understanding and making sense of it. That\u2019s why we are rolling out tools on the console for rich visualization of the data domains and their interactions. On top of that, users can now drill down into each of the models and commands and browse through further details, such as model-level analytics, fields, permissions, and relationship details.A large part of going to production successfully is to quickly debug and troubleshoot errors and issues. We have enhanced our monitoring capabilities in Hasura DDN, providing users the ability to get detailed traces and spans of the request as it traverses from the Hasura server all the way to the data source. That way, with all the context and information easily available to them, users quickly detect any latency issues while locating the root cause of the error in real time.Hasura DDN also features a new database administration tool that\u2019s ideal for exploring and editing data in PostgreSQL for now and going forward in most of the other supported databases and it\u2019s geared to scale to massive workloads.Code-first declarative API authoring and iterationOne of the main pain points of our developer community over the past few years has revolved around metadata authoring, management, and deployment. We\u2019ve heard complaints from users on the time a metadata change takes to get built and rolled out, mainly because of Hasura cold-start issues. On top of that, the hard coupling of Hasura to upstream services (such as the database or remote API servers) led to Hasura not being available unless upstream services were available. And lastly, being dependent on a metadata database implied that users have another thing to run and maintain in production.In Hasura DDN, we have introduced a build and runtime separation \u2013 a pattern that is analogous to compiler-based programming languages. The DDN now has a serverless runtime built-in with per-request tenancy. Each metadata change will now produce an atomic build and an associated artifact that is loaded to Hasura production instantly. This means users no longer experience cold starts and can achieve zero downtime rollouts without any restarts or downtimes. Each atomic build represents an immutable API that can be incrementally tested and applied to a project so that it is made available to API consumers. Hasura DDN is also no longer tied to the upstream services and introspection is not decoupled from runtime. As a result, there is no dependence on upstream services and users can now safely go to production multiple times in a day or even an hour.In Hasura DDN, users can interact with Hasura in a code-first way. To aid in declarative authoring and editing of metadata, we are introducing some potent metadata management tools such as a YAML-based metadata file type HML \u2013 a brand new language server protocol (shipped with VScode extension) to provide language features like autocomplete and code actions. Users also have access to a brand-new CLI that offers useful scaffolding and helps provide logical separations of the metadata via folders and files. You no longer need to browse a large metadata file and can instead navigate a file structure that makes the most sense for your team. The new CLI will stitch the metadata together and build the API from different folders and files.Lastly, to leverage the convenience of DDN cloud\u2019s out-of-the-box features, yet easily prototype using local resources like a local database or a local authentication server, the CLI now provides secure connect options in the form of a local daemon and a cloud-based tunnel server system. This would mean that you can develop your APIs to do all local development and testing scenarios while still being deployed on Hasura Cloud, making it even faster to go to production from the development stages. The experience is designed to be highly secure ensuring only those involved in the user's projects have access. It also offers a seamless experience, aiming to replicate the feel of true local development as closely as possible.Developers accustomed to using other web frameworks in a code-first approach can now quickly adapt and efficiently develop in code with DDN.Feature-rich data domain modelingHasura users have often wanted more control over how the schema is generated, what names are used, and how types are generated. As we have expanded to other types of data sources, such as NoSQL databases and API servers, we realized our users have been spending time to understand the Hasura metadata structure. Furthermore, the metadata specification was predominantly centered on SQL and GraphQL, which limited its adaptability to other protocols such as gRPC.In Hasura DDN, we\u2019ve made significant changes to our metadata structure with the release of the new OpenDataDomain specification that provides fixed and a small set of types such as Types, Models, Commands, Permissions, and Relationships to compose and author all forms of APIs encompassing complex business logic.As a result, users can employ contract-first or schema-first types of development practices while developing APIs with Hasura, freeing up time to focus more on domain design than how the APIs are actually implemented.The domain-led design approach using Models and Commands could be of great benefit to separate the command (write) and query (read) responsibilities in APIs. This pattern can be beneficial in reflecting the distinct needs and optimizations required for data modification versus data retrieval, and help users design, develop, and deploy scalable command and read graphs in production. Similarly, Permissions type can help set bounded contexts and define specific boundaries for different parts of the domain, while Relationships type can aid in dealing with interconnected contexts or bounded contexts and facilitating communication and integration between these contexts.With the new metadata specification, users have a single ubiquitous language across data sources and domains that can be shared by domain experts, developers, and stakeholders. Furthermore, API generation now becomes protocol agnostic as the same terms and concepts from this language could be used for GraphQL, Relay, REST, gRPC, and so on, ensuring consistency and clarity across the domain and the API.Given the strict and typed metadata contract, there\u2019s a higher correctness guarantee and more opportunities for Hasura to automate query planning and caching across future domains.Embed business logic using TypeScriptOur users have asked for more flexibility and feature support when it comes to implementing custom business logic in Hasura. While the Actions functionality provided a lot of support in this area, it was limited and also led to a learning curve for most users. There was also the burden of hosting functions in a different environment and bringing that to Hasura to implement a simple but core piece of business logic for the API.In Hasura DDN alpha, users can code in TypeScript, host the TypeScript functions in Hasura Cloud, and embed that function-backed business logic into the Hasura API instantly. This allows users to use the tooling and libraries of their choice while leveraging the instant production-grade API server creation capabilities of Hasura. It also enables highly requested features, such as data transformations with mutations, and provides new and innovative ways to do pre-mutation input validations and integrate with APIs on the Hasura platform. As part of alpha, users can leverage the new TypeScript Deno Connector, write their functions in index.ts, use the Hasura LSP to automatically generate Command based metadata, and instantly build to start using the API and a functional command graph.Native extensibility with data connectorsHasura DDN is a completely modular and extensible platform. Users can bring in any data connectors that follow the new Hasura native data connector specification to leverage the type of functionalities that Hasura currently offers with native databases such as PostgreSQL. With the recent release of the Hasura Connector Hub, all native data connectors are open source and free to use with Hasura DDN.We have two types of data connectors:1. Verified and Hasura-hosted connectorsThese connectors are deployed in multi-region and have stricter performance and availability SLAs. Currently, in alpha, we support PostgreSQL and its flavors in this category.2. Community-developed connectorsThese connectors need to be separately deployed before building Hasura metadata, either in the Hasura Cloud infrastructure or in your own environment. Currently in alpha, we support ClickHouse, SendGrid, and TypeScript (Deno).To help in fast iteration and less overhead, we have a CLI plugin to help deploy community-developed connectors on Hasura Cloud. After deployment, you can use the connector URL to embed in Hasura metadata as a data source for a Model.We also have extensive resources to aid in developing your own connectors with the NDC learn course and SDKs in TypeScript and Rust. How to explore Hasura DDN alphaTake a sneak peek into the future of data delivery \u2013 building supergraphs at the API layer \u2013 and contribute to shaping our product vision. Help us build the best, most useful product for your needs.Be among the first to experience Hasura DDN. Check out the following resources:Docs on getting started: https://hasura.io/docs/3.0/Register here to access the playback recording of our November 30 Community Call, which featured powerful Hasura DDN demos and the product roadmap.Connect with other alpha users and our product development team on Discord. We would love to hear from you! You can also reach out to us via email at team-product@hasura.io.Try out our Learn course here, where you can follow a tutorial and get started today!The road ahead for Hasura DDNAs we push forward in our mission to revolutionize microservice development and move towards the release of the beta version of Hasura DDN in early 2024, we're eager to iron out any rough edges and improve the experience based on invaluable feedback from our alpha users.The beta roadmap is razor-focused on improving and tightening the developer experience as we acknowledge our product is still evolving. Your participation will be crucial in identifying areas for refinement and we urge you to stay engaged, share your thoughts, and help us ensure the beta launch delivers the best developer experience possible. Areas of focus in the coming weeks:Metadata authoring enhancements: LSP code generation and auto-complete enhancements. Making it easier to build permissions and add relationships to models and commands. Ease of use for debugging metadata errors at the time of authoring. Improvements to the CLI watch command for rapid iteration loops.Avoiding API breaking changes: Build validations with dry run mode and compile time guarantees. Improved error handling at build time. Enhanced operational health dashboard and metrics for advanced error handling, insights, troubleshooting, and production monitoring of APIs across builds.Integrating business logic DX: Improvements to the TypeScript SDK and connector for accelerating connector and business logic development with fast iteration loops. Ease of use for importing REST and GraphQL.Team collaboration: Enhancements to the multi-repo workflow for building a supergraph across multiple data domains owned by different teams.API features: Complete the API feature set including mutations and other query features.V3-engine open source release: Making the API server behind DDN open source.Migration: Streamlined migration utilities for current v2 users to transition to DDN.We greatly value your feedback, and our commitment to crafting an exceptional developer experience is deeply rooted in the insights and suggestions we receive from our community.Thank you for being the driving force behind Hasura DDN's evolution.Let\u2019s continue this transformative journey together. We encourage you to share your thoughts, queries, or feedback about the alpha launch. Get access to Hasura DDN alpha and get started today!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Debunking GraphQL Myths and Misconceptions Description: In this myth-busting blog post, we dive deep into the top misconceptions surrounding GraphQL. From concerns about complexity, performance and developer experience. Author: Akshaya Acharya"}, {"role": "assistant", "content": "People well-versed in GraphQL are often found discussing the latest compilation strategy, or the best way to optimize performance of federation. But sometimes, for people new to GraphQL, it\u2019s simply not clear what GraphQL is doing in the first place. (And the name doesn\u2019t help.)Here are some common concerns/myths about GraphQL, from people getting acquainted with GraphQL, and clarifications to resolve them.Myth: GraphQL, the query language is designed to let clients perform arbitrary queriesThis is true, but just applicable during the development phase for developers who are building the client app. That's what makes GraphQL great. The DX of GraphQL consumption is amazing. The idea is to use tools like GraphiQL, test out different queries required for the client and integrate it in your application. GraphQL is self-documenting and tooling is much better than the equivalent REST spec - Swagger docs.But, when in production, you typically restrict what queries are allowed with an allow list or persisted queries and simply call them from the client. Wrongly implemented security doesn't make GraphQL bad. Any API can be vulnerable if you choose it to be. REST APIs already sends boatload of objects that clients didn't need anyways.Myth: GraphQL is a query language, like SQL for graph databasesThis is just due to the sheer coincidence of naming!The name GraphQL is a hint at the modeling and execution semantics. Queries are composed of a root Query field, which has nested fields. Each field can have further nested \u2013 and related \u2013 fields. This makes for a tree structure, which is a subgraph of the whole data graph that we can query.A graph is a very generic structure \u2013 any data source can be mapped to it. For instance, for a relational database like PostgreSQL, you could map tables to nodes, and relationships to edges in the graph. The key thing about a graph is that it emphasizes the connected nature of data, encouraging good modeling of the domain by identifying the different entities or models and laying out the relationships.graphql\nquery {\n  posts(where: user_id == 4) {\n    created_at\n    content\n\n    user {\n      name\n    }\n\n    comments(order_by: created_at desc, limit: 10) {\n      body\n    }\n  }\n}\n\nEvery field in a GraphQL schema (which specifies all the allowed queries as a collection of related types) can be backed by arbitrary code \u2013 exactly like an RPC endpoint \u2013 but with the additional advantage of having complete context of where it sits in the query tree.Further, because clients query all the related data they need in one go, the whole query is available to the server, which means the server can perform global optimizations across the whole query during the execution. For example, the server can push down requests for related models into database joins, which is otherwise not possible if the client is making successive queries to data related to a previous query in an N+1 pattern.Myth: GraphQL servers are hard to build and maintainEveryone agrees that they want a GraphQL API, the question then becomes one of cost.The majority of complaints about the complexity of implementation and cost of maintenance come from working at the wrong level of abstraction.Programmers building GraphQL like this often complain about repeated code and boilerplate. You also hear complaints about building layers of wrappers and adapters, and how it's difficult to keep these layers up to date as modeling and functionality evolves.Teams that successfully scale their GraphQL APIs all seem to converge on the same pattern: They're all building supergraphs! They're all using an engine like Hasura, where they specify the data modeling \u2013 the models and relations \u2013 and the engine automatically builds the GraphQL API (see how here).When your GraphQL APIs are based on supergraphs, there's no cruft and you can iterate rapidly. Want to change something in your data modeling or underlying microservice? Simply change the engine configuration and your API keeps pace effortlessly.Comparing writing resolvers manually to authoring supergraphs is like comparing JavaScript to React or NextJS. There is no comparison. And nothing stops you from using JavaScript when you exceed the limitations of the framework.Myth: GraphQL has the N+1 query problemWell, it does and it does not. A naive GraphQL server implementation has the N+1 query problem. With dataloader batching, it really isn't N+1 anymore. Tools like Hasura, Postgraphile compile GraphQL to SQL efficiently. It is much faster than a dev team trying to optimize from scratch. A badly implemented server doesn't make GraphQL bad.But yes, most tutorials start with the naive resolver methods and that is a problem. This goes back to the previous myth as well \u2013 GraphQL servers are hard to build and maintain. Myth: GraphQL is unnecessarily complex for small teamsThe truth is that, for a small project, your tech stack doesn't really matter. At that point, it matters a lot more what tech your team has experience with! If you can get away with using a Google Form instead of building something, you probably should. A landing page could be your startup's MVP to validate a product. As simple as possible... but no simpler.If your team has a single database and full stack engineers iterating rapidly across the entire stack, building and iterating on features both on mobile apps and the server, especially when you're iterating rapidly on the API itself, then RPC can be far easier to work with than GraphQL. The additional structure imposed by GraphQL can begin to look like a burden. At this stage, you'll often hear developers claim that it's so easy to add and modify RPC endpoints that GraphQL isn't worth it. And maybe they're right.GraphQL shines as a method to query richly connected data. Building a supergraph creates this richly connected data. You can now model relations across different teams and domains in organizations, a supergraph can model relations across different data sources. Siloed data comes alive and is discoverable.Supergraphs solve the harder problem of communication. (See Conway's Law). What this means is that every team can own and build their own graph with crystal clear interfaces, and then the supergraph just stitches these together, while making sure that all the constraints are enforced.This is better than using just a simple API gateway because the supergraph is based on models. Unlike an API gateway, a supergraph is not limited by the structure of the data that the service layers return. We can go further, and begin to look at all our data across all sources as part of a single large source that\u2019s queried flexibly, with a query planner that optimizes queries across the whole supergraph.When does a small product become a large product, and where do the tradeoffs begin to make sense? Perhaps when the product is validated? Perhaps when you add another data source. Perhaps when you have some clarity on the contracts that you want the API to have and you're ready to share it with the world!Myth: Caching is hard in GraphQL\u201cThere are only two hard things in Computer Science: cache invalidation and naming things.\u201d \u2013 Phil KarltonThe whole caching lifecycle is tricky irrespective of protocols involved. In GraphQL, there is client side caching and there is server side caching. GraphQL with its schema design and the client tooling like URQL and Apollo Client, the frontend caching story is much better than REST. Pre GraphQL/TypeScript, frontend caching wasn't even on the radar. TypeSafe clients are being used on the frontend with granular caching rules.On the server side, the fact that everything is a HTTP POST request in GraphQL makes it different to reason about caching with existing tooling that is built for years supporting REST spec at the network layer. The mental model is different. Tooling already exists (@stellate) and there is more work to be done in this space. The truth is, caching is tricky irrespective of whether you are dealing with GraphQL or REST. In GraphQL, you cannot rely on HTTP verbs like GET that you typically use to cache in REST. Instead, you need to look deeper in query responses which also may have errors inside.{\n\u201cdata\u201d: [], -> cache fields inside this\n\u201cerrors\u201d: [] -> look for errors here and ignore\n}\n\nConclusionIt\u2019s often not easy for engineers deep in a tech ecosystem to adopt a first-principles point of view and evaluate technology from a first-principles basis.It is important to understand the tradeoffs and work as a team on both the frontend and backend and you will get the value of different tools. GraphQL is great for being the data layer that stitches together different microservices into a supergraph. The consumption layer can be anything \u2013 REST/GraphQL/gRPC. It really doesn't matter at the end of the day. Incrementally adopt any tool that fits your needs.Hopefully, recognizing these common myths help you better evaluate GraphQL for your needs. And better yet, if GraphQL is a good fit, help evangelize it as a solution for your team or organization.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Remote Table Authorization: Write permission rules across all your data Description: We are thrilled to introduce our latest feature: Remote Table Authorization. \n With Remote Table Authorization, you can now use data from different sources to perform authorization checks on any table. Author: Sooraj Sanker"}, {"role": "assistant", "content": "In the ever-evolving world of data management, allowing complex access control becomes paramount for business success. We are thrilled to introduce our latest feature: Remote Table Authorization. With Remote Table Authorization, you can now use data from different sources to perform authorization checks on any table.Background and ProposalThis innovative project was born out of a pressing need raised by our Cloud Team and AI Innovations team. The use case is not only common but also incredibly powerful, as evidenced by its recent showcase in the HasuraCon 2023 AI talk through an ad-hoc PoC (Proof of Concept).Hasura's permission system has always excelled in filtering data, treating permissions as a special case of filtering. However, a significant challenge emerged when it came to filtering based on fields from remote relationships.What is Remote Table Authorization?Consider you have two tables on your database as follows: Defining a permission based on this setup was already easy with Hasura when the 2 tables are in the same data source, or as long as there is a column in the target table to compare with. ie. you could add a row access permission logic on posts that check for user_id equals to x-hasura-user-id as followsBut, as we see the data growing and getting distributed across different systems, you may notice the individual data units could be managed by different teams or even different tools.Now imagine the posts table, which is mentioned above, is coming from an external CRM system, where the user_id is different, and you don\u2019t have the luxury to modify the posts table schema or add an additional column to it.Usually in such cases what we will do is, to create a column mapping on existing user table to accommodate the crm_user_id which the CRM is using as follows, and add a Hasura remote relationship as follows.Now that we have the next set of problems, how do we write the permission for posts?This is where the Remote Table Authorization helps! You can now define permissions across data sources and reference remote relationship row values to contain the permission rule.So in this example, let\u2019s define a remote relationship as follows:and now we will be able to define a permission on the posts table with the remote relationship that we created as follows:As you can see, the permission rule is agnostic on the crm_user_id (id on the posts table), and it uses the user_id from the user table to grant access to the posts.So if you are a data architect, you don\u2019t have to think about the access permissions logic, as it is already taken care of by Hasura. Previously this could not be done directly. And the developers can simply query as:query MyPosts {\n  posts {\n    id\n  title\n    author_id\n    content\n  }\n}\n\nNow, let's make it more interesting! \u00a0Imagine the column author_d in the table posts, is something you don\u2019t want in your API layer- say it's an internal reference or some sensitive identifier.Now, we can remove column select permissions of user_id from posts table and crm_id from users table so that these fields are not visible in the API layer but play a critical role in granting access to the posts. How it worksImagine this scenario where each table resides in a different source. A remote relationship permission (on Table A) might appear as follows:{\n  \"relAB\": {\n          \"id_B\": {\n              \"_eq\": \"X-Hasura-User-Id\"\n          }\n      }\n}\n\nTo evaluate this permission expression, we follow these steps:Retrieve all the id values from table B where id_B equals X-Hasura-User-Id.Then, the boolean expression for table A is id_B in the result set from step 1.So, to evaluate any remote relationship permission, we need to perform these steps. It involves evaluating the inner node and then working our way up, generating a boolean expression for the table on which the permission is defined.LimitationsWhile Remote Table Authorization unlocks exciting possibilities, we acknowledge certain limitations:As we explained the execution, it fetches the remote relationship side first and then the actual table- this means if the response from the remote relationship table is too long, the query to RHS would also be a lengthy `in` query. This has to be considered while using this solution with many-to-many sort of relationship permissions.Currently, the console will show this option for GDC-based data sources by default, if you want to see the console for native data sources, you can enable the \u201cData Tab Performance Mode\u201d feature flag from the settings.ConclusionWith Remote Table Authorization, Hasura continues to lead the way in simplifying and enhancing data access control in GraphQL APIs. We're excited to empower you with the ability to define intricate permissions for remote relationships, offering a robust solution for your most complex data access scenarios.Discover more about Remote Table Authorization by exploring our documentation and giving it a try in your projects. Sign up now to unlock the full potential of your data relationships with Hasura if you are not on Hasura yet!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Architect\u2019s Dilemma: Navigating the world of GraphQL Description: I think a big part of the hype boils down to the fact that GraphQL done right is simply better than any possible equivalent of its REST counterpart. Author: Tristen Harr"}, {"role": "assistant", "content": "Remember not so long ago when GraphQL was the new kid on the block and people were rushing to adopt it? Lots of technologies come and go over the years but few end up getting their own foundation. (Much less a foundation that\u2019s tied into the Linux foundation.) I think a big part of the hype boils down to the fact that GraphQL done right is simply better than any possible equivalent of its REST counterpart. It\u2019s not better because I want it to be better, or because I like how it works more. It\u2019s better in the same way one might say MergeSort is better than BubbleSort. When it comes to all the hard-tech metrics \u2013 primarily \u201chow fast and efficient is it?\u201d \u2013 there's a clear winner. Yet if you Google \u201cIs GraphQL better than REST?\u201d you\u2019ll get mixed reviews. I\u2019ve even seen some GraphQL hate going around and have heard my fair share of nightmare stories. When we say MergeSort is better, the metric we\u2019re tending to use is time complexity. O(n log(n)) beats O(n^2) hands down. If we measure which sorting algorithm is better based on which is easier to implement, BubbleSort wins. There are also edge cases like when evaluating space complexity but let\u2019s be real, nobody is arguing for BubbleSort in production. The comparison between GraphQL and REST is similar in many regards. GraphQL \u2013 if implemented perfectly (hard to do), while properly solving all the known difficulties associated with implementing it elegantly (also hard to do) \u2013 is strictly better, if not for the simple fact that by reducing over-fetching and under-fetching we can literally reduce the amount of electricity our applications use while also speeding up our page-loads. The catch is precisely how hard this is to do. Unlike most sorting algorithms, you can\u2019t implement GraphQL in an afternoon by hand. GraphQL was never intended to be a quick win.I\u2019m of the personal opinion that the companies likely to get GraphQL right have great communication structures. And I believe this is a consequence of Conway\u2019s law, which states any organization that designs a system will produce a design whose structure is a copy of the organization's communication structure. Stealing a term from DDD circles, perhaps the most important discussions to be had around adopting GraphQL are the ones around your company's ubiquitous language. I\u2019d guess the better an organization is at breaking down the social boundaries within their organization, the stronger their ubiquitous language becomes, and the more likely they are to have great success when adopting GraphQL.There\u2019s also absolutely no reason GraphQL and REST can\u2019t play nicely together. If you can only pick one, I\u2019d vote for GraphQL, but nobody says you can\u2019t have both! So, too, can GraphQL and gRPC find ways to get along. The end motivation of adopting something like GraphQL oftentimes doesn\u2019t center specifically around GraphQL itself, but rather around this idea that\u2019s been starting to gain popularity in enterprise circles of creating supergraphs, which is something GraphQL can enable you to do. This is also one of the core problems that Hasura wants to help solve, by making it easy for Enterprise Architects to build supergraphs, which we\u2019ll peek into a bit deeper at the end of this blog. Building these supergraphs isn't just a technical hurdle; it's an endeavor riddled with complexity, especially when it comes to correctly implementing GraphQL APIs. And trust me, this isn't just a theoretical problem.Table of Contents\n\nPersonal experience has taught me that building GraphQL APIs correctly is hard\nConsuming a well-thought-out GraphQL API is great\nThinking like a node \u2013 some practical examples\n\n1. How much money did we make in February in 2009?\n2. A customer named Frank called in and said he emailed support last week but hasn't heard back. Who's his rep and why haven\u2019t they contacted him?\n3. Which Playlist is the most popular?\n4. Which Tracks are my best sellers?\n\n\nFrom simple to supergraph, tying it all together.\n\nPersonal experience has taught me that building GraphQL APIs correctly is hard.\nA few years before I discovered GraphQL, I was building a lot of REST API\u2019s. My workflow at the time included tools like Postman, and I was a big fan of building things using the OpenAPI spec. As a Python nerd, that often meant building things using FastAPI. And when Python wasn\u2019t a good fit, I would resort to things like Spring Boot. I remember the first time I laid eyes on GraphQL: The promise was beautiful, and right away I went to work on a personal project to give things a try. I quickly realized that when it comes to building things with GraphQL, writing GraphQL resolvers by hand is simply downright painful. If you\u2019re trying to build things with GraphQL and you\u2019re handwriting all of your resolvers, then in nearly every case you\u2019d likely be better off building things using REST. I didn\u2019t realize this until after a decent amount of time breaking myself out of the REST mindset (which is a whole chore on its own).Writing GraphQL resolvers by hand is downright painful.If you\u2019re coming from a REST background, one of the first things you need to do when adopting GraphQL is forget all the rules you\u2019ve been using for what\u2019s likely the majority if not the entirety of your professional development career so you can learn a bunch of new rules that contradict most everything you\u2019ve done or been taught before. One of my favorite posts about GraphQL is the ironically titled, \u201cHow not to learn GraphQL.\u201d While I was learning about GraphQL, at first, I found myself frequently falling victim to one of the biggest mistakes people make \u2013 trying to design GraphQL APIs the way the industry has spent the past 20 years teaching us to design APIs. (Who would\u2019ve thought?) I\u2019ve seen highly talented people and top-tier teams fall victim to this, even with all the warnings out there. When you take a team that\u2019s used to thinking in REST patterns, and you ask them to build something in GraphQL, I\u2019d guess there\u2019s a 90% chance they\u2019re going to end up with a spaghetti-monster schema as developers are going to superimpose a REST-ful mindset onto the whole thing. It\u2019s unbelievably easy to read this or something like it and think, Oh, I\u2019ll keep that in mind, note to self, don\u2019t do that, then go and do precisely that anyways. (I\u2019m guilty of having done it.)Another pattern I\u2019ve seen that often happens once people have managed to get over the mindset switch is they start building out a big pile of fancy new tooling from scratch. I\u2019ve seen multiple roll-your-own attempts at things that automate building resolvers. Then you have to handle the N+1 problem in your resolvers and suddenly things get more complex. If you want subscriptions that\u2019s an entirely different and equally complex conversation. Permissioning and handling authentication with GraphQL is another complexity because there\u2019s a lot larger of an attack-vector when you expose your entire business domain as a graph. You need to think about pagination, and sorting, and by the end of it all, you truly are better off with REST. If you still aren\u2019t convinced, then the final complexity monster I\u2019ll toss on top of it all is federation and handling multi-tenant applications. Or, if you want to push your engineers to the edge of insanity, you can try schema stitching by hand with cross-functional teams and watch the CI/CD build pipeline clog up for hours as each commit that makes a schema change kicks off your test suites. (Bonus points if some of the tests are flaky.) \u00a0I\u2019d posit that herein lies the problem: The bottom line is that using GraphQL is one big complexity monster. If you use it right, it\u2019s great, but good luck using it right on your own. It\u2019s especially tricky because chances are the more \u201cveteran\u201d an engineering team you have, the more likely they are to fall into using the patterns they\u2019ve spent their careers learning \u2013 and then they\u2019ll make a big mess. To adopt GraphQL correctly, you need to dedicate time and resources toward training your team on how to use GraphQL. There also should be an understanding from the top down that the decision to adopt GraphQL is a long-term strategic business decision. Hasura\u2019s Co-Founder and CEO Tanmai Gopal recently spoke at GraphQL Conf about how GraphQL is a strategic choice that can set the business up for success, but it isn\u2019t a tactical choice for quick wins.Lots of people jumped onto the GraphQL bandwagon, and many got burned. That\u2019s what happens when a technology is new and hot and nobody knows how to properly use it but everyone decides to try. As humans, we tend not to want to get burned and quickly learn not to stick our hands into the fire. Lots of developers have been a part of big, ugly, messy GraphQL projects where their job title quickly changed from \u201cSoftware Engineer\u201d to \u201cFirefighter.\u201d That\u2019s been the only experience many have had with GraphQL, and I can\u2019t say I blame those individuals if they\u2019ve sworn off ever trying GraphQL again. I try to avoid flame-breathing complexity monsters and would suspect most smart programmers and architects do, too.</rant>Consuming a well-thought-out GraphQL API is great.\nThat\u2019s the flip side. Once a GraphQL API is built, if it\u2019s been built well with a good structure and a well-thought-out design, it\u2019s a lovely experience to use. It\u2019s mostly the building and maintenance that sucks \u2013 for the consumers, it adds tremendous value.I\u2019m certainly biased in thinking that Hasura is the best way to work with GraphQL, but even without Hasura, there are plenty of better ways than doing it yourself. I think the biggest key is not trying to do it yourself. There are lots of difficult problems to solve when you try to DIY GraphQL. And while it\u2019s certainly a fun exercise, in the same way writing your own database might be, it\u2019s not something I\u2019d be planning to do for systems I wanted to send into production (unless I had the resources to dedicate for what will likely be a years-long engineering effort to get right). We at Hasura have worked to refine the techniques we use to overcome the N+1 problem for a while, and if you want to reinvent them, here\u2019s a potential starting point on how we did it. Good luck and have fun! \ud83d\ude05For us at least, solving these problems is an entire product that requires the dedication of a small army of talented engineers. Trying to handle all of these things on your own in the background, while also working to solve the difficult and unique challenges specific to your product, is perhaps a bit silly.Despite the above rant, there is a reason why GraphQL continues to get so much love and attention from developers and companies alike. When set up correctly, it\u2019s a whole new world. It doesn\u2019t even really have to be GraphQL if we\u2019re being honest. It just needs to be something that does all the same things that GraphQL does, and is as easy to consume as GraphQL is, so we can build a supergraph. A supergraph is an architecture and operating model to build and scale multiple data domains as a single graph of composable entities and operations. It gives you the best of both worlds and can be viewed from the lens of taking a \u201cmonolithic microservices\u201d approach. You can create an application that is logically monolithic, but physically distributed and micro. Building a supergraph is typically a large undertaking. At Hasura we believe it should be as simple as declaratively defining your data model and tucking your custom business logic away as operations on the graph. To build a supergraph all you need to learn to do is to think like a node, and the rest should be after-thoughts. We think that the most important questions should be around the ubiquitous language your organization uses to describe your business domain, rather than the implementation details.Thinking like a node \u2013 some practical examples\nGraphs are foundational to most everything we do inside of a computer, and they are the best way to model real-world systems and processes we have.In these examples, we\u2019ll examine the Chinook Music DB in the context of an organization's data domain. (You can download and play with this dataset if you want to from here). While we won\u2019t go over the schema in detail, there are plenty of examples online that use it, and we\u2019ve included an ERD diagram below. If you want to give Hasura a spin and follow along with the examples, simply deploy this onto Hasura Cloud to get started quickly. Feel free to skip ahead to the next section if this content feels familiar.How does our data model define what we see, talk about, and understand in regards to our business? Let\u2019s jump into a day in the life of our theoretical music store owner, by examining four common real-world questions we might encounter while running a music store.1. How much money did we make in February in 2009?\nWhat this question is asking: What\u2019s the sum total of all the Invoices that have an InvoiceDate that happened in February 2009?When phrased that way, it\u2019s easy to answer and think about. Start at the Invoice Entity, go into that entity's data, find all the invoices that occurred the month that we want, and sum up the total. It\u2019s easy to answer that question even outside of the context of computer science. Not so long ago answering that question meant going to the Invoices filing cabinet, getting the folder for the month and year, and then pulling out each invoice and adding the totals together. This is a simple query, as we aren\u2019t traversing any relationships. The root of our question, which is the place to start looking for the answer, is at the Invoice.Here\u2019s how you could ask this if you were using Hasura: query HowMuchMoneyDidIMakeInFebruary2009 {\n  answer: Invoice_aggregate(where: {InvoiceDate: {_gte: \"2009-02-01T00:00:00\", _lt: \"2009-03-01T00:00:00\"}}) {\n    aggregate {\n      sum {\n        Total\n      }\n    }\n  }\n}\n\nWhich would give you the answer:{\n  \"data\": {\n    \"answer\": {\n      \"aggregate\": {\n        \"sum\": {\n          \"Total\": 37.62\n        }\n      }\n    }\n  }\n}\n\n2. A customer named Frank called in and said he emailed support last week but hasn't heard back. Who's his rep and why haven\u2019t they contacted him?\nThe most important part of knowing how to approach answering a question when data is on the graph is knowing where to start. Once you know how to \u201ctake a walk through your data-graph\u201d you can easily determine the answer.In this case, we start on the Customer node, and we need to find customers named Frank, from there we need to traverse the edge (or, relationship) between Customer and Employee. The answer to the question depends on which Frank we\u2019re talking about. Frank Harris\u2019s support rep is Margaret Park, whereas Frank Ralston\u2019s is Jane Peacock.Asking this in Hasura looks like:query WhoIsFranksSupportRep {\n  answer: Customer(where: {FirstName: {_eq: \"Frank\"}}) {\n    FirstName\n    LastName\n    Employee {\n      FirstName\n      LastName\n    }\n  }\n}\n\nWhich gives you the answer:{\n  \"data\": {\n    \"answer\": [\n      {\n        \"FirstName\": \"Frank\",\n        \"LastName\": \"Harris\",\n        \"Employee\": {\n          \"FirstName\": \"Margaret\",\n          \"LastName\": \"Park\"\n        }\n      },\n      {\n        \"FirstName\": \"Frank\",\n        \"LastName\": \"Ralston\",\n        \"Employee\": {\n          \"FirstName\": \"Jane\",\n          \"LastName\": \"Peacock\"\n        }\n      }\n    ]\n  }\n}\n\n3. Which Playlist is the most popular?\nThis one\u2019s a bit of a trick question, the answer is that this question can\u2019t be answered right now. If this question is important to the business domain, that isn\u2019t represented in the data model. To answer it might mean asking things like which playlist has been listened to the most, or which playlist has the most upvotes. As of right now, all we can say about a Playlist is its name, id, and what Tracks it has. For the current state of our database, this would require a discussion around what metrics we want to track and what we would need to alter the schema to track those. Cue a roadmap planning meeting.4. Which Tracks are my best sellers?\nEach time a track is sold, there will be an Invoice that contains that track in its InvoiceLines. (The InvoiceLine can be thought of as a line item on an itemized receipt.) Thus we can imagine this by thinking about starting at the Track node, and then for each Track we can walk across all the incoming edges from InvoiceLine and find every time the track has shown up as a line item for an Invoice. From there we sum the Quantity for that InvoiceLine, which will give us the total number of times each specific track has been purchased.To answer this question, we\u2019d then want to sort our results and determine which track is the most popular. Hasura provides a great way to do this for you:query WhichTracksAreMyBestSellers {\n  answer: Track(limit: 3, order_by: {InvoiceLines_aggregate: {sum: {Quantity: desc_nulls_last}}}) {\n    TrackId\n    Name\n    InvoiceLines_aggregate {\n      aggregate {\n        sum {\n          Quantity\n        }\n      }\n    }\n  }\n}\n\nIn this dataset, it seems that the best-selling tracks have many ties for first place, and the output of the top three for this query is:{\n  \"data\": {\n    \"answer\": [\n      {\n        \"TrackId\": 2,\n        \"Name\": \"Balls to the Wall\",\n        \"InvoiceLines_aggregate\": {\n          \"aggregate\": {\n            \"sum\": {\n              \"Quantity\": 2\n            }\n          }\n        }\n      },\n      {\n        \"TrackId\": 8,\n        \"Name\": \"Inject The Venom\",\n        \"InvoiceLines_aggregate\": {\n          \"aggregate\": {\n            \"sum\": {\n              \"Quantity\": 2\n            }\n          }\n        }\n      },\n      {\n        \"TrackId\": 9,\n        \"Name\": \"Snowballed\",\n        \"InvoiceLines_aggregate\": {\n          \"aggregate\": {\n            \"sum\": {\n              \"Quantity\": 2\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n\nFrom simple to supergraph, tying it all together.\nI know what you might be thinking. How does a toy data model relate to building supergraphs? I think the real question is who said that this toy model couldn\u2019t be a toy supergraph?Physically, we can put data and compute wherever makes the most sense. (In today\u2019s age, that could mean spilling compute or data onto the edge.) There\u2019s no reason that the logical model we use to represent and talk about things can\u2019t be split up into pieces that are more micro when we physically deploy them. The capability to efficiently perform remote joins across disparate data sources and API\u2019s opens lots of doors for lots of people. We can design and talk about things as if we\u2019re working on a monolith that yields high cohesion and easy governance, yet we can still get the benefits of microservices like loose coupling and scaling ownership. It\u2019s especially compelling when we start to imagine all the different things we could plug into the supergraph. Some of the data connectors I\u2019ve been building that I\u2019m excited about include connectors for databases like Qdrant, Turso, and DuckDB. The supergraph is super-flexible. It\u2019s not too difficult to learn how to build a custom data connector for whatever you might want to connect to. This makes it easy to add whatever capabilities you need directly onto your supergraph and ultimately helps you ship faster.While the path to mastering GraphQL and building a supergraph is riddled with complexities, the journey is well worth the effort. For organizations considering the leap, it's important to approach the design and implementation of a supergraph not as a quick fix but as a strategic choice. Embracing this technology with the right tools and mindset, like leveraging solutions such as Hasura, can significantly simplify the transition. This approach not only eases the technical challenges but also aligns with the long-term vision of efficient and scalable data management. As we continue to innovate in the digital landscape, GraphQL, with its capacity to power supergraphs, stands as a testament to our evolving technological capabilities. Let's embrace this future, with all its challenges and opportunities, to build more robust, efficient, and interconnected systems.I invite you, the reader, to share your experiences, thoughts, or questions about GraphQL and supergraphs. Let's foster a community of learning and exchange around these exciting technologies, and together, let's embrace the future of web development. Reach out to me at tristen.harr@hasura.io"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL for data products Description: To connect backend data products with the chatbot frontend, a reliable, scalable, and secure communication mechanism is needed. This blog post posits that the link should be GraphQL. Author: Hasura"}, {"role": "assistant", "content": "*This blog was originally published by Sanjeev Mohan on his Medium page.\u200b\u200bThe promise of using a chatbot to derive data insights threatens the decades-old dominance of business intelligence reports and dashboards. Some thought leaders have even gone as far as suggesting it disrupts the entire World Wide Web and existing data catalogs.Gen AI isn\u2019t the only transformational trend of 2023. The other shift is in how we build and deploy data outcomes through data products. A data product offers superior, consistent, and reliable data access which allows consumers to get answers to their questions (or a chain of questions) to support business decisions or outcomes.What if the worlds of data products and large language models collide?Could we have a chat interface that simply asks questions to data products in natural language and gets back responses? And if so, what would that access pattern look like?To connect backend data products with the chatbot frontend, a reliable, scalable, and secure communication mechanism is needed. Traditionally this has been the REST APIs, but we need a more dynamic approach to accommodate the changing nature of queries and consumer needs. This blog post posits that the link should be GraphQL.But first, let\u2019s see how REST APIs and GraphQL differ.GraphQL introductionREST is a specification that defines the standard format in which the server transfers information. Clients make a stateless request to the server, which the server fulfills. REST APIs provide a technology-independent, simple, and scalable interface, which decouples clients and servers.GraphQL is a query language for APIs. Meta (Facebook) developed it in 2012 to optimize traffic between its mobile clients and the servers. Its efficiency stems from the fact that clients can request exactly what they need using typed schema.Meta shared the specification with the developer community in 2015. Since 2019, the Linux Foundation has hosted it.BenefitsAs GraphQL has matured, this specification to implement APIs provides benefits over traditional RESTful approaches:Reduces over fetching of data \u2013 REST APIs return more data than needed unless you create a specific endpoint for every data attribute, which is not practical.Reduces under fetching of data \u2013 multiple REST API calls are needed to fetch the desired data if it spans multiple tables and locations unless more specific endpoints are created for joined data.Provides a standard for data access \u2013 REST is a specification and not a standard. Hence, many organizations have implemented APIs that may not be consistent with others.In summary, GraphQL provides consistency through a standard specification while providing flexibility and higher control over the data received from the API calls. The term \u2018graph\u2019 is used because the API treats data as a logical graph structure. The abbreviation QL stands for an expressive \u2018query language\u2019 for the APIs.Typically, GraphQL is implemented over HTTP and its responses are delivered in JSON format but it is protocol agnostic. Its query language is very similar to the JSON syntax. GraphQL can be implemented over existing JSON interfaces.This document examines the benefits of GraphQL as an interface for data products.Challenges and opportunitiesREST is a well-established mature ecosystem for security and monitoring. While GraphQL has demonstrated its efficiencies, its adoption has lagged behind that of REST APIs. Also, GraphQL is more complex to set up than REST. But if we drill down into its complexity, it is in setting up the infrastructure to serve APIs, and not in its actual usage. Luckily, a few companies are easing the creation of the infrastructure framework, like Hasura.In addition, GraphQL can be converted and saved as a REST (or gRPC) endpoint. This allows REST APIs to work out of the box but at the efficiency of GraphQL. Open source options that provide \u201cRESTification of GraphQL\u201d include Hasura, Apollo, and The-Guild.Finally, the HTTP standard is being enhanced with the new HTTP QUERY request method. This upcoming standard will allow GraphQL reads using HTTP QUERY instead of HTTP POST and give a standard way to cache data.These opportunities make GraphQL an excellent choice to be the default access mechanism for data products. But, what exactly is a data product? To answer that, let\u2019s first explore the concept of data products.What is a data product?A data product is a container of data, metadata, and business logic with support for diverse access methods that offer a superior, consistent, and highly reliable consumption experience for a business user to get answers to their questions or a chain of questions to support business decisions.A data product stands out on two important characteristics: user experience and trust. It must have an owner who is accountable for its quality and reliability from the design, build, and runtime, to the retirement of the product. It is a single framework to get answers to all kinds of business questions and in most cases consumed via a self-service interface.Simply publishing a data set does not make it a data product. It must have the other components \u2013 the domain wrapper comprising a semantic layer, business logic, and metrics, and an access layer. Figure 1 illustrates the components of data products.Figure 1. Components of data products.Data products came into prominence with the rise of the concept of a data mesh, which has four principles, including \u201cdata as a product.\u201d While data as a product is a concept, data products are a physical manifestation of the concept. Data products stand out on their own merits and are independent of the concept of data mesh. Its key attributes are:Product management: It must have been developed using product management discipline. Data products have an end-to-end lifecycle \u2014 cradle to grave.Ownership: A data product should have an owner who is accountable for its quality and reliability. The owner should ensure that the data is accurate, up-to-date, and documented.Self-service: A data product should be consumed via self-service. This will allow users to get the information they need when they need it without having to go through a lengthy process of requesting access from IT.User experience: A data product should be easy to use and understand. The user interface should be intuitive and the data should be presented clearly and concisely. A \u201cdata contract\u201d should specify its schema, and access patterns, like APIs and SLAs.Trust: A data product should be reliable and accurate. The data should be curated and the consumers should have the ability to rate it.Single framework: A data product should be a single framework for getting answers to all kinds of business questions. This will make it easier for users to find the information they need and improve the overall efficiency of the business.As applications have adopted the microservices architecture, data products are doing the same in the world of data and analytics. While the entire concept of data products is new, the unique benefits of using GraphQL to access data products make them so compelling. The next section goes into deeper details.Benefits of using GraphQL for data productsData products are typically designed to be accessed through multiple output ports \u2013 user interface (UI), SQL API, and even a command-line interface (CLI). In the future, a chatbot using natural language will allow queries on data products. Many of these access metaphors will use large language models (LLMs) which will expand queries from simply being keywords, to adding semantic or similarity search capabilities.In each of these scenarios, a GraphQL framework can substantially accelerate data product access layer development while bringing consistent abstraction to the data product interface specification.But how does one evaluate if GraphQL is the right data access method?Figure 2 depicts various evaluation considerations. These criteria should apply to your business needs to select the optimal approach.Figure 2. A GraphQL framework provides benefits across many dimensions.User Experience\u201cWhat is easy to understand, gets adopted.\u201d Case in point: HTML. Before HTML, many markup languages existed, like SGML, but they were too complex for mainstream use. For GraphQL to be adopted, nothing is as important as the developer user experience. Developers have in recent years risen to be the most significant influencers of organizations\u2019 technical buying decisions.Traditionally, developers have written SQL code against the raw tables\u2019 schema. GraphQL appeals to front-end developers not proficient in SQL. They can declaratively describe the needed data which is returned in a single request.In addition, using a visual editor, developers can connect to the source database and construct their GraphQL query in a visual explorer. The engine then converts the GraphQL into SQL statements. A GraphQL frontend is like the well-known Postman tool that software engineers using APIs are familiar with.GraphQL has strongly typed schema and introspection capabilities that help developers build applications faster via code generation, code completion, API exploration, and documentation.GraphQL has backward compatibility with built-in version control using the schema. New types can be incrementally added to the schema without breaking changes to existing clients.GraphQL can vastly expand the number of data consumers who may not be proficient in SQL skills. These roles are often described as citizen roles, as in citizen analysts, and citizen developers. This benefit is similar to how chatbots now generate SQL code (and other languages) to run on behalf of the data consumers.ArchitectureDiverse data access patterns require that the underlying architecture is vendor-neutral. The users can access disparate backend systems in a loosely coupled manner.When developers write queries directly against the source schema, any changes in the schema can cause production bugs in downstream applications. In a loosely coupled architecture, GraphQL reduces the impact of changes to the underlying architecture. Technologies change, but business requirements don\u2019t change that fast. Decoupling increases the agility of the backend data products as they are developed and deployed independently of the data consumers\u2019 tight coupling. This allows them to scale on demand.Poorly implemented APIs can cause performance issues. Because of the nature of GraphQL APIs allowing arbitrary queries, it is important to limit the API calls based on unique parameters like IP or the user. GraphQL query depth can be restricted for users with the right authorization and security rules.CIOs and CDOs are seeking simplification of their burgeoning data and analytics architecture. Modern Data Stack has introduced new complexities that reduce transparency in the pipelines. GraphQL is an attempt to simplify data access. As AI matures, unified access for analytics and AI will become table stakes.Performance and latencyIn economics, Jevons Paradox states that when resources become efficient (and cheap), the demand for the resources, and hence the usage goes up instead of the other way around. Organizations have become more data-driven and have increased consumption of data products. However, data can easily become a victim of its success, unless we address performance and latency aspects.Today, many dashboards have to rely on syncing a local copy of the data warehouse to their desktops to provide ultra-fast analysis. Unfortunately, this model means users are working on stale data and are repeatedly replicating data. GraphQL is a more efficient way to connect dashboards to the server and allow analysis of real-time up-to-date data.GraphQL is like an API with a \u201cwhere\u201d clause. This is one of GraphQL\u2019s standout capabilities. It reduces over-fetching and under-fetching, making APIs more responsive. Complex multi-level and high-frequency queries are much more efficient when using GraphQL.Concomitant to the previous point about reducing rows that are fetched and returned, network traffic is reduced, further leading to higher efficiency.REST APIs support caching which reduces the number of requests sent to the server. GraphQL query responses can be cached granularly based on who is querying, and which fields they want to cache. With directives and cache-control headers, GraphQL queries can be cached effectively, reducing the number of requests sent to the server.Higher query and analysis performance leads to higher user experience and efficient operations. GraphQL can help improve the overall productivity of developers and end-users.Security and complianceMass adoption of the new LLMs will only happen when organizations can guarantee that sensitive data is protected and all applicable data privacy regulations are being met.Securing access control to underlying data sources is a constant challenge as different personas can access data at various layers of the stack \u2013 from raw object stores to curated persistence layers. GraphQL can be used to limit direct access to the underlying data sources.In addition, GraphQL access to a data product can also guarantee data quality and trust.Data residency laws require that personal or sensitive information should be collected, processed, and stored within a jurisdiction. Data sovereignty regulates who has access to which data. To achieve these data protection mandates, organizations can apply authorization techniques anywhere in the stack:at the storage level,on compute engines orinside application code, libraries, and reporting toolsin middleware or API gateway.Each option has its tradeoffs between cost, performance, and complexity. As every architect\u2019s goal is to isolate their IT stack from future changes and to minimize point solutions, it is safe to apply policies at the API later, as long as it is the only access point into the data storage.GraphQL can help to achieve dynamic authorization so that row-level security (RLS) and column-level security (CLS) can be achieved. At the API layer of abstraction, policies are applied consistently.CostOne of the biggest focus areas in 2023 has been on cost optimization. Growth at any cost is no longer acceptable when the economy faces headwinds. GraphQL can help alleviate wasted costs.As discussed earlier, GraphQL APIs apply checks and filters, which reduce the amount of data requested and returned from the underlying data sources. We can specify the filters at the row-level and at the column-level. As many DBaaS products charge for the amount of data scanned, GraphQL can reduce the database cost. In addition, lower network traffic also leads to lower costs.GraphQL makes it more efficient to build, deploy, and run data and analytics stacks. And it is also cheaper to maintain than a traditional API stack.Use casesOrganizations are facing a litany of tech challenges:IT teams are under pressure to address business consumers\u2019 growing data requests. They are often a bottleneck due to overburdened resources.Tech stack complexity is one of the biggest roadblocks to modernization or digital transformation initiatives.They are keen to do more with their data assets, but not at the risk of unpredictable costs.The key to their success is through more modern access based on GraphQL. GraphQL enables front-end applications using Node.js or React framework to modularly integrate data products to deliver disparate use cases.Business teams are developing multiple data products for various use cases. GraphQL can ease the integration of capabilities that can span operational, streaming, transactional, and analytical to deliver a derived data product.Before data products, IT teams have tried to unify operational and analytical databases (HTAP), but using GraphQL APIs as the data products interface is faster and more flexible.Because of GraphQL\u2019s inherent design that includes GraphQL subscriptions, real-time streaming use cases, such as anomaly detection, monitoring, auditing, and notification are easier to implement.We have so far examples of the use of GraphQL on analytical architectures and data products. But, for reasons like regulatory, some organizations are not allowed to consolidate data into a cloud data warehouse or a lakehouse. In such a case, standardized and consistent GraphQL APIs can federate data products with other data sources. GraphQL access is commonly used by data virtualization products, like Denodo.GraphQL can provide seamless communication between mobile clients and data products. This use case extends how GraphQL is already used to connect from a mobile device to a server database, likeAgility and developer productivityDevelopers yield higher influence than at any time in the past. A successful enterprise today needs very high developer productivity. GraphQL can make developers more productive.Developers may not be the final decision-makers in selecting new technologies, but they are heavy influencers. If developers adopt certain technologies that improve their productivity, organizations tend to make them the standards. Products like Hasura provide a user interface to build and run queries which improves the release cycle time for data products.Typically, developers spend a lot of time developing exact endpoints for their APIs. If a change is requested, then the APIs need to be further version-controlled. GraphQL overcomes the constant need for versioning of traditional APIs.GraphQL APIs are self-documenting. As and when the API changes, GraphQL can be used as a testing ground to quickly explore and test the APIs. Hasura console comes with a built-in GraphQL interface for exploring the APIs. Hasura\u2019s schema registry helps developers prevent breaking changes from being added inadvertently to production. After looking at the GraphQL schema diff, developers will have a clear view of what has changed and whether it affects their API in production among other things.EcosystemTypically, the success of a technology is less predicated on its features, but more on how good is the overall ecosystem across the end-to-end supply chain. This requires the technology to have an engaged community and integrations with other products within the ecosystem.A major appeal of open source software is because of a vibrant community of developers who build and contribute features and functionality. Although GraphQL is new, companies like Hasura have built communities to foster the development of this new specification.REST is not a standard but a set of principles that lead to inconsistencies although it provides flexibility.The data access framework should be well integrated with the orchestration tools, such as Apache Airflow, Prefect, and Dagster. It should also be closely integrated with the operations environment consisting of DevOps, DataOps, MLOps, LLMOps.Finally, It should also integrate with the data Governance environment. Data products catalog is a concept to enable their discovery as well as track their usage. In addition, the GraphQL framework should allow users access to business glossary, lineage, observability, and other metadata management features.A rich ecosystem is developing around GraphQL which is essential for its mainstream adoption.SummaryIn conclusion, some of the benefits of GraphQL are as follows:Efficiency \u2013 only fetch the data you need e.g. specific fields. This reduces network trafficking and improves concurrency, especially when the bandwidth is limited. This batching and data loader pattern also reduces the load on databases.Concurrency \u2013 Multiple resources can be requested in a single call thereby reducing the overhead of managing multiple HTTP requests.Latency \u2013 Data fetching can be optimized, with different parts of the query being executed concurrently as long as there are no dependenciesReal-time \u2013 Real-time subscriptions allow clients to receive updates as soon as the data changes.Prioritization \u2013 Assign different levels of concurrency or rate limiting to different types of queries or mutations based on their importance and resource requirements.Error handling \u2013 A consistent and predictable error format can make it easier to identify and handle errors gracefully, especially when multiple requests are being processed simultaneously.Recent surveys show organizations, on average, have over 100 SaaS applications. Analytical architectures are complex and schemas change frequently. As a result, the REST API should rest in peace. It has served us well and is a good choice for simple applications. However, its lack of standards and inefficiency leads to unsustainable costs in the modern era. It is time to explore the next generation of API access, via GraphQL that is better suited for AI applications.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How to convert TypeScript types to GraphQL types: better serverless functions Description: What if you could write TypeScript functions and that was enough to make a fully typed API in GraphQL? And while we\u2019re at it, can we get better serverless functions? Author: Akshaya Acharya"}, {"role": "assistant", "content": "What if you could write TypeScript functions and that was enough to make a fully typed API in GraphQL? And while we\u2019re at it, can we get better serverless functions?Write this...... and get this...type Email {\n  id: Int!\n  address: String!\n  body: String!\n}\n\ntype Mutation {\n  sendEmail(address: String!, body: String!): number!\n  sendEmailWithSignature(address: String!, body: String!): number!\n  getEmail(id: Int!): Email!\n}\n\nThese aren't just types \u2013 we can get a full API.How it worksHere's the code that makes getting a fully typed API possible: https://github.com/hasura/ndc-typescript-denoThe gist of it is parsing the TypeScript code (the TypeScript compiler exposes libraries for this), and then building out the GraphQL API.Hasura v3 has a connector architecture, where the engine (that receives and executes the query) uses connectors to connect different data sources to the graph. In Hasura, the TypeScript code is run by a connector, and the engine is configured by way of Hasura metadata (a data modeling tool) with the types and models that are backed by the functionsWe can literally just write TypeScript functions and run a few commands to get a GraphQL API. To me, this is the most compelling implementation of serverless functions. Other serverless functions today don't really run functions as much as they get you to implement http servers. The advantage of defining only functions is that the engine can now expose different APIs on top of it, be it GraphQL, RPC, or any other. Functions also compose far better than http endpoints, which allows for building semantic graphs.ConclusionHasura wants to empower TypeScript developers to deploy their work and make it usable for a wide audience. Larger organizations can incorporate the functions into their supergraph implementations.We believe in a declarative future, to bring maximum power to domain engineers, and the Hasura TypeScript connector is a big part of that story \u2013 by allowing architects to extend their model-based supergraph with custom logic and side-effects.ResourcesBuild TypeScript connectors: https://github.com/hasura/ndc-typescript-denoA real example that integrates with Sendgrid: https://github.com/hasura/ndc-sendgrid-deno"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Hasura open source observability stack Description: Explore how Hasura seamlessly integrates state-of-the-art, open source observability tools via OpenTelemetry giving developers a holistic view of performance, user interactions, and potential bottlenecks. Author: Raju Sunny"}, {"role": "assistant", "content": "In today's fast-paced tech landscape, understanding the intricacies of your application's performance is paramount. In this blog post, we'll explore how Hasura seamlessly integrates state-of-the-art, open source observability tools via OpenTelemetry giving developers a holistic view of performance, user interactions, and potential bottlenecks.But first, what is observability?Observability allows us to gain insight into a system from the outside by asking questions about it, and without knowing how it works internally. Furthermore, it enables us to quickly diagnose and tackle unexpected issues.To be able to ask those questions of a system, the application must be appropriately instrumented. That is, the application code must emit signals such as traces, metrics, and logs (the tree pillars of observability). When developers don't need to add further instrumentation to troubleshoot an issue because they have all of the information they need, an application is appropriately instrumented.What is Telemetry?Telemetry is one of the most reliable ways to monitor the stability and health of software in the realm of software development and application monitoring. Telemetry aids developers in maintaining awareness of the software's performance and alerts them when an issue arises with the application. Applications are no longer monolithic. Maintainers and developers clearly need a simple method to collect data from these complex, and diverse systems, and transmit it for monitoring and analysis.What is OpenTelemetry?OpenTelemetry is an open source observability framework that provides a unified way to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) from software applications. It is vendor-neutral and supports a wide range of languages and platforms.The figure below shows the rough architecture and capabilities of an OpenTelemetry implementation.OpenTelemetry Reference ArchitectureSo... why is this a big deal?Hasura supporting OpenTelemetry standards means a couple of important things for you and your organization.Saves timeYou will not have to deal with the inconvenience of installing various proprietary agents for various backend systems or re-instrumenting code. As a result, there is no need to create new connections as new technologies emerge because the OpenTelemetry collector will continue to trace data and export it without any issues.Avoid vendor lock-inWhen an APM vendor is no-longer suitable for your organization (cost, capabilities, etc.) you don't need to worry about their own custom monitoring tools or agents or connectors. You can freely move across to a better or cheaper APM vendor who will support OpenTelemetry almost seamlessly. You keep your end-to-end visibility with any APM vendor.TransparencyWith OpenTelemetry, you define what data is transmitted outside your application. No more wondering what a proprietary agent might be collecting and sending. You have full visibility and control over collected and transmitted data from your organization or application.So, how can I use OpenTelemetry with Hasura?Hasura fully supports OpenTelemetry for its logs, traces, and metrics. This applies to Hasura Cloud (Enterprise) and Hasura self-hosted (Enterprise). Navigate to your Settings/Monitoring and Observability section from both Cloud and Enterprise console, enable OpenTelemetry Exporter and configure your endpoints according to our documentation here. And that's it. Your Hasura signals are now available on your preferred APM tool.Introducing the Hasura observability stackEnterprise observability doesn't have to cost you a lot. Sure, there is a wide variety of APM platforms you can choose from. But one of the greatest observability stacks is available for free.As a reference, we at Hasura have created a pre-built, pre-configured, fully open-source-based observability stack for you. It comes with everything you need to have an enterprise grade observability using open source tools. Prometheus for your metrics, Grafana for visualizations, Jaeger for your tracing data, and AlertManager for event monitoring and alerting. Giving it a try is as easy as downloading a dockerfile and running docker compose. It brings up the latest Hasura GraphQL engine and all the components needed for end-to-end observability.Our observability stack comes with dashboards hand-built by Hasura engineers. They are one-click installable, and designed to show the right information that matters to you. Head over to our community repo here to give it a spin.Getting startedThe quickest way to get started with our open source observability stack is to clone the repository and use docker. There are two important files here you would need changes made todocker-compose.ymlModify the file to choose components you want to deploy. By default, Hasura GraphQL Engine, Prometheus, Grafana, Redis, Postgres (as the metadata DB and data source and an additional instance as a read-replica), Node-exporter, Alertmanager, Blackbox-exporter, Jaeger, Loki and Otel-collector. The selection depends on what components you want to deploy and what type of observability matters to you most.2. Dotenv\ta. Add your Hasura Enterprise license key to the variable \tHGE_EE_LICENSE_KEY.\tb. Set up your GraphQL admin secret key\tc. Optionally, protect your metrics endpoint with a secret. Although optional, this is a recommended step.\td. Choose the Hasura image version you would like to deploy by setting the value of HGE_IMAGE\te. Optionally, choose specific versions of the components listed previously\tf. Rename the file to .env in the same location.3. Deploy the system using docker compose up -d or using the swarm scripts provided in the \u201cutils\u201d folder.The default configuration enables the Native Prometheus exporter. If you want to push metrics to 3rd-party services that support OpenTelemetry, first disable the metrics API and configure the OpenTelemetry metrics endpoint to the otel-collector service. Browse the console -> Settings -> OpenTelemetry Exporter and configure the metrics endpoint.Embedded dashboardsTo help you get started quickly, we have included a few hand-built dashboards with this distribution. These are built to give you the best view into what matters most in terms of observing Hasura. The dashboards are broadly classified into two categories \u2013 general and Hasura. GeneralSystem Monitor gives you basic system information of the environment where you have deployed this stack. This includes CPU, Memory and Storage use, and system load.HTTP-Ping monitors various components you have deployed here, including the status of Hasura GraphQL Engine\u2019s health, and various data connectors.HasuraAn information-dense dashboard showing you a quick overview of most of the golden signals emitted by Hasura. This dashboard could be your \u201cquick-glance\u201d into your Hasura deployment\u2019s health.Hasura Subscriptions is a subscription specific dashboard giving you a detailed view of your subscription metrics. You can monitor the current active subscriptions, historical subscription information, websocket usage, latencies, and your subscription poller statistics.Hasura Logs and Traces: All logs generated by Hasura GraphQL Engine, and tracing information is available here. Trace information is available on the Jaeger web interface, too, which you can access using http://<your_deployment_address>:4002Hasura - HTTP GraphQL: Quick overview of your GraphQL metrics (including historical data) on your queries, mutations, HTTP connections and your cache metrics.Hasura Health: Health information of your Hasura deployment including health-check stats for your data sources, and connections to your data sources.Extending the Hasura observability stackYou are not limited by the dashboards we have provided or the components involved here. You are free to extend the functionality of this deployment to integrate with systems in your environment. For example, if you need Alert Manager to integrate with your enterprise IRP (Say PagerDuty), you can configure one of the many first-class integrations available. You can configure your own alerts using Grafana Expressions. An example where we need to alert users if your GraphQL error rates increased by a certain percentage in the last five minutes can be created using the following Grafana expression (increase(hasura_graphql_requests_total{response_status=\"failed\"}[5m]) / increase(hasura_graphql_requests_total[5m]))>= 0.5\n``\nSet a severity level (INFO, WARNING, CRITICAL etc), create an error message for your alert The error rate of GraphQL requests from your instance {{ $labels.instance }} exceeds {{ $value | humanizePercentage }}\n``\nAnd you are all set. You can configure more alert-rules by going to the Alert Rules (The Hamburger menu on the left, expand, and go to Alerts).Wrapping up our observability journeyAs we've journeyed through the Hasura observability stack, I\u2019m sure you will agree that it empowers you to proactively address issues, optimize user experiences, and confidently scale our application without being tied to a vendor, or an agent that you need to install \u2013 but do not know a lot about. As the digital world continues to evolve, staying ahead of challenges is crucial, and with OpenTelemetry by our side, we're equipped to navigate the future. Thank you for joining us on this exploration, and here's to building more resilient, efficient, and user-friendly applications together! Happy observability, everyone."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Compiling GraphQL for optimal performance: going beyond Dataloader Description: An implementation-level description of the architecture that allows Hasura v3 to achieve better-than-Dataloader performance across multiple different data sources. Author: Akshaya Acharya"}, {"role": "assistant", "content": "Updated 2023-11-17: Added benchmarks comparing simple dataloader approach to optimized database query using JSON variables. Jump to benchmarks.In this article: An implementation-level description of the architecture that allows Hasura v3 to achieve better-than-Dataloader performance across multiple different data sources.\n\nFor GraphQL practitioners: How to serve GraphQL more efficiently, including example code to generate efficient database queries.\nFor architects of entity based frameworks that construct a supergraph to relate and query different data sources: The method that underlies Hasura's execution engine, including a complete spec of the API contracts.\n\nPerformance is a foundational feature of Hasura; after all, Hasura is built to leverage ALL of your data. This has been true all the way since 2015, when the query execution engine started off written in Haskell, to now \u2013 hundreds and thousands of iterations later \u2013 with Hasura v3 being written in Rust. The fundamental techniques, however, are applicable to anybody who is building a GraphQL server, in any language, and with any architecture.\nThe venerable Dataloader from Facebook makes an appearance, and we'll see where in the architecture to place it for maximum effectiveness. We\u2019ll also see how we can go further.\nTOC\nContext and terminology\nThe technique has three parts (for architects):\n\nDecompose the query based on subgraphs\nQuery compilation into efficient database queries, and execution (for backend engineers)\n2a. catalog connector (three methods)\n2b. feedback connector (three methods)\n- Supergraph Top-N Challenge\nConsolidation of result parts\n\nPrior work\nConclusion\nResources\nContext and terminology\nA supergraph is composed of Models that form the nodes, and Relationships that form the edges. A Model is backed by a data source like tables in a database or RPC endpoints. (In addition, there are also Commands for mutations, not discussed further here.)\nA Relationship maps a field on a Model [1] to either one or many of another Model. Like a foreign key relationship in a database.\nWe'll use this example query:\nquery {\n    store(id: id_x) {\n        location\n        products(first: first_n) {\n            title\n            reviews(first: first_k) {\n                content\n                user {\n                    name\n                }\n            }\n        }\n    }\n}\n\nWhere the stores and products Models are from the catalog database, and the reviews and users Models are from the feedback database.\nproducts.id field maps to reviews.product_id\nreviews.user_id field maps to user.id.\n1. Decompose the query based on subgraphs\nThe Hasura engine receives the query and decomposes it into subqueries that can execute against different databases. [2]\nHasura is built on a connector architecture that allows us to rapidly add support for any database by building a connector for that data source. The data source can be a database, or existing domain microservice.\nThe connector can incrementally implement more of the Native Data Connector (NDC) Spec and the engine will unlock more and more complex supergraph capabilities for the connector, such as aggregations or cross data source relations.\n2. Query compilation into efficient database queries\nIn this section:\n\nAbstract Syntax Tree (AST) to SQL compilation\nJSON aggregation\n\nWhile everyone agrees that moving joins from your client into the server is a very good idea, Hasura goes further: It moves the joins right into the database.\n2a. catalog database connector\nThe connector for the catalog database receives an NDC Query.\nndcQuery {\n    store(id: id_x) {\n        location\n        products(first: first_n) {\n            id\n            title\n        }\n    }\n}\n\nThe connector knows its domain and can generate optimized queries.\nImportantly, this NDC Query also fetches the products.id field so that the engine can later compose the feedback data.\nTo illustrate the idea, consider how we can convert a GraphQL Query to an optimized SQL query for PostgreSQL.\nFirst, the GraphQL query was converted to the AST. The AST contains all the information that the GraphQL query did, just parsed into an object tree, so we can be convinced that it contains all the information we need to generate our SQL queries.\nA partial implementation of the GraphQL AST to SQL compiler - GitHub\nThe output we want has a nested form:\nResultObject<stores> {\n    Row {\n        location: String\n        products: ResultSet<products> {\n            [\n                Row {\n                    title: String\n                    user_id: Integer\n                }\n            ]\n    }\n}\n\n2a (i) Two separate queries\nWe might consider making two separate queries.\nselect stores.id, stores.location\nfrom stores\nwhere stores.id = id_x\n\nfollowed by\u2026\nselect\n  products.store_id,\n  products.title, products.user_id\nfrom products\nwhere products.store_id in ( id_x )\n\nThe connector would then need to transform this data into the nested form.\nThis approach has the problem that database connectors don't parametrise a variable number of arguments for IN clauses, which means awkward string manipulation. Also, there is a hard limit on the number of parameters that you can pass in.\nWe can do better.\n2a (ii) Lateral joins\nWe can consider an SQL query with left join lateral.\nselect stores.id, stores.location, products.title, products.user_id\nfrom (\n    select *\n    from stores\n    where id = id_x\n) stores\nleft join lateral (\n    select title, user_id\n    from products\n    where store_id = stores.id\n    limit first_n\n) products on true\n\nWe need lateral because this allows us to use sorting and limiting on the products query, independently for each stores row (there could be many, although there is only one here), which allows for complex nested queries.\nThis returns the outer product rows with multiple rows that repeat the store information for each store product, which the connector then needs to transform into the nested structure.\n| stores.id | stores.location | products.title | products.user_id |\n\nThis is already far better than the previous approach. There is only a single query, and we don't need to go back and forth to the database to get all our data.\nHowever, the connector still needs to transform this into our nested structure, which is additional computation to go row by row, group stuff by unique (sets of) columns, and create more objects to generate the nested structure.\nAnd these problems get worse when there's more nesting/joining.\nWe can still do better.\n2a (iii) JSON aggregation\nToday, both PostgreSQL and MySQL support JSON aggregation. Just like the sum aggregation maps a set of rows to a single number, JSON aggregation functions map a set of rows to a single string i.e. a JSON string that represents the set of rows.\nselect json_agg(row_to_json(root_stores))\nfrom (\n    select stores.id, stores.name, products.products\n    from (\n        select *\n        from stores\n        where id = id_x\n    ) stores\n    left join lateral (\n        select json_agg(row_to_json(root_products)) products\n        from (\n            select products.id, products.title\n            from products\n            where products.store_id = stores.id\n            limit first_n\n        ) root_products\n    ) products on true\n) root_stores;\n\nBy moving the data transformation as close to the source as possible, we avoid parsing and transforming the data multiple times, eking out the last drop of performance possible. The JSON that the database sends back has exactly the nested structure that we need!\nOnce the engine receives the response, it collects the products.id fields and sends off the second part of the query to the feedback connector with this information.\n2b. feedback database connector\nIn this section\n\nTop-N Queries\n\nThis connector receives the next NDC Query and the array of products.id as a variable.\n\nvariables: foreach $var in [ p_id1, p_id2, p_id3 ]\n\nndcQuery {\n  reviews(where: product_id = $var, first: first_k) {\n      content\n      user {\n          name\n      }\n  }\n}\n\nThe connector implements the NDC foreach capability, which means it can return the result of executing the NDC query for each of the variables [3]. Specifically, the connector returns an array of results that map to the array of variables; exactly like Dataloader!\nThe crucial bit here is that the engine knows nothing about the implementation, foreach is implemented by the connector, allowing different connectors to optimally choose different strategies depending on the underlying data source.\nThe obvious way to do this is to execute n parallel database queries for each of the n variables. This is what's conventionally known as the n+1 problem.\nLet\u2019s look at some other ways to solve this problem. We already know how to generate the most efficient query for a single variable instance. Now let's add variables. (The joins from the nested query as in the previous section are elided to focus on other techniques.)\n2b (i) IN clause\nselect\nfrom (\n    select * from reviews\n    limit first_k\n) root\nwhere root.product_id in ( p_id1, p_id2, p_id3 );\n\nThis has the limitations of the IN clause discussed above.\nMoreover, this doesn't work for more complex variable use cases, where we want to parametrise other parts of the query.\n2b (ii) JSON operators\nAnother way to do this is to materialize a temporary table with the variable ids, and then construct a query using that table.\nHere's one way to do this using JSON operators to pass in all variables as a JSON string;\nselect\nfrom json_array_elements('[1,2,3]') vars(var)\nleft join lateral (\n    select * from reviews\n    where reviews.product_id = var::text::int\n    limit first_k\n) root on true;\n\nThe database returns the nested structure we need, with top level keys as the variable instances. We can use the variables in any bit of our parametrized query!\nThe connector grabs each top level ResultSet value, puts them in the right order, and sends it back to the engine.\nBenchmarks!\nAs expected, using the JSON operators method to move the foreach into the database performs far better than repeated queries, even for modest number of values for the variable. Here are some benchmarks and waterfall charts for v3.\nSee that sharp drop from 430ms to 130ms towards the end? And that's only for 6 values for the variable!Before: back and forth to the databaseAfter: optimized to a single query. The scale is different though, this takes much less time.2b (iii) Other techniques\nThe pattern of returning the Top N rows for each element of another set is extremely common!\nFor instance, your favorite messaging app might show the last n messages for every thread. Or a fantasy league website might show the Top N people with the largest sum of points, for every tournament.\nConsidering how common this pattern is, it's surprising how few tools naturally support it! And even more surprising how poorly many of them perform. State of GraphQL Gateways in 2023 - The Guild.\nHasura hosts the Supergraph Top N Challenge where you can post your solution and win 1000USD for the best performing solution. At the time of this article, we are hosting this in conjunction with Hacktoberfest 2023.\nHere's a compilation of some solutions that could be useful implementing this pattern against different databases that support different features. Database techniques - Github.\n3. Consolidation of result parts\nThe connectors have really already done most of the work. The engine takes the results returned, and stitches them back to each other at the connector/subgraph/subquery boundaries.\nAt this stage, the engine can also implement features across the whole supergraph. For instance, it can add a Relay global id field for each returned Model instance, and this id is globally unique across all data sources/connectors.\nIt's worth looking at some of the implicit assumptions we made that enable all this to work.\nThe engine must have an excellent idea of the data model to plan the query. This includes how Models map to connectors (i.e. data sources), their Types, and the Relationship definitions. The engine must also know what capabilities each connector supports. In Hasura, all of this (and more) is specified at build time as the metadata, which means the engine can cold start on a project in milliseconds.\nThe fact that this metadata captures the entire domain model is also excellent from a discovery point of view. The supergraph serves as an always up-to-date and executable domain model for the entire organization!\nPrior work\nThere's quite a few projects that do cross data source joins.\nFor instance, PostgreSQL Foreign Data Wrappers and Apache Calcite.\nHasura adapts some of these ideas to deliver a ready to use GraphQL API, adding features like application user permissions. The metadata based model allows building effective tooling like our LSP-based editor and console.\nWe've also made efforts to ensure that our connectors are extremely easy to write. The NDC spec is JSON over HTTP. Our TypeScript connector automatically adds TypeScript functions in a file to your supergraph with no additional effort.\nConclusion\nAt Hasura, we know that every last millisecond adds up fast when you want to leverage your data. Our engineers wear every last millisecond shaved off as a badge of honor; it's not uncommon to see optimizations and benchmarks posted on our internal Slack channels! The methods we use today are borne out of lessons learned in practice across several years, product versions, and a/b testing with different approaches.\nWe believe that the supergraph approach with query compilation is the way forward to ensure maximum performance with maximum flexibility to bring data in from any type of source. We hope to see these implementation patterns used by different entity based supergraph frameworks, GraphQL gateways, and backend engineers writing their own GraphQL servers alike.\nResources\nPartial implementation of GraphQL AST to SQL compiler in Javascript - Github\nHasura Native Data Connectors (NDC) Spec\nThe Supergraph Top N Challenge - GitHub\nAlternate supergraph implementations: ent.go - Meta DGS - Netflix and more...\nNDC Spec - GitHub\nTypeScript Connector, connector for TypeScript functions - GitHub\nSQL Techniques to Solve the Top N Problem - GitHub\nFootnotes\n[1] Multiple Models can share the same ObjectType (which maps to a GraphQL Type), and it\u2019s actually the ObjectType that is related to other Models.\n[2] More properly, the engine parses the GraphQL query into an internal representation, and executes NDC Requests (Queries / Commands) against connectors that are backed by data sources.\n[3] Variables are more general: multiple variables can be used to parametrise any portion of the query.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Supercharging development: The power of Hasura and ClickHouse for real-time analytics Description: Combining Hasura\u2019s ability to instantly generate GraphQL APIs and ClickHouse\u2019s powerful real-time analytics capabilities allows developers to rapidly innovate and deliver blazing-fast applications that include real-time analytics. Author: David Meleney"}, {"role": "assistant", "content": "Teams of developers large and small are trying to do more with less, deliver results faster, all while maintaining lightning-fast, secure applications that scale. Combining Hasura\u2019s ability to instantly generate GraphQL APIs and ClickHouse\u2019s powerful real-time analytics capabilities allows developers to rapidly innovate and deliver blazing-fast applications that include real-time analytics.What is ClickHouse?ClickHouse is an open source column-oriented database management system that\u2019s designed for blazing-fast real-time analytics. It is primarily used to store and analyze large volumes of data, leveraging its column-oriented nature to efficiently run fast query processing over large datasets. The ability to retrieve data quickly from huge volumes of data in real time, makes ClickHouse an ideal data store for GenAI and analytics applications.What is HasuraHasura is a cutting-edge platform that allows users to seamlessly build and deploy REST- and GraphQL-powered applications. By providing instant GraphQL APIs over existing databases, Hasura enables rapid development without compromising on performance or security. Its intuitive interface and powerful features, such as caching and role-based access controls, empower developers and businesses alike to build scalable, secure, and efficient applications. Whether you're a seasoned developer or just getting started, Hasura promises to simplify and accelerate your application development journey.ChallengesData accessTeams across organizations of all sizes need access to specific data and leverage that data to build applications that serve internal and external users. APIs and microservices are a natural choice for connecting data to consumers in a fast, secure, scalable, and decoupled way. But the traditional API development process is time consuming. Manual API integration is time-consuming, requiring developer teams to create boilerplate CRUD code, figure out authorization (input / output validation, etc.. And then even more work to get the API production-ready \u2013 putting in place the right security controls, wiring it up to be observable for monitoring, performance tuning, etc.Limited flexibility means REST enforces a fixed set of endpoints with specific data structures, causing developers to manage multiple endpoints and make multiple requests for related data.Real-time analyticsTraditional OLTP (Online Transaction Processing) and Cloud data warehouses are not optimized for building interactive, data-driven applications. These applications demand low latency queries, to provide responsive user experiences, even when exposed to high levels of concurrency. As a result, organizations turn to real-time analytical databases, such as ClickHouse.Benefits of Hasura + ClickHouseData access that\u2019s instant, fast, and secureHasura's ClickHouse integration changes the game, especially when we consider the inclusion of GraphQL. Hasura's enterprise-ready API management and RBAC features simplify the adoption of new databases such as ClickHouse. Users can enjoy ClickHouse's ability to perform analytical queries over billions of rows in milliseconds, without needing a deep understanding of the data structure and schema. Application development can be accelerated with development teams working with a familiar API interface, and avoiding the initial cost of writing and refining SQL queries.Existing ClickHouse users can exploit Hasura's ability to provide a data agnostic query layer in front of ClickHouse \u2013 simplifying the adoption and allowing rapid prototyping through automatically generated JSON APIs.GraphQL's flexibility: As previously emphasized, GraphQL\u2019s capability allows developers to request exactly the data they need, no more and no less. This precision helps in reducing data transfer loads and enhancing application performance.Automatic API generation: With Hasura, as soon as the schema is set, it auto-generates a live, real-time GraphQL API, reducing the manual labor of setting up APIs.Data federations: Hasura GraphQL Engine provides a uniform interface to the data consumer across multiple data domains. This reduces the need for manual data integration and API orchestration, by integrating data across multiple diverse sources providing rich capabilities of retrieval, so that the data consumer does not have to incur the cost of data integration or API orchestration.Developer-friendly JSON format: GraphQL\u2019s innate use of JSON for both queries and responses is an advantage. The readability and simplicity of JSON are much loved by developers, and its universality makes data parsing effortless across different programming languages.Rapid prototyping: Hasura accelerates the development process by offering instant real-time APIs, which in turn aids in quick prototyping and validation of ideas.Real-time analytics: Fully managed, open source, and lightning fastClickHouse addresses the challenge of efficiently storing, managing, and querying massive volumes of data in real time. Traditional relational databases can struggle with performance issues when handling large-scale data analytics. ClickHouse, being a column-oriented database management system, is designed to perform real-time analytical queries on large datasets with high speed. Its architecture supports distributed processing, enabling it to crunch millions, or even billions, of rows in a fraction of the time that conventional databases might take. This makes it particularly suited for scenarios that require swift analysis of big data, such as telemetry data analysis, real-time analytics dashboards, and high-speed logging. In essence, ClickHouse offers a solution to the growing need for rapid, real-time data analytics on voluminous datasets.What can I build with Hasura and ClickHouse?1. AI-powered SRE assistantBuilding a GenAI-powered SRE assistant with ClickHouse and HasuraBefore: Traditional systems struggled with real-time data analysis, often bottlenecking SRE operations and making it challenging to harness the full potential of GenAI for system reliability enhancement.After: With ClickHouse integrated into Hasura, SRE teams now experience blazing-fast data analytics, unlocking the true capabilities of Gen-AI to power a sophisticated SRE assistant, ensuring enhanced system reliability and efficiency.2. Real-time analyticsBefore: Earlier, building real-time analytics dashboards required developers to write intricate SQL queries, entailing considerable time and expertise to manage and optimize data retrieval processes.After: Now, by utilizing GraphQL, developers can effortlessly construct dynamic dashboards that provide real-time updates, showcasing the most current data insights and fostering data-driven decision-making.3. E-commerce platformsBefore: In the past, e-commerce platforms had to rely on multiple endpoints for fetching various data like user details, product information, and order histories, which often resulted in slower load times and a complex backend architecture.After: With Hasura GraphQL engine, e-commerce applications streamline data fetching through a single API endpoint, thereby enhancing performance, reducing load times, and offering a more refined user experience.4. Content aggregation platformsBefore: Aggregating content from various sources traditionally required an intricate network of API calls, making the process less efficient and harder to manage.After: Now, Hasura facilitates a unified data access layer, allowing content aggregation platforms to fetch data from disparate sources with ease, presenting a consolidated view of information through a simplified query structure.Getting started connecting Hasura to ClickHouseCreate a free ClickHouse Cloud projectYou can connect Hasura to an open source or cloud-hosted version of ClickHouse. This example will cover connecting a Hasura Cloud project to a ClickHouse Cloud deployment. First, create a free ClickHouse Cloud trial. You will be prompted with several questions regarding what type of deployment and cloud provider you would like to use. You can connect Hasura to any type of ClickHouse deployment. For this demo, we recommend you select \u201cAnywhere\u201d to simplify things.Note: It is recommended that you host your Hasura and ClickHouse deployments in the same cloud provider and region, if possible, to reduce latency.Add sample data to your ClickHouse deploymentOnce you have created your ClickHouse Cloud deployment, you can add some sample data. Luckily, ClickHouse makes it easy to get started with a number of sample data sets. From the ClickHouse dashboard click the Actions dropdown and select add data. This will launch the console for the ClickHouse service you created.You will be presented with several options to add data to ClickHouse. You can choose any of these options to get started. For this example, we will choose one of the predefined sample data sets ClickHouse provides.You can see the predefined sample data offered by ClickHouse as of writing this post below. You can use any data to connect to Hasura. For this example, we will use the New York Taxi Data.After you import the data into your ClickHouse service, you should see the following table and data in the ClickHouse console.Getting started with HasuraTo get started, head over to cloud.hasura.io. Create a project and then click Launch Console.Connect ClickHouse to HasuraNext, go to the Data tab to connect ClickHouse with Hasura.Click on Data Connector Agents and then click Add Agent.You are presented with a form that requires you to add your connector endpoint.Prerequisite for the next step requires installation of the Hasura CLI. If you need to install and configure the Hasura CLI you can find more information in the ClickhHouse Getting Started - Cloud documentation.Follow the below commands to deploy the ClickHouse connector and fetch the endpoint.# Create connector endpoint using Hasura CLI\nhasura connector create <connector name> --github-repo-url \nhttps://github.com/hasura/clickhouse_gdc_v2/tree/main/\n\n# Review logs\nhasura connector logs <connector name>\n\n# Review hosted connectors and fetch endpoint\nhasura connector list\n\nTake note of the URL for the endpoint displayed in the console. You will need it for the next step.Once you have the ClickHouse connector deployed you can fill and submit the form to Connect a Data Connector Agent.Name - name to identify the connectorURL - endpoint of your deployed connectorNow, click Connect Database at the top of the page and select ClickHouse to connect the existing database.Find the connection string for your ClickHouse project by clicking the Connect dropdown and selecting View connection string.Navigate to the HTTPS section of the dialog. Within the information provided you can find the connection.You are now ready to name your Hasura database connection and enter in the required information: connection URL, username, and password.Track ClickHouse tablesNow that we have connected ClickHouse to Hasura, we can track the nyc_taxi table we added to ClickHouse in Hasura to expose it over Hasura\u2019s auto-generated GraphQL API. While in the Data tab, click on the ClickHouse database you just connected. Select the nyc_taxi table and click Track Selected.Congratulations! \ud83c\udf89 You have now successfully connected Hasura to ClickHouse and exposed a table over the GraphQL API.Try some GraphQL queriesThe Hasura ConsoleYour ClickHouse data is now available over a GraphQL API. Using the Hasura GraphiQL console you can explore the API making it easy for developers to explore and transform data to suit their needs. This is especially useful for use cases involving analytics. Using Hasura\u2019s GraphQL engine you now have a secure API that leverages the capabilities of ClickHouse for real-time analytics. Let\u2019s try some examples. Note that all of the queries have a caching directive added to them with a total time to live (ttl) of 3600 seconds or 1 hour. This means all queries will be cached in Hasura cloud for 1 hour, making for faster response times and less load on ClickHouse.Simple query with filterThe query below will return trips with a specific id.query MyQuery @cached(ttl: 3600) {\n  nyc_taxi(where: {trip_id: {_eq: 1199999883}}) {\n    trip_id\n    total_amount\n    trip_distance\n    tip_amount\n    passenger_count\n    pickup_datetime\n  }\n}\n\nFilter results based on conditionThe query below will return all trips that were longer than 30 miles. Note that the schema or fields being returned has changed.query MyQuery2 @cached(ttl: 3600) {\n  nyc_taxi(where: {trip_distance: {_gt:30}}) {\n    fare_amount\n    trip_id\n    trip_distance\n    total_amount\n  }\n}\n\nAggregationThe following query will return the average and max values for fares, tips, and distance. Note: This is what ClickHouse is more efficient at than traditional databases.query MyQuery3 @cached(ttl: 3600) {\n  nyc_taxi_aggregate {\n    aggregate {\n      avg {\n        fare_amount\n        tip_amount\n        trip_distance\n      }\n      max {\n        fare_amount\n        passenger_count\n        tip_amount\n        trip_distance\n      }\n    }\n  }\n}\n\nNext stepsThese are just the first steps to fully leveraging the capabilities of Hasura and ClickHouse. To build out your API further you can:Add sample data that includes relationships that will also be reflected in the GraphQL API. For more information see this demo.Add caching directives.Manage access to your data and APIs by defining roles and permissions.Create a REST endpoint.Read more about a use case we found for using ClickHouse here at Hasuara: Building a GenAI-powered SRE assistant with ClickHouse and Hasura By using Hasura to expose ClickHouse data over a GraphQL API, developers can significantly reduce the time and complexity involved when creating and iterating on real-time analytics applications. The user-friendly features and tools provided by Hasura streamline the development process, allowing developers to focus more on building powerful applications and less on wrestling with complex data retrieval processes. This synergy brings unparalleled efficiency and performance to development, setting a new standard for data management and analytics in the modern business landscape.\ud83d\udcda Documentation and resourcesTo help you get started, we've prepared detailed documentation, guides, and examples:Hasura Clickhouse documentationBuilding a GenAI-powered SRE assistant with ClickHouse and HasuraBuild an SRE app powered by AIHasura on Clickhouse\ud83d\ude80 Get started today!We can't wait to see the amazing applications you'll build using Hasura and Native Queries. Get started today by signing up for Hasura Cloud and connecting to one of the supported databases.If you have any questions or need assistance, feel free to reach out to our team on Discord or GitHub."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a supergraph backend with Hasura Description: In this blog post, we'll guide you through the process of building such a backend, highlighting the use of Snowflake and PostgresSQL databases, as well as demonstrating individually executed and nested queries.\n Author: Akshay Bhat"}, {"role": "assistant", "content": "Using Snowflake DB and Neon PostgreSQLIn today's fast-paced world, building scalable and efficient backends for applications is essential. Developers are constantly searching for tools that can simplify the process and improve performance. Hasura Cloud, Snowflake DB, and PostgreSQL on Neon offer a powerful combination to create a supergraph backend that handles complex data requirements, like joining data across multiple data sources, filtering data on model relations. It's a supergraph architecture with a unified layer, made from modules, continuously evolving which is present in many different companies across the globe.In this blog post, we'll guide you through the process of building such a backend, highlighting the use of Snowflake and PostgresSQL databases, as well as demonstrating individually executed and nested queries.In this tutorial/guide, we'll guide you through the process of building such a backend, highlighting the adding of Snowflake tables and PostgresSQL tables as models to the supergraphs, as well as demonstrating complex queries with nesting that can relate models from across different data sources.1. Setting up Hasura CloudHasura Cloud is a fantastic tool for creating GraphQL APIs quickly. This is a preliminary tutorial to get started, a more detailed guide will be coming up soon. Here, we will see how to:Sign up for a Hasura Cloud account.Create a new project and connect it to your version control system (e.g., GitHub).Define your data model using Hasura's metadata.Run some exciting queries2. Integrating Snowflake DBSnowflake is a cloud-based data warehousing platform that seamlessly integrates with Hasura Cloud. To integrate Snowflake:Obtain your Snowflake credentials (account, username, password).In Hasura Cloud, navigate to the \"Data\" tab and add a new data source.Select \"Snowflake\" and input your credentials.\u201cImport all tables\u201d to add all the tables as models to the supergraph3. Incorporating PostgreSQL on NeonNeon is a managed platform for PostgreSQL databases. To incorporate PostgreSQL on Neon into your supergraph backend:Sign up for Neon and create a new PostgreSQL database instance.Obtain the connection details for your Neon PostgreSQL database.In Hasura Cloud, add another data source, but this time, select \"Neon Database\" and authorize the Neon database connection details.Add the models and relationships you want to track in the data tab.4. Executing individual queriesNow that you have Snowflake and PostgreSQL on Neon integrated into your Hasura Cloud project, you can execute queries individually.Querying SnowflakeTo query data from Snowflake, you can use Hasura's GraphQL API:query MyQuery {\n  SNOWFLAKETABLE {\n    COLUMN1\n    COLUMN2\n  }\n}\n\nReplace snowflakeTable, column1, and column2 with your Snowflake table and columns.Querying PostgreSQL on NeonSimilarly, to query data from your Neon PostgreSQL database:query MyQuery {\n  neontable {\n    column1\n    column2\n    column3\n  }\n}\n\nReplace neonTable, column1, and column2 with your Neon table and columns.5. Nested queries for complex data retrievalOne of the advantages of GraphQL is its ability to handle complex data retrieval through nested queries. Let's demonstrate this with an example:query {\n  getUser(userId: 123) {\n    username\n    email\n    posts {\n      title\n      content\n    }\n  }\n}\n\nIn this query:We retrieve user details by their ID from either Snowflake or PostgreSQL.Then, we fetch the user's posts with their titles and content.By defining appropriate relationships and foreign keys in Hasura's metadata for both Snowflake and PostgreSQL, you can seamlessly perform nested queries like the one above, aggregating data from multiple sources into a single response.ConclusionBuilding a supergraph backend using Hasura Cloud, Snowflake DB, and PostgreSQL on Neon offers incredible flexibility and performance. You can efficiently integrate multiple data sources and execute both individual and nested queries, making it suitable for a wide range of applications. Explore the power of these tools and start creating your supergraph backend today for a seamless and efficient data-driven application experience.Sign up now for Hasura Cloud to get started!"}]}
