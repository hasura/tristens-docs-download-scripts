{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura Data Delivery Network Alpha Description: We are excited to unveil the alpha version of Hasura Data Delivery Network (DDN) and share key features of our new platform building on the advancements introduced in Hasura version 3.  Author: Rahul Agarwal"}, {"role": "assistant", "content": "We are excited to announce the alpha version of Hasura Data Delivery Network (DDN) and sharing key features of our new platform, building on the advancements introduced in Hasura version 3. In speaking with developers building APIs and microservices, we realize the community is faced with significant challenges. Hasura DDN is being built to address these problems and simplify the lives of software developers and architects everywhere. Here\u2019s what we heard most frequently in our conversations:1. Developers want to create APIs in their preferred programming languageAdopting a new language involves a steep learning curve and requires adapting to different tools. As a result, in creating a new microservice or API, developers often opt for the programming language they\u2019re most comfortable with \u2013 a preference driven by concerns about performance, reliability, backward compatibility, and integration, especially for crucial applications. Also, a new language and framework usually means more training, adding overhead to the business.Not just a black box \u2013 Hasura DDN is a low-code API development platform, fully language agnostic, so developers are free to augment Hasura's capabilities and compose business logic in the programming languages they\u2019ve already invested in.2. Developers want to create complex and feature-rich APIs \u2013 fast.Creating an API server that aligns with business needs and user expectations involves addressing various requirements \u2013 \u00a0supporting different API styles (such as Relay, GraphQL queries, subscriptions, pagination, and sorting) and formats like REST and gRPC. Backend developers are navigating complexity, designing for scalability, defining user experiences, testing and monitoring, and maintaining the codebase \u2013 and it\u2019s exceptionally time-consuming.Hasura DDN enables developers to create complex API features in minutes, rather than the days or months it would have taken to build manually. The capabilities for creating feature-rich APIs extend across all of our data connectors. Our platform is also extensible \u2013 our GraphQL capabilities extend to other API protocols, like gRPC and REST, with our open metadata specification. 3. Developers want the ability to quickly test and iterate.Building a feature-rich microservice that optimally utilizes the data source demands significant expertise and effort to make inferences from the available information. Another challenge is implementing continuous integration (CI) to handle frequent code changes and database schema adjustments. This process involves writing extensive boilerplate code, managing dependencies, and coordinating with other developers.Hasura DDN offers a free and intuitive developer environment with robust tooling, seamlessly combining local and cloud development for efficient API design, testing, iteration, and maintenance. The platform also provides code generation and reusable components for rapid development, allowing full flexibility for customization.Hasura DDN Alpha FeaturesOn the path to achieving the vision of Hasura being a \"CDN\"-like layer for real-time (transactional), analytical, and streaming data, the following are features found in the Hasura DDN alpha release.New console geared for rapid onboarding and troubleshootingWe have introduced major changes to the console in Hasura DDN. We have added features that help in onboarding developers in your team on your data domain. That way, you\u2019re able to collaborate and go to market faster. As the number of models and relationships increases, so does the complexity of understanding and making sense of it. That\u2019s why we are rolling out tools on the console for rich visualization of the data domains and their interactions. On top of that, users can now drill down into each of the models and commands and browse through further details, such as model-level analytics, fields, permissions, and relationship details.A large part of going to production successfully is to quickly debug and troubleshoot errors and issues. We have enhanced our monitoring capabilities in Hasura DDN, providing users the ability to get detailed traces and spans of the request as it traverses from the Hasura server all the way to the data source. That way, with all the context and information easily available to them, users quickly detect any latency issues while locating the root cause of the error in real time.Hasura DDN also features a new database administration tool that\u2019s ideal for exploring and editing data in PostgreSQL for now and going forward in most of the other supported databases and it\u2019s geared to scale to massive workloads.Code-first declarative API authoring and iterationOne of the main pain points of our developer community over the past few years has revolved around metadata authoring, management, and deployment. We\u2019ve heard complaints from users on the time a metadata change takes to get built and rolled out, mainly because of Hasura cold-start issues. On top of that, the hard coupling of Hasura to upstream services (such as the database or remote API servers) led to Hasura not being available unless upstream services were available. And lastly, being dependent on a metadata database implied that users have another thing to run and maintain in production.In Hasura DDN, we have introduced a build and runtime separation \u2013 a pattern that is analogous to compiler-based programming languages. The DDN now has a serverless runtime built-in with per-request tenancy. Each metadata change will now produce an atomic build and an associated artifact that is loaded to Hasura production instantly. This means users no longer experience cold starts and can achieve zero downtime rollouts without any restarts or downtimes. Each atomic build represents an immutable API that can be incrementally tested and applied to a project so that it is made available to API consumers. Hasura DDN is also no longer tied to the upstream services and introspection is not decoupled from runtime. As a result, there is no dependence on upstream services and users can now safely go to production multiple times in a day or even an hour.In Hasura DDN, users can interact with Hasura in a code-first way. To aid in declarative authoring and editing of metadata, we are introducing some potent metadata management tools such as a YAML-based metadata file type HML \u2013 a brand new language server protocol (shipped with VScode extension) to provide language features like autocomplete and code actions. Users also have access to a brand-new CLI that offers useful scaffolding and helps provide logical separations of the metadata via folders and files. You no longer need to browse a large metadata file and can instead navigate a file structure that makes the most sense for your team. The new CLI will stitch the metadata together and build the API from different folders and files.Lastly, to leverage the convenience of DDN cloud\u2019s out-of-the-box features, yet easily prototype using local resources like a local database or a local authentication server, the CLI now provides secure connect options in the form of a local daemon and a cloud-based tunnel server system. This would mean that you can develop your APIs to do all local development and testing scenarios while still being deployed on Hasura Cloud, making it even faster to go to production from the development stages. The experience is designed to be highly secure ensuring only those involved in the user's projects have access. It also offers a seamless experience, aiming to replicate the feel of true local development as closely as possible.Developers accustomed to using other web frameworks in a code-first approach can now quickly adapt and efficiently develop in code with DDN.Feature-rich data domain modelingHasura users have often wanted more control over how the schema is generated, what names are used, and how types are generated. As we have expanded to other types of data sources, such as NoSQL databases and API servers, we realized our users have been spending time to understand the Hasura metadata structure. Furthermore, the metadata specification was predominantly centered on SQL and GraphQL, which limited its adaptability to other protocols such as gRPC.In Hasura DDN, we\u2019ve made significant changes to our metadata structure with the release of the new OpenDataDomain specification that provides fixed and a small set of types such as Types, Models, Commands, Permissions, and Relationships to compose and author all forms of APIs encompassing complex business logic.As a result, users can employ contract-first or schema-first types of development practices while developing APIs with Hasura, freeing up time to focus more on domain design than how the APIs are actually implemented.The domain-led design approach using Models and Commands could be of great benefit to separate the command (write) and query (read) responsibilities in APIs. This pattern can be beneficial in reflecting the distinct needs and optimizations required for data modification versus data retrieval, and help users design, develop, and deploy scalable command and read graphs in production. Similarly, Permissions type can help set bounded contexts and define specific boundaries for different parts of the domain, while Relationships type can aid in dealing with interconnected contexts or bounded contexts and facilitating communication and integration between these contexts.With the new metadata specification, users have a single ubiquitous language across data sources and domains that can be shared by domain experts, developers, and stakeholders. Furthermore, API generation now becomes protocol agnostic as the same terms and concepts from this language could be used for GraphQL, Relay, REST, gRPC, and so on, ensuring consistency and clarity across the domain and the API.Given the strict and typed metadata contract, there\u2019s a higher correctness guarantee and more opportunities for Hasura to automate query planning and caching across future domains.Embed business logic using TypeScriptOur users have asked for more flexibility and feature support when it comes to implementing custom business logic in Hasura. While the Actions functionality provided a lot of support in this area, it was limited and also led to a learning curve for most users. There was also the burden of hosting functions in a different environment and bringing that to Hasura to implement a simple but core piece of business logic for the API.In Hasura DDN alpha, users can code in TypeScript, host the TypeScript functions in Hasura Cloud, and embed that function-backed business logic into the Hasura API instantly. This allows users to use the tooling and libraries of their choice while leveraging the instant production-grade API server creation capabilities of Hasura. It also enables highly requested features, such as data transformations with mutations, and provides new and innovative ways to do pre-mutation input validations and integrate with APIs on the Hasura platform. As part of alpha, users can leverage the new TypeScript Deno Connector, write their functions in index.ts, use the Hasura LSP to automatically generate Command based metadata, and instantly build to start using the API and a functional command graph.Native extensibility with data connectorsHasura DDN is a completely modular and extensible platform. Users can bring in any data connectors that follow the new Hasura native data connector specification to leverage the type of functionalities that Hasura currently offers with native databases such as PostgreSQL. With the recent release of the Hasura Connector Hub, all native data connectors are open source and free to use with Hasura DDN.We have two types of data connectors:1. Verified and Hasura-hosted connectorsThese connectors are deployed in multi-region and have stricter performance and availability SLAs. Currently, in alpha, we support PostgreSQL and its flavors in this category.2. Community-developed connectorsThese connectors need to be separately deployed before building Hasura metadata, either in the Hasura Cloud infrastructure or in your own environment. Currently in alpha, we support ClickHouse, SendGrid, and TypeScript (Deno).To help in fast iteration and less overhead, we have a CLI plugin to help deploy community-developed connectors on Hasura Cloud. After deployment, you can use the connector URL to embed in Hasura metadata as a data source for a Model.We also have extensive resources to aid in developing your own connectors with the NDC learn course and SDKs in TypeScript and Rust. How to explore Hasura DDN alphaTake a sneak peek into the future of data delivery \u2013 building supergraphs at the API layer \u2013 and contribute to shaping our product vision. Help us build the best, most useful product for your needs.Be among the first to experience Hasura DDN. Check out the following resources:Docs on getting started: https://hasura.io/docs/3.0/Register here to access the playback recording of our November 30 Community Call, which featured powerful Hasura DDN demos and the product roadmap.Connect with other alpha users and our product development team on Discord. We would love to hear from you! You can also reach out to us via email at team-product@hasura.io.Try out our Learn course here, where you can follow a tutorial and get started today!The road ahead for Hasura DDNAs we push forward in our mission to revolutionize microservice development and move towards the release of the beta version of Hasura DDN in early 2024, we're eager to iron out any rough edges and improve the experience based on invaluable feedback from our alpha users.The beta roadmap is razor-focused on improving and tightening the developer experience as we acknowledge our product is still evolving. Your participation will be crucial in identifying areas for refinement and we urge you to stay engaged, share your thoughts, and help us ensure the beta launch delivers the best developer experience possible. Areas of focus in the coming weeks:Metadata authoring enhancements: LSP code generation and auto-complete enhancements. Making it easier to build permissions and add relationships to models and commands. Ease of use for debugging metadata errors at the time of authoring. Improvements to the CLI watch command for rapid iteration loops.Avoiding API breaking changes: Build validations with dry run mode and compile time guarantees. Improved error handling at build time. Enhanced operational health dashboard and metrics for advanced error handling, insights, troubleshooting, and production monitoring of APIs across builds.Integrating business logic DX: Improvements to the TypeScript SDK and connector for accelerating connector and business logic development with fast iteration loops. Ease of use for importing REST and GraphQL.Team collaboration: Enhancements to the multi-repo workflow for building a supergraph across multiple data domains owned by different teams.API features: Complete the API feature set including mutations and other query features.V3-engine open source release: Making the API server behind DDN open source.Migration: Streamlined migration utilities for current v2 users to transition to DDN.We greatly value your feedback, and our commitment to crafting an exceptional developer experience is deeply rooted in the insights and suggestions we receive from our community.Thank you for being the driving force behind Hasura DDN's evolution.Let\u2019s continue this transformative journey together. We encourage you to share your thoughts, queries, or feedback about the alpha launch. Get access to Hasura DDN alpha and get started today!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Debunking GraphQL Myths and Misconceptions Description: In this myth-busting blog post, we dive deep into the top misconceptions surrounding GraphQL. From concerns about complexity, performance and developer experience. Author: Akshaya Acharya"}, {"role": "assistant", "content": "People well-versed in GraphQL are often found discussing the latest compilation strategy, or the best way to optimize performance of federation. But sometimes, for people new to GraphQL, it\u2019s simply not clear what GraphQL is doing in the first place. (And the name doesn\u2019t help.)Here are some common concerns/myths about GraphQL, from people getting acquainted with GraphQL, and clarifications to resolve them.Myth: GraphQL, the query language is designed to let clients perform arbitrary queriesThis is true, but just applicable during the development phase for developers who are building the client app. That's what makes GraphQL great. The DX of GraphQL consumption is amazing. The idea is to use tools like GraphiQL, test out different queries required for the client and integrate it in your application. GraphQL is self-documenting and tooling is much better than the equivalent REST spec - Swagger docs.But, when in production, you typically restrict what queries are allowed with an allow list or persisted queries and simply call them from the client. Wrongly implemented security doesn't make GraphQL bad. Any API can be vulnerable if you choose it to be. REST APIs already sends boatload of objects that clients didn't need anyways.Myth: GraphQL is a query language, like SQL for graph databasesThis is just due to the sheer coincidence of naming!The name GraphQL is a hint at the modeling and execution semantics. Queries are composed of a root Query field, which has nested fields. Each field can have further nested \u2013 and related \u2013 fields. This makes for a tree structure, which is a subgraph of the whole data graph that we can query.A graph is a very generic structure \u2013 any data source can be mapped to it. For instance, for a relational database like PostgreSQL, you could map tables to nodes, and relationships to edges in the graph. The key thing about a graph is that it emphasizes the connected nature of data, encouraging good modeling of the domain by identifying the different entities or models and laying out the relationships.graphql\nquery {\n  posts(where: user_id == 4) {\n    created_at\n    content\n\n    user {\n      name\n    }\n\n    comments(order_by: created_at desc, limit: 10) {\n      body\n    }\n  }\n}\n\nEvery field in a GraphQL schema (which specifies all the allowed queries as a collection of related types) can be backed by arbitrary code \u2013 exactly like an RPC endpoint \u2013 but with the additional advantage of having complete context of where it sits in the query tree.Further, because clients query all the related data they need in one go, the whole query is available to the server, which means the server can perform global optimizations across the whole query during the execution. For example, the server can push down requests for related models into database joins, which is otherwise not possible if the client is making successive queries to data related to a previous query in an N+1 pattern.Myth: GraphQL servers are hard to build and maintainEveryone agrees that they want a GraphQL API, the question then becomes one of cost.The majority of complaints about the complexity of implementation and cost of maintenance come from working at the wrong level of abstraction.Programmers building GraphQL like this often complain about repeated code and boilerplate. You also hear complaints about building layers of wrappers and adapters, and how it's difficult to keep these layers up to date as modeling and functionality evolves.Teams that successfully scale their GraphQL APIs all seem to converge on the same pattern: They're all building supergraphs! They're all using an engine like Hasura, where they specify the data modeling \u2013 the models and relations \u2013 and the engine automatically builds the GraphQL API (see how here).When your GraphQL APIs are based on supergraphs, there's no cruft and you can iterate rapidly. Want to change something in your data modeling or underlying microservice? Simply change the engine configuration and your API keeps pace effortlessly.Comparing writing resolvers manually to authoring supergraphs is like comparing JavaScript to React or NextJS. There is no comparison. And nothing stops you from using JavaScript when you exceed the limitations of the framework.Myth: GraphQL has the N+1 query problemWell, it does and it does not. A naive GraphQL server implementation has the N+1 query problem. With dataloader batching, it really isn't N+1 anymore. Tools like Hasura, Postgraphile compile GraphQL to SQL efficiently. It is much faster than a dev team trying to optimize from scratch. A badly implemented server doesn't make GraphQL bad.But yes, most tutorials start with the naive resolver methods and that is a problem. This goes back to the previous myth as well \u2013 GraphQL servers are hard to build and maintain. Myth: GraphQL is unnecessarily complex for small teamsThe truth is that, for a small project, your tech stack doesn't really matter. At that point, it matters a lot more what tech your team has experience with! If you can get away with using a Google Form instead of building something, you probably should. A landing page could be your startup's MVP to validate a product. As simple as possible... but no simpler.If your team has a single database and full stack engineers iterating rapidly across the entire stack, building and iterating on features both on mobile apps and the server, especially when you're iterating rapidly on the API itself, then RPC can be far easier to work with than GraphQL. The additional structure imposed by GraphQL can begin to look like a burden. At this stage, you'll often hear developers claim that it's so easy to add and modify RPC endpoints that GraphQL isn't worth it. And maybe they're right.GraphQL shines as a method to query richly connected data. Building a supergraph creates this richly connected data. You can now model relations across different teams and domains in organizations, a supergraph can model relations across different data sources. Siloed data comes alive and is discoverable.Supergraphs solve the harder problem of communication. (See Conway's Law). What this means is that every team can own and build their own graph with crystal clear interfaces, and then the supergraph just stitches these together, while making sure that all the constraints are enforced.This is better than using just a simple API gateway because the supergraph is based on models. Unlike an API gateway, a supergraph is not limited by the structure of the data that the service layers return. We can go further, and begin to look at all our data across all sources as part of a single large source that\u2019s queried flexibly, with a query planner that optimizes queries across the whole supergraph.When does a small product become a large product, and where do the tradeoffs begin to make sense? Perhaps when the product is validated? Perhaps when you add another data source. Perhaps when you have some clarity on the contracts that you want the API to have and you're ready to share it with the world!Myth: Caching is hard in GraphQL\u201cThere are only two hard things in Computer Science: cache invalidation and naming things.\u201d \u2013 Phil KarltonThe whole caching lifecycle is tricky irrespective of protocols involved. In GraphQL, there is client side caching and there is server side caching. GraphQL with its schema design and the client tooling like URQL and Apollo Client, the frontend caching story is much better than REST. Pre GraphQL/TypeScript, frontend caching wasn't even on the radar. TypeSafe clients are being used on the frontend with granular caching rules.On the server side, the fact that everything is a HTTP POST request in GraphQL makes it different to reason about caching with existing tooling that is built for years supporting REST spec at the network layer. The mental model is different. Tooling already exists (@stellate) and there is more work to be done in this space. The truth is, caching is tricky irrespective of whether you are dealing with GraphQL or REST. In GraphQL, you cannot rely on HTTP verbs like GET that you typically use to cache in REST. Instead, you need to look deeper in query responses which also may have errors inside.{\n\u201cdata\u201d: [], -> cache fields inside this\n\u201cerrors\u201d: [] -> look for errors here and ignore\n}\n\nConclusionIt\u2019s often not easy for engineers deep in a tech ecosystem to adopt a first-principles point of view and evaluate technology from a first-principles basis.It is important to understand the tradeoffs and work as a team on both the frontend and backend and you will get the value of different tools. GraphQL is great for being the data layer that stitches together different microservices into a supergraph. The consumption layer can be anything \u2013 REST/GraphQL/gRPC. It really doesn't matter at the end of the day. Incrementally adopt any tool that fits your needs.Hopefully, recognizing these common myths help you better evaluate GraphQL for your needs. And better yet, if GraphQL is a good fit, help evangelize it as a solution for your team or organization.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Remote Table Authorization: Write permission rules across all your data Description: We are thrilled to introduce our latest feature: Remote Table Authorization. \n With Remote Table Authorization, you can now use data from different sources to perform authorization checks on any table. Author: Sooraj Sanker"}, {"role": "assistant", "content": "In the ever-evolving world of data management, allowing complex access control becomes paramount for business success. We are thrilled to introduce our latest feature: Remote Table Authorization. With Remote Table Authorization, you can now use data from different sources to perform authorization checks on any table.Background and ProposalThis innovative project was born out of a pressing need raised by our Cloud Team and AI Innovations team. The use case is not only common but also incredibly powerful, as evidenced by its recent showcase in the HasuraCon 2023 AI talk through an ad-hoc PoC (Proof of Concept).Hasura's permission system has always excelled in filtering data, treating permissions as a special case of filtering. However, a significant challenge emerged when it came to filtering based on fields from remote relationships.What is Remote Table Authorization?Consider you have two tables on your database as follows: Defining a permission based on this setup was already easy with Hasura when the 2 tables are in the same data source, or as long as there is a column in the target table to compare with. ie. you could add a row access permission logic on posts that check for user_id equals to x-hasura-user-id as followsBut, as we see the data growing and getting distributed across different systems, you may notice the individual data units could be managed by different teams or even different tools.Now imagine the posts table, which is mentioned above, is coming from an external CRM system, where the user_id is different, and you don\u2019t have the luxury to modify the posts table schema or add an additional column to it.Usually in such cases what we will do is, to create a column mapping on existing user table to accommodate the crm_user_id which the CRM is using as follows, and add a Hasura remote relationship as follows.Now that we have the next set of problems, how do we write the permission for posts?This is where the Remote Table Authorization helps! You can now define permissions across data sources and reference remote relationship row values to contain the permission rule.So in this example, let\u2019s define a remote relationship as follows:and now we will be able to define a permission on the posts table with the remote relationship that we created as follows:As you can see, the permission rule is agnostic on the crm_user_id (id on the posts table), and it uses the user_id from the user table to grant access to the posts.So if you are a data architect, you don\u2019t have to think about the access permissions logic, as it is already taken care of by Hasura. Previously this could not be done directly. And the developers can simply query as:query MyPosts {\n  posts {\n    id\n  title\n    author_id\n    content\n  }\n}\n\nNow, let's make it more interesting! \u00a0Imagine the column author_d in the table posts, is something you don\u2019t want in your API layer- say it's an internal reference or some sensitive identifier.Now, we can remove column select permissions of user_id from posts table and crm_id from users table so that these fields are not visible in the API layer but play a critical role in granting access to the posts. How it worksImagine this scenario where each table resides in a different source. A remote relationship permission (on Table A) might appear as follows:{\n  \"relAB\": {\n          \"id_B\": {\n              \"_eq\": \"X-Hasura-User-Id\"\n          }\n      }\n}\n\nTo evaluate this permission expression, we follow these steps:Retrieve all the id values from table B where id_B equals X-Hasura-User-Id.Then, the boolean expression for table A is id_B in the result set from step 1.So, to evaluate any remote relationship permission, we need to perform these steps. It involves evaluating the inner node and then working our way up, generating a boolean expression for the table on which the permission is defined.LimitationsWhile Remote Table Authorization unlocks exciting possibilities, we acknowledge certain limitations:As we explained the execution, it fetches the remote relationship side first and then the actual table- this means if the response from the remote relationship table is too long, the query to RHS would also be a lengthy `in` query. This has to be considered while using this solution with many-to-many sort of relationship permissions.Currently, the console will show this option for GDC-based data sources by default, if you want to see the console for native data sources, you can enable the \u201cData Tab Performance Mode\u201d feature flag from the settings.ConclusionWith Remote Table Authorization, Hasura continues to lead the way in simplifying and enhancing data access control in GraphQL APIs. We're excited to empower you with the ability to define intricate permissions for remote relationships, offering a robust solution for your most complex data access scenarios.Discover more about Remote Table Authorization by exploring our documentation and giving it a try in your projects. Sign up now to unlock the full potential of your data relationships with Hasura if you are not on Hasura yet!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Architect\u2019s Dilemma: Navigating the world of GraphQL Description: I think a big part of the hype boils down to the fact that GraphQL done right is simply better than any possible equivalent of its REST counterpart. Author: Tristen Harr"}, {"role": "assistant", "content": "Remember not so long ago when GraphQL was the new kid on the block and people were rushing to adopt it? Lots of technologies come and go over the years but few end up getting their own foundation. (Much less a foundation that\u2019s tied into the Linux foundation.) I think a big part of the hype boils down to the fact that GraphQL done right is simply better than any possible equivalent of its REST counterpart. It\u2019s not better because I want it to be better, or because I like how it works more. It\u2019s better in the same way one might say MergeSort is better than BubbleSort. When it comes to all the hard-tech metrics \u2013 primarily \u201chow fast and efficient is it?\u201d \u2013 there's a clear winner. Yet if you Google \u201cIs GraphQL better than REST?\u201d you\u2019ll get mixed reviews. I\u2019ve even seen some GraphQL hate going around and have heard my fair share of nightmare stories. When we say MergeSort is better, the metric we\u2019re tending to use is time complexity. O(n log(n)) beats O(n^2) hands down. If we measure which sorting algorithm is better based on which is easier to implement, BubbleSort wins. There are also edge cases like when evaluating space complexity but let\u2019s be real, nobody is arguing for BubbleSort in production. The comparison between GraphQL and REST is similar in many regards. GraphQL \u2013 if implemented perfectly (hard to do), while properly solving all the known difficulties associated with implementing it elegantly (also hard to do) \u2013 is strictly better, if not for the simple fact that by reducing over-fetching and under-fetching we can literally reduce the amount of electricity our applications use while also speeding up our page-loads. The catch is precisely how hard this is to do. Unlike most sorting algorithms, you can\u2019t implement GraphQL in an afternoon by hand. GraphQL was never intended to be a quick win.I\u2019m of the personal opinion that the companies likely to get GraphQL right have great communication structures. And I believe this is a consequence of Conway\u2019s law, which states any organization that designs a system will produce a design whose structure is a copy of the organization's communication structure. Stealing a term from DDD circles, perhaps the most important discussions to be had around adopting GraphQL are the ones around your company's ubiquitous language. I\u2019d guess the better an organization is at breaking down the social boundaries within their organization, the stronger their ubiquitous language becomes, and the more likely they are to have great success when adopting GraphQL.There\u2019s also absolutely no reason GraphQL and REST can\u2019t play nicely together. If you can only pick one, I\u2019d vote for GraphQL, but nobody says you can\u2019t have both! So, too, can GraphQL and gRPC find ways to get along. The end motivation of adopting something like GraphQL oftentimes doesn\u2019t center specifically around GraphQL itself, but rather around this idea that\u2019s been starting to gain popularity in enterprise circles of creating supergraphs, which is something GraphQL can enable you to do. This is also one of the core problems that Hasura wants to help solve, by making it easy for Enterprise Architects to build supergraphs, which we\u2019ll peek into a bit deeper at the end of this blog. Building these supergraphs isn't just a technical hurdle; it's an endeavor riddled with complexity, especially when it comes to correctly implementing GraphQL APIs. And trust me, this isn't just a theoretical problem.Table of Contents\n\nPersonal experience has taught me that building GraphQL APIs correctly is hard\nConsuming a well-thought-out GraphQL API is great\nThinking like a node \u2013 some practical examples\n\n1. How much money did we make in February in 2009?\n2. A customer named Frank called in and said he emailed support last week but hasn't heard back. Who's his rep and why haven\u2019t they contacted him?\n3. Which Playlist is the most popular?\n4. Which Tracks are my best sellers?\n\n\nFrom simple to supergraph, tying it all together.\n\nPersonal experience has taught me that building GraphQL APIs correctly is hard.\nA few years before I discovered GraphQL, I was building a lot of REST API\u2019s. My workflow at the time included tools like Postman, and I was a big fan of building things using the OpenAPI spec. As a Python nerd, that often meant building things using FastAPI. And when Python wasn\u2019t a good fit, I would resort to things like Spring Boot. I remember the first time I laid eyes on GraphQL: The promise was beautiful, and right away I went to work on a personal project to give things a try. I quickly realized that when it comes to building things with GraphQL, writing GraphQL resolvers by hand is simply downright painful. If you\u2019re trying to build things with GraphQL and you\u2019re handwriting all of your resolvers, then in nearly every case you\u2019d likely be better off building things using REST. I didn\u2019t realize this until after a decent amount of time breaking myself out of the REST mindset (which is a whole chore on its own).Writing GraphQL resolvers by hand is downright painful.If you\u2019re coming from a REST background, one of the first things you need to do when adopting GraphQL is forget all the rules you\u2019ve been using for what\u2019s likely the majority if not the entirety of your professional development career so you can learn a bunch of new rules that contradict most everything you\u2019ve done or been taught before. One of my favorite posts about GraphQL is the ironically titled, \u201cHow not to learn GraphQL.\u201d While I was learning about GraphQL, at first, I found myself frequently falling victim to one of the biggest mistakes people make \u2013 trying to design GraphQL APIs the way the industry has spent the past 20 years teaching us to design APIs. (Who would\u2019ve thought?) I\u2019ve seen highly talented people and top-tier teams fall victim to this, even with all the warnings out there. When you take a team that\u2019s used to thinking in REST patterns, and you ask them to build something in GraphQL, I\u2019d guess there\u2019s a 90% chance they\u2019re going to end up with a spaghetti-monster schema as developers are going to superimpose a REST-ful mindset onto the whole thing. It\u2019s unbelievably easy to read this or something like it and think, Oh, I\u2019ll keep that in mind, note to self, don\u2019t do that, then go and do precisely that anyways. (I\u2019m guilty of having done it.)Another pattern I\u2019ve seen that often happens once people have managed to get over the mindset switch is they start building out a big pile of fancy new tooling from scratch. I\u2019ve seen multiple roll-your-own attempts at things that automate building resolvers. Then you have to handle the N+1 problem in your resolvers and suddenly things get more complex. If you want subscriptions that\u2019s an entirely different and equally complex conversation. Permissioning and handling authentication with GraphQL is another complexity because there\u2019s a lot larger of an attack-vector when you expose your entire business domain as a graph. You need to think about pagination, and sorting, and by the end of it all, you truly are better off with REST. If you still aren\u2019t convinced, then the final complexity monster I\u2019ll toss on top of it all is federation and handling multi-tenant applications. Or, if you want to push your engineers to the edge of insanity, you can try schema stitching by hand with cross-functional teams and watch the CI/CD build pipeline clog up for hours as each commit that makes a schema change kicks off your test suites. (Bonus points if some of the tests are flaky.) \u00a0I\u2019d posit that herein lies the problem: The bottom line is that using GraphQL is one big complexity monster. If you use it right, it\u2019s great, but good luck using it right on your own. It\u2019s especially tricky because chances are the more \u201cveteran\u201d an engineering team you have, the more likely they are to fall into using the patterns they\u2019ve spent their careers learning \u2013 and then they\u2019ll make a big mess. To adopt GraphQL correctly, you need to dedicate time and resources toward training your team on how to use GraphQL. There also should be an understanding from the top down that the decision to adopt GraphQL is a long-term strategic business decision. Hasura\u2019s Co-Founder and CEO Tanmai Gopal recently spoke at GraphQL Conf about how GraphQL is a strategic choice that can set the business up for success, but it isn\u2019t a tactical choice for quick wins.Lots of people jumped onto the GraphQL bandwagon, and many got burned. That\u2019s what happens when a technology is new and hot and nobody knows how to properly use it but everyone decides to try. As humans, we tend not to want to get burned and quickly learn not to stick our hands into the fire. Lots of developers have been a part of big, ugly, messy GraphQL projects where their job title quickly changed from \u201cSoftware Engineer\u201d to \u201cFirefighter.\u201d That\u2019s been the only experience many have had with GraphQL, and I can\u2019t say I blame those individuals if they\u2019ve sworn off ever trying GraphQL again. I try to avoid flame-breathing complexity monsters and would suspect most smart programmers and architects do, too.</rant>Consuming a well-thought-out GraphQL API is great.\nThat\u2019s the flip side. Once a GraphQL API is built, if it\u2019s been built well with a good structure and a well-thought-out design, it\u2019s a lovely experience to use. It\u2019s mostly the building and maintenance that sucks \u2013 for the consumers, it adds tremendous value.I\u2019m certainly biased in thinking that Hasura is the best way to work with GraphQL, but even without Hasura, there are plenty of better ways than doing it yourself. I think the biggest key is not trying to do it yourself. There are lots of difficult problems to solve when you try to DIY GraphQL. And while it\u2019s certainly a fun exercise, in the same way writing your own database might be, it\u2019s not something I\u2019d be planning to do for systems I wanted to send into production (unless I had the resources to dedicate for what will likely be a years-long engineering effort to get right). We at Hasura have worked to refine the techniques we use to overcome the N+1 problem for a while, and if you want to reinvent them, here\u2019s a potential starting point on how we did it. Good luck and have fun! \ud83d\ude05For us at least, solving these problems is an entire product that requires the dedication of a small army of talented engineers. Trying to handle all of these things on your own in the background, while also working to solve the difficult and unique challenges specific to your product, is perhaps a bit silly.Despite the above rant, there is a reason why GraphQL continues to get so much love and attention from developers and companies alike. When set up correctly, it\u2019s a whole new world. It doesn\u2019t even really have to be GraphQL if we\u2019re being honest. It just needs to be something that does all the same things that GraphQL does, and is as easy to consume as GraphQL is, so we can build a supergraph. A supergraph is an architecture and operating model to build and scale multiple data domains as a single graph of composable entities and operations. It gives you the best of both worlds and can be viewed from the lens of taking a \u201cmonolithic microservices\u201d approach. You can create an application that is logically monolithic, but physically distributed and micro. Building a supergraph is typically a large undertaking. At Hasura we believe it should be as simple as declaratively defining your data model and tucking your custom business logic away as operations on the graph. To build a supergraph all you need to learn to do is to think like a node, and the rest should be after-thoughts. We think that the most important questions should be around the ubiquitous language your organization uses to describe your business domain, rather than the implementation details.Thinking like a node \u2013 some practical examples\nGraphs are foundational to most everything we do inside of a computer, and they are the best way to model real-world systems and processes we have.In these examples, we\u2019ll examine the Chinook Music DB in the context of an organization's data domain. (You can download and play with this dataset if you want to from here). While we won\u2019t go over the schema in detail, there are plenty of examples online that use it, and we\u2019ve included an ERD diagram below. If you want to give Hasura a spin and follow along with the examples, simply deploy this onto Hasura Cloud to get started quickly. Feel free to skip ahead to the next section if this content feels familiar.How does our data model define what we see, talk about, and understand in regards to our business? Let\u2019s jump into a day in the life of our theoretical music store owner, by examining four common real-world questions we might encounter while running a music store.1. How much money did we make in February in 2009?\nWhat this question is asking: What\u2019s the sum total of all the Invoices that have an InvoiceDate that happened in February 2009?When phrased that way, it\u2019s easy to answer and think about. Start at the Invoice Entity, go into that entity's data, find all the invoices that occurred the month that we want, and sum up the total. It\u2019s easy to answer that question even outside of the context of computer science. Not so long ago answering that question meant going to the Invoices filing cabinet, getting the folder for the month and year, and then pulling out each invoice and adding the totals together. This is a simple query, as we aren\u2019t traversing any relationships. The root of our question, which is the place to start looking for the answer, is at the Invoice.Here\u2019s how you could ask this if you were using Hasura: query HowMuchMoneyDidIMakeInFebruary2009 {\n  answer: Invoice_aggregate(where: {InvoiceDate: {_gte: \"2009-02-01T00:00:00\", _lt: \"2009-03-01T00:00:00\"}}) {\n    aggregate {\n      sum {\n        Total\n      }\n    }\n  }\n}\n\nWhich would give you the answer:{\n  \"data\": {\n    \"answer\": {\n      \"aggregate\": {\n        \"sum\": {\n          \"Total\": 37.62\n        }\n      }\n    }\n  }\n}\n\n2. A customer named Frank called in and said he emailed support last week but hasn't heard back. Who's his rep and why haven\u2019t they contacted him?\nThe most important part of knowing how to approach answering a question when data is on the graph is knowing where to start. Once you know how to \u201ctake a walk through your data-graph\u201d you can easily determine the answer.In this case, we start on the Customer node, and we need to find customers named Frank, from there we need to traverse the edge (or, relationship) between Customer and Employee. The answer to the question depends on which Frank we\u2019re talking about. Frank Harris\u2019s support rep is Margaret Park, whereas Frank Ralston\u2019s is Jane Peacock.Asking this in Hasura looks like:query WhoIsFranksSupportRep {\n  answer: Customer(where: {FirstName: {_eq: \"Frank\"}}) {\n    FirstName\n    LastName\n    Employee {\n      FirstName\n      LastName\n    }\n  }\n}\n\nWhich gives you the answer:{\n  \"data\": {\n    \"answer\": [\n      {\n        \"FirstName\": \"Frank\",\n        \"LastName\": \"Harris\",\n        \"Employee\": {\n          \"FirstName\": \"Margaret\",\n          \"LastName\": \"Park\"\n        }\n      },\n      {\n        \"FirstName\": \"Frank\",\n        \"LastName\": \"Ralston\",\n        \"Employee\": {\n          \"FirstName\": \"Jane\",\n          \"LastName\": \"Peacock\"\n        }\n      }\n    ]\n  }\n}\n\n3. Which Playlist is the most popular?\nThis one\u2019s a bit of a trick question, the answer is that this question can\u2019t be answered right now. If this question is important to the business domain, that isn\u2019t represented in the data model. To answer it might mean asking things like which playlist has been listened to the most, or which playlist has the most upvotes. As of right now, all we can say about a Playlist is its name, id, and what Tracks it has. For the current state of our database, this would require a discussion around what metrics we want to track and what we would need to alter the schema to track those. Cue a roadmap planning meeting.4. Which Tracks are my best sellers?\nEach time a track is sold, there will be an Invoice that contains that track in its InvoiceLines. (The InvoiceLine can be thought of as a line item on an itemized receipt.) Thus we can imagine this by thinking about starting at the Track node, and then for each Track we can walk across all the incoming edges from InvoiceLine and find every time the track has shown up as a line item for an Invoice. From there we sum the Quantity for that InvoiceLine, which will give us the total number of times each specific track has been purchased.To answer this question, we\u2019d then want to sort our results and determine which track is the most popular. Hasura provides a great way to do this for you:query WhichTracksAreMyBestSellers {\n  answer: Track(limit: 3, order_by: {InvoiceLines_aggregate: {sum: {Quantity: desc_nulls_last}}}) {\n    TrackId\n    Name\n    InvoiceLines_aggregate {\n      aggregate {\n        sum {\n          Quantity\n        }\n      }\n    }\n  }\n}\n\nIn this dataset, it seems that the best-selling tracks have many ties for first place, and the output of the top three for this query is:{\n  \"data\": {\n    \"answer\": [\n      {\n        \"TrackId\": 2,\n        \"Name\": \"Balls to the Wall\",\n        \"InvoiceLines_aggregate\": {\n          \"aggregate\": {\n            \"sum\": {\n              \"Quantity\": 2\n            }\n          }\n        }\n      },\n      {\n        \"TrackId\": 8,\n        \"Name\": \"Inject The Venom\",\n        \"InvoiceLines_aggregate\": {\n          \"aggregate\": {\n            \"sum\": {\n              \"Quantity\": 2\n            }\n          }\n        }\n      },\n      {\n        \"TrackId\": 9,\n        \"Name\": \"Snowballed\",\n        \"InvoiceLines_aggregate\": {\n          \"aggregate\": {\n            \"sum\": {\n              \"Quantity\": 2\n            }\n          }\n        }\n      }\n    ]\n  }\n}\n\nFrom simple to supergraph, tying it all together.\nI know what you might be thinking. How does a toy data model relate to building supergraphs? I think the real question is who said that this toy model couldn\u2019t be a toy supergraph?Physically, we can put data and compute wherever makes the most sense. (In today\u2019s age, that could mean spilling compute or data onto the edge.) There\u2019s no reason that the logical model we use to represent and talk about things can\u2019t be split up into pieces that are more micro when we physically deploy them. The capability to efficiently perform remote joins across disparate data sources and API\u2019s opens lots of doors for lots of people. We can design and talk about things as if we\u2019re working on a monolith that yields high cohesion and easy governance, yet we can still get the benefits of microservices like loose coupling and scaling ownership. It\u2019s especially compelling when we start to imagine all the different things we could plug into the supergraph. Some of the data connectors I\u2019ve been building that I\u2019m excited about include connectors for databases like Qdrant, Turso, and DuckDB. The supergraph is super-flexible. It\u2019s not too difficult to learn how to build a custom data connector for whatever you might want to connect to. This makes it easy to add whatever capabilities you need directly onto your supergraph and ultimately helps you ship faster.While the path to mastering GraphQL and building a supergraph is riddled with complexities, the journey is well worth the effort. For organizations considering the leap, it's important to approach the design and implementation of a supergraph not as a quick fix but as a strategic choice. Embracing this technology with the right tools and mindset, like leveraging solutions such as Hasura, can significantly simplify the transition. This approach not only eases the technical challenges but also aligns with the long-term vision of efficient and scalable data management. As we continue to innovate in the digital landscape, GraphQL, with its capacity to power supergraphs, stands as a testament to our evolving technological capabilities. Let's embrace this future, with all its challenges and opportunities, to build more robust, efficient, and interconnected systems.I invite you, the reader, to share your experiences, thoughts, or questions about GraphQL and supergraphs. Let's foster a community of learning and exchange around these exciting technologies, and together, let's embrace the future of web development. Reach out to me at tristen.harr@hasura.io"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL for data products Description: To connect backend data products with the chatbot frontend, a reliable, scalable, and secure communication mechanism is needed. This blog post posits that the link should be GraphQL. Author: Hasura"}, {"role": "assistant", "content": "*This blog was originally published by Sanjeev Mohan on his Medium page.\u200b\u200bThe promise of using a chatbot to derive data insights threatens the decades-old dominance of business intelligence reports and dashboards. Some thought leaders have even gone as far as suggesting it disrupts the entire World Wide Web and existing data catalogs.Gen AI isn\u2019t the only transformational trend of 2023. The other shift is in how we build and deploy data outcomes through data products. A data product offers superior, consistent, and reliable data access which allows consumers to get answers to their questions (or a chain of questions) to support business decisions or outcomes.What if the worlds of data products and large language models collide?Could we have a chat interface that simply asks questions to data products in natural language and gets back responses? And if so, what would that access pattern look like?To connect backend data products with the chatbot frontend, a reliable, scalable, and secure communication mechanism is needed. Traditionally this has been the REST APIs, but we need a more dynamic approach to accommodate the changing nature of queries and consumer needs. This blog post posits that the link should be GraphQL.But first, let\u2019s see how REST APIs and GraphQL differ.GraphQL introductionREST is a specification that defines the standard format in which the server transfers information. Clients make a stateless request to the server, which the server fulfills. REST APIs provide a technology-independent, simple, and scalable interface, which decouples clients and servers.GraphQL is a query language for APIs. Meta (Facebook) developed it in 2012 to optimize traffic between its mobile clients and the servers. Its efficiency stems from the fact that clients can request exactly what they need using typed schema.Meta shared the specification with the developer community in 2015. Since 2019, the Linux Foundation has hosted it.BenefitsAs GraphQL has matured, this specification to implement APIs provides benefits over traditional RESTful approaches:Reduces over fetching of data \u2013 REST APIs return more data than needed unless you create a specific endpoint for every data attribute, which is not practical.Reduces under fetching of data \u2013 multiple REST API calls are needed to fetch the desired data if it spans multiple tables and locations unless more specific endpoints are created for joined data.Provides a standard for data access \u2013 REST is a specification and not a standard. Hence, many organizations have implemented APIs that may not be consistent with others.In summary, GraphQL provides consistency through a standard specification while providing flexibility and higher control over the data received from the API calls. The term \u2018graph\u2019 is used because the API treats data as a logical graph structure. The abbreviation QL stands for an expressive \u2018query language\u2019 for the APIs.Typically, GraphQL is implemented over HTTP and its responses are delivered in JSON format but it is protocol agnostic. Its query language is very similar to the JSON syntax. GraphQL can be implemented over existing JSON interfaces.This document examines the benefits of GraphQL as an interface for data products.Challenges and opportunitiesREST is a well-established mature ecosystem for security and monitoring. While GraphQL has demonstrated its efficiencies, its adoption has lagged behind that of REST APIs. Also, GraphQL is more complex to set up than REST. But if we drill down into its complexity, it is in setting up the infrastructure to serve APIs, and not in its actual usage. Luckily, a few companies are easing the creation of the infrastructure framework, like Hasura.In addition, GraphQL can be converted and saved as a REST (or gRPC) endpoint. This allows REST APIs to work out of the box but at the efficiency of GraphQL. Open source options that provide \u201cRESTification of GraphQL\u201d include Hasura, Apollo, and The-Guild.Finally, the HTTP standard is being enhanced with the new HTTP QUERY request method. This upcoming standard will allow GraphQL reads using HTTP QUERY instead of HTTP POST and give a standard way to cache data.These opportunities make GraphQL an excellent choice to be the default access mechanism for data products. But, what exactly is a data product? To answer that, let\u2019s first explore the concept of data products.What is a data product?A data product is a container of data, metadata, and business logic with support for diverse access methods that offer a superior, consistent, and highly reliable consumption experience for a business user to get answers to their questions or a chain of questions to support business decisions.A data product stands out on two important characteristics: user experience and trust. It must have an owner who is accountable for its quality and reliability from the design, build, and runtime, to the retirement of the product. It is a single framework to get answers to all kinds of business questions and in most cases consumed via a self-service interface.Simply publishing a data set does not make it a data product. It must have the other components \u2013 the domain wrapper comprising a semantic layer, business logic, and metrics, and an access layer. Figure 1 illustrates the components of data products.Figure 1. Components of data products.Data products came into prominence with the rise of the concept of a data mesh, which has four principles, including \u201cdata as a product.\u201d While data as a product is a concept, data products are a physical manifestation of the concept. Data products stand out on their own merits and are independent of the concept of data mesh. Its key attributes are:Product management: It must have been developed using product management discipline. Data products have an end-to-end lifecycle \u2014 cradle to grave.Ownership: A data product should have an owner who is accountable for its quality and reliability. The owner should ensure that the data is accurate, up-to-date, and documented.Self-service: A data product should be consumed via self-service. This will allow users to get the information they need when they need it without having to go through a lengthy process of requesting access from IT.User experience: A data product should be easy to use and understand. The user interface should be intuitive and the data should be presented clearly and concisely. A \u201cdata contract\u201d should specify its schema, and access patterns, like APIs and SLAs.Trust: A data product should be reliable and accurate. The data should be curated and the consumers should have the ability to rate it.Single framework: A data product should be a single framework for getting answers to all kinds of business questions. This will make it easier for users to find the information they need and improve the overall efficiency of the business.As applications have adopted the microservices architecture, data products are doing the same in the world of data and analytics. While the entire concept of data products is new, the unique benefits of using GraphQL to access data products make them so compelling. The next section goes into deeper details.Benefits of using GraphQL for data productsData products are typically designed to be accessed through multiple output ports \u2013 user interface (UI), SQL API, and even a command-line interface (CLI). In the future, a chatbot using natural language will allow queries on data products. Many of these access metaphors will use large language models (LLMs) which will expand queries from simply being keywords, to adding semantic or similarity search capabilities.In each of these scenarios, a GraphQL framework can substantially accelerate data product access layer development while bringing consistent abstraction to the data product interface specification.But how does one evaluate if GraphQL is the right data access method?Figure 2 depicts various evaluation considerations. These criteria should apply to your business needs to select the optimal approach.Figure 2. A GraphQL framework provides benefits across many dimensions.User Experience\u201cWhat is easy to understand, gets adopted.\u201d Case in point: HTML. Before HTML, many markup languages existed, like SGML, but they were too complex for mainstream use. For GraphQL to be adopted, nothing is as important as the developer user experience. Developers have in recent years risen to be the most significant influencers of organizations\u2019 technical buying decisions.Traditionally, developers have written SQL code against the raw tables\u2019 schema. GraphQL appeals to front-end developers not proficient in SQL. They can declaratively describe the needed data which is returned in a single request.In addition, using a visual editor, developers can connect to the source database and construct their GraphQL query in a visual explorer. The engine then converts the GraphQL into SQL statements. A GraphQL frontend is like the well-known Postman tool that software engineers using APIs are familiar with.GraphQL has strongly typed schema and introspection capabilities that help developers build applications faster via code generation, code completion, API exploration, and documentation.GraphQL has backward compatibility with built-in version control using the schema. New types can be incrementally added to the schema without breaking changes to existing clients.GraphQL can vastly expand the number of data consumers who may not be proficient in SQL skills. These roles are often described as citizen roles, as in citizen analysts, and citizen developers. This benefit is similar to how chatbots now generate SQL code (and other languages) to run on behalf of the data consumers.ArchitectureDiverse data access patterns require that the underlying architecture is vendor-neutral. The users can access disparate backend systems in a loosely coupled manner.When developers write queries directly against the source schema, any changes in the schema can cause production bugs in downstream applications. In a loosely coupled architecture, GraphQL reduces the impact of changes to the underlying architecture. Technologies change, but business requirements don\u2019t change that fast. Decoupling increases the agility of the backend data products as they are developed and deployed independently of the data consumers\u2019 tight coupling. This allows them to scale on demand.Poorly implemented APIs can cause performance issues. Because of the nature of GraphQL APIs allowing arbitrary queries, it is important to limit the API calls based on unique parameters like IP or the user. GraphQL query depth can be restricted for users with the right authorization and security rules.CIOs and CDOs are seeking simplification of their burgeoning data and analytics architecture. Modern Data Stack has introduced new complexities that reduce transparency in the pipelines. GraphQL is an attempt to simplify data access. As AI matures, unified access for analytics and AI will become table stakes.Performance and latencyIn economics, Jevons Paradox states that when resources become efficient (and cheap), the demand for the resources, and hence the usage goes up instead of the other way around. Organizations have become more data-driven and have increased consumption of data products. However, data can easily become a victim of its success, unless we address performance and latency aspects.Today, many dashboards have to rely on syncing a local copy of the data warehouse to their desktops to provide ultra-fast analysis. Unfortunately, this model means users are working on stale data and are repeatedly replicating data. GraphQL is a more efficient way to connect dashboards to the server and allow analysis of real-time up-to-date data.GraphQL is like an API with a \u201cwhere\u201d clause. This is one of GraphQL\u2019s standout capabilities. It reduces over-fetching and under-fetching, making APIs more responsive. Complex multi-level and high-frequency queries are much more efficient when using GraphQL.Concomitant to the previous point about reducing rows that are fetched and returned, network traffic is reduced, further leading to higher efficiency.REST APIs support caching which reduces the number of requests sent to the server. GraphQL query responses can be cached granularly based on who is querying, and which fields they want to cache. With directives and cache-control headers, GraphQL queries can be cached effectively, reducing the number of requests sent to the server.Higher query and analysis performance leads to higher user experience and efficient operations. GraphQL can help improve the overall productivity of developers and end-users.Security and complianceMass adoption of the new LLMs will only happen when organizations can guarantee that sensitive data is protected and all applicable data privacy regulations are being met.Securing access control to underlying data sources is a constant challenge as different personas can access data at various layers of the stack \u2013 from raw object stores to curated persistence layers. GraphQL can be used to limit direct access to the underlying data sources.In addition, GraphQL access to a data product can also guarantee data quality and trust.Data residency laws require that personal or sensitive information should be collected, processed, and stored within a jurisdiction. Data sovereignty regulates who has access to which data. To achieve these data protection mandates, organizations can apply authorization techniques anywhere in the stack:at the storage level,on compute engines orinside application code, libraries, and reporting toolsin middleware or API gateway.Each option has its tradeoffs between cost, performance, and complexity. As every architect\u2019s goal is to isolate their IT stack from future changes and to minimize point solutions, it is safe to apply policies at the API later, as long as it is the only access point into the data storage.GraphQL can help to achieve dynamic authorization so that row-level security (RLS) and column-level security (CLS) can be achieved. At the API layer of abstraction, policies are applied consistently.CostOne of the biggest focus areas in 2023 has been on cost optimization. Growth at any cost is no longer acceptable when the economy faces headwinds. GraphQL can help alleviate wasted costs.As discussed earlier, GraphQL APIs apply checks and filters, which reduce the amount of data requested and returned from the underlying data sources. We can specify the filters at the row-level and at the column-level. As many DBaaS products charge for the amount of data scanned, GraphQL can reduce the database cost. In addition, lower network traffic also leads to lower costs.GraphQL makes it more efficient to build, deploy, and run data and analytics stacks. And it is also cheaper to maintain than a traditional API stack.Use casesOrganizations are facing a litany of tech challenges:IT teams are under pressure to address business consumers\u2019 growing data requests. They are often a bottleneck due to overburdened resources.Tech stack complexity is one of the biggest roadblocks to modernization or digital transformation initiatives.They are keen to do more with their data assets, but not at the risk of unpredictable costs.The key to their success is through more modern access based on GraphQL. GraphQL enables front-end applications using Node.js or React framework to modularly integrate data products to deliver disparate use cases.Business teams are developing multiple data products for various use cases. GraphQL can ease the integration of capabilities that can span operational, streaming, transactional, and analytical to deliver a derived data product.Before data products, IT teams have tried to unify operational and analytical databases (HTAP), but using GraphQL APIs as the data products interface is faster and more flexible.Because of GraphQL\u2019s inherent design that includes GraphQL subscriptions, real-time streaming use cases, such as anomaly detection, monitoring, auditing, and notification are easier to implement.We have so far examples of the use of GraphQL on analytical architectures and data products. But, for reasons like regulatory, some organizations are not allowed to consolidate data into a cloud data warehouse or a lakehouse. In such a case, standardized and consistent GraphQL APIs can federate data products with other data sources. GraphQL access is commonly used by data virtualization products, like Denodo.GraphQL can provide seamless communication between mobile clients and data products. This use case extends how GraphQL is already used to connect from a mobile device to a server database, likeAgility and developer productivityDevelopers yield higher influence than at any time in the past. A successful enterprise today needs very high developer productivity. GraphQL can make developers more productive.Developers may not be the final decision-makers in selecting new technologies, but they are heavy influencers. If developers adopt certain technologies that improve their productivity, organizations tend to make them the standards. Products like Hasura provide a user interface to build and run queries which improves the release cycle time for data products.Typically, developers spend a lot of time developing exact endpoints for their APIs. If a change is requested, then the APIs need to be further version-controlled. GraphQL overcomes the constant need for versioning of traditional APIs.GraphQL APIs are self-documenting. As and when the API changes, GraphQL can be used as a testing ground to quickly explore and test the APIs. Hasura console comes with a built-in GraphQL interface for exploring the APIs. Hasura\u2019s schema registry helps developers prevent breaking changes from being added inadvertently to production. After looking at the GraphQL schema diff, developers will have a clear view of what has changed and whether it affects their API in production among other things.EcosystemTypically, the success of a technology is less predicated on its features, but more on how good is the overall ecosystem across the end-to-end supply chain. This requires the technology to have an engaged community and integrations with other products within the ecosystem.A major appeal of open source software is because of a vibrant community of developers who build and contribute features and functionality. Although GraphQL is new, companies like Hasura have built communities to foster the development of this new specification.REST is not a standard but a set of principles that lead to inconsistencies although it provides flexibility.The data access framework should be well integrated with the orchestration tools, such as Apache Airflow, Prefect, and Dagster. It should also be closely integrated with the operations environment consisting of DevOps, DataOps, MLOps, LLMOps.Finally, It should also integrate with the data Governance environment. Data products catalog is a concept to enable their discovery as well as track their usage. In addition, the GraphQL framework should allow users access to business glossary, lineage, observability, and other metadata management features.A rich ecosystem is developing around GraphQL which is essential for its mainstream adoption.SummaryIn conclusion, some of the benefits of GraphQL are as follows:Efficiency \u2013 only fetch the data you need e.g. specific fields. This reduces network trafficking and improves concurrency, especially when the bandwidth is limited. This batching and data loader pattern also reduces the load on databases.Concurrency \u2013 Multiple resources can be requested in a single call thereby reducing the overhead of managing multiple HTTP requests.Latency \u2013 Data fetching can be optimized, with different parts of the query being executed concurrently as long as there are no dependenciesReal-time \u2013 Real-time subscriptions allow clients to receive updates as soon as the data changes.Prioritization \u2013 Assign different levels of concurrency or rate limiting to different types of queries or mutations based on their importance and resource requirements.Error handling \u2013 A consistent and predictable error format can make it easier to identify and handle errors gracefully, especially when multiple requests are being processed simultaneously.Recent surveys show organizations, on average, have over 100 SaaS applications. Analytical architectures are complex and schemas change frequently. As a result, the REST API should rest in peace. It has served us well and is a good choice for simple applications. However, its lack of standards and inefficiency leads to unsustainable costs in the modern era. It is time to explore the next generation of API access, via GraphQL that is better suited for AI applications.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How to convert TypeScript types to GraphQL types: better serverless functions Description: What if you could write TypeScript functions and that was enough to make a fully typed API in GraphQL? And while we\u2019re at it, can we get better serverless functions? Author: Akshaya Acharya"}, {"role": "assistant", "content": "What if you could write TypeScript functions and that was enough to make a fully typed API in GraphQL? And while we\u2019re at it, can we get better serverless functions?Write this...... and get this...type Email {\n  id: Int!\n  address: String!\n  body: String!\n}\n\ntype Mutation {\n  sendEmail(address: String!, body: String!): number!\n  sendEmailWithSignature(address: String!, body: String!): number!\n  getEmail(id: Int!): Email!\n}\n\nThese aren't just types \u2013 we can get a full API.How it worksHere's the code that makes getting a fully typed API possible: https://github.com/hasura/ndc-typescript-denoThe gist of it is parsing the TypeScript code (the TypeScript compiler exposes libraries for this), and then building out the GraphQL API.Hasura v3 has a connector architecture, where the engine (that receives and executes the query) uses connectors to connect different data sources to the graph. In Hasura, the TypeScript code is run by a connector, and the engine is configured by way of Hasura metadata (a data modeling tool) with the types and models that are backed by the functionsWe can literally just write TypeScript functions and run a few commands to get a GraphQL API. To me, this is the most compelling implementation of serverless functions. Other serverless functions today don't really run functions as much as they get you to implement http servers. The advantage of defining only functions is that the engine can now expose different APIs on top of it, be it GraphQL, RPC, or any other. Functions also compose far better than http endpoints, which allows for building semantic graphs.ConclusionHasura wants to empower TypeScript developers to deploy their work and make it usable for a wide audience. Larger organizations can incorporate the functions into their supergraph implementations.We believe in a declarative future, to bring maximum power to domain engineers, and the Hasura TypeScript connector is a big part of that story \u2013 by allowing architects to extend their model-based supergraph with custom logic and side-effects.ResourcesBuild TypeScript connectors: https://github.com/hasura/ndc-typescript-denoA real example that integrates with Sendgrid: https://github.com/hasura/ndc-sendgrid-deno"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Hasura open source observability stack Description: Explore how Hasura seamlessly integrates state-of-the-art, open source observability tools via OpenTelemetry giving developers a holistic view of performance, user interactions, and potential bottlenecks. Author: Raju Sunny"}, {"role": "assistant", "content": "In today's fast-paced tech landscape, understanding the intricacies of your application's performance is paramount. In this blog post, we'll explore how Hasura seamlessly integrates state-of-the-art, open source observability tools via OpenTelemetry giving developers a holistic view of performance, user interactions, and potential bottlenecks.But first, what is observability?Observability allows us to gain insight into a system from the outside by asking questions about it, and without knowing how it works internally. Furthermore, it enables us to quickly diagnose and tackle unexpected issues.To be able to ask those questions of a system, the application must be appropriately instrumented. That is, the application code must emit signals such as traces, metrics, and logs (the tree pillars of observability). When developers don't need to add further instrumentation to troubleshoot an issue because they have all of the information they need, an application is appropriately instrumented.What is Telemetry?Telemetry is one of the most reliable ways to monitor the stability and health of software in the realm of software development and application monitoring. Telemetry aids developers in maintaining awareness of the software's performance and alerts them when an issue arises with the application. Applications are no longer monolithic. Maintainers and developers clearly need a simple method to collect data from these complex, and diverse systems, and transmit it for monitoring and analysis.What is OpenTelemetry?OpenTelemetry is an open source observability framework that provides a unified way to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) from software applications. It is vendor-neutral and supports a wide range of languages and platforms.The figure below shows the rough architecture and capabilities of an OpenTelemetry implementation.OpenTelemetry Reference ArchitectureSo... why is this a big deal?Hasura supporting OpenTelemetry standards means a couple of important things for you and your organization.Saves timeYou will not have to deal with the inconvenience of installing various proprietary agents for various backend systems or re-instrumenting code. As a result, there is no need to create new connections as new technologies emerge because the OpenTelemetry collector will continue to trace data and export it without any issues.Avoid vendor lock-inWhen an APM vendor is no-longer suitable for your organization (cost, capabilities, etc.) you don't need to worry about their own custom monitoring tools or agents or connectors. You can freely move across to a better or cheaper APM vendor who will support OpenTelemetry almost seamlessly. You keep your end-to-end visibility with any APM vendor.TransparencyWith OpenTelemetry, you define what data is transmitted outside your application. No more wondering what a proprietary agent might be collecting and sending. You have full visibility and control over collected and transmitted data from your organization or application.So, how can I use OpenTelemetry with Hasura?Hasura fully supports OpenTelemetry for its logs, traces, and metrics. This applies to Hasura Cloud (Enterprise) and Hasura self-hosted (Enterprise). Navigate to your Settings/Monitoring and Observability section from both Cloud and Enterprise console, enable OpenTelemetry Exporter and configure your endpoints according to our documentation here. And that's it. Your Hasura signals are now available on your preferred APM tool.Introducing the Hasura observability stackEnterprise observability doesn't have to cost you a lot. Sure, there is a wide variety of APM platforms you can choose from. But one of the greatest observability stacks is available for free.As a reference, we at Hasura have created a pre-built, pre-configured, fully open-source-based observability stack for you. It comes with everything you need to have an enterprise grade observability using open source tools. Prometheus for your metrics, Grafana for visualizations, Jaeger for your tracing data, and AlertManager for event monitoring and alerting. Giving it a try is as easy as downloading a dockerfile and running docker compose. It brings up the latest Hasura GraphQL engine and all the components needed for end-to-end observability.Our observability stack comes with dashboards hand-built by Hasura engineers. They are one-click installable, and designed to show the right information that matters to you. Head over to our community repo here to give it a spin.Getting startedThe quickest way to get started with our open source observability stack is to clone the repository and use docker. There are two important files here you would need changes made todocker-compose.ymlModify the file to choose components you want to deploy. By default, Hasura GraphQL Engine, Prometheus, Grafana, Redis, Postgres (as the metadata DB and data source and an additional instance as a read-replica), Node-exporter, Alertmanager, Blackbox-exporter, Jaeger, Loki and Otel-collector. The selection depends on what components you want to deploy and what type of observability matters to you most.2. Dotenv\ta. Add your Hasura Enterprise license key to the variable \tHGE_EE_LICENSE_KEY.\tb. Set up your GraphQL admin secret key\tc. Optionally, protect your metrics endpoint with a secret. Although optional, this is a recommended step.\td. Choose the Hasura image version you would like to deploy by setting the value of HGE_IMAGE\te. Optionally, choose specific versions of the components listed previously\tf. Rename the file to .env in the same location.3. Deploy the system using docker compose up -d or using the swarm scripts provided in the \u201cutils\u201d folder.The default configuration enables the Native Prometheus exporter. If you want to push metrics to 3rd-party services that support OpenTelemetry, first disable the metrics API and configure the OpenTelemetry metrics endpoint to the otel-collector service. Browse the console -> Settings -> OpenTelemetry Exporter and configure the metrics endpoint.Embedded dashboardsTo help you get started quickly, we have included a few hand-built dashboards with this distribution. These are built to give you the best view into what matters most in terms of observing Hasura. The dashboards are broadly classified into two categories \u2013 general and Hasura. GeneralSystem Monitor gives you basic system information of the environment where you have deployed this stack. This includes CPU, Memory and Storage use, and system load.HTTP-Ping monitors various components you have deployed here, including the status of Hasura GraphQL Engine\u2019s health, and various data connectors.HasuraAn information-dense dashboard showing you a quick overview of most of the golden signals emitted by Hasura. This dashboard could be your \u201cquick-glance\u201d into your Hasura deployment\u2019s health.Hasura Subscriptions is a subscription specific dashboard giving you a detailed view of your subscription metrics. You can monitor the current active subscriptions, historical subscription information, websocket usage, latencies, and your subscription poller statistics.Hasura Logs and Traces: All logs generated by Hasura GraphQL Engine, and tracing information is available here. Trace information is available on the Jaeger web interface, too, which you can access using http://<your_deployment_address>:4002Hasura - HTTP GraphQL: Quick overview of your GraphQL metrics (including historical data) on your queries, mutations, HTTP connections and your cache metrics.Hasura Health: Health information of your Hasura deployment including health-check stats for your data sources, and connections to your data sources.Extending the Hasura observability stackYou are not limited by the dashboards we have provided or the components involved here. You are free to extend the functionality of this deployment to integrate with systems in your environment. For example, if you need Alert Manager to integrate with your enterprise IRP (Say PagerDuty), you can configure one of the many first-class integrations available. You can configure your own alerts using Grafana Expressions. An example where we need to alert users if your GraphQL error rates increased by a certain percentage in the last five minutes can be created using the following Grafana expression (increase(hasura_graphql_requests_total{response_status=\"failed\"}[5m]) / increase(hasura_graphql_requests_total[5m]))>= 0.5\n``\nSet a severity level (INFO, WARNING, CRITICAL etc), create an error message for your alert The error rate of GraphQL requests from your instance {{ $labels.instance }} exceeds {{ $value | humanizePercentage }}\n``\nAnd you are all set. You can configure more alert-rules by going to the Alert Rules (The Hamburger menu on the left, expand, and go to Alerts).Wrapping up our observability journeyAs we've journeyed through the Hasura observability stack, I\u2019m sure you will agree that it empowers you to proactively address issues, optimize user experiences, and confidently scale our application without being tied to a vendor, or an agent that you need to install \u2013 but do not know a lot about. As the digital world continues to evolve, staying ahead of challenges is crucial, and with OpenTelemetry by our side, we're equipped to navigate the future. Thank you for joining us on this exploration, and here's to building more resilient, efficient, and user-friendly applications together! Happy observability, everyone."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Compiling GraphQL for optimal performance: going beyond Dataloader Description: An implementation-level description of the architecture that allows Hasura v3 to achieve better-than-Dataloader performance across multiple different data sources. Author: Akshaya Acharya"}, {"role": "assistant", "content": "Updated 2023-11-17: Added benchmarks comparing simple dataloader approach to optimized database query using JSON variables. Jump to benchmarks.In this article: An implementation-level description of the architecture that allows Hasura v3 to achieve better-than-Dataloader performance across multiple different data sources.\n\nFor GraphQL practitioners: How to serve GraphQL more efficiently, including example code to generate efficient database queries.\nFor architects of entity based frameworks that construct a supergraph to relate and query different data sources: The method that underlies Hasura's execution engine, including a complete spec of the API contracts.\n\nPerformance is a foundational feature of Hasura; after all, Hasura is built to leverage ALL of your data. This has been true all the way since 2015, when the query execution engine started off written in Haskell, to now \u2013 hundreds and thousands of iterations later \u2013 with Hasura v3 being written in Rust. The fundamental techniques, however, are applicable to anybody who is building a GraphQL server, in any language, and with any architecture.\nThe venerable Dataloader from Facebook makes an appearance, and we'll see where in the architecture to place it for maximum effectiveness. We\u2019ll also see how we can go further.\nTOC\nContext and terminology\nThe technique has three parts (for architects):\n\nDecompose the query based on subgraphs\nQuery compilation into efficient database queries, and execution (for backend engineers)\n2a. catalog connector (three methods)\n2b. feedback connector (three methods)\n- Supergraph Top-N Challenge\nConsolidation of result parts\n\nPrior work\nConclusion\nResources\nContext and terminology\nA supergraph is composed of Models that form the nodes, and Relationships that form the edges. A Model is backed by a data source like tables in a database or RPC endpoints. (In addition, there are also Commands for mutations, not discussed further here.)\nA Relationship maps a field on a Model [1] to either one or many of another Model. Like a foreign key relationship in a database.\nWe'll use this example query:\nquery {\n    store(id: id_x) {\n        location\n        products(first: first_n) {\n            title\n            reviews(first: first_k) {\n                content\n                user {\n                    name\n                }\n            }\n        }\n    }\n}\n\nWhere the stores and products Models are from the catalog database, and the reviews and users Models are from the feedback database.\nproducts.id field maps to reviews.product_id\nreviews.user_id field maps to user.id.\n1. Decompose the query based on subgraphs\nThe Hasura engine receives the query and decomposes it into subqueries that can execute against different databases. [2]\nHasura is built on a connector architecture that allows us to rapidly add support for any database by building a connector for that data source. The data source can be a database, or existing domain microservice.\nThe connector can incrementally implement more of the Native Data Connector (NDC) Spec and the engine will unlock more and more complex supergraph capabilities for the connector, such as aggregations or cross data source relations.\n2. Query compilation into efficient database queries\nIn this section:\n\nAbstract Syntax Tree (AST) to SQL compilation\nJSON aggregation\n\nWhile everyone agrees that moving joins from your client into the server is a very good idea, Hasura goes further: It moves the joins right into the database.\n2a. catalog database connector\nThe connector for the catalog database receives an NDC Query.\nndcQuery {\n    store(id: id_x) {\n        location\n        products(first: first_n) {\n            id\n            title\n        }\n    }\n}\n\nThe connector knows its domain and can generate optimized queries.\nImportantly, this NDC Query also fetches the products.id field so that the engine can later compose the feedback data.\nTo illustrate the idea, consider how we can convert a GraphQL Query to an optimized SQL query for PostgreSQL.\nFirst, the GraphQL query was converted to the AST. The AST contains all the information that the GraphQL query did, just parsed into an object tree, so we can be convinced that it contains all the information we need to generate our SQL queries.\nA partial implementation of the GraphQL AST to SQL compiler - GitHub\nThe output we want has a nested form:\nResultObject<stores> {\n    Row {\n        location: String\n        products: ResultSet<products> {\n            [\n                Row {\n                    title: String\n                    user_id: Integer\n                }\n            ]\n    }\n}\n\n2a (i) Two separate queries\nWe might consider making two separate queries.\nselect stores.id, stores.location\nfrom stores\nwhere stores.id = id_x\n\nfollowed by\u2026\nselect\n  products.store_id,\n  products.title, products.user_id\nfrom products\nwhere products.store_id in ( id_x )\n\nThe connector would then need to transform this data into the nested form.\nThis approach has the problem that database connectors don't parametrise a variable number of arguments for IN clauses, which means awkward string manipulation. Also, there is a hard limit on the number of parameters that you can pass in.\nWe can do better.\n2a (ii) Lateral joins\nWe can consider an SQL query with left join lateral.\nselect stores.id, stores.location, products.title, products.user_id\nfrom (\n    select *\n    from stores\n    where id = id_x\n) stores\nleft join lateral (\n    select title, user_id\n    from products\n    where store_id = stores.id\n    limit first_n\n) products on true\n\nWe need lateral because this allows us to use sorting and limiting on the products query, independently for each stores row (there could be many, although there is only one here), which allows for complex nested queries.\nThis returns the outer product rows with multiple rows that repeat the store information for each store product, which the connector then needs to transform into the nested structure.\n| stores.id | stores.location | products.title | products.user_id |\n\nThis is already far better than the previous approach. There is only a single query, and we don't need to go back and forth to the database to get all our data.\nHowever, the connector still needs to transform this into our nested structure, which is additional computation to go row by row, group stuff by unique (sets of) columns, and create more objects to generate the nested structure.\nAnd these problems get worse when there's more nesting/joining.\nWe can still do better.\n2a (iii) JSON aggregation\nToday, both PostgreSQL and MySQL support JSON aggregation. Just like the sum aggregation maps a set of rows to a single number, JSON aggregation functions map a set of rows to a single string i.e. a JSON string that represents the set of rows.\nselect json_agg(row_to_json(root_stores))\nfrom (\n    select stores.id, stores.name, products.products\n    from (\n        select *\n        from stores\n        where id = id_x\n    ) stores\n    left join lateral (\n        select json_agg(row_to_json(root_products)) products\n        from (\n            select products.id, products.title\n            from products\n            where products.store_id = stores.id\n            limit first_n\n        ) root_products\n    ) products on true\n) root_stores;\n\nBy moving the data transformation as close to the source as possible, we avoid parsing and transforming the data multiple times, eking out the last drop of performance possible. The JSON that the database sends back has exactly the nested structure that we need!\nOnce the engine receives the response, it collects the products.id fields and sends off the second part of the query to the feedback connector with this information.\n2b. feedback database connector\nIn this section\n\nTop-N Queries\n\nThis connector receives the next NDC Query and the array of products.id as a variable.\n\nvariables: foreach $var in [ p_id1, p_id2, p_id3 ]\n\nndcQuery {\n  reviews(where: product_id = $var, first: first_k) {\n      content\n      user {\n          name\n      }\n  }\n}\n\nThe connector implements the NDC foreach capability, which means it can return the result of executing the NDC query for each of the variables [3]. Specifically, the connector returns an array of results that map to the array of variables; exactly like Dataloader!\nThe crucial bit here is that the engine knows nothing about the implementation, foreach is implemented by the connector, allowing different connectors to optimally choose different strategies depending on the underlying data source.\nThe obvious way to do this is to execute n parallel database queries for each of the n variables. This is what's conventionally known as the n+1 problem.\nLet\u2019s look at some other ways to solve this problem. We already know how to generate the most efficient query for a single variable instance. Now let's add variables. (The joins from the nested query as in the previous section are elided to focus on other techniques.)\n2b (i) IN clause\nselect\nfrom (\n    select * from reviews\n    limit first_k\n) root\nwhere root.product_id in ( p_id1, p_id2, p_id3 );\n\nThis has the limitations of the IN clause discussed above.\nMoreover, this doesn't work for more complex variable use cases, where we want to parametrise other parts of the query.\n2b (ii) JSON operators\nAnother way to do this is to materialize a temporary table with the variable ids, and then construct a query using that table.\nHere's one way to do this using JSON operators to pass in all variables as a JSON string;\nselect\nfrom json_array_elements('[1,2,3]') vars(var)\nleft join lateral (\n    select * from reviews\n    where reviews.product_id = var::text::int\n    limit first_k\n) root on true;\n\nThe database returns the nested structure we need, with top level keys as the variable instances. We can use the variables in any bit of our parametrized query!\nThe connector grabs each top level ResultSet value, puts them in the right order, and sends it back to the engine.\nBenchmarks!\nAs expected, using the JSON operators method to move the foreach into the database performs far better than repeated queries, even for modest number of values for the variable. Here are some benchmarks and waterfall charts for v3.\nSee that sharp drop from 430ms to 130ms towards the end? And that's only for 6 values for the variable!Before: back and forth to the databaseAfter: optimized to a single query. The scale is different though, this takes much less time.2b (iii) Other techniques\nThe pattern of returning the Top N rows for each element of another set is extremely common!\nFor instance, your favorite messaging app might show the last n messages for every thread. Or a fantasy league website might show the Top N people with the largest sum of points, for every tournament.\nConsidering how common this pattern is, it's surprising how few tools naturally support it! And even more surprising how poorly many of them perform. State of GraphQL Gateways in 2023 - The Guild.\nHasura hosts the Supergraph Top N Challenge where you can post your solution and win 1000USD for the best performing solution. At the time of this article, we are hosting this in conjunction with Hacktoberfest 2023.\nHere's a compilation of some solutions that could be useful implementing this pattern against different databases that support different features. Database techniques - Github.\n3. Consolidation of result parts\nThe connectors have really already done most of the work. The engine takes the results returned, and stitches them back to each other at the connector/subgraph/subquery boundaries.\nAt this stage, the engine can also implement features across the whole supergraph. For instance, it can add a Relay global id field for each returned Model instance, and this id is globally unique across all data sources/connectors.\nIt's worth looking at some of the implicit assumptions we made that enable all this to work.\nThe engine must have an excellent idea of the data model to plan the query. This includes how Models map to connectors (i.e. data sources), their Types, and the Relationship definitions. The engine must also know what capabilities each connector supports. In Hasura, all of this (and more) is specified at build time as the metadata, which means the engine can cold start on a project in milliseconds.\nThe fact that this metadata captures the entire domain model is also excellent from a discovery point of view. The supergraph serves as an always up-to-date and executable domain model for the entire organization!\nPrior work\nThere's quite a few projects that do cross data source joins.\nFor instance, PostgreSQL Foreign Data Wrappers and Apache Calcite.\nHasura adapts some of these ideas to deliver a ready to use GraphQL API, adding features like application user permissions. The metadata based model allows building effective tooling like our LSP-based editor and console.\nWe've also made efforts to ensure that our connectors are extremely easy to write. The NDC spec is JSON over HTTP. Our TypeScript connector automatically adds TypeScript functions in a file to your supergraph with no additional effort.\nConclusion\nAt Hasura, we know that every last millisecond adds up fast when you want to leverage your data. Our engineers wear every last millisecond shaved off as a badge of honor; it's not uncommon to see optimizations and benchmarks posted on our internal Slack channels! The methods we use today are borne out of lessons learned in practice across several years, product versions, and a/b testing with different approaches.\nWe believe that the supergraph approach with query compilation is the way forward to ensure maximum performance with maximum flexibility to bring data in from any type of source. We hope to see these implementation patterns used by different entity based supergraph frameworks, GraphQL gateways, and backend engineers writing their own GraphQL servers alike.\nResources\nPartial implementation of GraphQL AST to SQL compiler in Javascript - Github\nHasura Native Data Connectors (NDC) Spec\nThe Supergraph Top N Challenge - GitHub\nAlternate supergraph implementations: ent.go - Meta DGS - Netflix and more...\nNDC Spec - GitHub\nTypeScript Connector, connector for TypeScript functions - GitHub\nSQL Techniques to Solve the Top N Problem - GitHub\nFootnotes\n[1] Multiple Models can share the same ObjectType (which maps to a GraphQL Type), and it\u2019s actually the ObjectType that is related to other Models.\n[2] More properly, the engine parses the GraphQL query into an internal representation, and executes NDC Requests (Queries / Commands) against connectors that are backed by data sources.\n[3] Variables are more general: multiple variables can be used to parametrise any portion of the query.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Supercharging development: The power of Hasura and ClickHouse for real-time analytics Description: Combining Hasura\u2019s ability to instantly generate GraphQL APIs and ClickHouse\u2019s powerful real-time analytics capabilities allows developers to rapidly innovate and deliver blazing-fast applications that include real-time analytics. Author: David Meleney"}, {"role": "assistant", "content": "Teams of developers large and small are trying to do more with less, deliver results faster, all while maintaining lightning-fast, secure applications that scale. Combining Hasura\u2019s ability to instantly generate GraphQL APIs and ClickHouse\u2019s powerful real-time analytics capabilities allows developers to rapidly innovate and deliver blazing-fast applications that include real-time analytics.What is ClickHouse?ClickHouse is an open source column-oriented database management system that\u2019s designed for blazing-fast real-time analytics. It is primarily used to store and analyze large volumes of data, leveraging its column-oriented nature to efficiently run fast query processing over large datasets. The ability to retrieve data quickly from huge volumes of data in real time, makes ClickHouse an ideal data store for GenAI and analytics applications.What is HasuraHasura is a cutting-edge platform that allows users to seamlessly build and deploy REST- and GraphQL-powered applications. By providing instant GraphQL APIs over existing databases, Hasura enables rapid development without compromising on performance or security. Its intuitive interface and powerful features, such as caching and role-based access controls, empower developers and businesses alike to build scalable, secure, and efficient applications. Whether you're a seasoned developer or just getting started, Hasura promises to simplify and accelerate your application development journey.ChallengesData accessTeams across organizations of all sizes need access to specific data and leverage that data to build applications that serve internal and external users. APIs and microservices are a natural choice for connecting data to consumers in a fast, secure, scalable, and decoupled way. But the traditional API development process is time consuming. Manual API integration is time-consuming, requiring developer teams to create boilerplate CRUD code, figure out authorization (input / output validation, etc.. And then even more work to get the API production-ready \u2013 putting in place the right security controls, wiring it up to be observable for monitoring, performance tuning, etc.Limited flexibility means REST enforces a fixed set of endpoints with specific data structures, causing developers to manage multiple endpoints and make multiple requests for related data.Real-time analyticsTraditional OLTP (Online Transaction Processing) and Cloud data warehouses are not optimized for building interactive, data-driven applications. These applications demand low latency queries, to provide responsive user experiences, even when exposed to high levels of concurrency. As a result, organizations turn to real-time analytical databases, such as ClickHouse.Benefits of Hasura + ClickHouseData access that\u2019s instant, fast, and secureHasura's ClickHouse integration changes the game, especially when we consider the inclusion of GraphQL. Hasura's enterprise-ready API management and RBAC features simplify the adoption of new databases such as ClickHouse. Users can enjoy ClickHouse's ability to perform analytical queries over billions of rows in milliseconds, without needing a deep understanding of the data structure and schema. Application development can be accelerated with development teams working with a familiar API interface, and avoiding the initial cost of writing and refining SQL queries.Existing ClickHouse users can exploit Hasura's ability to provide a data agnostic query layer in front of ClickHouse \u2013 simplifying the adoption and allowing rapid prototyping through automatically generated JSON APIs.GraphQL's flexibility: As previously emphasized, GraphQL\u2019s capability allows developers to request exactly the data they need, no more and no less. This precision helps in reducing data transfer loads and enhancing application performance.Automatic API generation: With Hasura, as soon as the schema is set, it auto-generates a live, real-time GraphQL API, reducing the manual labor of setting up APIs.Data federations: Hasura GraphQL Engine provides a uniform interface to the data consumer across multiple data domains. This reduces the need for manual data integration and API orchestration, by integrating data across multiple diverse sources providing rich capabilities of retrieval, so that the data consumer does not have to incur the cost of data integration or API orchestration.Developer-friendly JSON format: GraphQL\u2019s innate use of JSON for both queries and responses is an advantage. The readability and simplicity of JSON are much loved by developers, and its universality makes data parsing effortless across different programming languages.Rapid prototyping: Hasura accelerates the development process by offering instant real-time APIs, which in turn aids in quick prototyping and validation of ideas.Real-time analytics: Fully managed, open source, and lightning fastClickHouse addresses the challenge of efficiently storing, managing, and querying massive volumes of data in real time. Traditional relational databases can struggle with performance issues when handling large-scale data analytics. ClickHouse, being a column-oriented database management system, is designed to perform real-time analytical queries on large datasets with high speed. Its architecture supports distributed processing, enabling it to crunch millions, or even billions, of rows in a fraction of the time that conventional databases might take. This makes it particularly suited for scenarios that require swift analysis of big data, such as telemetry data analysis, real-time analytics dashboards, and high-speed logging. In essence, ClickHouse offers a solution to the growing need for rapid, real-time data analytics on voluminous datasets.What can I build with Hasura and ClickHouse?1. AI-powered SRE assistantBuilding a GenAI-powered SRE assistant with ClickHouse and HasuraBefore: Traditional systems struggled with real-time data analysis, often bottlenecking SRE operations and making it challenging to harness the full potential of GenAI for system reliability enhancement.After: With ClickHouse integrated into Hasura, SRE teams now experience blazing-fast data analytics, unlocking the true capabilities of Gen-AI to power a sophisticated SRE assistant, ensuring enhanced system reliability and efficiency.2. Real-time analyticsBefore: Earlier, building real-time analytics dashboards required developers to write intricate SQL queries, entailing considerable time and expertise to manage and optimize data retrieval processes.After: Now, by utilizing GraphQL, developers can effortlessly construct dynamic dashboards that provide real-time updates, showcasing the most current data insights and fostering data-driven decision-making.3. E-commerce platformsBefore: In the past, e-commerce platforms had to rely on multiple endpoints for fetching various data like user details, product information, and order histories, which often resulted in slower load times and a complex backend architecture.After: With Hasura GraphQL engine, e-commerce applications streamline data fetching through a single API endpoint, thereby enhancing performance, reducing load times, and offering a more refined user experience.4. Content aggregation platformsBefore: Aggregating content from various sources traditionally required an intricate network of API calls, making the process less efficient and harder to manage.After: Now, Hasura facilitates a unified data access layer, allowing content aggregation platforms to fetch data from disparate sources with ease, presenting a consolidated view of information through a simplified query structure.Getting started connecting Hasura to ClickHouseCreate a free ClickHouse Cloud projectYou can connect Hasura to an open source or cloud-hosted version of ClickHouse. This example will cover connecting a Hasura Cloud project to a ClickHouse Cloud deployment. First, create a free ClickHouse Cloud trial. You will be prompted with several questions regarding what type of deployment and cloud provider you would like to use. You can connect Hasura to any type of ClickHouse deployment. For this demo, we recommend you select \u201cAnywhere\u201d to simplify things.Note: It is recommended that you host your Hasura and ClickHouse deployments in the same cloud provider and region, if possible, to reduce latency.Add sample data to your ClickHouse deploymentOnce you have created your ClickHouse Cloud deployment, you can add some sample data. Luckily, ClickHouse makes it easy to get started with a number of sample data sets. From the ClickHouse dashboard click the Actions dropdown and select add data. This will launch the console for the ClickHouse service you created.You will be presented with several options to add data to ClickHouse. You can choose any of these options to get started. For this example, we will choose one of the predefined sample data sets ClickHouse provides.You can see the predefined sample data offered by ClickHouse as of writing this post below. You can use any data to connect to Hasura. For this example, we will use the New York Taxi Data.After you import the data into your ClickHouse service, you should see the following table and data in the ClickHouse console.Getting started with HasuraTo get started, head over to cloud.hasura.io. Create a project and then click Launch Console.Connect ClickHouse to HasuraNext, go to the Data tab to connect ClickHouse with Hasura.Click on Data Connector Agents and then click Add Agent.You are presented with a form that requires you to add your connector endpoint.Prerequisite for the next step requires installation of the Hasura CLI. If you need to install and configure the Hasura CLI you can find more information in the ClickhHouse Getting Started - Cloud documentation.Follow the below commands to deploy the ClickHouse connector and fetch the endpoint.# Create connector endpoint using Hasura CLI\nhasura connector create <connector name> --github-repo-url \nhttps://github.com/hasura/clickhouse_gdc_v2/tree/main/\n\n# Review logs\nhasura connector logs <connector name>\n\n# Review hosted connectors and fetch endpoint\nhasura connector list\n\nTake note of the URL for the endpoint displayed in the console. You will need it for the next step.Once you have the ClickHouse connector deployed you can fill and submit the form to Connect a Data Connector Agent.Name - name to identify the connectorURL - endpoint of your deployed connectorNow, click Connect Database at the top of the page and select ClickHouse to connect the existing database.Find the connection string for your ClickHouse project by clicking the Connect dropdown and selecting View connection string.Navigate to the HTTPS section of the dialog. Within the information provided you can find the connection.You are now ready to name your Hasura database connection and enter in the required information: connection URL, username, and password.Track ClickHouse tablesNow that we have connected ClickHouse to Hasura, we can track the nyc_taxi table we added to ClickHouse in Hasura to expose it over Hasura\u2019s auto-generated GraphQL API. While in the Data tab, click on the ClickHouse database you just connected. Select the nyc_taxi table and click Track Selected.Congratulations! \ud83c\udf89 You have now successfully connected Hasura to ClickHouse and exposed a table over the GraphQL API.Try some GraphQL queriesThe Hasura ConsoleYour ClickHouse data is now available over a GraphQL API. Using the Hasura GraphiQL console you can explore the API making it easy for developers to explore and transform data to suit their needs. This is especially useful for use cases involving analytics. Using Hasura\u2019s GraphQL engine you now have a secure API that leverages the capabilities of ClickHouse for real-time analytics. Let\u2019s try some examples. Note that all of the queries have a caching directive added to them with a total time to live (ttl) of 3600 seconds or 1 hour. This means all queries will be cached in Hasura cloud for 1 hour, making for faster response times and less load on ClickHouse.Simple query with filterThe query below will return trips with a specific id.query MyQuery @cached(ttl: 3600) {\n  nyc_taxi(where: {trip_id: {_eq: 1199999883}}) {\n    trip_id\n    total_amount\n    trip_distance\n    tip_amount\n    passenger_count\n    pickup_datetime\n  }\n}\n\nFilter results based on conditionThe query below will return all trips that were longer than 30 miles. Note that the schema or fields being returned has changed.query MyQuery2 @cached(ttl: 3600) {\n  nyc_taxi(where: {trip_distance: {_gt:30}}) {\n    fare_amount\n    trip_id\n    trip_distance\n    total_amount\n  }\n}\n\nAggregationThe following query will return the average and max values for fares, tips, and distance. Note: This is what ClickHouse is more efficient at than traditional databases.query MyQuery3 @cached(ttl: 3600) {\n  nyc_taxi_aggregate {\n    aggregate {\n      avg {\n        fare_amount\n        tip_amount\n        trip_distance\n      }\n      max {\n        fare_amount\n        passenger_count\n        tip_amount\n        trip_distance\n      }\n    }\n  }\n}\n\nNext stepsThese are just the first steps to fully leveraging the capabilities of Hasura and ClickHouse. To build out your API further you can:Add sample data that includes relationships that will also be reflected in the GraphQL API. For more information see this demo.Add caching directives.Manage access to your data and APIs by defining roles and permissions.Create a REST endpoint.Read more about a use case we found for using ClickHouse here at Hasuara: Building a GenAI-powered SRE assistant with ClickHouse and Hasura By using Hasura to expose ClickHouse data over a GraphQL API, developers can significantly reduce the time and complexity involved when creating and iterating on real-time analytics applications. The user-friendly features and tools provided by Hasura streamline the development process, allowing developers to focus more on building powerful applications and less on wrestling with complex data retrieval processes. This synergy brings unparalleled efficiency and performance to development, setting a new standard for data management and analytics in the modern business landscape.\ud83d\udcda Documentation and resourcesTo help you get started, we've prepared detailed documentation, guides, and examples:Hasura Clickhouse documentationBuilding a GenAI-powered SRE assistant with ClickHouse and HasuraBuild an SRE app powered by AIHasura on Clickhouse\ud83d\ude80 Get started today!We can't wait to see the amazing applications you'll build using Hasura and Native Queries. Get started today by signing up for Hasura Cloud and connecting to one of the supported databases.If you have any questions or need assistance, feel free to reach out to our team on Discord or GitHub."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a supergraph backend with Hasura Description: In this blog post, we'll guide you through the process of building such a backend, highlighting the use of Snowflake and PostgresSQL databases, as well as demonstrating individually executed and nested queries.\n Author: Akshay Bhat"}, {"role": "assistant", "content": "Using Snowflake DB and Neon PostgreSQLIn today's fast-paced world, building scalable and efficient backends for applications is essential. Developers are constantly searching for tools that can simplify the process and improve performance. Hasura Cloud, Snowflake DB, and PostgreSQL on Neon offer a powerful combination to create a supergraph backend that handles complex data requirements, like joining data across multiple data sources, filtering data on model relations. It's a supergraph architecture with a unified layer, made from modules, continuously evolving which is present in many different companies across the globe.In this blog post, we'll guide you through the process of building such a backend, highlighting the use of Snowflake and PostgresSQL databases, as well as demonstrating individually executed and nested queries.In this tutorial/guide, we'll guide you through the process of building such a backend, highlighting the adding of Snowflake tables and PostgresSQL tables as models to the supergraphs, as well as demonstrating complex queries with nesting that can relate models from across different data sources.1. Setting up Hasura CloudHasura Cloud is a fantastic tool for creating GraphQL APIs quickly. This is a preliminary tutorial to get started, a more detailed guide will be coming up soon. Here, we will see how to:Sign up for a Hasura Cloud account.Create a new project and connect it to your version control system (e.g., GitHub).Define your data model using Hasura's metadata.Run some exciting queries2. Integrating Snowflake DBSnowflake is a cloud-based data warehousing platform that seamlessly integrates with Hasura Cloud. To integrate Snowflake:Obtain your Snowflake credentials (account, username, password).In Hasura Cloud, navigate to the \"Data\" tab and add a new data source.Select \"Snowflake\" and input your credentials.\u201cImport all tables\u201d to add all the tables as models to the supergraph3. Incorporating PostgreSQL on NeonNeon is a managed platform for PostgreSQL databases. To incorporate PostgreSQL on Neon into your supergraph backend:Sign up for Neon and create a new PostgreSQL database instance.Obtain the connection details for your Neon PostgreSQL database.In Hasura Cloud, add another data source, but this time, select \"Neon Database\" and authorize the Neon database connection details.Add the models and relationships you want to track in the data tab.4. Executing individual queriesNow that you have Snowflake and PostgreSQL on Neon integrated into your Hasura Cloud project, you can execute queries individually.Querying SnowflakeTo query data from Snowflake, you can use Hasura's GraphQL API:query MyQuery {\n  SNOWFLAKETABLE {\n    COLUMN1\n    COLUMN2\n  }\n}\n\nReplace snowflakeTable, column1, and column2 with your Snowflake table and columns.Querying PostgreSQL on NeonSimilarly, to query data from your Neon PostgreSQL database:query MyQuery {\n  neontable {\n    column1\n    column2\n    column3\n  }\n}\n\nReplace neonTable, column1, and column2 with your Neon table and columns.5. Nested queries for complex data retrievalOne of the advantages of GraphQL is its ability to handle complex data retrieval through nested queries. Let's demonstrate this with an example:query {\n  getUser(userId: 123) {\n    username\n    email\n    posts {\n      title\n      content\n    }\n  }\n}\n\nIn this query:We retrieve user details by their ID from either Snowflake or PostgreSQL.Then, we fetch the user's posts with their titles and content.By defining appropriate relationships and foreign keys in Hasura's metadata for both Snowflake and PostgreSQL, you can seamlessly perform nested queries like the one above, aggregating data from multiple sources into a single response.ConclusionBuilding a supergraph backend using Hasura Cloud, Snowflake DB, and PostgreSQL on Neon offers incredible flexibility and performance. You can efficiently integrate multiple data sources and execute both individual and nested queries, making it suitable for a wide range of applications. Explore the power of these tools and start creating your supergraph backend today for a seamless and efficient data-driven application experience.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: A graph is not a set of nodes. Getting to the root of GraphQL complaints. Description: When you treat your graph like a set of nodes, you miss the point of GraphQL. The beautiful thing about data living in a graph is that each piece of data becomes enriched by virtue of its relationships with other pieces of data.  Author: Akshaya Acharya"}, {"role": "assistant", "content": "When you treat your graph like a set of nodes, you miss the point of GraphQL. Graphs have edges. The beautiful thing about data living in a graph is that each piece of data becomes enriched by virtue of its relationships with other pieces of data.\t\t\t\t\t\t\t\t\t\t\t\t\t\u00a7Common GraphQL federation is merely used to wrap existing RPC endpoints in GraphQL resolvers and then aggregate them at a single endpoint. The lack of building relations between these aggregated endpoints (i.e. collection of nodes, as opposed to a graph) leads to the common impression that building out a GraphQL API is busy work, and can be better supplanted by other patterns like BFFs. And moreover, this is true! This way of doing things sells GraphQL short.\t\t\t\t\t\t\t\t\t\t\t\t\t\u00a7Hasura is better in that it\u2019s built on the pillars of models, types, and relations which come together to define a supergraph. Once you declaratively define where your data is and how it's connected, Hasura automatically builds an execution engine that leverages these relationships to unlock the value of your data. You get to specify what data you want, and don't have to worry about reimplementing the how for every new what.\t\t\t\t\t\t\t\t\t\t\t\t\t\u00a7A model based approach to building a supergraph that exposes a GraphQL API is not a new idea. In fact, serious GraphQL users like Meta (Entgo), Netflix (DGS), Goldman Sachs, and Airbnb have their own entity based frameworks to construct supergraphs.Hasura is unique in that the supergraph is declaratively defined. Even authorization predicates can be declaratively defined on individual model rows and signed-in user parameters such as a role or user ID. What this means is that the execution engine can compile extremely efficient queries pushing graph traversals into database joins where possible, and pushing down authorization predicates into database where clauses.\t\t\t\t\t\t\t\t\t\t\t\t\t\u00a7The supergraph is a perfect abstraction to bring together disparate data sources as the concepts of models (nodes) and relationships (edges) can map cleanly to any kind of database such as relational (PostgreSQL), document stores (MongoDB), or even graph databases. The supergraph also cleanly incorporates any API/interface, including CRUD/RPC, CQRS, and other GraphQL endpoints.\t\t\t\t\t\t\t\t\t\t\t\t\t\u00a7Building an app or API without a supergraph is like storing your data in files instead of a relational database. Relational databases come with filtering and joins as basic features because everyone uses them. Databases assume this complexity as high-quality science based implementations to save their users the effort of implementing these common patterns (often poorly), and bring an increase in productivity and efficiency of several orders of magnitude to everyone.Further, databases bring about clear separation of concerns by building the storage and retrieval of data into the execution engine, leaving application developers free to focus on the business and domain logic.\t\t\t\t\t\t\t\t\t\t\t\t\t\u00a7Every data source is a model to Hasura: tables in your database, types in object stores, and RPC endpoints (Hasura understands the input and output types). Cross data source joins and filtering a model based on its relations work consistently across all models.The query interface with filtering, joins, and aggregations is isomorphic across the nature of the backing data stores.People suddenly find themselves in a platonically ideal world where they can invest energy and mindshare in the semantics of data, units of pure meaning and structure without a care for what the underlying implementation looks like.ConclusionIronically, it is in this world that GraphQL begins to show its true potential, merely as a query language to query the rich supergraph that we've already built. Yet the supergraph itself is versatile enough to service any protocol like RPC or even SQL.Build your supergraph with Hasura and GraphQL today."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Get 10x efficient in building production-ready GenAI RAG applications Description: Boost your productivity with Hasura and Portkey This blog illustrates how Hasura and Portkey can boost your productivity and help you stay competitive. Author: Simrat Hanspal"}, {"role": "assistant", "content": "Large Language Models (LLMs) like OpenAI\u2019s ChatGPT have been around for some time now and have shown they are great for automating many tasks like content generation, text processing, information retrieval, etc. The only thing limiting us at the moment is our creativity and context window. \ud83d\ude09Context window?Yes, LLMs are frozen in time. At the time of writing this post, ChatGPT\u2019s most recent context is September of 2021. Because your data is evolving and not seen by the LLM at the time of training, it can\u2019t interpret your proprietary data. If you ask it something about your internal or real-time data, which an LLM has never seen, it will make up an answer, also called hallucinating. Retrieval Augmented Generation (RAG) pipelines fetch relevant context related to the user query so that LLMs can generate their answer based on the context, thereby generating grounded contextual responses.Unfortunately, we are limited by every LLM\u2019s context window length, which requires us to be selective about the information we add to the prompt. In some cases, we can\u2019t cover all of the necessary context for an LLM to answer our queries. For example, consider the case of summarizing or answering queries on your application code from GitHub. You can\u2019t understand all the details from just a few functions or pages. Likewise, LLMs can\u2019t answer accurately with limited context in all cases.Bullish on context window size increase!As Moore\u2019s law has it, the GenAI industry is striving for a larger context window. Context window length of OpenAI\u2019s GPT models has doubled in 3-4 months release frame.ReferenceAnthropic also released a 100k context window Claude model, which can process 240 pages of API docs in a single prompt.But bigger is not always better. Recent research shows that a longer context decreases performance. You can read more about this fantastic research in \"Lost in the Middle\" paper.So, how much context is good context?Building GenAI production application is challenging.We\u2019re still in the early days of GenAI, with the technology quickly evolving.While context length is a critical component to the quality of application results, AI engineers have to deal with many such decisions and have to keep revisiting these questions as tech advances:What is the best vector DB in terms of cost and performance?What is the best chunking strategy?How do we avoid malicious attacks for different LLM data query patterns?What is the best model in terms of cost and performance?And then issues around reliably serving the app itself \u2013 how to debug when something goes wrong? How to monitor the model\u2019s performance?And many more.Spending a lot of time outside of building core GenAI product.Let us review a slightly abstracted tech stack of LLM applications. The first layer \u2013 applications is what you are building for your customers, e.g., chatbot to resolve issues with online orders. Second, business logic determines the execution logic, e.g., how the chatbot responds to incomplete order complaints. The third and fourth layers are the foundational layers with context-augmented LLMs and data sources.Given the fast-paced environment, it's equally important to avoid dedicating excessive time to constructing data APIs and setting up LLMOps (crucial yet somewhat mundane tasks \ud83e\udd37\u200d\u2640\ufe0f) when you could be focused on creating exciting and lucrative applications. \ud83d\ude80Boost your productivity with Hasura and Portkey \u2728Let us understand how Hasura and Portkey can boost your productivity and help you stay competitive.Understanding HasuraData-driven applications have always required secure data access. GenAI applications bring in new data query challenges: Time-consumingBuilding data APIs for multiple data sources with authentication and authorization, like role-based access control, is time-consuming and not optimized for performance, scalability, or reliability.Security is not easy Securely querying vector databases is challenging. Vector databases are not connected to the application data model, making it difficult to query vectors for specific entities or apply access permissions.Integration is complexVector data stores with additional metadata that require syncing data between application and vector databases. There is a high risk of the vector database going out of sync, leading to stale results.Hasura enables you to build production-grade data APIs in no time, unifying your multiple private data sources to give you secure, unified query capabilities with authentication and role-based access control.Hasura can help you to keep your data stores in sync with Event and Scheduled Triggers to automatically vectorize data whenever a CRUD event happens on your data.Build a secure LLM application API with Hasura.Easily convert any business logic into a secure API using Hasura Actions, including your LLM queries.Understanding PortkeyPortkey, in simple words, brings the DevOps principle of reliability to the LLM world \u2013 it adds production capabilities to your RAG app extremely easily without you having to change anything. And it does that by helping you answer four questions about your app:How can I see all my RAG calls, and identify which are failing?What is my latency and how can I reduce it?How can I stay compliant and keep my LLM calls secure?How can I ensure reliability when any upstream component fails?Portkey essentially operates as a gateway between your app and your LLM provider, and makes your workflows production-grade with its observability and reliability layers.Integrating Portkey is a breeze. Just set baseURL and pass Portkey headers, and you\u2019re set.Portkey provides:Observability LayerLogging: Portkey logs all requests by default so you can see the details of a particular call and debug easily.Tracing: Trace various prompt chains to a single id and analyze them better.Custom Tags: Add production-critical tags to requests that you can segment and categorize later for deeper insights on how your app is being used.Reliability LayerAutomated Fallbacks and Retries: Ensure your application remains functional even if a primary service fails.Load Balancing: Efficiently distribute incoming requests among multiple models to reduce latencies and maintain rate-limiting thresholds.Semantic Caching: Reduce costs and latency by intelligently caching of requests. (20% of requests can become 20x times faster).All of this, in your existing code, with just one line of change to integrate Portkey.Are you excited about Hasura and Portkey? We would love to hear what you are building, share your thoughts and feedback in the comments.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a GenAI-powered SRE assistant with ClickHouse and Hasura Description: To leverage LLMs and build an SRE assistant, we must augment the business context (of being an SRE) to LLMs by prompt engineering. ClickHouse is a database designed to allow us to fetch the relevant context for a query, and Hasura acts as a secure data-access layer. Author: Simrat Hanspal"}, {"role": "assistant", "content": "What is an SRE assistant?Site Reliability Engineers (SREs) are entrusted with the critical task of minimizing downtime and disruptions, often requiring them to navigate through a deluge of data flowing in from all directions. However, the stress and pressure of these scenarios and the overwhelming influx of data and signals can hamper an SRE's effectiveness and result in a cascade of service interruptions.Imagine a solution that could provide SREs with a dedicated assistant capable of seamlessly synthesizing all this information to provide useful insights about incidents and other events as they take place. For example, we could provide aggregated telemetry data to LLM and ask to provide root cause analysis (RCA) for an incident. Enter the concept of an SRE assistant: a conversational bot supercharged by Large Language Models (LLMs) designed to effectively manage the wide variety of inputs and challenges encountered in this high-stakes environment. While LLMs excel at comprehending and engaging in natural language, they do encounter limitations when confronted with novel problems.To leverage LLMs and build an SRE assistant, we must augment the business context (of being an SRE) to LLMs by prompt engineering.What is ClickHouse?ClickHouse is a fast, open source, column-oriented database management system that\u2019s designed for real-time analytics using SQL queries. Storing huge volumes of telemetry data and retrieving relevant context becomes possible through ClickHouse, which can crunch millions of records in a fraction of a second because of its distributed processing capabilities. The ability to retrieve data quickly from huge volumes of data at inference time, makes Clickhouse an ideal data store for GenAI applications.What is Hasura?Hasura is a platform to build secure production-grade data APIs in no time with role-based access control (RBAC). You can use native connectors or create your own custom connector.Hasura can connect to ClickHouse using a custom connector, which you can find here: https://github.com/hasura/clickhouse_gdc_v2.Hasura enables you to prototype rapidly with a Notebook. You can get started quickly and incrementally add value.Data and consumersWithin this environment, distinct teams need access to specific data as dictated by their permissions, aligning with the granular nature of modern data management. Further, the origin of the data, whether from mobile or web platforms, holds significance in determining the permissions governing its access, adding an additional layer of context to the process.Data layer challengesTo build highly performant microservices, we need to develop high-concurrency, low-latency data APIs with security and RBAC. Building data APIs is time-consuming and not the best use of a developer\u2019s time in the fast-paced GenAI environment.Why Hasura + ClickHouse?Hasura's enterprise-ready API management and RBAC features simplify the adoption of new databases such as ClickHouse. Users can enjoy ClickHouse's ability to perform analytical queries over billions of rows in milliseconds, without needing a deep understanding of the data structure and schema. Conversely, ClickHouse users can exploit Hasura's ability to provide a data agnostic query layer in front of ClickHouse \u2013 simplifying the adoption and allowing rapid prototyping through automatically generated JSON APIs.To demonstrate this value, we build an example application below.Tutorial: SRE assistantConsider a scenario where we are operating a web application, receiving an average traffic of around 100 requests per minute. To effectively manage this load, our infrastructure consists of 10 servers, each assigned a portion of the incoming requests through a round-robin distribution.However, there are instances when hardware issues come into play, leading to a delay in the request execution and subsequently causing increased latency. Our system is configured to monitor latency closely, and any sudden surge beyond the hypothetical threshold of 20 milliseconds triggers an automatic incident creation mechanism. This proactive approach ensures that performance anomalies are promptly addressed, maintaining a seamless user experience even during periods of heightened demand.In response to the volume of potential incidents that could arise, we want to develop an SRE assistant with the primary goal of swiftly uncovering valuable insights and helping to restore service. These insights are derived through the utilization of an LLM, which is thoughtfully equipped with pertinent incident context via a prompt. By feeding the assistant this contextual information, we enable it to effectively generate meaningful insights in a timely manner.We will execute the following steps to build an SRE assistant:ClickHouse account creationGenerate synthetic data and load it into ClickHouseConnect ClickHouse to HasuraBuild a secure data API with HasuraBuild an SRE assistant using LangChain via Hasura NotebookPrerequisites1. Hasura CLIIf this is your first time using Hasura CLI, you will need to install it. Follow the installation instructions in this document.2. Hasura NotebookFollow the steps in the document to initiate your Hasura Notebook. You will know you are ready when you are able to access the Hasura Notebook with a landing page that looks like this on your Hasura Notebook URL. Let\u2019s get building! \ud83d\ude80Getting started with ClickHouseCreate a ClickHouse account if you don\u2019t already have one. Login into the console and start a new service.During service creation you will be prompted to specify from where to allow connections. For this demo application, and to simplify getting started, ensure you select \u201cAnywhere.\u201dGenerate synthetic data and load it into the ClickHouse DBLaunch notebook.Open clickhouse_sre/synthetic_data.ipynb. Execute all the cells to generate the synthetic data. You\u2019ll find a data folder generated, if not already available, under the current location with three files.We can upload these files to ClickHouse using the file upload feature.Review all details. The types of data will be automatically detected and a schema proposed.Please don\u2019t forget to define Sort key for efficient querying.request.csvSort key: request_idincident.csvSort key: incident_idaggregated_server_metrics.csvSort key: server_idTables are now ready in ClickHouse with synthetic data.Getting started with HasuraTo get started, head over to cloud.hasura.io. Create a project and then click Launch Console.Connect ClickHouse DB to HasuraNext, go to the Data tab to connect ClickHouse with Hasura.Click on Data Connector Agents and then click Add Agent.You are presented with a form that requires you to add your connector endpoint.Follow the below commands to deploy the ClickHouse connector and fetch the endpoint.# Create connector endpoint using Hasura CLI\nhasura connector create <connector name> --github-repo-url \nhttps://github.com/hasura/clickhouse_gdc_v2/tree/main/\n\n# Review logs\nhasura connector logs <connector name>\n\n# Review hosted connectors and fetch endpoint\nhasura connector list\n\nOnce you have the ClickHouse connector deployed you can fill and submit the form to Connect a Data Connector Agent.Name - name to identify the connectorURL - endpoint of your deployed connector.Now, click Connect Database at the top of the page and select ClickHouse to connect the existing database.Add the connection details with the database name as SRE. You can fetch the connection details from the ClickHouse console.Now you can find the SRE ClickHouse database and track all the tables on Hasura Console.Great, now we have the database connected to ClickHouse.Congratulations! \ud83c\udf89You are through the first part of setting up the systems. Let\u2019s verify by heading to the API tab and executing a query to fetch records from any of the three tables. We can see that the data is now accessible through the Hasura console.Note how we\u2019ve integrated ClickHouse into our data stack without writing a single line of SQL!Setting role-based access controlSetting up granular permissions through Hasura is easy. Head to the Data tab, click on the SRE database, and then the request table.Click on the Permissions tab, define a new role called sre, and provide select permissions. This is helpful if you have restricted content and you want SRE to access only projects that he/she is authorized to. You can do so by accepting a header called X-Hasura-Project-Id and setting up a custom check. The custom check will return only records where the project-id matches X-Hasura-Project-Id.Don\u2019t forget to select all columns to return.Let us test it out by heading to the API tab again and, this time, adding two new header fields.x-hasura-role = sreX-hasura-project-id = Project_3 (can be anything of your choice \ud83d\ude09)Voila! \ud83c\udf89We get only records for Project_3. You can define complex checks, too.Context retrieval for LLM augmentationLLM trained on a public dataset wouldn\u2019t understand your real-time business data. So, we will need to fetch relevant context to pass along with our instruction prompt to the LLM. When an SRE requests to analyze a particular incident, we fetch two context datasets to pass to the LLM: Incident details and associated request detailsGeneral aggregated server metricsLet us define a relationship between the incident and request tables to fetch our first context.On the data tab, select the incident table and then choose the Relationships tab. From here, we'll create a new relationship by clicking on Add Relationship. We'll name this incident_request_relationship and choose to reference the SRE / request table. Our relationship should be of type Object and map from the request_id column in our incident table to the request_id column in the request table. Then, click Create Relationship.We test it out on the API tab.Query to fetch request information along with the incident.query IncidentDetails($IncidentId: int64!) {\n incident(where: {incident_id: {_eq: $IncidentId}}) {\n   incident_id\n   request_id\n   timestamp\n   incident_request_relationship {\n     cpu_usage\n     execution_time\n     memory_usage\n     number_of_active_requests\n     request_id\n     server_id\n     project_id\n     timestamp\n   }\n }\n}\n\nApply the below query variable:{\"incident_id\": \"2\"}\n\nQuery to fetch aggregated data for a specific time window.query MyQuery($StartTime: date_time64!, $EndTime: date_time64!, $ServerId: string!) {\n  aggregated_server_metrics(where: {timestamp: {_gt: $StartTime, _lt: $EndTime}, server_id: {_eq: $ServerId}}) {\n    avg_cpu_usage\n    avg_memory_usage\n    server_id\n    timestamp\n    total_requests\n  }\n}\n\nQuery variables:{ \"ServerId\": \"Server_9\", \"StartTime\":\"2023-08-01 00:00:00.000000000\", \"EndTime\": \"2023-08-02 00:00:00.000000000\" }\n\nNow let us put it all together. Build ChatBot APILet us create a Hasura Action using an endpoint hosted from our Hasura Notebook.You can find the code under clickhouse_sre/sre_assistant.ipynb.Let us uncover the magic. We use OpenAI chatmodel API ChatCompletion.First, we use the chat model and OpenAI\u2019s function calling capability to extract intent and entity. Intent in this case is checking if the user is asking for analysis on the incident and the entity is incident id.Finally, we setup the prompt to provide the instructions and context to LLM.Python\n\"\"\"\nYou are provided with request and server stats data for the incident.\nCan you run RCA and identify the root cause of the incident?\nRequest data:\n{request_data}\nServer stats data:\n{server_stats}\nAnalyze the data step by step and list down your analysis in bullet.\nShare next steps to resolve the incident.\n\"\"\"\n\nAnd finally, the cell is commented as POST, which converts the whole cell into a POST request handler at /sre_query Python\n# POST /sre_query\n\n...\n\nOn the Hasura Notebook home page, select clickhouse_sre_assistant/server.ipynb and press Start API to start serving the endpoints and Restart API if you would like to reload existing server settings.Create Hasura Action with Chatbot APIHead to the Hasura console and click the Actions tab. Click Create to setup a new Hasura Action.Your webhook URL is your notebook URL (ends with .app) + invoke/sre_query.You can also fetch your webhook URL by running this CLI command again hasura notebook status.Add the authorization header required to reach the endpoint from the Hasura Notebook.Generate base64 encoded value. You will need to execute this on the terminal of your choice.echo -n \"<username>:<password>\" | base64\n\nYou can fetch the username and password by executing the following command.hasura notebook status\n\nKey = Authorization\nValue = Basic <base64 encoded value here>\n\nYou will also need to increase the Action\u2019s timeout as LLM responses can be slow.Set a large timeout; 1000 seconds (16 minutes) is a reasonably safe value. You can always update the timeout to a more reasonable value based on your system SLAs.That\u2019s a wrap!Through harnessing Hasura\u2019s role-based access control via an instant GraphQL API, ClickHouse\u2019s rapid data analytics, and the intelligent automation capabilities of LLMs, we\u2019ve been able to create a helpful, knowledgeable, and low-cost SRE assistant. As technology advances and complexities grow, tools like this will continue to help us solve problems and stay ahead of operational challenges. What are you looking forward to building? Let us know in the comments below. \ud83c\udf89Hope you enjoyed building an SRE assistant alongside me!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL backends becoming more efficient and performant: Takeaways from GraphQL Conf 2023 Description: In this post, I would like to summarize some key takeaways from the conference and how exciting the future of GraphQL looks like. Author: Praveen Durairaju"}, {"role": "assistant", "content": "The GraphQL Community came together in San Francisco, between Sept 19 to 21, for the first ever GraphQL Conf hosted by GraphQL Foundation.In this post, albeit late, I would like to summarize some key takeaways from the conference and how exciting the future of GraphQL looks like.If you haven't watched any of the talks or want to do a quick check, here's the YouTube playlist of all the recordings. There were interesting workshops, talks, and discussions in sponsor booths.In a continued commitment to GraphQL, Hasura was a Diamond sponsor for the conference and we had a booth interacting with GraphQL users and enthusiasts.Hasura Booth at the GraphQL ConfSupergraph ChallengeWe ran a challenge for attendees in the conference. The challenge was to build and execute the fastest GraphQL federated query. Any framework or language. The constraint was to fetch data from two different Postgres DBs and to make a top N query across two tables.We had a couple of attempted submissions and one successful submission with the GraphQL Mesh solution. Here's the GitHub repo for reference.\u2728 The Supergraph Top-N Challenge Winner \u2728Congratulations to @Kdawgwilk! \ud83d\ude80 Kaden mastered new GraphQL libraries, built the fastest GraphQL gateway, and experimented with supergraph methods.Our kudos go out to you for your outstanding performance and innovative ideas \ud83c\udf89\u2014 Hasura (@HasuraHQ) October 4, 2023\n\nGraphQL FusionSince we are on the topic of Supergraph and Federation, it was interesting to see the community come together to launch a more open spec called GraphQL Fusion to address the distributed GraphQL gateway problems. It will be released later this year with MIT license. The idea is that no single entity owns the GraphQL spec and the GraphQL Foundation will drive the spec related updates.There are existing solutions like Apollo's federation and the Fusion was built to support composable subgraphs from Apollo. Hasura is working on an Open Domain Data specification for Data APIs, powered by GraphQL and this is an exciting time to be a developer in the GraphQL space as the new proposals make it more open, more flexible, and composable to build APIs for your business needs.There have been a few interesting recap blogposts about the conference, from the team at The Guild, the team at Inigo and so on. I would also recommend you to take a look at those to understand what else went in the conference. Let us look at some takeaways from Hasura's talks.Is GraphQL BFF necessary in a RSC World?Tanmai's keynote in GraphQL ConfCheck out Tanmai's keynote here.GraphQL was popular for client side data fetching with the ability to specify what you want in a single query. With modern architectures in frontend frameworks and \u00a0the shift to server side component rendering with that, data fetching and rendering is moving to the server. React Server Components solves client-side overfetching and underfetching that GraphQL used to solve for clients and now GraphQL can be used to solve backend optimizations like n-hops, query planning, and query optimization. Why Relay is a must for GraphQL APIsMarion Schleifer from our team talked about how Relay is critical to UI development and GraphQL without Relay leads us to reinventing the wheel on state management and API libraries again and again. We believe that GraphQL without Relay might not even be worth it.You might be interested in this post - Scaling frontend app teams using RelayThe future of GraphQL Backends: Efficient, composable, performant, and more openThere have been various announcements in the conference that suggest the backend story for GraphQL is getting interesting and more open. Let us do a quick recap of those.FederationIt was a natural path to open standards when it comes to Federation. Each framework had their own custom implementation and it was becoming harder. With more open specifications proposed during the conf like the GraphQL Fusion and everyone is collaborating to build this, GraphQL will help power a more top level abstraction like a supergraph. Hasura is building a data connector model to support composable APIs that enable much more powerful and performant federation with supergraph.Grafast: Add query planning to your schemaBenjie Gillam, Graphile maintainer, announced Grafast, an entirely new approach to GraphQL execution to improve performance, reducing operational costs. What more? You can add this to a new or existing GraphQL APIs as well.Benjie's Hot take: GraphQL's execution model is wrong for most servers.Dataloader 3.0Jens Neuse from Wundergraph talked about a new way to approach Dataloader to solve N+1 problems. The DataLoader pattern solves the N+1 problem by introducing a concurrency problem - Jens Neuse.Hasura Data Delivery Network (DDN)A lot of attendees were excited about Hasura's future with DDN. Do signup on the waitlist to get an early access preview.With around 400 attendees across three days, the conference was a great event to catchup with interesting usage of GraphQL, how large companies are adopting GraphQL and modernizing their data stack.I will just leave you with this trends graph for the graphql term. It is growing and has a new exciting direction for backend developers and there's no better time to start considering GraphQL in your data stack.GraphQL Interest over time"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Harness the full potential of Snowflake with Hasura Logical Models Description: In this blog post, we'll explore the exciting capabilities of Hasura Logical Models and Native Queries allowing users to execute SQL queries in Snowflake and seamlessly operationalize them over the Hasura schema. Author: David Meleney"}, {"role": "assistant", "content": "The world of data management and analytics has been revolutionized with the rise of Snowflake, a cloud data platform that provides almost infinite scale, concurrency, and performance. But as developers and data architects, how can we truly harness its potential and bring it closer to the frontend of our applications? Enter Hasura \u2013 the GraphQL engine that accelerates API development by connecting directly to your databases. The Hasura Snowflake connector combines the flexibility and efficiency of GraphQL with the powerful analytics capabilities of Snowflake.In this blog post, we'll explore the exciting capabilities of Hasura Logical Models and Native Queries allowing users to execute SQL queries in Snowflake and seamlessly operationalize them over the Hasura schema.This feature, when leveraged correctly, turbocharges your development cycle and introduces an unparalleled level of flexibility.How Hasura worksHasura can introspect Snowflake and instantly expose tables, views, and UDFs over a fast and secure API.1. Instant APIs from Snowflake tables and viewsAs soon as you connect Hasura to your Snowflake instance, it immediately introspects the database schema. For every table and view in Snowflake, Hasura will generate corresponding GraphQL APIs. This not only means that you can instantly query your data but also ensures that any relationships or constraints defined in Snowflake are respected and represented in the GraphQL schema.For example, if you have a users table and an orders table in Snowflake with a relationship, Hasura will allow you to make nested GraphQL queries that span across these tables, reflecting the underlying relationship.2. Leverage UDFs (user-defined functions) seamlesslySnowflake's UDFs enable custom computations and transformations, tailoring your database operations to specific business needs. Hasura can utilize these UDFs, allowing them to be invoked directly through the GraphQL API. This means that the custom logic encapsulated within a Snowflake UDF can be easily consumed by frontend applications, all with minimal setup.3. Logical Models and Native Queries (new for Snowflake!!)Hasura's Native Queries provides a powerful tool for enhancing your GraphQL API with the flexibility and control of raw SQL queries. By leveraging Native Queries, you can create custom and advanced behavior in your Hasura-generated GraphQL schema without the need for additional database objects or DDL privileges. In addition to the auto-generated API you can now unlock the full potential of SQL while building robust and efficient applications with Hasura.This is just a subset of features that can be leveraged using Hasura. We have not mentioned other features such as data federation, Schema Registry, and more. Check out or documentation for more information.Benefits1. Instant API iteration: decoupling at its bestOne of the most powerful aspects of Hasura\u2019s integration with Snowflake is the decoupling it provides. By separating the models exposed in the API from the underlying database (Snowflake in this case), Hasura enables rapid iterations on the API without making fundamental changes to the database structure.Why is this beneficial?Flexibility: As your application requirements evolve, you can easily make changes to your GraphQL API without having to refactor the underlying database or its schema.Isolation: Protects your Snowflake database by serving as a middle layer, allowing you to expose only the data and operations you want while keeping other parts of your data private and secure.Performance: Instead of manually writing complex data fetching logic that ties into Snowflake, you can rely on Hasura's optimized query execution. You write the query once, and Hasura ensures it's executed efficiently.2. Unleashing the power of Snowflake's native queriesWhile GraphQL provides a standardized way of fetching and manipulating data, sometimes you need to tap into the raw power and flexibility of Snowflake's native SQL. Hasura acknowledges this by letting you write and execute native Snowflake queries, which can then be tied to your GraphQL schema.This approach provides several advantages:Optimized queries: Leverage Snowflake-specific functionalities, syntax, and optimizations, ensuring you're getting the most out of your database.Custom logic: Not all data operations can be generalized. By executing native Snowflake queries, you can implement highly custom business logic directly.Seamless integration: Even though you're executing native Snowflake queries, they're seamlessly integrated into your GraphQL API. This means your frontend team can continue to consume the data in the GraphQL format they're familiar with.3. A unified data platform for modern appsBy integrating Snowflake and Hasura, you're essentially bridging the gap between your data powerhouse and your application frontend. Hasura serves as the conduit, translating GraphQL requests into efficient Snowflake queries and vice versa. The result? A robust, scalable, and agile data platform suitable for the demands of modern applications. Whether you're building a data-intensive analytical tool or a dynamic web application, the Snowflake-Hasura integration is a game-changer.Native queries use casesSome examples of when you would use Logical Models with Snowflake are:Query semi-structured data: Snowflake can handle semi-structured data like JSON and VARIANT data types, making it easier to work with flexible and nested data structures.User-defined functions: You can create user-defined functions (UDFs) using JavaScript, which can be integrated into your SQL queries to perform custom calculations or transformations.In-database analytics: Snowflake supports various analytic functions, allowing you to perform complex calculations, aggregations, and window functions directly within SQL queries.Snowflake AI function:Forecasting predicts future metric values from past trends in time-series data.Anomaly Detection flags metric values that differ from typical expectations.Contribution Explorer helps you find surprising dimensions and values that affect the metric.Example of leveraging semi-structured dataOne of the distinguishing features of Snowflake is its support for semi-structured data types, with VARIANT being a popular choice. The VARIANT data type allows for flexible storage of JSON, Avro, ORC, and Parquet formats, among others. This flexibility often comes with the challenge of querying and interpreting this data in traditional systems. However, with Hasura's Native Query capabilities, this becomes a seamless process.Understanding VARIANT in SnowflakeBefore diving into the querying process, it's crucial to understand the nature of VARIANT. It's a column type in Snowflake that can store values with different structures, primarily JSON data. For instance, consider a user_preferences column in a users table where each user might have a different set of preferences stored as a JSON object.Hasura's ability to execute native queries in Snowflake and tie them to a GraphQL schema is nothing short of revolutionary for developers and data architects. It paves the way for a flexible, efficient, and rapid development process, all while ensuring you're harnessing the full capabilities of your Snowflake database.As businesses and applications continue to demand more from their data infrastructure, tools that foster agility, performance, and integration will lead the way. Hasura's integration with Snowflake is a testament to this future-forward approach, and we're excited to see the innovative applications that will emerge from it.Using a Hasura Native Query you can flatten, transform, and normalize the semi-structured data of a variant in real-time removing the need to duplicate data or create a view. Using this ability API consumers and application developers can iterate faster without accessing Snowflake directly or relying on Snowflake practitioners for support.Try Yourself1. Add VARIANT data to SnowflakeLog in to your Snowflake instance.Select or create a database and a schemaCreate a table to store the dataCREATE TABLE sample_variant_table (data VARIANT);\n\nInsert data that is of type VARIANTINSERT INTO sample_variant_table (data)\nSELECT PARSE_JSON(column1)\nFROM VALUES\n('{\"name\": \"John\", \"age\": 30, \"city\": \"New York\", \"state\": \"NY\"}'),\n('{\"name\": \"Jane\", \"age\": 25, \"city\": \"Los Angeles\", \"state\": \"CA\"}'),\n('{\"name\": \"Alice\", \"age\": 28, \"city\": \"Chicago\", \"state\": \"IL\"}'),\n('{\"name\": \"Bob\", \"age\": 32, \"city\": \"Houston\", \"state\": \"TX\"}'),\n('{\"name\": \"Charlie\", \"age\": 29, \"city\": \"Phoenix\", \"state\": \"AZ\"}'),\n('{\"name\": \"David\", \"age\": 34, \"city\": \"Philadelphia\", \"state\": \"PA\"}'),\n('{\"name\": \"Eva\", \"age\": 27, \"city\": \"San Antonio\", \"state\": \"TX\"}'),\n('{\"name\": \"Frank\", \"age\": 31, \"city\": \"San Diego\", \"state\": \"CA\"}'),\n('{\"name\": \"Grace\", \"age\": 26, \"city\": \"Dallas\", \"state\": \"TX\"}'),\n('{\"name\": \"Hannah\", \"age\": 33, \"city\": \"San Jose\", \"state\": \"CA\"}')\nAS t(column1);\n\n2. Querying data from a VARIANT:You can now query the semi-structured data stored as a variant using Snowflake SQL.You can perform a simple query to fetch the data you have stored as a VARIANT:SELECT data FROM SAMPLE_VARIANT_TABLE;\n\nThis query will return all of the data as a single column.Querying addresses from a specific state, e.g., California (CA), is straightforward:SELECT data\nFROM sample_variant_table\nWHERE data:\"state\"::STRING = 'CA';\n\nThis query will return all the rows where the state key in the VARIANT column address matches the value CA.You can define the structure and data types of the data you want a query to return:SELECT \n    data:\"name\"::STRING AS \"name\",\n    data:\"state\"::STRING AS \"state\" \nFROM sample_variant_table;\n\nThis query returns the name and state of users as strings in two columns.Using common table expressions (CTEs), you can structure how you want the data to be returned as well as filter data based on the semistructured data stored as a VARIANT data type.WITH parsed_data as (\nSELECT \n    data:\"name\"::STRING AS \"name\",\n    data:\"state\"::STRING AS \"state\", \n    data:\"age\"::INT AS \"age\"\nFROM sample_variant_table)\nSELECT name, state, age FROM parsed_data\nWHERE state = 'CA';\n\nThis query will return the name, age, and state of the address of users from a specific state, e.g. California (CA).3. Add native query to HasuraConnect your Snowflake instance to Hasura by following the directions outlined here. Be sure to include the schema that contains the sample_variant_table table you created in step1.Navigate to the Native Queries section of the Data tab.Click \u201cCreate Native Query.\u201dEnter the name you want to use for the Native Query. This name will appear as a new type in your GraphQL schema.Select the database connection you want to create a Native Query for. In this case, I have named my Snowflake connection \u201csnowflake-demo.\u201dAdd a query parameter named \u2018state\u2019 as the type \u2018String\u2019 that will be used in the next step as a query parameter.Enter the following for the \u201cNative Query Statement\u2019.\u201d This example is similar to the last query covered in step 2 but we have replaced CA with {{state}} in order to use a query parameter.WITH parsed_data as (\nSELECT \n    data:\"name\"::STRING AS \"name\",\n    data:\"state\"::STRING AS \"state\", \n    data:\"age\"::INT AS \"age\"\nFROM sample_variant_table)\nSELECT \"name\", \"state\", \"age\" FROM parsed_data\n\nClick the \u201cAdd Logical Model\u201d button. The logical model will determine the types and shapes of the data returned by the Native Query we are creating. In this case, we have the fields name (string), state (string), and age (int). Give the logical model a name and select it to be used for your native query. You can use a single Logical Model for multiple Native Queries. For example, you could create a query that simply flattens the nested data name, state, and age using the same Logical Model.4. Call the native query typeNavigate to the API tab.You can use the GraphiQL UI to create a query that leverages the native query you created. If you followed the naming conventions used in this tutorial you can use the following query to return the name and age of results from California (CA).query MyQuery {\n  users_age_bystate(args: {state: \"CA\"}) {\n    name\n    age\n  }\n}\n\n\nKey TakeawaysSnowflake's emergence has transformed the landscape of data management, offering unparalleled scalability, concurrency, and performance. Hasura, with its GraphQL engine, introduces a robust integration, enabling developers to tap into Snowflake's full potential effortlessly. Through Hasura's Native Queries, not only can users execute SQL queries in Snowflake, but they can also operationalize them over the Hasura schema, ensuring that data retrieval is efficient and precise. This capability significantly enhances the development cycle, offering unprecedented flexibility. The integration between Hasura and Snowflake serves as a bridge, harmoniously connecting the backend data powerhouse with application frontends. With the introduction of native queries and logical models Hasura can address any use case possible while also significantly reducing time and effort with the API elements auto-generated from introspection of tables, views, and UDFs.\ud83d\udcda Documentation and resourcesTo help you get started, we've prepared detailed documentation, guides, and examples:Get started with Hasura\u2019s Snowflake documentationEffortless APIs for Snowflake with HasuraGet GraphQL API on Snowflake with Hasura \u2013 Sample AppLess pipelines, more shipping: Renaissance\u2019s journey to Hasura with SnowflakeInstant APIs on Snowflake using UDFs and Hasura\ud83d\ude80 Get started today!We can't wait to see the amazing applications you'll build using Hasura and Native Queries. Get started today by signing up for Hasura Cloud and connecting to one of the supported databases.If you have any questions or need assistance, feel free to reach out to our team on Discord or GitHub."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Hasura Hacktoberfest Fiesta: Build a Supergraph and win $1000! Description: The deal is simple: Participants with four pull requests (PRs) accepted between October 1-31, 2023 will receive a unique digital reward from Hacktoberfest, and if you\u2019re lucky, win $1000 with our Hasura supergraph Top-N challenge! Author: Estefani Moreno"}, {"role": "assistant", "content": "Hey there, open source enthusiasts and developers! October is here, and you know what that means: The open source community gears up for a month of collaboration and celebration. For the 6th consecutive year, Hasura proudly participates in Digital Ocean's Hacktoberfest, a month-long event that encourages developers to make meaningful contributions to open source software.About HacktoberfestIn case you're new to the party, Hacktoberfest is a cool open source event, brought to you by Digital Ocean. The deal is simple: Participants with four pull requests (PRs) accepted between October 1-31, 2023 will receive a unique digital reward from Hacktoberfest, and if you\u2019re lucky, win $1000 with our Hasura supergraph Top-N challenge!Contribute to HasuraThe supergraph Top-N challengeFor this year\u2019s Hacktoberfest, we have prepared a special challenge for you:Build and execute the fastest GraphQL gatewaySubmit a link to your git repository as a GitHub issueAny stack or approach, 1 month, 3 prizesTop prize of $1000Find more information and instructions on this GitHub repo. Reach out to us in the #contrib channel on Discord if you have any questions.Valid / invalid contributionsTo submit your supergraph, create a PR on the submissions.md file and add your name, submission date and the link to your supergraph repository. If the team accepts the contribution as valid, your PR will be merged and we\u2019ll add the hacktoberfest-accepted label.Contributing to Hasura DocsYou can also contribute to the Hasura Docs. We are thrilled to share this list of issues that were carefully selected for you. Feel free to go through this set of issues and pick up the one that you\u2019d like to help with. Submit a PR for one or more of our issues listed above which are labeled as \u201chacktoberfest\u201dYour PR will be reviewed within two weeksIf it is valid, we will either merge it or label it as \u201chacktoberfest-accepted\u201dWe\u2019ll reply with a comment to your PR asking you to fill out an order formA t-shirt and a sticker pack is sent to everyone who submits an accepted contributionBefore getting started with your docs PR, please read through the docs contributing wiki.Invalid PRsTypically, PRs that don't directly address and resolve an issue labeled as \"hacktoberfest\" are considered invalid, and we're not specifically seeking typo fixes this time around. However, if you submit a PR that we find particularly valuable, we may still consider it for Hacktoberfest by adding the \"hacktoberfest-accepted\" label. Please note that this doesn't come with any guarantees.DeadlineFor your contribution to qualify for Hacktoberfest, you must submit it on or before October 31We will be reviewing contributions until November 10GuidelinesSetting some ground rules to help maintain the quality of contributions and to ensure that Hacktoberfest remains a positive and rewarding experience for everyone involved.Code of conductOpen source thrives on the power of learning and giving back to our amazing community. We're committed to maintaining an inclusive environment where everyone can learn and grow together. We kindly ask you to take a moment to go through and uphold our Code of conduct. Any violations of these guidelines will be taken seriously and may result in disqualification from the challenge.Quality, not quantity, is our measureWe prioritize quality contributions above all else. While quantity may serve as a tempting goal, our true measure of success lies in the impactful, well-crafted contributions our community brings to the open source table.Stay in touchIn order to get the most out of your Hacktoberfest experience, all participants are encouraged to join our Discord #contrib channel is a great place to get help if you encounter any issues.Interested in Hasura updates? Join us for our Community Call: Launch Day on October 26 as we share our newest features, updates, and demos. We\u2019d love to see you there!Ready to make your mark in the open source universe? Happy hacking!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Less pipelines, more shipping: Renaissance\u2019s journey to Hasura with Snowflake Description: After attempting a few DIY options, Renaissance settled on the Hasura-Snowflake connector as their preferred solution for pulling the required data from Snowflake into the app via APIs.  Author: Hasura"}, {"role": "assistant", "content": "Renaissance is a leading K-12 educational software company. Founded in 1986, the company offers popular solutions like Freckle, Schoolzilla, and more to students and teachers in over 100 countries. Renaissance products are deployed in more than 50% of U.S. schools. Their unwavering mission: Accelerate learning for children and adults across all ability levels and backgrounds. Renaissance has standardized on the Snowflake Data Cloud for their analytics and reporting needs. Since Snowflake aggregates data from across its full suite of products, it contains powerful insights across the whole user journey. Renaissance recently decided to surface some of these rich insights from Snowflake in one of their new applications, tentatively referred to as the Teacher Journey, to view insights and recommendations for their students from across all of Renaissance's products.After attempting a few DIY options, Renaissance settled on the Hasura-Snowflake connector as their preferred solution for pulling the required data from Snowflake into the app via APIs. \u201cHasura eliminates so much of the work of building and maintaining APIs,\u201d said Jesse Matlock, one of the core Renaissance engineers on the Teacher Journey team. He estimates that without Hasura, they would need a few extra full-time engineers to build and maintain the API application. This blog describes how Matlock and his team evolved their application architecture and the steps they took along the way to simplify and improve their data pipeline.Snowflake at RenaissanceMatlock and the Teacher Journey team use Snowflake to pull together data from Renaissance products like Freckle, an app where students practice math and English Language Arts (ELA). Each Renaissance product has its own data store. \u201cRenaissance has acquired best-of-breed products, and not all of those products write to the same backend data store. \u00a0Having an analytic data warehouse allows us to have a view into student learning while allowing our product teams to continue to innovate and develop quickly,\u201d Matlock noted. The different Renaissance educational products sync their data into Snowflake daily. All this aggregated data from our full suite of apps had lots of great information and insights. Renaissance wanted to enrich the experience for their end users \u2013 teachers, students, etc. \u2013 by pulling some of those insights into their consumer apps, leading to the development of the Teacher Journey.\u201cSnowflake is very good at building reports and aggregating data on a cadence,\u201d said Matlock. \u201cIt's very cost-effective to just run it a few times a day to build reports that feed into the business, but it\u2019s not as cost-effective to have an API hit it 100,000 times a day with very small queries.\u201dTo tackle this challenge, Matlock's team initially decided on a pipeline that would funnel the required data from Snowflake to PostgreSQL. PostgreSQL would serve as the operational data store for the app. They decided to use Hasura to accelerate API development on PostgreSQL.The first data pipeline: Snowflake to PostgreSQLThe Renaissance team used a combination of lambdas, SNS topics, and SQS queues to push the required data from Snowflake into PostgreSQL. \u201cIt worked, but there were bugs with the data integrity and lots of nuances,\u201d said Matlock. Their initial data pipeline used a Snowflake feature that copies data into Amazon S3 buckets. The tables in Snowflake are huge \u2013 300 million records and gigabytes of data. Dropping the data into S3 is fast; the problems start with moving it to PostgreSQL. \u201cIt's slow because there's so much data,\u201d explained Matlock. \u201cYou\u2019re limited on RDS instance size and how many connections it can handle. We had massive instances that we would spin up when we were consuming data.\u201dLarge RDS instances are also pricey. \u201cIt costs us $6,000-7,000 dollars a month just to run them for a few hours a day,\u201d Matlock noted. \u201cThe instances had to be that big to get sync times down to an hour or two instead of eight\u201d he added. Minimizing that window is important since the new app needs the freshest data possible, as the data in Snowflake already lags a bit behind live prod data.Data mapping hiccupsMapping data from Snowflake to PostgreSQL took some effort. Data comes into Snowflake from multiple software products, and each is structured and typed differently. Snowflake handles variations gracefully, so the discrepancies were not evident until the data got to PostgreSQL. Take UUIDs, for example. \u201cOne app may use a UUID, but not be strict on how it is formatted,\u201d said Matlock. \u201cIt could be a UUID or a string or a number. In migrating that data to PostgreSQL, we would set the ID field to UUID, and halfway through syncing we realized this product uses a string. Then we would have to go into PostgreSQL and make that work by storing all UUIDs as a string.\u201dConstant pipeline issuesThe data pipeline was becoming unwieldy because it moved so much data from one system to the next and required strict schedule coordination. The team was spending a lot of time managing and debugging this pipeline. In short, running the pipeline took a long time, and it was expensive. It was also hard to maintain. \u201cWe realized it's going to cost us more money to build and maintain this pipeline than to find a cost-effective way to leverage Snowflake directly,\u201d said Matlock.Rapidly building APIs on PostgreSQL with HasuraMatlock and team were already using Hasura on PostgreSQL in this architecture, and they had experienced its benefits for accelerated GraphQL API development. Matlock first heard about Hasura through a coworker who was dabbling with it for a personal project. The coworker suggested using Hasura for a different project they were both working on and it worked really well. \u201cWe had previously handwritten a GraphQL API using Express and some Node libraries,\u201d said Matlock. That handcrafted API performed well enough, but it had been a lot of work to create. \u201cHasura is hands-off. I don't have to make a lot of changes. And I don't have to make new endpoints. Hasura does it all for you,\u201d said Matlock.When they decided to explore ways to directly connect the app to Snowflake, the then newly-launched Hasura connector for Snowflake was an obvious first choice. Hasura + Snowflake = Goodbye unwieldy pipelinesUsing the Hasura Snowflake connector means no more unwieldy pipelines; instead, Snowflake is now the operational store for the new application. \u201cWe transitioned to using the Snowflake connector in the last month. We've gotten rid of this big pipeline of copying things from Snowflake to PostgreSQL,\u201d Matlock explained. \u201cNow we just hit Snowflake directly.\u201dFrom a Hasura standpoint, moving to the Snowflake connector took no time at all. \u201cIt's super easy,\u201d said Matlock. \u201cWe pointed from PostgreSQL to Snowflake and added some config files.\u201dThe smooth migration and simpler architecture was a great start. However, the team needed to make sure two other criteria were met since the new app will be used by thousands of teachers multiple times per day. Avoid ballooning costs from frequent queries to SnowflakeKeep end-to-end latencies low for the end userThe Hasura caching feature helped with both of these requirements. When teachers log in each morning, their results come from Snowflake and are cached in Hasura. If they log in again at lunch, the app goes straight to the cached results in Hasura. \u00a0On the performance side, one of the big questions going into this transition was whether Snowflake could handle a live API call, which isn't necessarily quick most of the time. Matlock says, \u201cSnowflake's data structure isn't built to handle those sorts of queries. We heavily structured the data and partitioned it in Snowflake so that when you request it by a specific user ID, the query returns very quickly.\u201dCombined with Hasura caching and query plan optimization, the team was able to achieve latencies in the 300-350 ms range in this new architecture. Previously, a query from Hasura to PostgreSQL took 200-250 ms. The difference isn\u2019t noticeable.Less engineering, more app development with HasuraThe Snowflake connector simplified their pipeline, and using Hasura saves Matlock\u2019s team a lot of time. Making changes to Hasura is also very simple. Matlock reported they spend just a few hours each month on these changes. \u201cWe just go into the config files and tweak the permissions or change a few tables here and there,\u201d he said.After changing the configuration files, the team applies them with a CI/CD pipeline that uses Hasura's command line tool. \u201cHasura is hands-off, low code. I don't have to spend a lot of time building and testing our endpoints,\u201d said Matlock.The team uses advanced Hasura features, including authentication via JWT and row-level permissions. They\u2019ve also started experimenting with Hasura observability.Matlock estimates that time savings from Hasura are equivalent to the work of a few full-time backend software engineers. \u201cIf we didn\u2019t use Hasura, we'd have to hire an engineer or allocate one of our engineers to be in charge of building out features in the API code \u2013 things that Hasura comes with off the shelf like pagination, caching, and all these things that we would have to build ourselves. Hasura just does it for us,\u201d said Matlock.Looking to the future \u00a0The Teacher Journey team is looking at combining Snowflake and PostgreSQL as a part of their roadmap. They anticipate that the need to serve more fresh and near real-time data in this new application will grow, such as displaying results from a recent test that a classroom has taken. Frequently hitting Snowflake with smaller queries to get the latest data will become less cost-effective than having some of the less complex data cached in an alternate database like PostgreSQL. Since Hasura seamlessly integrates into multiple databases, the team can keep the architecture simple by making the data from both of the databases available under the same GraphQL endpoint.In addition to multiple databases, the team is also looking at implementing a few Remote Schemas to interact with other internal tools and services at Renaissance. This will allow the GraphQL API to serve data from more than just the databases.Ready to learn more about the Hasura-Snowflake connector? Discover all the details and get started today!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Scaling telehealth to millions: Hasura tips and tricks from Henry Meds Description: As backend engineers, they spent a lot of time figuring out how to connect routes to controllers, controllers to services, and services to the data access level. All of this work is replaced by Hasura. Author: Hasura"}, {"role": "assistant", "content": "We\u2019re bringing you blogs that summarize some of the very best talks from HasuraCon 2023. This talk, \u201cHow Henry Meds Scaled Rapidly with Hasura,\u201d was presented by Nathaniel Armer, CTO and Founder of Henry, a subscription healthcare service.Henry uses Hasura to improve developer productivity, build scalable architectures, and handle security in a fast-growing telehealth startup. The following diagram shows how Hasura fits into Henry\u2019s architecture. Hasura amplifies developer productivity. Henry solutions look simple but have a lot of moving pieces: customers, providers, pharmacies, and internal teams all have different needs. One might think that Henry needs a very large engineering team, but they actually started using Hasura with one and a half engineers, and today they have a single small team.Watch Henry\u2019s full HasuraCon 2023 presentation here, or keep reading to get the full breakdown: Your data structure is your APIWith Hasura, Henry shifted from building APIs to modeling their data. As backend engineers, they spent a lot of time figuring out how to connect routes to controllers, controllers to services, and services to the data access level. All of this work is replaced by Hasura \u2013 there\u2019s so much busy work they don't have to do anymore. Using Hasura does require a change in thinking \u2013 your data structure is now your API.Tip!Think about how your tables are structured and about your naming system. Decide up front on a consistent naming system because your names are not just internal, but the names for your public API. With PostgreSQL, Henry uses schemas, which are like virtual databases, to logically group tables. For example, a customer schema might have a customer table, a profile table, and a settings table. That schema makes it much easier to see how these tables are connected.Use built-in migrationsHasura has built-in migrations, not just for Hasura schemas, but for Henry\u2019s SQL. This is a huge time saver. Nathaniel Armer, said, \u201cI've worked at startups where we developed our own SQL migration strategies or used off-the-shelf solutions. Having SQL migrations built into Hasura gives us one less tool to manage.\u201d With Hasura, once you've figured out your data structure, you get your API for free. \u201cWhether it's filtering or joining or sorting, or even really advanced features like live queries, Hasura provides it out of the box,\u201d said Armer.Double backend engineer productivityThe startups Armer has worked with had a one-to-one ratio between frontend engineers and backend engineers. With Hasura, Henry\u2019s backend engineers are so productive that the ratio is two to one, doubling their productivity. \u201cHasura freed up one of our engineers to work on other backend focus projects,\u201d said Armer. \u201cAs a result, we're shipping features to customers faster than ever before.\u201dBenefits for frontend engineersHenry\u2019s frontend team appreciates Hasura\u2019s GraphQL API. They no longer wait for backend engineers to painstakingly model the API. They have the control they want, a fully fledged API, and automatic type generation. They still work closely with the backend team, but discussions are around data models and architecture, not details like sorting on a particular column.Focus on the architectureOne engineer at Henry describes Hasura\u2019s benefits this way: \"The change in focus allows you as a developer to focus on the architecture and less on boilerplate.\" That architectural shift has led the Henry team to explore other ways to handle business logic rather than writing controllers. They use triggers, remote schemas, and turning actions into data.Triggers for business logicUsing stored procedures or triggers leverages functionality that's built into PostgreSQL, such as setting a date when a row is created or adding validating logic to a complex mutation before letting it commit. Henry uses pg_cron to schedule some of this logic in the database.Remote schemas to expose external endpointsRemote schemas enable Henry to expose external REST or GraphQL endpoints as part of their Hasura deployment. They use remote schemas for file uploads and downloads. Armer built a microservice that returns signed Google Cloud storage links. It is federated with Hasura and accessed like any other Hasura resource. Tip!With remote schemas, you can also join external resources. For example, you could join a profile table to a Google Cloud storage resource with profile images and return signed links that let users view them. This technique keeps everything in the same API, but the files are uploaded and downloaded external to Hasura for greater efficiency.Turning actions into dataArmer explains how Henry turns actions (such as emailing) into data. \u00a0\u201cAs a backend engineer, if I'm asked to send an email, I grab my favorite email library,\u201d says Armer. \u201cBut what happens if the email doesn't go out, or the code quits halfway through? As the project gets bigger and is split into multiple services, where should email sending be \u2013 in the onboarding flow or in the main application?\u201dTo decouple data from coding workflows, Armer and his team created an email table with typical email fields and a sent-at field. A worker service polls the table for unsent emails and sends them automatically. If that service goes down, it sends emails the next time it polls. The table includes a sent-at timestamp, which makes it easier to diagnose and debug any problems.When a new customer is added, Armer turns that action into data too. A trigger writes a welcome email to the email table, and the same worker service ensures the email is sent. \u201cTurning actions into data is simple, transactional, and really elegant when you get used to it,\u201d says Armer.Hasura + Cloud Run: Serverless on the cheapWith Hasura doing so much heavy lifting, Henry avoids the monolith that startups typically build. But they still need a platform for logic that runs separately. They chose Cloud Run, which complements Hasura and combines the low cost of serverless with the ability to run Docker containers. Cloud Run supports complex multi-threaded code and scales to 100 instances.Henry architecture diagramOne of the downsides of serverless is the need for long-running instances to support services like email service workers. \u201cNormally, I would model this as a 24/7 service on a compute instance,\u201d said Armer. \u201cBut what happens if that compute instance goes down?\u201dInstead, Armer made the service a Cloud Run instance and uses Cloud Scheduler to send regular cron-style events. \u201cEvery minute or so we send an event to check for emails and send them,\u201d said Armer. \u201cThis lets us simulate a 24-hour service with the scaling and fault tolerance of serverless.\u201dIn Cloud Run, versions are free; when unused, they scale to zero and cost nothing. With this in mind, the Henry team is experimenting with per-feature branch deployments. Every feature branch has its own URL and its own deployment live in the dev environment, which simplifies testing and sharing builds.Even though Henry\u2019s architecture looks sophisticated, it is very cost-effective. They don\u2019t need dedicated DevOps staff, and their serverless cost per customer is less than pennies per month.Authentication and authorizationHealthcare involves sensitive data and is heavily regulated. At Henry, they think about security and auditing from the start of any project. Henry uses Google Cloud Identity with a custom microservice in front. The microservice generates JWTs and sends custom emails rather than using Identity's email feature. Tip!When you generate JWTs, don't forget to set useful session variables like user email or user ID. You'll be able to use those session variables in Hasura. Like any JWT implementation, your client simply passes the token to Hasura to authenticate every request.Armer appreciates Hasura\u2019s authorization features. \u201cI've really enjoyed using authorization with Hasura. It's built-in, it's powerful, and it's free,\u201d he said.Henry has implemented basic user roles in Hasura including a provider role, a customer service role, and an end-user role. They added a few feature-level roles to control specific granular functionality and use Hasura\u2019s role-level control to show a user only their own profile. Tip!Each role you set up requires some configuration, so strike a balance between control and speed.Log everythingAuthentication and authorization are only half the story; Henry also needs to log every action. They use database triggers to write audit logs on every mutation. Hasura includes useful context like session variables. However, this approach doesn't work for read queries and data sources that don't support triggers. The second approach is to use the logs that Hasura generates. These logs cover both read and write actions, but aren't accessible from the database. For that, Henry uses Datadog to fetch the logs and send them to Google Cloud storage for long-term persistence. In Henry\u2019s architecture, many services go directly to Hasura rather than the database. Having Hasura centralize all data access gives Henry a central hub for auditing."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How we validated and tested Hasura v3 with an internal hackathon Description: With our v3 release around the corner, we ran an internal two-day hackathon in an effort to validate and test our features, as well as to find potential bugs. In this post, we share how this hackathon helped us prepare for a successful user-facing launch. Author: Marion"}, {"role": "assistant", "content": "With our v3 release around the corner, we ran an internal two-day hackathon in an effort to validate and test our features, as well as to find potential bugs. In this post, we share how this hackathon helped us prepare for a successful user-facing launch.PurposeThe new Hasura version 3 \u2013 which powers the new product offering called Data Delivery Network (DDN) \u2013 comes with significant architecture changes, as well as a ton of new features. So, it was important to stress-test this new system as well as find any remaining bugs and validate the user experience. We also wanted to create awareness within the company and educate engineers on what DDN is all about. We wanted to ensure everyone is familiar with the new concepts, to help create buzz around them and help people visualize what this new product is all about.At the same time, we wanted to create time and space for innovation, for thinking outside the box, and for discussions on how we can make DDN even more impactful. All of this in an effort to get ready for our public-facing launch.The EventThe hackathon was two days long, and developers were free to work on whatever they wanted. Since the future of Hasura will be largely connector based (we offer the ability to build connectors to any data source you can imagine), we encouraged folks to look into the new connector specifiction and to even start building out connectors.In the end, we had an independent jury vote on the submitted projects and we handed out prizes for gold, silver, and bronze.OutcomeAs the clock ticked down, a total of seven projects and lots of feedback were shared. There\u2019s no doubt that our teams care deeply about our users as we witnessed folks crafting solutions that exceeded expectations and inspired delight.The three winning projects were not only recognized and celebrated but were also granted the opportunity to see their ideas brought to life! We are happy this event helped drive awareness of Hasura DDN internally, and the v3 team got a lot of useful reactions, responses, questions, and feedback.We had projects submitted around the following topics:Building connectors with TypeScriptBuilding new connectors for HasuraMulti region capabilitiesDeveloper experience enhancementsCaching enhancementsHere\u2019s a highlight of two extraordinary projects in a bit more detail.HasNeT stackHasNet stack is an opinionated project starter kit that aims to help people ship from day one. It is an acronym for \u201cHasura + Next.js + Tailwind\u201d and is inspired by the epic-stack. One of the biggest challenges in web development right now is that we have a lot of tools and frameworks to choose from and building the app gets sidelined in the process of choosing the right tools and frameworks.The stack guarantees that the core choice of \u00a0\u201cHasura\u201d + \u201cNext.js\u201d + \u201cTailwind\u201d will remain in the stack forever. The stack will also come with some default swappable choices (for example, PostgreSQL for data storage, GitHub actions for CI/CD, etc.) ready to be used out of the box and switchable with other tools if one wishes.TypeScript ConnectorOne project was a TypeScript-powered connector prototype using the Deno platform to host functions. The configuration for this connector takes the form of a TypeScript file, which implements the Hasura v3 specification functions exposed by the connector. The schema and documentation are inferred from the TypeScript source. We showed a demo at Community Call \u2013 Launch Day event, and we\u2019ll keep you posted on its development.What we learnedOverall, we can say that we were able to validate and test Hasura DDN, and identify a lot of small fixes and improvements from this hackathon. We also created awareness inside the company on the technical details of DDN, as well as the new concepts it comes with. We can recommend conducting a hackathon to validate and test a new version, and we hope it leads to as many incredible projects as ours \ud83c\udf89.Stay tuned for our v3 launch!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Deploy Data Connectors to Hasura Cloud Description: Last year, we announced the Hasura Data Connector framework, which lets you build connectors and bring your own data sources to Hasura. Today, we are now excited to announce a way to deploy these data connectors you build to Hasura Cloud. Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "Last year, we announced the Hasura Data Connector framework, which lets you build connectors and bring your own data sources to Hasura.Today, we are now excited to announce a way to deploy these data connectors you build to Hasura Cloud.Hasura Connector (Beta) is a CLI plugin that can be installed along with Hasura CLI to deploy and manage Data Connectors to the same Hasura Cloud platform that is running your GraphQL APIs. Hasura Connector deploys can take any public GitHub repo with a Dockerfile, build it and deploy it on Hasura Cloud, giving you a URL that can then be used as a Data Connector Agent.DeployingLet's take a look at how we can deploy the Hasura Connector for ClickHouse.First, install the Hasura CLI. Once you have it installed, install the Hasura Connector Plugin and the Cloud plugin:hasura plugins install connector\nhasura plugins install cloud\n\nNext, login to Hasura Cloud:hasura cloud login\n\nOnce you're logged in, you can execute new commands.Deploy ClickHouse agent to Hasura Cloud:hasura connector create --github-url https://github.com/hasura/clickhouse_gdc_v2/tree/main\n\nNote that you have to use a full branch or commit url so that Hasura can pull the right code. Check the status of the connector:hasura connector status clickhouse:r1\n\nWe can see the build logs from connector:Once connector is deployed you will see the URL when status command is executed:Now, head to your Hasura console and navigate to Data -> Data Connector Agents -> Add Agent and enter a name (say clickhouse) and the url you got from the output above. And then click Connect.Now if you head to the connect database screen, you will see ClickHouse showing up as a new data source available to connect.You can now connect to any ClickHouse database using this connector.Note that each connector deployment is immutable, if you need to deploy a new version, please create a new connector.DebuggingIn case you need to debug or troubleshoot, you can use the following command to stream logs from the connector:hasura connector logs clickhosue:r1\n\nLimitationsThe provided repository must be publicly accessible.The provided repository should follow the GDC spec.The provided repository should have a Dockerfile at the root directory which will be used to build the image.PricingThere is no cost for using the connector while it is in Beta. There will be 3 connector deployments allowed per user if you only have free tier projects and 10 if you have at least one Professional tier project. Reach out to us if you need to deploy more.Reach out to us \u2013 we'd like to hear from you how you're using Hasura Data Connectors. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Efficiently compiling GraphQL queries for MongoDB performance \u26a1 Description: How do we do it? The user sends us a GraphQL request, which we extract, enrich with relations, permissions, and predicates, then compile before sending to the MongoDB data source. Author: Martin Mark"}, {"role": "assistant", "content": "We are thrilled to announce the availability of our Hasura MongoDB 1.0 connector.This improvement shows our dedication to giving our users cutting-edge database solutions, not just simple interfaces to run queries.Since its release, the performance of our database connector has emerged as a recurrent theme in the feedback we have gotten. One of the most common questions we have heard is, \"How does it perform?\" And then another popular follow-up is, \"What is the magic going on under the hood to achieve performance with MongoDB and GraphQL?\" In this blog post, we break down and explore these important issues.How do we achieve lightning-quick performance?So how do we manage to make our Hasura MongoDB 1.0 connector so quick?It essentially comes down to a few key tactics. The main overarching tactic is database push-downs and query compiling.Simply put, we aim to transfer less data and make fewer server requests. This helps speed things up. To do this, we reduce unnecessary data fetching and processing on the server's (or clients') side and let the database handle the bulk of the work.How do we do it? The user sends us a GraphQL request, which we extract, enrich with relations, permissions, and predicates, then compile before sending to the MongoDB data source.This means we only get the exact data we need and nothing extra. The result?Faster data retrieval and a smoother experience for users.In the following examples, we\u2019ll be using a Chinook sample database, which can be found here.Optimizing simple queriesEvery query counts when it comes to improving performance, even the simplest ones. For these basic queries, we begin by compiling them with projection. This makes sure that only the necessary fields are retrieved from the database.Without Hasura, these queries frequently execute without such projection, resulting in the retrieval of extra fields that are subsequently removed at the server level.Below is an example of only retrieving the necessary fields for a query from a Collection.GraphQL Query:query getArtists {\n  Artist {\n    Name\n  }\n}\n\nCompiled Database Query:{\n  \"command\": {\n    \"aggregate\": \"Artist\",\n    \"pipeline\": [\n      { \"$match\": {} },\n      {\n        \"$replaceWith\": {\n          \"Name\": \"$Name\"\n        }\n      }\n    ],\n    \"cursor\": {},\n    \"$db\": \"chinook\"\n  }\n}\n\nHowever, it is important to keep in mind that even when data flow between the database and the server is optimized through projection, there is still a performance cost (at the database layer) associated with querying Documents that are overstuffed.This is why it's crucial to structure your Documents appropriately for performance.Ensure that Documents are right-sized, and split any non-essential embedded fields to reference Collections where it makes sense.Really great cross-collection relationshipsWhen it comes to handling MongoDB, one challenging aspect is establishing relationships across Collections, especially if we're keeping our Documents right-sized and making use of reference Collections. Outside of Hasura, these joins are commonly seen at either the server or even client-client level. This often leads to slowdowns due to excessive data fetching and subsequent matching and filtering.Our solution? We turn to lookup queries. Using these, we can create queries that efficiently push joins down to the MongoDB database. This ensures that we only retrieve the specific related fields we require, avoiding unnecessary data bloat.In the following example, we show how easy it is to take multiple cross-Collection relationships and execute the join at the database level using Hasura:GraphQL query:query getArtistsAlbumsTracks {\n  Artist {\n    Name\n    Albums {\n      Title\n      Tracks {\n        Name\n        Composer\n        Milliseconds\n      }\n    }\n  }\n}\n\n\nCompiled database query:{\n  \"command\": {\n    \"aggregate\": \"Artist\",\n    \"pipeline\": [\n      { \"$match\": {} },\n      {\n        \"$lookup\": {\n          \"from\": \"Album\",\n          \"let\": { \"v_ArtistId\": { \"$getField\": { \"$literal\": \"ArtistId\" } } },\n          \"pipeline\": [\n            {\n              \"$match\": {\n                \"$and\": [\n                  {\n                    \"$expr\": {\n                      \"$eq\": [\n                        \"$$v_ArtistId\",\n                        { \"$getField\": { \"$literal\": \"ArtistId\" } }\n                      ]\n                    }\n                  }\n                ]\n              }\n            },\n            { \"$match\": {} },\n            {\n              \"$lookup\": {\n                \"from\": \"Track\",\n                \"let\": {\n                  \"v_AlbumId\": { \"$getField\": { \"$literal\": \"AlbumId\" } }\n                },\n                \"pipeline\": [\n                  {\n                    \"$match\": {\n                      \"$and\": [\n                        {\n                          \"$expr\": {\n                            \"$eq\": [\n                              \"$$v_AlbumId\",\n                              { \"$getField\": { \"$literal\": \"AlbumId\" } }\n                            ]\n                          }\n                        }\n                      ]\n                    }\n                  },\n                  { \"$match\": {} },\n                  {\n                    \"$replaceWith\": {\n                      \"Name\": \"$Name\",\n                      \"Composer\": \"$Composer\",\n                      \"Milliseconds\": \"$Milliseconds\"\n                    }\n                  }\n                ],\n                \"as\": \"Tracks\"\n              }\n            },\n            {\n              \"$replaceWith\": {\n                \"Title\": \"$Title\",\n                \"Tracks\": { \"rows\": { \"$getField\": { \"$literal\": \"Tracks\" } } }\n              }\n            }\n          ],\n          \"as\": \"Albums\"\n        }\n      },\n      {\n        \"$replaceWith\": {\n          \"Albums\": { \"rows\": { \"$getField\": { \"$literal\": \"Albums\" } } },\n          \"Name\": \"$Name\"\n        }\n      }\n    ],\n    \"cursor\": {},\n    \"$db\": \"chinook\"\n  }\n}\n\n\nAccelerated access control and permissionsAfter discussing how we optimize simple queries and relationships directly at the database level, you may be wondering how we incorporate access controls and permissions into this efficient framework. After all, it\u2019s a critical aspect. The good news is that our permission engine and query compilation system are tightly integrated. This design makes sure that the process is quick and seamless, even when handling complex queries with layered permissions.As a result, we provide a solution that abstracts away complexities, ensuring both robust security and lightning-fast performance.In the following example, we\u2019ll be taking the cross-Collection relationship we executed previously while also enforcing access control permission for only seeing records that are assigned to you:Permission rule (on Artist Collection, for role = `user`):{\"ArtistId\":{\"_eq\":\"X-Hasura-User-Id\"}}\n\nGraphQL query (role = `user`, user-id = `1`):query getArtistsAlbumsTracksWithPermission {\n  Artist {\n    Name\n    Albums {\n      Title\n      Tracks {\n        Name\n        Composer\n        Milliseconds\n      }\n    }\n  }\n}\n\n\nCompiled database query:{\n  \"command\": {\n    \"aggregate\": \"Artist\",\n    \"pipeline\": [\n      { \"$match\": { \"ArtistId\": { \"$eq\": 1 } } },\n      {\n        \"$lookup\": {\n          \"from\": \"Album\",\n          \"let\": { \"v_ArtistId\": { \"$getField\": { \"$literal\": \"ArtistId\" } } },\n          \"pipeline\": [\n            {\n              \"$match\": {\n                \"$and\": [\n                  {\n                    \"$expr\": {\n                      \"$eq\": [\n                        \"$$v_ArtistId\",\n                        { \"$getField\": { \"$literal\": \"ArtistId\" } }\n                      ]\n                    }\n                  }\n                ]\n              }\n            },\n            { \"$match\": {} },\n            {\n              \"$lookup\": {\n                \"from\": \"Track\",\n                \"let\": {\n                  \"v_AlbumId\": { \"$getField\": { \"$literal\": \"AlbumId\" } }\n                },\n                \"pipeline\": [\n                  {\n                    \"$match\": {\n                      \"$and\": [\n                        {\n                          \"$expr\": {\n                            \"$eq\": [\n                              \"$$v_AlbumId\",\n                              { \"$getField\": { \"$literal\": \"AlbumId\" } }\n                            ]\n                          }\n                        }\n                      ]\n\n\nFlexibly crafting and composing GraphQL queriesWith all of the talk about optimizing cross-Collection joins and our intricate permissions layer, let us take a look at another key feature: the art of composing GraphQL queries. We understand that developers frequently require more complex query constructs, particularly when adding arguments and specific predicates, such as \"where\" clauses.Here is where a new difficulty arises.How do we seamlessly incorporate our auto-generated query modeling into this mix? Our generated query model, like joins and permissions, is seamlessly integrated into our query compiling mechanism. This means that whether you are writing simple queries or complex queries with multiple arguments, the underlying MongoDB queries will always be efficient and optimized.In the following example, we\u2019ll take the complex relationship query above, including its permissions, and then add in some arbitrary where arguments to additionally filter the records that are returned.Permission rule (on Artist Collection, for role = `user`):{\"ArtistId\":{\"_eq\":\"X-Hasura-User-Id\"}}\n\nGraphQL query (role = `user`, user-id = `1`):query getSpecificAlbumPaginatedTracks {\n  Artist {\n    Name\n    Albums(where: {Title: {_eq: \"Let There Be Rock\"}}) {\n      Title\n      Tracks(limit: 5, offset: 5, order_by: {TrackId: desc}) {\n        Name\n        Composer\n        Milliseconds\n      }\n    }\n  }\n}\n\n\nCompiled database query:{\n  \"command\": {\n    \"aggregate\": \"Artist\",\n    \"pipeline\": [\n      { \"$match\": { \"ArtistId\": { \"$eq\": 1 } } },\n      {\n        \"$lookup\": {\n          \"from\": \"Album\",\n          \"let\": { \"v_ArtistId\": { \"$getField\": { \"$literal\": \"ArtistId\" } } },\n          \"pipeline\": [\n            {\n              \"$match\": {\n                \"$and\": [\n                  {\n                    \"$expr\": {\n                      \"$eq\": [\n                        \"$$v_ArtistId\",\n                        { \"$getField\": { \"$literal\": \"ArtistId\" } }\n                      ]\n                    }\n                  }\n                ]\n              }\n            },\n            { \"$match\": { \"Title\": { \"$eq\": \"Let There Be Rock\" } } },\n            {\n              \"$lookup\": {\n                \"from\": \"Track\",\n                \"let\": {\n                  \"v_AlbumId\": { \"$getField\": { \"$literal\": \"AlbumId\" } }\n                },\n                \"pipeline\": [\n                  {\n                    \"$match\": {\n                      \"$and\": [\n                        {\n                          \"$expr\": {\n                            \"$eq\": [\n                              \"$$v_AlbumId\",\n                              { \"$getField\": { \"$literal\": \"AlbumId\" } }\n                            ]\n                          }\n                        }\n                      ]\n                    }\n                  },\n                  { \"$match\": {} },\n                  { \"$sort\": { \"TrackId\": 1 } },\n                  { \"$skip\": 5 },\n                  { \"$limit\": 5 },\n                  {\n                    \"$replaceWith\": {\n                      \"Milliseconds\": \"$Milliseconds\",\n                      \"Name\": \"$Name\",\n                      \"Composer\": \"$Composer\"\n                    }\n                  }\n                ],\n                \"as\": \"Tracks\"\n              }\n            },\n            {\n              \"$replaceWith\": {\n                \"Tracks\": { \"rows\": { \"$getField\": { \"$literal\": \"Tracks\" } } },\n                \"Title\": \"$Title\"\n              }\n            }\n          ],\n          \"as\": \"Albums\"\n        }\n      },\n      {\n        \"$replaceWith\": {\n          \"Name\": \"$Name\",\n          \"Albums\": { \"rows\": { \"$getField\": { \"$literal\": \"Albums\" } } }\n        }\n      }\n    ],\n    \"cursor\": {},\n    \"$db\": \"chinook\"\n  }\n}\n\n\nPushing down as much as we canWhile we\u2019re able to compile and push down the vast majority of queries, like:ProjectionPermissionsRelationships (joins)Arguments and filteringThere are a couple of areas where we still depend on the server to implement logic in-memory, such as cross-database joins and multiple unrelated (batch) queries.Cross-database joinsWhile we have made significant progress in optimizing queries within the same database, cross-database joins present a unique challenge.A server-side mechanism is required to perform these joins. However, even with this server-side intervention, we remain dedicated to efficiency. According to our philosophy of lean and optimized data handling, the queries aimed at the related data sources are carefully compiled at each of their respective sources to ensure minimal data retrieval.Multiple unrelated (batch) queriesOne point worth mentioning is our approach to unrelated queries in PostgreSQL. Currently, these unrelated queries are also routed directly to the database. Once executed, the results are transformed into JSON format within the database before being sent back to the Hasura server.This methodology ensures streamlined processing and minimizes unnecessary back-and-forths.This method is not currently available in MongoDB, but we\u2019re continually exploring ways to refine our approach. Specifically, batching these kinds of queries is on our radar for potential enhancements in the future.The future for MongoDB performance and HasuraOur journey to optimize and improve Hasura's performance is ongoing, and we are looking forward to what comes next. We are hard at work on benchmarks, and you can expect a detailed report on our findings in an upcoming blog post. We have compiled a selection of articles below for those of you interested in delving deeper into the complexities of Hasura's performance and getting a more comprehensive view on our perspectives on query compiling. We invite you to learn more about the work we do and our commitment to delivering exceptional results.Building a GraphQL to SQL compiler on Postgres, MS SQL, and MySQLArchitecture of a high-performance GraphQL to SQL engine"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Highlights of August Community Call: Launch Day Description: In our Community Call: Launch Day event on August 31, we launched some exciting new features. In this blog, we\u2019ll share details about these features, as well as the demo recordings and some useful content to get started with them. Author: Marion"}, {"role": "assistant", "content": "In our Community Call: Launch Day event on August 31, we launched some exciting new features. In this blog, we\u2019ll share details about these features, as well as the demo recordings and some useful content to get started with them.MongoDB supportOur MongoDB connector hit its 1.0 milestone. \ud83c\udf89 You can now create and update your GraphQL schema from NoSQL Documents, join efficiently between collections, and enforce access control permissions on your data.Watch the demoResourcesRead the launch postRead the documentationGenAI applications with Hasura NotebookHasura Notebook and Hasura Connectors allow you to quickly bootstrap your AI applications by connecting your favorite vector databases, using Hasura Event Triggers to auto-vectorize data, and giving controlled data access to LLMs.Also, check out our new AI page and stay up to date on what Hasura is up to on the AI front!Watch the demoResourcesRead the launch postRead the documentationCLI plugin for GraphQL federation CI/CDThis new CLI plugin allows you to verify metadata consistency of your subgraph against your supergraph before \u201cpublishing\u201d changes to your supergraph. This allows you to handle multiple remote schemas and data federation with confidence!Watch the demoResourcesRead the documentationRemote table authorizationIn polyglot data environments where identity/authz-related information is in one database and domain data is in another database, combining data from both databases to provide authorized data can be challenging. You can now achieve this easily by creating remote relationships and using them in permissions in Hasura.Watch the demoResourcesRead the documentationReleases in AugustWe had three major releases in August:v2.31.0: Newly-improved suggested relationships UIv2.32.0: GA for SQL Server stored procedures supportv2.33.0: ClickHouse support (Beta)Stay tunedJoin the next Community Call Launch Day later this month on September 28 to get a first glimpse of new Hasura features on the very day they get released!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Complexity tradeoffs with GraphQL and BFFs: What MVC can teach us [infographic] Description: Where does the Backend for Frontend (BFF) and related tools fit in your stack? Understand how teams can scale, and what the separation of concerns should be, by tracing their evolution step by step, through MVC, GraphQL, to BFFs and React Server Components. Author: Akshaya Acharya"}, {"role": "assistant", "content": "Upcoming technologies like React Server Components has led to a resurgence of interest in backend architectures. What exactly belongs in this layer, and how does it fit in the stack?Enterprises that have for ages used data-bus like patterns now see that similar patterns can ease supporting multiple frontend applications or Backend for Frontends (BFF) by building out a Backend for Data (BFD), or capability based API.GraphQL makes an appearance as well, being ideally suited for data integration tasks. Here's a look at how a backend stack can evolve from a simple monolith, through microservices, and then to GraphQL based data layers.Every good architect knows that complexity is the enemy. To an outsider, building decoupled systems can look counterintuitive, yet these architectures actually help scale complexity and teams.Keeping in mind the tradeoffs that each pattern or technology exposes is a good way to stay grounded, and introduce patterns in a manner that yields maximum impact.We've seen the how backend stacks can evolve. The next post will touch on the frontend, and the pressures that lead to different decisions there.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Scaling frontend app teams using Relay Description: Learn how to use modern technologies and ideas like Relay and Backend for Frontend, to scale out frontend applications and teams. Author: Akshaya Acharya"}, {"role": "assistant", "content": "The UI is decomposed into multiple React components.The basic idea behind scaling the frontend, much like scaling any other part of the stack, is factoring it into multiple components:Can be owned by multiple independent specialised teamsReduce coupling between componentsHave clearly specified interfaces between componentsLet's look at how this strategy plays out, and how to use modern technologies and ideas like Relay and backend-for-frontend (BFF) to scale out frontend applications and teams.Independent isolated teams own different componentsThis is a useful baseline from which to iterate. It represents the naive scenario in which teams independently develop different components in a larger app but in isolation from each other.Independently developed components with independent data fetching. The circles are different teams.Problem: Independent data fetchingPoor performance and UX jankA component may make multiple network requestsComponent data request may depend on ancestor data, leading to waterfall style requestsComponents may fetch redundant dataProblem: Independent state managementData and UI inconsistencyBatching of network requestsTeams can manually coordinate data fetching:Batch all data requests at the root levelDistribute data through the component tree via propsTeams manually batch data requests through a shared root level query.Problem: Coupling at root queryAdding a query: can duplicate other similar queries, and fetch redundant dataRemoving or editing a query:\tCan break another component that (implicitly) depends on the same data\tNot removing unused data leads to over-fetch and cruftProblem: Poor developer ergonomicsThe data requirements for a component are no longer colocated with the component itself, which breaks encapsulationTRPC / React Query is not a complete solutionBatches parallel network queries into single network requestsCannot batch all queries needed for a pageCannot solve waterfall requestsQueries are batched over the network layer, but still execute against the data layer as independent queries i.e. batching cannot leverage the internal structure or relations of queriesCoordinate data access through a centralized cache storeTeams manually coordinate shared state through a central store.Introduces coupling between teams, with similar issues as with batching queriesLots of boilerplate to normalize data, update stores, and plumb data to componentsBackend for frontendBFF is useful for making lighter and more performant client applications by moving compute and data transformations to the server.Backend for frontend: Moves compute and data transformations to the server. Collectively owned by all frontend teams.Owned by the client app teamDoesn't solve any of the coupling problems, just moves it aroundGraphQLBatched queries organized around client pages instead of server functionality couple the frontend and the backend teams.Backend developers build APIs organized around Frontend pages/routes.Instead, a GraphQL API exposes all features of the backend at a single endpoint.The client app can craft queries that fetch exactly the data needed in one shotBackend GraphQL API organized around server capabilities allows for flexible frontend queries.Even better, we begin to see that the query structure beings to mirror the component tree!This means we can refactor a root level query into fragmentsQueries decompose into fragments.Putting it all together with RelayIn Relay, every component defines its own data requirementsData dependencies colocated with components. Fragment structure mirrors the component tree.Significantly, a component can only access the data it has explicitly requested. This is \"data masking\" and is enforced through the useFragment hook.This means that teams can modify individual components with confidence knowing that nothing will break as there are no implicit data dependenciesIndependently declared data dependencies are compiled into a single root level query by the Relay compiler.At build time, the Relay compiler builds an optimized set of top level queries from all the fragmentsThe compiler can check for common errors, and run optimizations such as deduplication across the whole codebaseYou get the developer ergonomics of independently developed components, but with the efficiency of globally optimized and batched data fetchingRelay can leverage the rich information present in the GraphQL schema. Incrementally adopt the global node id and connection spec.Relay uses the rich type information in the GraphQL schema, and automatically builds a local cache of data from all queries.Further, by incrementally adopting features such as the node global id spec and the connection spec, you get advanced features such as:Reloading only a portion of a query via fragmentsCursor based paginationConclusionThe JavaScript ecosystem has produced a variety of solutions for frontend development, data fetching, and state management; but solutions often fall short for ambitious projects.GraphQL, Relay, and React were built to work together and have the huge benefit of being driven by Meta's (Facebook) experience building and maintaining extremely large and and complex applications developed by many teams.if you\u2019re not composing GraphQL fragments from multiple components into one query (as Relay does), i think you\u2019re missing 80% of the point of GraphQL.which is ok but isn\u2019t talked about enough https://t.co/ooohCDV6ZO\u2014 danabramov.bsky.social (@dan_abramov) March 12, 2023 \nIt seems that GraphQL and React has taken over the world, but people are often put off by the new conventions espoused by the Relay library.The good news is that Relay can be incrementally adopted.The client library will work with any existing GraphQL APIAdopting bits of the Relay spec unlocks additional featuresRelay can be adopted selectively by some components and not others, for an easy migration pathEven better, it seems that the people who've implemented Relay, have been happy with it for quite a while.How Coinbase is scaling their app with Relay\"Relay is unique among GraphQL client libraries in how it allows an application to scale to more contributors while remaining malleable and performant.\" https://t.co/D45yEid8l0\u2014 Relay (@RelayFramework) May 6, 2022 \nJoin us for a live Q&A session with Tanmai Gopal on the @GraphQL Discord. \ud83c\udf99\ufe0f Discover how to scale UI development with Relay #GraphQL.\u23f0 July 12, 11AM PT Join GraphQl Discord server \u27a1\ufe0f https://t.co/MM3HN7DpbW pic.twitter.com/hhKYYcnbKc\u2014 Hasura (@HasuraHQ) July 7, 2023 \nIt still surprises me how Twitter embraces GraphQL / Relay (for RWeb) but keeping their Timeline model instead of adapting the Relay connection spec https://t.co/skOcPvrmMv\u2014 Jane Manchun Wong (@wongmjane) November 19, 2022 \n\ud83d\udca1 #RelayJs feature I appreciate - Relay is optimized for performance by default. It \u201cforces\u201d you to break down the data requirements of your UI in small, reusable parts, like React does with components. It'll then subscribe to changes of only the data each component asks for 1/x\u2014 Gabriel Nordeborn (@___zth___) April 3, 2022 \nUse Hasura to easily generate a Relay compatible API with no code, across multiple data stores, with cross data store joins, filtering, and aggregations, and declaratively defined permissions.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura Notebook: Prototype fast on your GenAI apps [updated] Description: As part of Hasura\u2019s evolution, today we\u2019re introducing Hasura Notebooks, a tool designed to facilitate the swift prototyping of cutting-edge GenAI applications. Author: Simrat Hanspal"}, {"role": "assistant", "content": "Hasura empowers you to rapidly create high-quality data APIs for production purposes. As data sources continue to evolve, Hasura evolves as well, providing you with the capability to construct secure data APIs over versatile data stores, including vectorized data.If you are new to Hasura, we recommend getting started for free with Hasura Cloud.As part of Hasura\u2019s evolution, today we\u2019re introducing Hasura Notebooks, a tool designed to facilitate the swift prototyping of cutting-edge GenAI applications.In this blog, we\u2019ll take you through this new tool to gain a comprehensive understanding of how it works and why you need it.\ud83d\ude42GenAI revolutionLarge Language Models (LLMs) are very large, deep neural networks trained on vast amounts of text data mostly scraped from the internet. LLMs have taken the world by storm because these generalized language models are so good at understanding context, retrieving information, and generating content that can help us with numerous applications. And why not? Automation is the key to efficient applications.At its core, LLMs learn the word probabilities to predict the next most suitable word given in a sentence. If the sentence is from a domain which the LLM hasn\u2019t been exposed in the training period, then the LLM will make low-confidence predictions, which are known as hallucinations. We can deal with this problem by providing LLM context in prompts and instructing it to use the context to complete the task. This process is called grounding.In order to maximize the potential of LLMs across a wide range of applications, it's essential to expand the scope of secure data sources. Given the fast-paced environment, it's equally important to avoid dedicating excessive time to constructing data APIs (a crucial yet somewhat mundane task \ud83e\udd37\u200d\u2640\ufe0f) when you could be focused on creating exciting and lucrative applications.Hasura to the rescue!Let\u2019s now go through how you can quickly build secure data APIs and prototypes with a product search demo.What is Hasura NotebookHasura Notebook is a remote Jupyter Notebook with ready-to-run examples and Jupyter Kernel Gateway baked-in to turn your code cells into endpoints. Hasura Notebooks are great if you want to quickly prototype GenAI applications with Hasura or learn GenAI from our existing templated projects.We are going to be building with Hasura Notebook in this post. If you haven\u2019t checked it out, we recommend looking over this post before proceeding: Hasura Jupyter Python Notebook & API Server documentation.Use case: Get a contextual product search powered by OpenAI in under 10 minutes \ud83d\ude80As you might have experienced yourself, searches on most e-commerce websites are keyword-oriented. This results in a lot of false positives or irrelevant results and can lead to dissatisfied users. Contextual search is an answer to this problem. Contextual search matches the user query with the product description.Prerequisites1. Hasura CLIIf this is your first time using Hasura CLI, you will need to install it.Follow the installation instructions on this doc. 2. Hasura NotebookFollow the steps in the documentation below to create your Hasura Notebook. You will know you are ready when you are able to access the Hasura Notebook with a landing page that looks like this on your Hasura Notebook URL. Let\u2019s get building! \ud83d\ude80Set up your PostgreSQL databaseStep 1: Start a new Neon Cloud PostgreSQL DB or link your existing DB.More details on Neon Cloud PostgreSQL DB here. Step 2: Create a new table called `base_products` and track it from the data tab.Table name - base_productsColumnsid - integer (auto-increment)name - TextDescription - TextTracking enables the table to be accessible through GraphQL query.Set up your vector databaseStep 1: Create a free 14-day cluster on Weaviate.Head to https://console.weaviate.cloud/ and register for an account. After confirming via email, click + Create cluster and fill in a name before clicking Create. Once Weaviate has provisioned your sandbox cluster, proceed to the next step.Step 2: Create the Product schema called Product_vectors.Product schema ID, Product Name, and Product Description.To execute this, run smart_product_search/setup_weaviate.ipynb Step 3: Add the Weaviate table to Hasura.Currently, the Weaviate connector is not natively available in Hasura. You can add it to Hasura in two simple steps using the Hasura CLI: Deploy the Weaviate connector using Hasura CLI.# create connector using our weaviate repo\nhasura connector create my_weaviate_connector:v1 --github-repo-url https://github.com/hasura/weaviate_gdc/tree/main/\n\n# check deployment status to get the endpoint\nhasura connector status my_weaviate_connector:v1\n# you can also use list command\nhasura connector list\n\n# view logs at any point of time\nhasura connector logs my_weaviate_connector:v1\n\n# for more commands explore help section\n\n2. Add the Weaviate connector to Hasura. Refer to this document for details https://hasura.io/docs/latest/databases/vector-databases/weaviate/Step 4: Track the table.Add a remote relationship between PostgreSQL and Weaviate tablesGo to the Product_vectors table and add a relationship with the PostgreSQL database as shown below.Set up an Event Trigger on a PostgreSQL tableStep 1: Fetch your Hasura GraphQL API endpoint and Admin secret.Move over to cloud.hasura.io and click Projects. Click on the Settings icon from your project to access Project details.Step 2: Configure event handler API.Open smart_product_search/server.ipynb. Head to the section Template for event trigger to ETL data, and you will see there are multiple functions with cells commented with # POST /handle_event. These are the functions that are available on the endpoint /handle_event.Update the details in the cell.On the Hasura Notebook home page select smart_product_search/server.ipynb and press Start API.Step 3: Add the event trigger in Hasura.Head back to the Hasura Console and click on the Event tab to create a new Event. Fill in the required details.Trigger Name - vectorize_productsDatabase - Postgres_dbSchema - Public - base_productsTrigger operations - Insert and updateYour webhook URL is your notebook URL (ends with .app) + invoke/handle_event. You can also fetch your webhook URL by running this CLI command again hasura notebook status.Add the Authorization header required to reach the endpoint from the Hasura notebook.You\u2019ll need to generate a base64-encoded value. You will need to execute this on the terminal of your choice.echo -n \"<username>:<password>\" | base64\n\nYou can fetch the username and password by executing the following command:hasura notebook status\n\nKey = Authorization\nValue = Basic <base64-encoded value here>\n\nInsert data into PostgreSQL and watch Vector DB update automaticallyNow that we have integrated Weaviate with Hasura, we can use a mutation in Hasura to insert data into our PostgreSQL table and auto-vectorize and update our vector DB! You can do this by executing smart_product_search/insert_data.ipynb. Voila! We have vectors with product name and description fields. Secure semantic search using HasuraFor more on building secure APIs with Hasura, refer to this overview: https://hasura.io/docs/latest/auth/overview/Let us now go one step further with a complex query powered by our LLM.LLM-powered product searchStep 1: Update Hasura GraphQL and OpenAI details in the notebook under Template for event trigger to ETL data in smart_product_search/insert_data.ipynb. After updating these values, it\u2019s important to restart your notebook. You can do this from the gateway generated earlier using the CLI.Step 2: Create an LLM prompt.Feel free to tweak the prompt and play with it.Step 3: Create Hasura Action. Action Description - query_productAction Definitiontype Query {\n\t\"\"\"\n\tquert_product\n\t\"\"\"\n\tqueryProduct(user_query: String!): String\n}\n\nLike before, you will need to add authorization headers.Step 4: You just created a secure API on your LLM query. Integrate the API with your app, or just play around on the Hasura Console. You now have the power of the Hasura by your side.ConclusionBy seamlessly deploying a Jupyter Notebook through the Hasura CLI, you've unlocked a world of possibilities. The ability to automatically vectorize relational data has not only accelerated your data manipulation processes but has also paved the way for more efficient and effective analyses. Moreover, the integration of robust security measures into your LLM queries ensures that your data remains protected at all times. This remarkable journey, powered by Hasura, empowers you to harness the true potential of your data, all while simplifying the intricate processes involved! With Hasura by your side, what will you build?Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura + NoSQL = \ud83d\ude80 Announcing GA for MongoDB database support Description: Hasura is happy to announce the general availability of our MongoDB database connector. Author: Martin Mark"}, {"role": "assistant", "content": "There have likely been times when you\u2019ve wanted access to a tool that unlocks data in dynamic, declarative ways, but were stuck using a relational database. Or maybe you\u2019ve wished you could more fluidly use NoSQL instead of tirelessly managing migrations. We\u2019ve got the solution for you!Hasura is happy to announce the general availability of our MongoDB database connector.Over the past couple of months, during our Beta offering, we\u2019ve focused on creating the most performant read experience with MongoDB databases using Hasura\u2019s underlying GraphQL technology. With the Hasura MongoDB database connector, you can:Instantly create a GraphQL API on MongoDBEfficiently query between your MongoDB collectionsJoin a MongoDB connection to another data source\u2026 and more!We've listened to your pain points and built a solution that simplifies the complex task of managing and querying your data while offering a seamless and efficient way to author your GraphQL schema over NoSQL (with MongoDB, specifically).\u201cWe are looking to have GraphQL as our foundational approach for whatever data we serve to our applications, and Hasura\u2019s gonna be the facilitator for piecing that together for us and not having to write our own resolvers. Our longer term vision has changed where we don\u2019t want a data source that's going to tie us to a cloud provider, and using MongoDB with Hasura will help us be more cloud agnostic.\u201d- Fortune 500 healthcare services provider on migrating to MongoDB and modernizing their infrastructure with HasuraCheck out this video playlist on how to get started with setting up your MongoDB data with Hasura.Generating your GraphQL schema for MongoDBWe\u2019ve worked hard to enhance your experience authoring and generating a GraphQL schema on MongoDB using Hasura. Our goal is to help you normalize your unstructured NoSQL workloads using GraphQL, enabling a more structured strategy for interacting with your data.Some of the improvements include:Logical models are now available for MongoDB, allowing you to write and customize your schema for your MongoDB collections (and views).This includes a new interface for importing schemas for collections from a single document sample, with the flexibility to share models and permissions across collections and even parts of collections (read more in our docs for more on how to author your own logical models).Validation schemas remain accessible for inferring your schema directly from your database, adding an extra layer of convenience and efficiency (read more in our docs for more information on validation schemas in your MongoDB database).The added ability to efficiently author in-database relationships between collections in your MongoDB database.Enhancements to our query capabilities by introducing where clauses and permissions on embedded (or nested) documents, providing you with more control and precision in handling your data permission structures.An additional enhancement for adding MongoDB views to your API, which allows for custom views of your data using aggregation pipelines.Enhanced functionality for creating relationships between remote databases using GraphQL in Hasura, including a visual builder and the ability to specify your MongoDB collections as targets for remote relationships.Blazing-fast queriesOur goal is to give you the tools to effortlessly work with large data sets and complex structures in MongoDB and other data sources. Our core query planning strategy revolves around sending the least amount of data while making the fewest requests to ensure optimal performance.Some of the performance-enhancing strategies we've implemented include:Field-level projection for document retrieval, even for embedded documents using runtime-generated `$where` queries.Permissions are pushed down to the data source, which also includes embedded documents.Within the database, cross-collection relationships are available through `$lookup` queries.Lateral permissions across those relationships are made possible with query-level pushdowns.We\u2019ve also enabled cross-database joins between MongoDB and other data sources, with permissions handled at their respective data sources and the join itself taking place in-memory within Hasura.These methodologies ensure a highly responsive and efficient querying experience, allowing you to manage your data with ease.NoSQL, no restrictionsOne of our core goals is that we want Hasura to be flexible enough to help you author your APIs and stay out of your way when you need it to.A few ways our system stands out from our competitors include:Allowing connections and relationships between MongoDB and other types of databases, something not commonly found with other providers.Not placing unnecessary limits on your work with any fixed-system restrictions on depth limits or resolvers.Using complex arrangements of data with our support for embedded array objects, adding more flexibility to your projects.Offering the ability to host your API on your own premises, giving you more control over your hosting while also providing our own cloud-hosted version if you prefer (you can read more on setting up your Cloud Hasura instance with MongoDB).Find out more about getting started with Hasura and MongoDB over here in our documentation.The same Hasura you know and loveWhile providing all of these enhancements to MongoDB, we also have all the features you\u2019ve come to know and love when authoring your APIs with Hasura, such as:Secure authentication for providing user access to your APIAuthorization and access control to keep your records safe and secureBlazing-fast query compilation and executionQuery caching for even more performance on top of efficient query compilingData federation across different data sourcesAn integration framework for external business logicSecurity for protecting against API scraping, malicious attacks, and excessive data exposureWhat\u2019s next?We\u2019re just getting started.Over the following months, we\u2019ll be working hard to support writing and updating documents in MongoDB and bring the same benefits we provide to the schema generation and read-layer of the database.Keep an eye out for an upcoming Beta program for mutation support for MongoDB."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Exploring an instant secure API layer on DuckDB Description: We explore use cases where a Data API over DuckDB with Authorization and Federation makes sense in an OLAP environment. Author: Praveen Durairaju"}, {"role": "assistant", "content": "As data enthusiasts, we keep a close eye on new developments in the database world. DuckDB has caught our attention due to its growing popularity for analytics use cases. In this post, we make an attempt to understand DuckDB, the use cases it is built for, and the advantages it has over existing solutions. We also explore use cases where we could achieve much more with DuckDB when we plug it into a Data API Platform like Hasura.Let\u2019s quickly go over the what and why of DuckDB before delving into the API explorations.What is DuckDBDuckDB is a free, open source, embedded database management system optimized for data analytics and online analytical processing (OLAP). It supports standard SQL to run queries, aggregations, joins, and other SQL functions on the data. The data can be imported from JSON, Parquet, and CSV files.DuckDB has a number of extensions available for use. The extensions allow reading data from database systems like PostgreSQL and SQLite. Like SQLite, it's a simple, file-based database so there's no separate server installation required. You just include the library in your application. DuckDB specifically caters to users from the analytical processing niche, where a full-fledged database would be overkill.Why DuckDBDuckDB is a columnar database. The data is organized by field, keeping all the data associated with a field next to each other. This structure helps since most OLAP queries typically access only a subset of the column but a large number of rows for those columns.It utilizes vectorized data processing. Large batches of values are processed in a single operation. Vectorized execution leads to better performance in analytical calculations, which are often performed over all values in a particular column.It uses multi-version concurrency control to become ACID compliant. This allows for modifying tables in parallel to running analytical queries.Quote from the White paper by DuckDB CTO\u201cWhile DuckDB is first in a new class of data management systems, none of DuckDB\u2019s components is revolutionary in its own regard. Instead, we combined methods and algorithms from the state of the art that were best suited for our use cases.\u201dSQLite for OLAP: Popular use casesWe can immediately relate to the statement that mentioned DuckDB as \u201cSQLite for OLAP,\u201d catering to analytical use cases. It is gaining popularity among data scientists and analysts. As we dig deeper to find what\u2019s making DuckDB popular over other solutions, it has a few quick wins from developer experience and ease of setup and usage.Some of the quick wins:You can now do local analysis on large datasets. This is preferred over setting up libraries with other solutions due to SQL familiarity and query engine optimizations.Analytics in application servers to serve dashboards, by virtue of it being an embedded OLAP system.With DuckDB being portable, you can work with up to a few GBs (like 100GB range) of data locally, mash up data from different sources and run SQL queries for last mile analytics.Familiar SQL syntax and a potential data API* (We are exploring a data API with Hasura).Cost savings by running locally. Running OLAP queries on cloud warehouses are relatively expensive in the long run.Data imports and mashup from different sources like CSV, Parquet, and PostgreSQLDuckDB has the ability to mash up data from different sources like CSV / Parquet along with support for dialects like PostgreSQL among other sources. Depending on the use case and workload, you can leverage DuckDB\u2019s memory usage and spilling to disk.If most of your workload is aggregations, you can run it without even creating the tables because DuckDB is capable of directly querying CSV and Parquet files. For example, you can issue a query like SELECT val1, sum(val2) FROM read_csv_auto('my_csv_file.csv') GROUP BY val 1 on 100GB+ CSV files without having a lot of memory.If your workload is join-heavy, then the more memory you have, the better, but joins can also spill on disk.The case for an API layer on top of DuckDBDuckDB has client libraries in popular languages to connect and execute the SQL queries directly. Question: Why not directly execute SQL against DuckDB?Answer: Well, you totally should execute directly for use cases where it is set up locally and fun to experiment, learn, and play with SQL for OLAP queries, as well as quick data mashup workloads.As fun as it sounded so far to play around and experiment with, it becomes trickier for a few real-world use cases. Let\u2019s delve into some of them.Team collaboration on OLAP dataIf you have multiple users collaborating on a huge data set, then of course it makes sense to use a cloud warehouse. Not everyone can or wants to download data sets into their local machine to set up DuckDB with associated network / bandwidth costs and the time taken. Also, in the many-terabyte range, DuckDB may not be practical anymore as it's limited to the resources of a single machine. This is where a cloud serverless platform like MotherDuck helps. It is in early preview right now, but once this is launched for everyone to try out, this will be a no-brainer for collaboration workflows. With DuckDB on the cloud, an API layer makes more sense for collaboration.Building real-world dashboards requires an APIDuckDB has been optimized for running OLAP queries and data analysis. There are use cases where you need dashboards / data visualizations against this data store. Building dashboards on the frontend and sharing the dashboard with a team of analysts wanting to look at the data requires building an API layer.There is support for WASM which enables executing inside browsers via JavaScript. Calling SQL directly from the frontend totally works fine as long as the use case is local and there is a single user. Beyond that, you would be exposing connection string/credentials and it makes sense to work on an API. Once you have an API layer, there are general concerns about an API that needs to be addressed. Some of them are authentication and authorization, which we will talk about next, and production-ready features like controlling execution time, query whitelisting, avoiding the obvious security concerns like SQL injection, etc. We will go over these concerns later.Authentication and authorizationDuckDB doesn\u2019t have a native authorization system and they shouldn\u2019t worry about it either apart from the authentication stuff that solutions like MotherDuck integrates with.Going back to the dashboards use case, different users collaborating require authorization depending on their access control rules configured for it. In the API layer, you would have to account for role-based access control (RBAC) rules. This is where a tool like Hasura can quickly bridge the gap with built-in declarative authorization.Data mesh and federationAs we know, DuckDB helps mash up data from multiple sources like files and databases. There are data sources for which you need pipelines to import into DuckDB. For example, if you are looking to import Snowflake or BigQuery data into DuckDB, you need to set up ingestion pipelines. But what if you need to just combine some metadata from another data source for reporting dashboards? And what if some of the data is not for OLAP but more metadata from an OLTP source or an external API?A federated API layer would help in this use case. Imagine if you can combine the API capabilities of Hasura over DuckDB + MySQL + Oracle + Stripe API + <name whatever source you have>.Running DuckDB workloads on edge environmentsLurking around in the MotherDuck Slack, we found a use case where DuckDB queries were required to be run on edge environments like Vercel Edge Functions and Cloudflare Workers with TypeScript. Currently, there are workarounds to set it up using the CLI but not all edge environments support this workflow.Hasura\u2019s Data Delivery Network connected to a cloud instance of DuckDB (hosted on MotherDuck) would expose an API (http interface) that can be used to execute queries on any edge environments natively.Of course, it is clear why a HTTP interface is required, so what is stopping you from building one yourself? Well, the idea is to not spend time writing boilerplate CRUD APIs and working on authorization rules when they are all available declaratively through platforms like Hasura.Going back to the dashboard use case, I would rather spend some time setting up the frontend to create tables/visualizations than spending time writing an API which solves filtering, permissions, pagination, and handle date times.API Architecture: Hasura + DuckDB on MotherDuckConclusionAs we write this, we are working on a data connector for DuckDB so that you can instantly query using GraphQL / REST without writing the boilerplate code, worrying about authorization, and running this whole system for a production use case.Do reach out to us for collaborating on building the connector or to get an early access to the connector by creating a GitHub issue here."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Alerts for Hasura Cloud Description: Today, we are announcing an Alerting system for Hasura Cloud that proactively notifies you about important events and incidents in your Hasura infrastructure. It ensures that you are informed about critical changes, potential performance, and reliability issues as soon as possible. Author: Toan Nguyen"}, {"role": "assistant", "content": "Although Hasura Cloud takes care of the performance, security and reliability requirements of your API layer, many issues are out of our control:Status of remote data sources and external webhooks that are hosted on your infrastructurePerformance of database query and webhook invocationsLogic errorsToday, we are announcing an Alerting system for Hasura Cloud that proactively notifies you about important events and incidents in your Hasura infrastructure. It ensures that you are informed about critical changes, potential performance, and reliability issues as soon as possible.Key benefits\nFaster incident resolution\nBy receiving timely alerts, users can quickly identify incidents and take immediate action to address them. This reduces the time it takes to detect and respond to issues, minimizing their impact on system performance and user experience.\nImproved reliability\nBy promptly detecting and resolving incidents, Hasura Alerts help improve overall reliability. Users can proactively address potential issues before they impact end-users, resulting in enhanced system stability and user satisfaction.\nPerformance issue detection\nPerformance issues can arise within or downstream from Hasura infrastructure. With alerting, users can monitor important performance metrics and receive notifications when thresholds are breached. This allows for timely intervention and optimization, ensuring optimal system performance.\nRoot cause identification\nIncidents often have downstream causes that originate from outside the Hasura infrastructure itself. Proactive alerting helps users quickly trace these issues to their root causes, enabling more efficient troubleshooting and resolution.\nAlert criteria\nAbnormal error rate\nErrors have a direct impact on end-user experience and can cause critical loss for your business.  It requires the engineering team to quickly respond to solve the issue. Hasura Cloud supports error rate alerts for GraphQL operations, Event, and Cron triggers.\nHigh latency\nEven though the functionality of your API works as expected, slow response times also cause bad user experience and potential timeout errors. Hasura Cloud also supports these alerts for GraphQL operations, Event, and Cron triggers.\nData source status\nThe health of remote data sources is important. Hasura can\u2019t work if remote data sources are unavailable. Postgres is natively supported by Hasura with health checks and concurrent connection metrics, as well as alerts for this data source.\nSchema changes\nThis alert rule helps you quickly act with inconsistent metadata issues, or just get notifications about metadata changes by someone in your team.\nWhat alert contents do I expect to see?\nThe alert message contains general information that explains what\u2019s going on in your project, including:The name of the alert.Description of the alert rule, and why it was triggered, e.g. The error rate exceeds 50 percent.Time when the alert was triggered.Alert status. The alert is sent when it happens or is resolved. The severity of the triggered status can be critical or warning. The email below is an example of an alert message. Concurrent alerts are grouped in a single email to avoid spamming your inbox. However, some content can be different depending on special rules, for example, Schema registry alertsSupported Alert channels\nAt this moment, Hasura Alerts support email and Slack notifications. You will receive alert emails like below if enabling the feature.Configuration\nTo enable alerts, head to Project Details -> Alerting tab. On this page, you can:Enable or disable the feature to stop receiving alert messages.Specify alert rules that are fit for your project.Add email addresses and integrate a Slack application to receive alert messages.What\u2019s coming next\nHasura Alerting is currently. Stay tuned to Hasura's blog for upcoming updates.Interested in Hasura Alert? You can give it a try on Hasura Cloud. If you have feedback, request new alert rules, channels, or just ask questions about this feature, join Hasura engineers and community on our Discord or GitHub Discussions."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Single Sign-on for Hasura Console Description: We're excited to announce the Single Sign-on (SSO) solution for Hasura Console. You can integrate existing SSO identity providers (IdP) with Hasura GraphQL Engine to enable authentication. Author: Toan Nguyen"}, {"role": "assistant", "content": "Hasura Console is a powerful tool that can control metadata and data sources. \u201cWith great power comes great responsibility.\u201d The default admin secret-based authentication isn\u2019t secure enough because:We have to share a single secret with team members.The secret should be long and hard to remember.If your backend has many projects, managing the secrets of individual Hasura instances is a nightmare.To help development teams strengthen security and have flexibility in accessing Hasura Console, we're excited to announce the Single Sign-on (SSO) solution for Hasura Console. You can integrate existing SSO identity providers (IdP) with Hasura GraphQL Engine to enable authentication. The feature is available on Hasura Enterprise Edition (EE). SSO is already available for Hasura Cloud customers via SAML.Enable SSO for Enterprise Edition\nHasura provides full support for OAuth 2.0 as a Single Sign-On (SSO) solution without the need for additional tooling or services. OAuth 2.0 is an industry-standard protocol that enables secure and delegated access to protected resources.Authorization Flow\nFirst, Hasura needs to know the authentication context and verification secrets of the Identity Provider (IdP). If JSON Web Key Sets (JWKS) are exposed on the remote URL, the server will fetch the JWT secret on startup.When clicking on the SSO login button, Console initiates an Authorization Code flow with the IdP.On successful authorization, the IdP issues a JWT ID token to the Console with an admin or equivalent role.The Console then uses the JWT to execute Metadata and GraphQL API calls.Configuration\nSet the --sso-providers argument (or HASURA_GRAPHQL_SSO_PROVIDERS environment variable) which takes a list of SSO provider objects.\n[\n {\n   \"client_id\": \"<client-id-from-idp>\",\n   \"name\": \"<display-name>\",\n   \"scope\": \"openid\",\n   \"authorization_url\": \"<login-url>\",\n   \"request_token_url\": \"<request-token-url>\",\n   \"admin_roles\": [\"admin\"],\n   \"jwt_secret\": {\n     \"type\": \"RS256\",\n     \"jwk_url\": \"https://...\",\n     \"issuer\": \"myapp\"\n   }\n }\n]\n\nclient_id: Client ID of the identity application.name: Display name of the SSO button in the login page of the Hasura Console.scope: The OAuth scope. It must contain openid so the identity provider can return the JWT id_token from the request token endpoint.authorization_url: The authorization URL that the browser redirects to from the Console.request_token_url: URL the Console uses to get the ID token using the authorization code.admin_roles: By default, the role should be admin in the token issued by IdP so that Console access is provided. But, if you have a configuration where the admin role is used for something else, set this key to indicate which roles should be treated as admin. This is an array of strings.jwt_secret: JWT secret the server uses to verify the JWT signature. It follows the JWT secret configuration.Other protocols\nFor other protocols, such as LDAP or SAML, you can use any OpenID Connect compliant middleware service such as Dex that acts as a portal to other identity providers. Hasura talks to the middleware to authorize and verify signatures without caring about how the middleware handles external providers. Dex supports SAML 2.0, LDAP protocols, as well as identity providers like GitHub, Google, and Active Directory.The authorization flow is similar to the OAuth / OpenID Connect flow, with an extra step to authenticate the IdP through middleware.Read more about the integration and integration guides for popular providers at Hasura Docs or get started with the SSO Demo repository.SSO on Hasura Cloud\nHasura Cloud Enterprise supports SSO integration with your identity management system through SAML 2.0 protocol. Hasura is a SAML 2.0-compliant service provider.To enable SSO integration with your Identity provider, you need to:Create a new service provider on your SAML IdP with ACS URL and identity to be https://auth.pro.hasura.io/saml/callback.Provide service provider details and metadata to Hasuar support to configure SAML SSO.Read more about the configuration details at Hasura Docs.Hasura Cloud Login PageVerdict\nSingle Sign-on is the security standard for large teams and enterprise organizations nowadays. The solution helps team members avoid common password mistakes, especially with powerful tools such as Hasura Console.Both Hasura Cloud and Enterprise Edition support SSO with simple configurations. If you are using Hasura Core and interested in the feature, register for your 30-day free Enterprise Trial or get started with some Hasura Cloud projects. If you have feedback or ask questions about this feature, join Hasura engineers and community on our Discord or GitHub Discussions."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top 5 pitfalls to look out for when deploying microservices Description: In this blog, we'll take a deep dive into the most famous architectural pattern of the current era: microservices. \nWe will focus on the main challenges when deploying microservices so we become mindful of when and when not to, instead of using them as a hammer for all problems.  Author: Smit Shah"}, {"role": "assistant", "content": "Every once in a while, specific architectural patterns are claimed to be silver bullets. They become the go-to way of solving any problem, undermining the design. In the past, there was an era of service-oriented architecture and layered architecture, both incredible patterns but each with its limitations.In this blog, we'll take a deep dive into the most famous architectural pattern of the current era: microservices. We will focus on the main challenges when deploying microservices so we become mindful of when and when not to, instead of using them as a hammer for all problems. Pitfall #1: Access control of data in microservices is complexWhen it comes to controlling access to data in monoliths, the problem is simple: You can have role-based access defined at one of your layers and delegate the responsibility for enforcing the access control.When it comes to microservices, the problem compounds \u2013 either there is a shared library that maintains the access control and has a standard service to which each service talks, or in the worst case, each service goes and implements its authorization layer.So the complexity of building an authorization layer is much greater than a monolithic system, and depending on the implementation, it can significantly slow down the rollouts of new roles or access control policies.Pitfall #2: Distributed transactions in microservices are complicatedImagine booking a holiday package, which gives a discounted price for hotels and flights; let\u2019s say your hotel booking goes through but not your flight booking.What would you do? Transactions guarantee either both bookings succeed or neither of them. They guarantee consistent behavior across multiple writes.Such guarantees across different services are complex; you can use two-phase commits or saga patterns to ensure consistency. However, these distributed patterns are complex, challenging to reason, and can come with performance penalties.Pitfall #3: Aggregating data in microservices is challenging to optimizeWhen fetching data from multi-services, applications need to take care of complex aggregations, and they could lead to massive fan outs due to N+1 queries and similar problems.It\u2019s challenging to do optimizations like predicate push-downs for microservices; most naive implementations will call the microservice multiple times or fetch more data than required.Looking at this example, to show a holiday package, we need to call different services multiple times to offer other options for hotels/flights and rentals.Pitfall #4: Observability in microservices is often elusiveMonitoring or triaging an issue across multiple services can be tricky without the right system; your workflow could fall apart due to a downstream system you are unaware of.It can be complicated to pinpoint an issue in production or debug which system is causing the outage. To tackle these problems, we can set up distributed tracing and monitoring, but that adds to infrastructure complexity and cost.Pitfall #5: Infrastructure complexity in microservices is costlyTo make microservice work, you must invest in various infrastructure pieces to make services easily deployable, scalable, and traceable. You also need the correct monitoring and alert setup, which adds to the terms of cost and management.To be productive with microservices while ensuring minimal outages and downtime is challenging. Ultimately, the cost of maintenance overshadows the cost of implementation, and it\u2019s true for microservices.ConclusionMicroservice is an excellent architecture pattern for the correct set of problems, but it shouldn\u2019t be the go-to pattern for all issues. This post shows that many challenges come in a microservices architecture, some non-trivial.In our next post, we will discuss monolithic architecture and how to work with its challenges.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Enhancing observability by modernizing an application Description: Explore how modernizing a legacy application with Hasura significantly enhances observability, leading to improved performance, increased agility, and better user experiences. Author: Akshay Bhat"}, {"role": "assistant", "content": "Modernizing legacy applications is a strategic imperative for businesses seeking to stay competitive in today's rapidly evolving digital landscape. However, modernization isn't just about updating the user interface or migrating to the cloud. It's also about improving observability \u2013 the ability to gain insights into application performance, diagnose issues, and make informed decisions. In this blog post, we'll explore how modernizing a legacy application with Hasura significantly enhances observability, leading to improved performance, increased agility, and better user experiences.The observability challenge in legacy applicationsLegacy applications often pose challenges when it comes to observability. These challenges include:Complex architecture: Legacy applications are typically built using monolithic architectures, making it difficult to isolate issues and understand the impact of changes.Limited insights: Inadequate logging, tracing, and monitoring capabilities hinder the ability to gain deep insights into application behavior.Opaque data flow: Inadequate logging and tracing result in limited visibility into data flow and interactions.Silos of data: Data is often scattered across various databases and systems, making it hard to correlate and analyze data effectively.Manual debugging: Traditional debugging methods in legacy applications are time-consuming and error-prone, slowing down issue resolution.Data Modernization with HasuraHasura, with its powerful GraphQL engine, is a game-changer for modernizing legacy applications. By transforming a legacy monolith into a set of microservices with Hasura, organizations can overcome many observability challenges, including these top five:Granular observability: Hasura allows you to expose granular GraphQL APIs for each microservice, enabling detailed monitoring and tracing of API requests. This granularity provides insights into individual service performance and helps pinpoint bottlenecks.Real-time monitoring: Hasura's real-time capabilities empower you to monitor changes in your data in real time. You can set up real-time subscriptions to track data modifications, enabling proactive monitoring and reducing the time between detecting and addressing issues.Centralized data access: By unifying data access through GraphQL, Hasura eliminates data silos and provides a single endpoint for querying and monitoring data. This centralization simplifies observability and enhances data analysis.Dynamic tracing: Hasura's support for distributed tracing allows you to track the flow of requests across microservices. This dynamic tracing provides a holistic view of request paths and helps identify performance bottlenecks and latency issues.Automated debugging: Hasura's powerful debugging capabilities, including error tracking and detailed query analysis, accelerate issue resolution. Developers can identify problematic queries and optimize them for improved performance.Observability options in HasuraHasura offers an array of default observability options to jumpstart your journey:Stats overview: Gain a holistic overview of project health and performance metrics.Errors: Real-time error tracking to diagnose and resolve issues promptly.Usage summaries: Insights into API usage patterns for informed decision-making.Operations: Monitor and analyze API operations to optimize performance.Websockets: Real-time tracking of data flows through Websockets.Subscription workers: Monitor subscription worker performance for seamless data streaming.Distributed tracing: Trace requests across microservices for performance insights.Query tags: Categorize queries for enhanced analysis and optimization.Hasura's observability integrationsHasura integrates seamlessly with observability platforms like Prometheus, Grafana, and Jaeger. This compatibility enhances existing observability ecosystems, providing deeper insights into performance metrics, real-time monitoring, and tracing.Use case: Real-time analytics dashboardConsider a legacy e-commerce application struggling with performance bottlenecks during peak shopping seasons. Modernizing with Hasura enables real-time monitoring of user interactions, transaction flows, and database queries. By integrating with Grafana and Prometheus, developers create custom dashboards displaying response times, user activity, and database latency. This empowers them to proactively optimize resources, ensuring a seamless shopping experience.Hasura Enterprise: Elevated observabilityFor Hasura Enterprise deployments, observability takes center stage:Monitoring for containers and pods: Self-hosted Hasura instances benefit from enabling monitoring, ensuring comprehensive insights into resource usage.Logs and metrics via Prometheus: Integration with Prometheus amplifies observability, offering granular insights into performance metrics.Database observability with HasuraModernization extends to the database observability:Historical query performance: Dive into historical query performance data to optimize database interactions and ensure efficient data retrieval.Host-level metrics: Monitor host-level metrics for efficient database operations.Use case: Microservices performance optimizationA legacy airline booking system faces challenges due to slow database queries. Modernizing with Hasura transforms the application into microservices with dedicated GraphQL APIs.By implementing distributed tracing with Jaeger, developers track each step of a booking transaction. Slow database queries are identified, enabling the team to optimize them and improve overall system performance.Enhanced database observability with HasuraHasura enhances database observability through strategic steps:Enabling observability agents: Activating observability agents offers real-time insights into query performance.Query tags: Leveraging query tags along with native database monitoring tools like pganalyze enables deep analysis of query performance and database health.Use case: Healthcare data monitoringA legacy healthcare application grapples with data inconsistencies. Modernization with Hasura transforms the application into microservices, each serving a specific healthcare function. By implementing Hasura's query tags and integrating them with pganalyze, the development team identifies slow queries affecting patient data retrieval. Query optimizations lead to a faster and more reliable healthcare system.Proactive alerting and alert propagationObservability culminates in proactive alerting:Configuring alerts: Set up alerts for critical metrics and events, ensuring prompt notifications for immediate action.Alert propagation: Ensure alerts reach the appropriate teams for swift resolution and proactive maintenance.Also, Hasura has instrumented all API queries, mutation, and subscriptions with the OpenTelemetry format, thus allowing you to export Traces and Metrics.Step-by-step modernization for enhanced observabilityIf you want to modernize and enhance the observability of your application, use these steps as your implementation guide:Assessment and planning: Evaluate the legacy application's architecture, identifying potential microservices and data sources. Plan the migration to microservices with Hasura.Data modeling: Design the GraphQL schema using Hasura's schema stitching feature to represent the microservices' data and relationships.Migration: Migrate the existing application logic to microservices, creating GraphQL APIs using Hasura's declarative data modeling.Real-time integration: Implement real-time subscriptions using Hasura, enabling instant notifications of data changes.Monitoring and tracing: Set up monitoring tools like Prometheus and Grafana, and enable Hasura's tracing features to track API requests and identify performance issues.Testing and validation: Thoroughly test the modernized application, analyzing observability metrics to ensure performance improvements.Iterative enhancement: Continuously monitor the application, analyze observability data, and optimize queries and services for optimal performance.ConclusionEnhancing observability in legacy applications is a critical step toward maintaining a competitive edge in today's dynamic digital landscape. Modernizing with Hasura enables organizations to overcome the challenges of legacy observability, offering granular insights, real-time monitoring, and efficient debugging. By following a strategic approach to modernization, businesses can transform their legacy applications into agile, observable, and high-performance systems that deliver exceptional user experiences.Hasura's transformative capabilities, combined with observability best practices, empower developers and organizations to not only modernize legacy applications but also elevate their observability to new heights. The result is a future-ready application ecosystem that's highly responsive, resilient, and primed for ongoing success.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura\u2019s top 5 session picks featuring data APIs at Google Cloud Next 2023 Description: Sharing with you the top five sessions regarding the world of APIs we are particularly excited for. See you there! Author: Hasura"}, {"role": "assistant", "content": "All geared up for Google Cloud Next 2023? We\u2019re busy getting Hasura\u2019s booth (#543!) ready for the event and really look forward to connecting with you there at the Moscone Center in San Francisco.But before we get ahead of ourselves, we wanted to share with you the top five sessions regarding the world of APIs we are particularly excited for.Guide to Choosing Between GraphQL, gRPC, AsyncAPI, or REST APIWe all know that when it comes to modern software development, choosing the right API protocol is crucial. This session looks promising in terms of demystify the decision-making process with a compare and contrast analysis to help gain insights into the strengths, weaknesses, and real-world applications. We like informed choices!Integrate Apps and Workflows with Google Meet APIs and SDKsBuilding integrated workflows? We also like smooth-sailing work situations! \u00a0Communication\u2026 collaboration\u2026 these \u00a0are the cornerstone of remote work and this session looks to showcase how to seamlessly integrate your applications and workflows with Google Meet APIs and SDKs, based on meeting data to enhance user experiences, streamline processes, and boost virtual interactions and experiences.Four Ways to Supercharge Your Startup\u2019s Market Entry and Monetization with APIsNavigating the fast-moving, competitive market landscape can be challenging, especially for startups with resource constraints. What gets us there faster? The freedom to integrate your essential tools while leveraging APIs and Google Cloud for building go-to-market tech stacks. This session looks to connect the dots from build to supporting speed-to-market strategies and tailored business models thato accelerate market entry and monetization. Stay Ahead of API Security Threats with ML DetectionWe all know a major part of working with data is staying one step ahead of quickly adapting security threats that come from seemingly disparate data points and sources. Essential learning as this sesion highlights machine learning (ML) detection techniques specifically tailored for API security. Gain insights into how ML can proactively identify and mitigate potential threats, safeguarding your applications and data in an increasingly complex digital environment.Integrate with Machine Learning APIs ChallengePut your skills to the test in this gamified challenge lab, where you are a member of the Analytics teams for Jooli, Inc. that develops and assess datasets for machine learning projects. The challenge is speed and accuracy, and there will be prizes! Obviously, there is so much more happening at Google Cloud Next, but you know where we\u2019ll be! And be sure to visit Hasura\u2019s booth #543 to learn more about our latest data API solutions for unblocking your teams in modernization efforts and much more.When you stop by, you\u2019ll have a chance to enter our raffle for a chance to win exciting prizes (more prizes!) and discover firsthand the power of Hasura's offerings. We also encourage you to book your 15-minute Hasura demo at Google Cloud Next ahead of the event to secure your spot. Book your demo now!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building dynamic ABAC authorization for banking Description: In this technical blog post, we explore how to implement a dynamic ABAC authorization system for the banking industry using Hasura's powerful GraphQL engine. Author: Akshay Bhat"}, {"role": "assistant", "content": "In the banking industry, data security and access control are of paramount importance. Traditional role-based access control (RBAC) mechanisms often fall short in providing the fine-grained control required to manage access to sensitive financial data. Attribute-based access control (ABAC) is a more flexible approach, allowing access decisions based on dynamic attributes. In this technical blog post, we explore how to implement a dynamic ABAC authorization system for the banking industry using Hasura's powerful GraphQL engine.Understanding attribute-based access control (ABAC)ABAC is an access control model that grants or denies access to resources based on the attributes of the user, the resource, and the environment. It allows for a more sophisticated access control system that considers multiple attributes in the decision-making process. For the banking industry, where access to financial data should be tightly controlled, ABAC is an ideal choice.Step 1: Define the data modelBefore implementing ABAC, we need to define the data model for the banking application. This includes defining entities such as Customers, Accounts, Transactions, and other relevant entities. Additionally, we should identify the attributes that will be used to make access control decisions, such as customer type, account type, transaction amount, location, and more.Step 2: Integrate Hasura's GraphQL EngineNext, we integrate Hasura's GraphQL Engine into our banking application. Hasura makes it easy to set up a GraphQL API by automatically generating a schema based on the underlying data sources. By leveraging Hasura's powerful data modeling capabilities, we can establish relationships between entities and set up custom resolvers to compute derived attributes that will be used in our ABAC policies.Step 3: Implementing dynamic ABAC authorizationTo build dynamic ABAC authorization, we need to define our access control policies using GraphQL permissions. Hasura provides a declarative way to specify these policies, which allows us to create rules based on various attributes and conditions. For example, we can create policies like:Allow customers to view their own account details.Allow bank employees to view and modify customer data but only for customers in the same branch.Allow managers to perform high-value transactions but only during business hours.These policies can be written in the Hasura Console or defined in configuration files as per our preference.Step 4: Leveraging external data sourcesIn the banking industry, access control decisions often rely on data from external sources such as customer credit scores, risk assessments, or regulatory compliance checks. Hasura allows us to connect to these external data sources through Remote Schemas or Remote Joins. This way, we can enrich our access control decisions with data from third-party systems, making our authorization system more robust.Step 5: Testing and auditingAfter implementing the dynamic ABAC authorization system, thorough testing is crucial. We should verify that access control policies are working as expected, allowing or denying access based on the specified attributes and conditions. Additionally, we should perform security audits to ensure no loopholes exist in our system and that unauthorized access attempts are appropriately logged and flagged for further investigation.DemoLet's consider a simplified example where we have a banking application that allows customers and bank employees to access account information, and managers to perform high-value transactions.1. Customer RoleCan view their own account details.Cannot access other customers' account details.2. Bank Employee RoleCan view and modify customer data but only for customers in the same branch.Cannot access customer data from other branches.3. Manager RoleCan perform high-value transactions but only during business hours.Cannot perform transactions outside business hours.ArchitectureFrontend: The user interface through which customers, bank employees, and managers interact with the banking application.Hasura GraphQL API Gateway: Acts as a single entry point for all GraphQL requests. It handles incoming queries and mutations, and based on the user roles, permissions, and rules defined, it routes the requests to the respective microservices.Microservice 1 (customer data): This microservice is responsible for managing customer-related data, such as customer profiles, accounts, and account details.Microservice 2 (transaction): This microservice handles high-value transactions and related functionalities.Databases: Two separate databases are used to store data for customers and transactions. These databases can be different types, such as SQL or NoSQL databases, depending on the requirements.ABAC authorization workflow:1. A user sends a request from the frontend to the Hasura GraphQL API Gateway.2. The API Gateway receives the request and checks the user's role and permissions based on the defined ABAC policies.3. If the user is authorized, the API Gateway routes the request to the appropriate microservice.4. The microservice processes the request, retrieves or modifies the data from the respective database, and sends the response back to the API Gateway.5. The API Gateway forwards the response to the frontend, completing the user's request.In this example demo, we have demonstrated how to implement dynamic ABAC authorization for a banking application using Hasura's GraphQL engine. By defining fine-grained access control policies based on dynamic attributes and roles, we can ensure the security and privacy of sensitive financial data in the banking industry. The architecture diagram illustrates the flow of requests and data between the frontend, Hasura API Gateway, and microservices, highlighting the flexibility and control provided by Hasura's powerful features.ConclusionIn the banking industry, ensuring secure access to sensitive financial data is of utmost importance. By implementing a dynamic ABAC authorization system with Hasura's GraphQL Engine, we can create a flexible and powerful access control mechanism. The ability to define fine-grained access control policies based on dynamic attributes provides the necessary security measures required in the banking industry. By leveraging Hasura's capabilities, we can build a robust and efficient authorization system that meets the stringent security standards of the banking sector.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Instant APIs on Snowflake using UDFs and Hasura Description: Within a few minutes, we replaced five different AWS components, implemented an API without writing a single line of code, added an admin secret for authentication, and added the ability to cache to provide low-latency data access. Author: Smit Shah"}, {"role": "assistant", "content": "The Hasura Snowflake connector became generally available in June. Like all other Hasura connectors, the value of the Snowflake connector is the ability to generate APIs (GraphQL and REST) on Snowflake data with minimal coding. I wanted to put it to the test by recreating this blog from Snowflake\u2019s team with Hasura.The original post showed how to create an analytics app using OAG: Global Airline Schedules data. The architecture includes using AWS Lambda functions to read from Snowflake and return the data as RESTful APIs.Architecture in the original blog.We will implement these data APIs over Snowflake using Hasura. Right off the bat, we can instantly replace all the different Amazon components with Hasura, greatly simplifying the architecture by reducing five components to one.Simplified architecture with Hasura.Importing sample data to SnowflakeFirst and foremost, we must import the sample data to our Snowflake instance \u2013 the data can be found in the marketplace here.Implementing the APIs in HasuraThis section will show how to instantly create the required APIs using Snowflake\u2019s user-defined functions (UDF) and Hasura\u2019s query engine. A UDF is a function you define to call it from SQL. UDF\u2019s logic typically extends or enhances SQL with functionality that SQL doesn\u2019t have or doesn\u2019t do well. A UDF also allows you to encapsulate functionality to call it repeatedly from multiple places in code.We will implement the busy_airports API using UDFs and Hasura from the original blog.Busiest airports APIThis API endpoint finds the busiest airports in the data. We will use an API key as an admin secret. We want to filter the data based on flight_date and restrict the number of rows returned from the API.1. Tracking UDFs in HasuraFirst, we will create the following UDF in Snowflake\u2019s SQL console:Now, we will import the UDF to Hasura: After that, we can query the UDF like any other table from Hasura:2. Cache your Snowflake APIYou can use the @cached directive to your query to decrease the response time. The default TTL is 60 seconds, reducing latency by about ~100 milliseconds.Caching is useful when querying Snowflake, as latencies could be a bottleneck, primarily when serving customers on web/mobile applications. It can improve latencies and reduce the load on data warehouse resources by fetching the results from Redis.You can also fetch the results using cURL:url -X POST -H \"x-hasura-admin-secret: admin-secret\" -H \"Content-Type: application/json\" -d '{\"query\":\"query MyQuery{BUSY_AIRPORTS_2(limit:20,order_by:{COUNT:desc}){ARRAPT COUNT}}\"}' https://choice-platypus-97.hasura.app/v1/graphql\n{\"data\":{\"BUSY_AIRPORTS_2\":[{\"ARRAPT\":\"ATL\",\"COUNT\":131184},{\"ARRAPT\":\"ORD\",\"COUNT\":120075},{\"ARRAPT\":\"LHR\",\"COUNT\":106354},{\"ARRAPT\":\"DFW\",\"COUNT\":105660},{\"ARRAPT\":\"JFK\",\"COUNT\":98454},{\"ARRAPT\":\"LAX\",\"COUNT\":91109},{\"ARRAPT\":\"CDG\",\"COUNT\":87035},{\"ARRAPT\":\"FRA\",\"COUNT\":86363},{\"ARRAPT\":\"AMS\",\"COUNT\":85004},{\"ARRAPT\":\"MAD\",\"COUNT\":61921},{\"ARRAPT\":\"BOS\",\"COUNT\":58124},{\"ARRAPT\":\"SEA\",\"COUNT\":56016},{\"ARRAPT\":\"SFO\",\"COUNT\":55467},{\"ARRAPT\":\"DEN\",\"COUNT\":53891},{\"ARRAPT\":\"EWR\",\"COUNT\":49122},{\"ARRAPT\":\"FCO\",\"COUNT\":48736},{\"ARRAPT\":\"IAH\",\"COUNT\":47115},{\"ARRAPT\":\"MIA\",\"COUNT\":46552},{\"ARRAPT\":\"YYZ\",\"COUNT\":45225},{\"ARRAPT\":\"CLT\",\"COUNT\":44266}]}}\n\nSimilarly, we can implement airport_daily and airport_daily_carriers following the same steps. Thus, we can access the data in minutes without writing a single line of code, just using UDFs in Snowflake and Hasura. The APIs are protected with API keys. Suppose you want a role-based authorization layer. In that case, this blog covers how to implement role-based access control on Snowflake: \u201cSnowflake using RBAC: A secure and scalable data access solution.\u201dConclusionWithin a few minutes, we replaced five different AWS components, implemented an API without writing a single line of code, added an admin secret for authentication, and added the ability to cache to provide low-latency data access.Hasura avoids getting entangled with details and lets you focus on building the end product.\ud83d\ude80 Get Started Today!We can't wait to see the amazing application you'll build using Hasura and Snowflake. Start by signing up for Hasura Cloud and connecting your Snowflake data warehouse. If you have any questions or need assistance, please contact our team on Discord or GitHub."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Improving Observability of Subscriptions and Event Triggers in Hasura \u2013 a Golden Signals approach Description: Taking inspiration from the Golden Signals monitoring approach described in the SRE book by Google, Hasura has made significant improvements to the observability of subscriptions and event triggers. Author: Sooraj Sanker"}, {"role": "assistant", "content": "In the world of modern application development, observability is a critical aspect of ensuring the reliability and performance of systems. Hasura, a powerful GraphQL engine, understands the importance of observability and is continuously working to enhance it. Taking inspiration from the Golden Signals monitoring approach described in the SRE book by Google, Hasura has made significant improvements to the observability of subscriptions and event triggers.In this blog, we will explore these enhancements and how they can help you monitor and debug your Hasura-based applications effectively.\u201cThe four golden signals of monitoring are latency, traffic, errors, and saturation. If you can only measure four metrics of your user-facing system, focus on these four.\u201dSubscription Observability Observing the performance and behavior of subscriptions is crucial for ensuring the reliability and efficiency of GraphQL-based applications. Considering subscriptions deal with real time data, the performance and load on the database needs to be observed and balanced in an efficient manner. This would require internal visibility on how Hasura processes a subscription and keep the data in sync on different stages. At Hasura, we have taken inspiration from the Golden Signals monitoring philosophy outlined in the SRE book by Google to enhance the observability of subscriptions. Hasura incorporates the four golden signals \u2013 latency, traffic, errors, and saturation \u2013 into subscription observability and more details on how each of these signals can be monitored with the exported metrics is documented here.For example, If you see a performance issue you may want know:LatencySubscriptions (by operation name / query hash)Database Execution Time (by operation name / query hash)Websocket Queue timeTrafficTotal number of active subscriptionsHasura websocket connections by workersWS Ingress/EgressErrorActive Subscription workers in error stateFor more details on how Hasura executes subscriptions, refer to the live query or streaming subscription documentation.The following screenshots shows an example Grafana dashboards rendered using the metrics exported using OpenTelemetry exporter from Hasura.Hasura empowers developers to proactively monitor, debug, and optimize their GraphQL-based applications. Leveraging these signals enables improved reliability, performance, and scalability, providing users with real-time data updates seamlessly.Event Triggers ObservabilityEvent triggers in Hasura allow you to react to database events and invoke webhooks. Considering the source of these events may or may not be user events, the traffic is very much connected to your business logic and in many cases unpredictable in nature. Which makes it more important to have an enhanced observability on each stages of an event trigger execution. For instance, you watch the overall success rate and processing time first, and if you notice changes in the processing time (latency), you may want to look into the internal latency at every stage of an event trigger operation such as Event fetch time, Queue time, HTTP webhook processing time, active HTTP workers to process these events from the queue etc.Or if you notice any errors or increasing error rates, you may want to look at the HTTP workers saturation rate, or how your event webhook service is processing the requests. \u00a0The following screenshots shows an example Grafana dashboards rendered using the metrics exported using OpenTelemetry exporter from Hasura.Monitoring these metrics helps identify saturation and take necessary actions, such as increasing the number of HTTP workers or scaling the Hasura instance.If you want to try out the observability setup with self-hosted Enterprise Edition version, checkout the following repository and follow the instructions: https://github.com/hasura/graphql-engine/tree/master/community/boilerplates/observability/enterpriseGet started now!Exporting observability data from the Hasura cloud is now easier than ever!Sign up now to try out the new observability metrics from Hasura.Create a new project and click on \u201cLaunch Console\u201d from the project details page.Head to the settings page and select \u201cOpenTelemetry Exporter (Beta)\u201d from the sidebar. Enable the service and provide valid OLTP endpoints for traces and (or) metrics, then save the configuration by clicking on the \u201cUpdate\u201d button.5. \u00a0 Viol\u00e0 \ud83c\udf89 That's it! You will now be getting trace and metrics information exported to the OLTP endpoint in the OpenTelemetry format based on the configuration!Stay tuned for more updates from Hasura as they continue to enhance observability and provide a seamless GraphQL experience for developers worldwide."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Ship faster using SQL Server stored procedures with Hasura GraphQL Engine Description: Explore the benefits of adding support for SQL Server stored procedures to the Hasura GraphQL Engine, along with some compelling use cases. Author: Samir Talwar"}, {"role": "assistant", "content": "GraphQL has revolutionized the way APIs are designed and consumed. Many application developers love its flexibility and declarative nature and these are top reasons why GraphQL has \u00a0garnered widespread adoption. Now, imagine combining the power of GraphQL with the performance and security of SQL Server stored procedures\u2026 In this blog, we'll explore the benefits of adding support for SQL Server stored procedures to the Hasura GraphQL Engine, along with some compelling use cases.Understanding SQL Server stored proceduresSQL Server stored procedures are precompiled database objects that encapsulate business logic and complex data operations. They are very popular with Microsoft SQL Server database administrators, who often use them as the sole point of access to the database from an application. As the database admins are often the SQL experts on the team, this allows them to manage performance, security, and releases in the environment in which they\u2019re most familiar.Use cases for SQL Server stored procedures:Data validation and sanitization: Stored procedures can validate and sanitize incoming data before persisting it in the database, ensuring data integrity and security.Complex data manipulation: For complex data transformations or calculations, stored procedures offer a performant way to process data within the database.Atomic operations: Stored procedures can group multiple queries into a single transaction, ensuring data consistency and integrity.Integration with legacy systems: When dealing with legacy systems, stored procedures can act as a bridge to connect modern applications with older databases.Security and access control: By implementing business logic in stored procedures, developers can control data access and protect sensitive information.Hasura GraphQL for stored proceduresHasura has recently announced the ability to make native queries, which offer complete flexibility when reading data from the database. In doing so, we also introduced logical models, which are a totally freeform way of specifying the structure of the returned data from the database, which has the potential to reach much wider than native queries. Today, we\u2019d like to announce another feature built on top of logical models: Invoking read-only stored procedures in SQL Server, directly from Hasura GraphQL Engine.Accessing stored procedures from Hasura allows you to:Leverage existing logic: For applications that interact with an already established database containing stored procedures, Hasura's integration allows developers to utilize existing logic without rewriting it, saving valuable development time.Improve performance: SQL Server stored procedures are optimized for execution, leading to faster response times and reduced server load. Hasura's support for stored procedures allows developers to tap into this performance advantage while utilizing GraphQL's efficient data retrieval capabilities.Control access to data: With GraphQL, app developers can fine-tune queries to request precisely the data they need. When paired with stored procedures, administrators can ensure that the application only has access to the data it needs, and no more.Simplify business logic: By exposing SQL Server stored procedures in Hasura's GraphQL schema, developers can abstract complex business logic from application code. This separation enhances code maintainability, reduces duplication, and streamlines development workflows.ExampleLet\u2019s imagine we\u2019re running an online shop, backed by SQL Server. We have a set of products that we want to feature on the homepage, but only for a certain amount of time. We could encode this logic in our various frontends, but this would lead to duplication of work, so we\u2019ve elected to create a stored procedure, managed by our database administrators.We\u2019ll create two tables:CREATE TABLE product (\n  id int NOT NULL IDENTITY(1, 1) PRIMARY KEY,\n  name varchar(255) NOT NULL\n);\n\nCREATE TABLE product_feature (\n  product_id int NOT NULL,\n  region varchar(255) NOT NULL,\n  featured_start datetime2,\n  featured_end datetime2,\n  CONSTRAINT fk_product_region_product_id FOREIGN KEY (product_id)\n    REFERENCES product (id) ON DELETE CASCADE ON UPDATE CASCADE\n);\n\nHere, our product_feature table identifies each region associated with a product, as well as the start and end timestamps we use to decide when to feature that product. (We tie this to a region so we can feature products in different places at different times.)So our stored procedure might look like this:CREATE PROCEDURE featured_products\n  @region varchar(255),\n  @current_time datetime2\nAS\n  SELECT DISTINCT product.*\n  FROM product\n  JOIN product_region ON product.id = product_region.product_id\n  WHERE region = @region\n\tAND @current_time BETWEEN featured_start AND featured_end;\n\nThis shows us all products to be featured for the given region, at the specified time.We can now track this stored procedure in Hasura GraphQL Engine. Using the Console, we can navigate to the Native Queries section of the Data tab, and add a logical model corresponding to the structure returned by this procedure:We then track a Stored Procedure specifying the arguments:We can then query it just as you would a table. You can even add extra clauses to filter or sort the stored procedure results.ConclusionThe synergy between SQL Server stored procedures and Hasura GraphQL Engine presents app developers with a powerful toolkit to build high-performance, secure, and scalable applications. By leveraging the benefits of stored procedures, such as improved performance, fine-grained data access, and simplified business logic, developers can optimize their application's data layer and streamline development processes.From data validation to complex data manipulation and real-time capabilities, the combination of SQL Server stored procedures and GraphQL empowers developers to create robust applications that meet the demands of today's dynamic digital landscape. Whether it's a new application or an existing one seeking optimization, Hasura's support for stored procedures opens up a lot of possibilities for app developers.\ud83d\udcda Documentation and resourcesTo help you get started, we've prepared detailed documentation, guides, and examples:https://hasura.io/docs/latest/schema/ms-sql-server/logical-models/stored-procedures/https://hasura.io/docs/latest/schema/ms-sql-server/logical-models/index/\ud83d\ude80 Get started today!We can't wait to see the amazing applications you'll build using Hasura and Native Queries. Get started today by signing up for Hasura Cloud and connecting to one of the supported databases.If you have any questions or need assistance, feel free to reach out to our team on Discord or GitHub."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Supercharge your application development with Hasura Remote Joins and Data Federation Description: We are thrilled to announce that Hasura now supports Remote Joins across all supported data sources, including PostgreSQL, Snowflake, MySQL, SQL Server, BigQuery, Oracle, Athena, and Remote Schemas! Author: David Meleney"}, {"role": "assistant", "content": "We are thrilled to announce that Hasura now supports Remote Joins across all supported data sources, including PostgreSQL, Snowflake, MySQL, SQL Server, BigQuery, Oracle, Athena, and Remote Schemas!In the world of modern application development, building complex data relationships across various sources has become a common requirement. As databases and data sources grow in complexity, developers often find themselves facing the challenge of integrating data from different databases, APIs, or remote GraphQL endpoints. Thankfully, Hasura\u2019s GraphQL engine provides a powerful solution through its remote relationships, also known as \"Remote Joins.\" In this blog, we will explore the concept of remote relationships and how they can be leveraged to join data across tables and remote data sources, empowering developers to build sophisticated and data-rich applications.Understanding remote relationshipsAt its core, a remote relationship in GraphQL allows you to connect data across different tables and remote data sources. These sources can include:Database to database relationships: This type of remote relationship enables you to join data between two different database sources. For instance, you can link order information stored in one PostgreSQL database with user information stored in another PostgreSQL or even a SQL Server database. GraphQL acts as the glue that seamlessly integrates the data from both sources.Database to GraphQL services: Here, you can merge data across tables with remote GraphQL APIs that we call Remote Schemas. For example, you might combine customer data from your database with account data from external services like Stripe, Spotify, or Auth0. This integration allows you to consolidate information from various sources into a single, cohesive GraphQL query.GraphQL services to database relationships: In this scenario, you can connect data from Remote Schemas (representing services like Stripe, Spotify, or Auth0) to customer data from your database. This bidirectional relationship unlocks a plethora of possibilities for building feature-rich applications that rely on data spanning across multiple services.Custom business logic to database relationships: Actions in GraphQL often correspond to REST APIs. With this type of remote relationship, you can join data across tables and Actions, such as fetching user data from your database and combining it with the response from a createUser action using the id field.How Hasura performs Remote Joins?When performing a RemoteJoin between \u201ctable A\u201d and \u201ctable B,\u201d Hasura will first query \u201ctable A\u201d for its rows. From these rows Hasura will collect all the IDs involved in the join to \u201ctable B.\u201d It will then query \u201ctable B\u201d as a separate query, filtering the rows by the IDs collected from \u201ctable A.\u201d Hasura will then stitch the rows returned from \u201ctable B\u201d into the results from \u201ctable A\u201d to provide the final response to the original GraphQL query.This approach avoids the usual N+1 query anti-pattern that can occur with naive implementations that join between two tables in different databases. The N+1 query anti-pattern is where you query \u201ctable B\u201d for every row returned by the query to \u201ctable A,\u201d which is inefficient. In Hasura, join queries are optimized to query each table once, limiting the number of separate queries issued to the smallest possible number.How do Remote Joins enhance application development?Businesses today are not confined to a single database. Data exists in a multitude of sources \u2013 on-premises, in the cloud, or distributed across multiple systems. To maintain a competitive edge, organizations must harness the power of these disparate data sources and seamlessly combine them to drive meaningful use cases and insights. In a matter of minutes, developers can create a single GraphQL API using Hasura that accesses data from multiple sources, including GraphQL APIs (Remote Schemas) and databases, in real time, without the need for complex data integration processes. This means developers can create powerful applications that access data from different sources, without worrying about data silos.In traditional SQL, joining tables from different databases requires maintaining a duplicate copy of the data from one database in the other database that is needed to perform the join. This process was not only time-consuming but also required a lot of resources. With Hasura's remote joins, developers can perform joins across data sources without copying any data. This allows organizations to federate their data into a single self-service data access layer.By providing a unified view of data, Hasura's remote joins can significantly reduce the time it takes to develop data intensive applications. Developers can quickly prototype applications, test different use cases, and get feedback from stakeholders. This enables enterprises to bring applications and data products to market faster, giving them a competitive edge.Simplified data managementHandling and managing data across multiple databases is a significant challenge for developers. It often involves writing complex queries and spending countless hours on data management tasks. However, with Hasura's Remote Joins, these challenges become a thing of the past. Remote Joins provide a way to access and unify data across various databases as if it were coming from a single data source.Boosts development speedBy offering a unified API across different databases, Hasura saves developers from the tiresome task of orchestrating data from various sources. This allows developers to focus more on the core functionality of the application, ultimately boosting the development speed.Streamlined workflowA GraphQL API that bridges different data sources simplifies your workflow. It not only helps you fetch data from multiple sources in a single request but also keeps the interface clean and organized, significantly streamlining the development process.Getting started with Remote Joins for databasesCheck out my colleague Vaishnavi\u2019s demo from her talk at HasuraCon 2023 on joining data from Postgres to MySQL.Step 1: Add two database sources.\u200bAdd a source database as described here and track the required tables. Then, repeat the process to add your target database.Step 2: Define and create the relationship.\u200bA remote database relationship is defined alongside the source database table (that is, the source side of the join).The following fields can be defined for a Remote Schema relationship:Relationship type: Either object or array \u2013 similar to normal relationships. Hasura supports both many-to-one (object) and one-to-many (array) relationships.Relationship name: A name for the relationship.Reference source: The name of the target database (that is, the target side of the join).Reference table: The table in the target database source that should be joined with the source table.Field mapping: A mapping between fields in the source table and their corresponding fields in the target table, just as a foreign key relationship would be defined by such mapping within a single database.For example, say we have a table Album(AlbumId int) in the source database and a table Track(id int, name text) in the target database.We can create an array remote database relationship between the AlbumId joining the Album table to the Track table using the Album.AlbumId and Track.AlbumId fields.ConclusionWhen it comes to generating APIs for multiple data sources, Hasura offers a powerful and developer-friendly solution. Its ease of use, automation, and feature-rich capabilities provide a significant advantage over the DIY approach, which requires significant time and effort to design and implement custom solutions. With Hasura, developers can create a single GraphQL API that can access data from multiple sources allowing developers to focus more on building innovative applications that leverage diverse data sources, without getting bogged down by the complexities of API generation.If you're looking to simplify the process of joining data from multiple sources, Hasura is the way to go.\ud83d\udcda Documentation and resourcesTo help you get started, we've prepared detailed documentation, guides, and examples:Remote Relationships DocumentationGraphQL Federation DocumentationOther Hasura Data Federation Blogs\ud83d\ude80 Get started today!We can't wait to see the amazing applications you'll build using Hasura and Native Queries. Get started today by signing up for Hasura Cloud and connecting to one of the supported databases.If you have any questions or need assistance, feel free to reach out to our team on Discord or GitHub."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Input Validation Permissions on Hasura: Enhancing data integrity and security Description: To empower developers with more control over their data validation process, Hasura is thrilled to announce the launch of a powerful new feature \u2013 Input Validations! Author: Sooraj Sanker"}, {"role": "assistant", "content": "In today's data-driven world, building applications with strong data integrity and security is challenging. Ensuring that the data being processed is accurate, valid, and aligned to predefined rules is a critical aspect of modern application development. To empower developers with more control over their data validation process, Hasura is thrilled to announce the launch of a powerful new feature \u2013 Input Validations!Why data integrity and security matterData integrity refers to the accuracy, consistency, and reliability of data stored and processed within an application. Ensuring data integrity is essential for a number of \u00a0reasons:Reliable decision-making: Accurate data enables sound decision-making, leading to better business outcomes and customer experiences.Data consistency: Inconsistent data can cause errors, leading to confusion and incorrect results.Data security: Validating input data helps prevent security vulnerabilities, such as SQL injections, and safeguards sensitive information.Compliance and regulations: Many industries have strict data compliance and privacy regulations, making data integrity a legal requirement.Maintaining data integrity and security can be a challenging task, especially in complex applications with numerous data mutations and interactions. Input Validations provide a robust solution to tackle these challenges.Introducing Input ValidationsHasura's Input Validations allow developers to implement custom data validation logic for GraphQL mutations. This feature acts as a pre-mutation hook, offering developers the ability to validate input arguments before executing insert, update, or delete operations. By defining rules and constraints on an HTTP service or a serverless function that can be pointed from Hasura as input validation endpoint, developers can ensure that only valid data is processed and stored in the database.How Input Validations workWhen a GraphQL mutation arrives, targeting specific tables and roles, the Input Validations feature comes into action. The mutation arguments are routed to the defined HTTP webhook, where custom validation logic is executed. If the validation is successful, the mutation proceeds, and the data is processed as intended. On the other hand, if the validation fails, the mutation is aborted, and appropriate error messages can be relayed back to the client.You can set up an Input Validation rule for a role directly from the Hasura Console as easy as configuring a normal permission rule in Hasura.Let\u2019s take an example of allowing user sign ups with age >18 years and I\u2019m going to create a table as shown below. Head to the table permissions page and you will now see a new section as shown below, lets add an input validation configuration as shown.Now lets create a simple web server to validate the data inputs. Authoring the webhook validator service is quite easy:To approve an input: Resolve the HTTP request with a 200To reject an input: Resolve the request with 400 and a message payload, the message payload will be forwarded to the client as a GraphQL error message for referenceHere is a simple NodeJS server that handles the input from Hasura and validate the input only if the user is more than 10 years old.const express = require(\"express\");\nconst bodyParser = require(\"body-parser\");\n\nconst app = express();\napp.use(bodyParser.json()); // to support JSON-encoded bodies\napp.use(bodyParser.urlencoded({\n  extended: true\n}));\n\napp.get(\"/\", (req, res) => {\n  res.send(\"Server is running!\");\n});\napp.post(\"/validateNewUserData\", (req, res) => {\n  console.log(\"INPUT_\", req?.body?.data?.input);\n  const DOB = req?.body?.data?.input?.[0]?.DOB;\n  const age = ~~((new Date() - new Date(DOB)) / 31557600000);\n\n  console.log(\"Age\", age);\n  if (age > 10) {\n    res.send(\"SUCCESS\");\n  } else {\n    res.status(400).json({\n      message: \"User should have a minimum age of 10\"\n    });\n  }\n  return;\n});\n\napp.listen(8080, function() {\n  console.log(\"Server is running on 8080\");\n});Now let\u2019s test this by making a mutation on HasuraYou can find a sample repository with the express-js server data validator here: https://github.com/soorajshankar/Hasura-Input-Validation-Demo \u00a0Read more about the configurations of Input Validations in detail from our official docs page here.Use cases and problems solvedInput Validations can address a wide range of use cases and solve common challenges faced by developers:User registration and authentication: Validate user registration details like email addresses, usernames, and enforce password complexity rules.Managing user-generated content: Implement content guidelines and moderation for user-generated data, ensuring compliance and preventing inappropriate content.Enforce restrictions on allowing requests: Ensure that an order cannot be placed with more than a certain amount of an item.Hasura's Input Validations offers a powerful tool for developers to enhance data integrity and security in their applications. With this feature, developers can ensure that the data flowing through their systems is accurate, valid, and protected from security vulnerabilities.By configuring custom validation logic through HTTP webhooks, developers gain control over the entire validation process, enabling them to build more reliable and secure applications.Stay tuned for more in-depth insights into this exciting new feature! Sign up now to try Input Validations on your project. Sign up to start your journey with Hasura Cloud."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Incremental database migration with Hasura Description: Revisit the database migration process and see how Hasura helps incrementally execute the migration, tackling the uncertainties but still delivering the results on time. Author: Smit Shah"}, {"role": "assistant", "content": "Database migration is a complex, multiphase, multi-process activity. The challenges scale up significantly when you move to a new database vendor or across different cloud providers or data centers.Despite the challenges, sometimes data migration is the only way. So how do we peel back the layers of ambiguity, keep the downtime minimal, and ensure our user's experience does not suffer when embarking on this journey?I had faced a similar change in the past when I was at one of Asia/India\u2019s largest e-commerce shops. Our database storage and compute capacity had maxed out, we were using ~90%+ of our compute resources, and we had about two months left before running out of storage space.We had to act fast and execute the entire process within that two-month period while ensuring our business continued to grow. It was a stressful period with some leap-of-faith calculations to make it happen.In this post, I want to revisit the migration process and see how, with Hasura, we could incrementally execute the migration, tackling the uncertainties but still delivering the results on time.The problemWe want to migrate data from one SQL database to another SQL database vendor on the cloud, and we want to achieve this process with minimal disruption.The challengesWhat are the typical challenges we must solve for when moving to a new database vendor?Database schema migrationUsually, there are differences in data types, index types, and architectures between different vendors that need to be addressed before starting the migration.Some characteristics need functional/performance testing to be sure.These differences arise even if we move to a different vendor for the same types of databases (E.g., MySQL to PostgreSQL).However, these differences further magnify when moving from SQL to NoSQL store, where write and access pattern change considerably due to schema/schema-less changes.Data transformationDue to changes in schema and data types, one would need to transform the data before persisting.These ETL pipelines can become complex, as we need to reason schema by schema.It can be a mammoth task if the table count is large (20+) with a large amount of data.Data migrationThis step could be challenging if the databases are co-located or on different cloud providers.Inside the same data centers, the network bandwidth is usually plenty to move 100s of GB of data in a few mins to a few hours.However, the same cannot be said for migrating the data over the internet, there are many failure scenarios to consider, and the time it takes can be a couple of days.Performance/functional testingIf one is migrating all the data simultaneously, testing the new database's migration and functionality/performance is paramount.There could be a need for tweaking data types, indexes, or queries that might not work from older databases.This step requires a thorough review of the data modeling, and the application code, specific queries, and consistency guarantees must change when undertaking such migrations.API migrationWhen migrating data across databases, data access patterns with existing APIs become crucial.If it\u2019s a migration across a couple of days or weeks, some requests must be handled while pointing to the new database vendor, while prior data access must be redirected to the older database.If the data is being asked in multiple databases, we must handle cross-DB queries/joins.For writes, moving partial data could be challenging, as new inserts have to be written in the new datastore, while updates for old rows have to be redirected to the old DB for consistency.However, if one moves entire tables to the new database, the data access layer can handle many such challenges.An approach to solving DB migration challengesGiven the challenges above, how do we go about this problem statement?First, the data migration problem is also an API migration problem. If we can provide the flexibility of querying multiple data sources while running existing business logic, we can support migrations using an incremental approach. Here are the steps, using Hasura, to help address the uncertainties:Introduce Hasura and connect it to the old database.Connect existing API service with Hasura.The existing APIs will become a source of business logic and data validation for Hasura, and with that, we can start migrating some of the client API calls to Hasura.For GraphQL, we can use remote schemas, and for REST APIs, we can use actions.Once we've migrated some client API calls, we can slowly move all clients to Hasura. Post that, we can start replicating some table schemas to the new database vendor. There are many ways to replicate data across different data sources.Once those tables have been replicated successfully and after conducting functional and performance tests over the new data source, we can connect Hasura with the new database.Connect Hasura to the new database.With this, queries to new tables will be directed to the latest data source, rest of the queries will be redirected to the existing database.Make remote joins across databases.In case of joins between tables between two databases, we can use Hasura\u2019s Remote Joins feature, which allows us to fetch data from multiple data sources in a performant manner.With this, we have achieved a framework for incrementally migrating data to the new vendor. By having a strategy where we can move a few tables at a time, we can take our time to squash any issues that arise due to changes in the database vendor, slowly increasing our data footprint over time.Remove old data sources from Hasura.Once we have moved all our data, we can discard and disconnect the old data source from Hasura.ConclusionWe saw how using Hasura, we could have a clear strategy for migrating API, which allowed us to move data to the new vendor incrementally while causing minimal client disruptions.Using this approach, we can easily guard against any uncertainties that may arise when moving to a new database vendor while providing a fantastic experience to our API consumers.Sign up now for Hasura Cloud and get started for free!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The why of GraphQL Client Side Nullability in Examples Description: A quick exposition of how client side nullability can inform client component design through comprehensive examples. Author: Akshaya Acharya"}, {"role": "assistant", "content": "A terse exposition of how client side nullability can inform client component design through comprehensive examples.A nullable field can represent a value that may or may not exist.Client side nullability can be used to solve common issues when defining the data fetch for client side components by:Validating that all the fields the component expects are available in one shotSimplifying the types on fetched data from nullable types to non nullable types (especially elegant when using statically typed languages where you have to unwrap the value from an optional type).Modifying how errors or null values affect the returned fields based on bubble-up logicBubbling on the server (a recap)Let's set up an example server API to use in the next sections on client side component data fetches.On the server, every field is nullable by default; the GraphQL spec allows the server to return a \"partial response.\" A resilient API can resolve all the fields it is able to and return errors on the side. Errors can include system failures (network/database/code) or even authorization errors. Nullable by default also eases API evolution by ensuring that clients are responsible for validating fields in the response data.A null value in a field that is not nullable bubbles up to the nearest nullable field.For example, with the following query and types:# sample query 1 BASIC\nquery {\n  user {\n    email\n    profile { # nullable\n      picture\n      address { # not nullable\n        street  # not nullable\n        country\n      }\n    }\n  }\n}\n\n# server schema\ntype User {\n  email: String\n  profile: Profile # fields in graphql are nullable by default\n}\ntype Profile {\n  picture: String\n  address: Address! # not nullable\n}\ntype Address {\n  street: String! # not nullable\n  country: String\n}\n\nIf something went wrong while fetching the non-nullable \"street\" field, then the server would bubble up that error to the nearest nullable field \"profile\", and you would get a null profile field. For this query, either you get a profile with a street, or no profile at all. This is true even if \"picture\" was a valid value.// return when non-nullable street not resolved\n{\n    user: {\n        email: ...\n        profile: null // null bubbled up to first nullable field\n    }\n}\n\n// return if street value is resolved\n{\n    user: {\n        email: ...\n        profile: {\n            picture: ...\n            address: {\n                street: ... // street has a non null value\n                country: ...\n            }\n        }\n    }\n}\n\nMoving Control to the Client With Nullability Designators\n\n        Client side nullability introduces two new operators ! named \"required\" and ? named \"optional\" that the client can use to specify how the server should bubble up null errors.\n    \nSolution 1: Narrow optional data with smaller error boundary in a profile widgetThe client can force the server to return some data while marking other data as optional.# sample query 2 PROFILE WIDGET, smaller boundary\nquery {\n  user {\n    email\n    profile { # nullable\n      picture\n      address? { # not nullable, optional\n        street!  # not nullable, required\n        country\n      }\n    }\n  }\n}\n\nIn case of an error or null value while resolving the required field \"street\", the null value would propagate up to the closest optional field \"address\", not the closest nullable field. So this allows for a client side developer to override overzealous nullability requirements that the server specifies and get this response:// sample response 2 PROFILE WIDGET\n{\n    user: {\n        email: ...\n        profile: {\n            picture: ...\n            address: null // closest optional field is null\n        }\n    }\n}\n\nThe profile widget can now display a profile picture even when the address is invalid.Solution 2: Simplify data validation with larger error boundary in a location widgetThe client can indicate that either all or no data should be returned to simplify validation of the returned data.# sample query 3 USER LOCATION WIDGET, larger boundary\nquery {\n  user? { # optional\n    email\n    profile { # nullable\n      picture\n      address { # not nullable\n        street!  # not nullable, required\n        country\n      }\n    }\n  }\n}\n\nA null value for the required street field still propagates to the closest optional field, so we get this result:// sample response 3 USER LOCATION WIDGET\n{\n    user: null\n}\n\nThis means that the user location widget doesn't need to dig into the internals of the returned user data and validate that the fields returned as expected, the null at the optional field already captures the bubbled up error.Solution With RelayThe Relay library has had this for some time through the @required directive specific to the Relay library, and different from the client side nullability GraphQL spec.query {\n  user {\n    email\n    ... profileFragment\n  }\n}\n\nfragment profileFragment on users {\n  profile {\n    picture\n    address @required {\n      street @required\n      country\n    }\n  }\n}\n\nIf \"street\", an @required field is missing, the null value will bubble up to the first field that is not @required, but only within the same fragment i.e. the \"profile\" field.{\n    user: {\n        email: ...\n        profile: null\n    }\n}\n\nThis is because Relay likes to use fragments to locally scope data fetching requirements with data masking. An advantage with the Relay approach is that the directives are implemented on the client, the server does not need to support the @required directive, unlike the client side nullability spec.Relay is also looking to push the client side nullability spec further with Fragment Response Keys which define fragment composition boundaries for the client side nullability designators (GraphQL Fails Modularity on YouTube).Using this todayThe client side nullability spec is currently in the RFC stage, and Hasura does not support it today.You can however use the @required directive with Relay today, and Hasura can generate a Relay API. Check it out if the idea of having data requirements colocated with your components using fragments stands out to you. It also makes for a phenomenal developer experience.If you're using Hasura backed by a single PostgreSQL database, you don't really have to worry about the server returning unexpected null values.In a federated setup, with Hasura backed by multiple data sources across the network, something like client side nullability could help clients account for partial failures when resolving data. Hasura v3 will handle these scenarios more elegantly by allowing partial fetches even with some of the backing data sources down.From a client perspective, a developer probably wants to focus on nailing down data requirements for every component regardless of the status of the backing data sources.The GraphQL spec continues to evolve to solve real problems that developers encounter.Here are some active discussions for GraphQL features that are still in the works! GraphQL Working Group RFCsReferenceshttps://github.com/graphql/graphql-wg/blob/main/rfcs/ClientControlledNullability.mdhttp://spec.graphql.org/October2021/#sec-Handling-Field-Errorshttps://relay.dev/docs/next/guides/required-directive/https://hasura.io/blog/graphql-nulls-cheatsheet/Other discussions:https://github.com/graphql/graphql-wg/discussions/1009https://github.com/graphql/graphql-wg/discussions/994https://www.youtube.com/watch?v=SVx4HG2bhIISign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Harnessing the power of MuleSoft and Hasura Description: Take a deep dive into MuleSoft and Hasura, exploring their individual merits, discussing real-world use cases, and providing architectural insights on how these platforms can complement each other to meet the diverse needs of modern enterprises. Author: Akshay Bhat"}, {"role": "assistant", "content": "In the rapidly evolving world of API development and integration, organizations often encounter complex challenges that require a combination of powerful tools and technologies. Two such platforms, MuleSoft and Hasura, offer unique capabilities that can be harnessed together to create a comprehensive and efficient API ecosystem. In this blog post, we will take a deep dive into MuleSoft and Hasura, exploring their individual merits, discussing real-world use cases, and providing architectural insights on how these platforms can complement each other to meet the diverse needs of modern enterprises.Understanding MuleSoftMuleSoft, acquired by Salesforce, is a leading integration platform that enables organizations to seamlessly connect disparate systems, applications, and data sources. With its extensive library of connectors, MuleSoft simplifies integration challenges by providing out-of-the-box connectivity to various systems, including enterprise applications, databases, cloud services, and more. It offers a visual integration development environment that empowers developers to design, build, and manage APIs, ensuring efficient data flow across the enterprise. MuleSoft's robust capabilities include data transformation, API orchestration, message routing, and comprehensive monitoring and management tools.MuleSoft shines as an API gateway layer, enabling organizations to manage, secure, and control their APIs. Its versatile workflow capabilities streamline the integration process, ensuring seamless orchestration of data and processes. Beyond its API gateway functionality, MuleSoft's specialized B2B/EDI integration platform deserves a special mention, catering to the specific requirements of B2B and EDI integration use-cases. With MuleSoft, organizations can effortlessly connect systems, enhance API governance, and drive efficient workflows.Deep dive into MuleSoftMessage transformation: MuleSoft provides powerful data transformation capabilities, allowing you to easily convert data between different formats, protocols, and systems. With its graphical mapping editor and built-in data mapping functions, you can efficiently handle complex data transformation scenarios.Orchestration and workflow: MuleSoft's visual flow designer enables the creation of complex integration workflows and orchestration processes. It allows you to define conditional routing, error handling, and exception management, making it suitable for implementing sophisticated integration scenarios.Exploring HasuraHasura, on the other hand, is an open source data platform that provides instant GraphQL APIs over existing databases and existing REST and GraphQL endpoints. With Hasura, developers can rapidly build secure and performant APIs without writing complex boilerplate code. It acts as a powerful data access layer, automatically generating GraphQL APIs based on the database schema. Hasura's real-time capabilities, powered by GraphQL subscriptions, enable developers to build interactive and collaborative applications with instant updates. It also offers granular access control and authorization mechanisms, allowing developers to define fine-grained permissions and ensure data security and compliance.Hasura takes the spotlight as a data connectivity layer, revolutionizing API development and transformation. By seamlessly integrating with OLTP and OLAP databases, Hasura provides an automated solution for generating GraphQL APIs. With its powerful API integration capabilities, developers can quickly build robust APIs and effortlessly handle data connectivity challenges. Hasura simplifies the complex task of API integration and transformation, empowering developers to accelerate their development cycles and deliver exceptional user experiences.Deep dive into HasuraRapid API development: Hasura's strength lies in its ability to generate GraphQL APIs from existing databases rapidly. With its intuitive UI and CLI tools, you can easily design, build, and deploy APIs in minutes, significantly accelerating your development cycles.Real-time capabilities: Hasura offers real-time GraphQL subscriptions, enabling instant updates and real-time data synchronization. This feature is particularly useful for applications requiring live updates, collaborative features, or notifications.Fine-grained data access control: Hasura provides granular access control and authorization mechanisms, allowing you to define permissions and policies at the API layer. This ensures that only authorized users have access to specific data, enhancing security and compliance.Real-world use cases and architectureLet's explore three real-world use cases where the combination of MuleSoft and Hasura can be particularly powerful:1. \u00a0E-commerce integration: MuleSoft can connect e-commerce platforms, inventory systems, payment gateways, and shipping providers, ensuring a seamless customer experience. Hasura can be used to provide instant GraphQL APIs over the product catalog and inventory databases, enabling real-time inventory updates and personalized product recommendations.2. \u00a0Healthcare data integration: MuleSoft can integrate patient management systems, electronic health records (EHR), and billing systems, streamlining data exchange and ensuring compliance with healthcare standards. Hasura can serve as the data access layer, allowing healthcare providers to query patient data through GraphQL APIs securely.3. \u00a0Financial services integration: MuleSoft can connect banking systems, payment processors, and customer relationship management (CRM) platforms, facilitating secure and efficient financial transactions. Hasura can be used to provide real-time insights into customer transactions and account balances, enabling personalized financial dashboards or fraud detection systems.ArchitectureIn the above architecture, the client and the application layer directly interacts with MuleSoft, which serves as the API gateway/layer and workflow platform. MuleSoft handles the routing, transformation, and management of API requests, providing a centralized point for API governance and control. MuleSoft also does the integration of APIs that Hasura doesn\u2019t work with or which MuleSoft handles best. This includes the workflow management and B2B EDI integration APIs.Hasura, on the other hand, is positioned as the data connectivity layer. It seamlessly connects to OLTP and OLAP databases and provides the ability to perform API integration and transformation. Hasura simplifies API development by auto-generating a GraphQL API layer from existing databases, enabling efficient data access and manipulation.The databases, represented in the architecture, can be OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing) databases. Hasura can handle both types, allowing developers to connect to and query the databases using GraphQL APIs.This architecture highlights the clear division of responsibilities between MuleSoft and Hasura, with MuleSoft focused on API gateway functionality and workflow management, while Hasura excels in data connectivity, API integration, and handling OLTP and OLAP databases.When to use Hasura with MuleSoft Let\u2019s look at the main scenarios where it makes sense to use Hasura with MuleSoft:1. \u00a0Simplified Data Access: When your primary focus is on efficient data access and manipulation, especially with real-time requirements, Hasura's GraphQL engine offers a powerful solution. It is perfect for applications that necessitate real-time updates, collaborative editing, or interactive data querying.2. \u00a0Hybrid Approaches: In certain cases, a combination of MuleSoft and Hasura can be beneficial. For instance, you can leverage MuleSoft's integration capabilities to connect disparate systems and use Hasura as a data access layer, providing a unified GraphQL API over the integrated data sources. This approach combines the best of both platforms to deliver a seamless and efficient solution.Combining MuleSoft and HasuraWhen considering the combination of MuleSoft and Hasura, it's essential to understand their complementary strengths and architectural implications. Here are some key scenarios where their combined usage can prove beneficial:1. \u00a0Hybrid Integration Scenarios: MuleSoft excels in integrating heterogeneous systems, enabling seamless communication between various enterprise applications, cloud services, and databases. In scenarios where real-time data access and manipulation are required, Hasura can be used as a data access layer for instant GraphQL APIs over the existing databases. This combination allows for efficient data integration and real-time capabilities while leveraging MuleSoft's comprehensive connectivity and transformation features.2. \u00a0Microservices Architecture: In a microservices architecture, MuleSoft can serve as the integration layer, orchestrating communication between different microservices and providing a unified API gateway. Hasura can be used within the microservices themselves, generating GraphQL APIs to access and manipulate the respective microservice's data. This approach allows for decoupled microservices with their own GraphQL APIs, while MuleSoft ensures seamless communication and integration between them.3. \u00a0Accelerating API Development: Hasura's automatic API generation capabilities provide developers with a rapid development experience. By integrating Hasura with MuleSoft, developers can leverage MuleSoft's powerful transformation and connectivity features to enrich the data returned by Hasura's GraphQL APIs. This combination allows for accelerated API development while ensuring data consistency, enrichment, and adherence to enterprise standards.4. \u00a0Real-time Data Synchronization: Hasura's real-time capabilities make it ideal for scenarios where real-time data synchronization is required, such as collaborative applications or real-time dashboards. MuleSoft can integrate with Hasura to fetch and transform data from various sources, and Hasura's real-time subscriptions can enable instant updates and real-time notifications to connected clients.Comparing MuleSoft and HasuraTo better understand why combining Hasura and MuleSoft is so powerful, let\u2019s compare the two systems. The contrast shows us how complimentary they really are:FeaturesMuleSoftHasuraIntegration typeEnterprise Integration PlatformData Platform with auto-generated APIsConnectivityExtensive connectors for various systems and protocolsConnects to existing databases for API generationMessage transformationPowerful data transformation capabilitiesDirectly maps database schema to GraphQL APIOrchestrationRobust visual flow designer for complex integration workflowsN/A (Focuses on API generation and real-time features)Real-time capabilitiesLimited support for real-time updates and notificationsGraphQL subscriptions for real-time data synchronizationRapid API developmentRequires configuration and development effortAuto-generates GraphQL APIs with a minimal development effortData access controlProvides fine-grained access control and authorization mechanismsOffers granular control over API permissions and policies with ABAC and AuthorizationScalabilityDesigned for complex enterprise integrationsSuitable for projects of all sizes, including startups and smaller applicationsLearning curvesSteeper learning curve due to its extensive feature setRelatively easier to learn and get started withUse casesLarge-scale enterprise integrations involving diverse systemsModern applications requiring real-time updates and collaborative featuresWhen comparing MuleSoft and Hasura, it's essential to understand their differences in focus and functionality. MuleSoft shines in enterprise-level integrations, offering extensive connectors, data mapping, and transformation capabilities. It excels in large-scale deployments, legacy system modernization, and hybrid cloud scenarios. Hasura, on the other hand, is tailored for efficient data access and manipulation through GraphQL. It provides real-time capabilities, automatic schema generation, and authorization mechanisms.ConclusionMuleSoft and Hasura, with their distinct capabilities, can be harnessed together to create a comprehensive and efficient API ecosystem. MuleSoft excels in complex integrations, enterprise-level security, and large-scale deployments. Hasura simplifies data access and manipulation through GraphQL, providing real-time capabilities.By combining MuleSoft's powerful integration capabilities with Hasura's rapid API development and real-time capabilities, organizations can achieve enhanced agility, seamless data flow, and improved user experiences. By architecting solutions with MuleSoft and Hasura and adding their complementary strengths to align them with your specific use cases, you can unlock new possibilities in API development and integration, empowering your organization to innovate and thrive in the digital era.Sign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Breaking up monolith into microservices with Hasura Description: In this blog post, we will explore how Hasura can help break up a monolithic application into microservices, enabling a more efficient and scalable architecture. Author: Akshay Bhat"}, {"role": "assistant", "content": "Unleashing scalability and agilityAs applications grow in size and complexity, the monolithic architecture that once served them well can become a bottleneck. Monolithic applications are difficult to scale, maintain, and deploy. Microservices, on the other hand, offer a more flexible and scalable architecture that allows for independent development and deployment of smaller, focused components. In this blog post, we will explore how Hasura can help break up a monolithic application into microservices, enabling a more efficient and scalable architecture.Understanding monoliths and microservicesBefore diving into the migration process, it is important to understand the key differences between monolithic and microservices architectures. In a monolithic architecture, the entire application is built as a single, tightly-coupled unit. This makes it difficult to isolate and scale individual components independently. On the other hand, microservices architecture breaks down the application into small, autonomous services that can be developed, deployed, and scaled independently.ArchitectureLet\u2019s look at the detailed process of breaking a monolithic application into microservices step-by-step:Step 1: Identify microservice boundariesThe first step in breaking up a monolith is to identify the boundaries of your microservices. This involves analyzing the existing monolithic application to identify cohesive and loosely coupled modules that can be decoupled and deployed as independent services. Each microservice should have its own bounded context and serve a specific business capability.Step 2: Create GraphQL APIs with HasuraHasura provides a powerful GraphQL engine that can act as a gateway to your microservices. Instead of exposing each microservice's API directly to clients, you can create a unified GraphQL API using Hasura. Hasura simplifies this process by automatically generating the GraphQL schema based on the underlying data sources. You can leverage Hasura's console or configuration files to define relationships, access control rules, and custom business logic.Step 3: Extract microservices from the monolithWith the microservice boundaries defined and the GraphQL API set up, you can start extracting individual microservices from the monolithic application. This can be done incrementally by identifying a well-defined module or functionality within the monolith and moving it into its own service. Hasura's GraphQL API acts as a bridge between the existing monolith and the newly created microservices, allowing for seamless integration and coexistence.Step 4: Refactor and redesignDuring the extraction process, you may need to refactor and redesign certain components to ensure they align with microservices principles. This may involve restructuring databases, revisiting domain models, and optimizing communication patterns between microservices. Hasura's powerful data modeling capabilities, such as handling relationships and custom resolvers, can assist in this process.Step 5: Deployment and scalabilityOnce the microservices are extracted and properly designed, you can deploy them individually using your preferred deployment strategy. Hasura integrates well with popular containerization technologies like Docker and orchestration platforms like Kubernetes, making it easier to deploy and scale microservices. Each microservice can have its own independent deployment pipeline, enabling continuous delivery and reducing the risk associated with deploying changes.Step 6: Monitor and maintainMonitoring and maintaining microservices is crucial to ensure their health and performance. Hasura provides monitoring and logging capabilities that can be leveraged to gain insights into the usage, performance, and error handling of your microservices. By utilizing these tools, you can proactively identify and address any issues that may arise, ensuring the smooth operation of your distributed application.ConclusionBreaking up a monolithic application into microservices can be a complex undertaking, but with the help of Hasura, it becomes more manageable. Hasura's GraphQL engine simplifies the creation of a unified API layer for your microservices, enabling seamless integration and coexistence with the existing monolith. By following the steps outlined in this blog post, you can transform your monolithic application into a scalable and flexible microservices architecture, unlocking the benefits of independent development, deployment, and scalability.Sign up now for Hasura Cloud to get started for free!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The complexity of building a GraphQL API permissions layer and how Hasura solves this Description: We will see what it takes to build authorization logic for a DIY GraphQL server to understand how it is more complex. We will also compare it with Hasura and how it solves this complexity by being declarative and leveraging predicate pushdown. Author: Praveen Durairaju"}, {"role": "assistant", "content": "IntroductionAPI security breaches are on the rise. Gartner predicts that by 2025, insecure APIs will account for more than 50% of data theft incidents. As enterprises continue to embrace an API-driven approach to software development (for all the benefits it brings), arming their developers with the tools to build secure APIs with proper data access and authorization logic needs to be a priority.Building an authorization layer involves many factors. In GraphQL, authorization belongs in the business logic layer and not typically inside resolvers. It is more complex to write an authorization layer for GraphQL APIs than REST APIs. In this blog, we will see what it takes to build authorization logic for a DIY GraphQL server to understand how it is more complex. We will also compare it with Hasura and how it solves this complexity by being declarative and leveraging predicate pushdown.Here's a quick TL'DR and summary before diving deep into this post.GraphQL AuthZ MethodsWithout HasuraWith HasuraPredicate pushdownVery hard to implement predicate pushdown. Data is fetched first and then filters are applied.Authorization is based on predicate pushdown approach. Rules are embedded in the SQL query generated to get exactly the data required.API-wideWrite custom logic code to throw unauthorized access error.Configure API limits based on user role / IP / requests per second / GraphQL query depth etc.Resolver-basedPass context info to resolvers and manually write logic to authorize requests for every field / resolverThere are no resolvers. Declaratively configure role based access control without any code involved.Custom directivesWrite logic for a custom directive like @auth and authorize requests based on explicit requests from client.Not applicable and not necessary. JWT or webhook mode has enough information about the request to apply AuthZ logic.Outside GraphQLCan only pass the header information to the downstream REST APICan pass header information. Can also define role-based permissions for queries and mutations that map to a REST API.Authorization in federationUse GraphQL schema models to write custom authorization rules.Again, there is no code involved for authorization. Declaratively define rules for cross-source authorization / federation.Key items to considerData modelingData models contain key information about the types of data, relationships between data. This is used to determine the kind of authorization system that needs to be built. For example, you have users of the application that can view public data of all users and certain private data of them. The data model now contains certain table columns relevant to this and also relationships that are relevant to this private data.Roles and attributesThe common ways to model your authorization system is through RBAC (role-based access control and ABAC (attribute-based access control).You could start defining roles for the application and list out use cases and privileges for each. RBAC could be flat, hierarchical, constrained and symmetrical. Authorization can also be modeled based on attributes where the user who logs in to the application will have certain attributes (type of user, type of resources, and the environment in which they are accessing) that can be checked for allowing access.Nested rulesThe GraphQL schema may or may not be a direct map to the data models. In some cases, the data model extends to multiple data sources. Even within the same data source, the GraphQL query could be nested with multiple fields spanning relationships. Applying authorization logic contextually to the nested query is vital.For example:query {\n  users {\n    id\n    name\n    orders { // apply permission rule here too\n      id\n      order_total\n    }\n  }\n}\nHere the orders is part of the users relationship and is nested. But the authorization logic needs to apply for both users, for orders, and for any level of nesting of the query, contextually.PerformanceIdeally, authorization checks shouldn\u2019t add a lot of overhead to the response latency. In reality, when you are writing a custom GraphQL server yourself, checks are done after the data fetching operation and there\u2019s a lot of unnecessary data fetched, which slows down the database if you consider millions of requests which slows down API performance as a result.Predicate pushdownIf the authorization logic is heavily dependent on the data being fetched, it is important to do a predicate pushdown of the Authorization rules. For example: If you are fetching orders placed by a user on an e-commerce app, you need to be able to apply the rule in the query that goes to the database to fetch only the orders placed by the user requesting them. This is not only faster, but also the most secure way to apply authorization logic.Building a DIY authorization layer for a GraphQL APIYou can build an AuthZ layer using middleware libraries. The maturity of AuthZ libraries in GraphQL depends on the language or framework you are in. Why is building an authorization layer complex?When you are writing your own GraphQL server with custom authorization rules, there are a number of methods to write this logic, depending on the use case. You have:API-wide authorizationResolver-based authorizationSchema / model-based authorizationThe resolver-based authorization quickly balloons into a lot of boilerplate code if authorization rules are applied to every field. Even with repeated rules and patterns that can be applied, it can easily expand to thousands of lines of code to secure your application. And this code becomes difficult to maintain.In the schema-based authorization, the authorization logic is dependent on the GraphQL schema and becomes independent of the underlying database or data fetching libraries and ORMs.Authorization in GraphQL is typically built using Context object that is available on every request. The context is a value that is provided to every resolver and is created at the start of a GraphQL server request. This means you can add authentication and authorization details to the context, such as user data.Here\u2019s how a typical AuthZ context gets passed in a DIY GraphQL Server written in Node.js:Parsing and validating JWT tokenThe first step is to parse and validate the incoming JWT token. Once the JWT token is verified, you can pass it to the context. We are using JWT tokens as an example here, because it is universal and works across platforms.An example request:Endpoint: myapp.com/v1/graphqlHeaders: Authorization: <Bearer token>The Authorization header is parsed, validated, and verified. The underlying authentication service used could be any solution that issues JWT. Here\u2019s some example code that verifies a JWT token:try {\n    if (token) {\n        return jwt.verify(token, YOUR_SECRET_KEY);\n    }\n    return null;\n} catch (err) {\n    return null;\n}Here\u2019s an example taken from a DIY GraphQL Server (Apollo) to parse the context:Create a new instance of Apollo Server by passing in the type definitions and resolvers.const server = new ApolloServer < MyContext > ({\n    typeDefs,\n    resolvers,\n});Create a standalone server that retrieves the token from headers and returns the user information as context.const {\n    url\n} = await startStandaloneServer(server, {\n    // Note: This example uses the `req` argument to access headers,\n    // but the arguments received by `context` vary by integration.\n    context: async ({\n        req,\n        res\n    }) => {\n        // Get the user token from the headers.\n        const token = req.headers.authorization || '';\n        // Try to retrieve a user with the token\n        const user = await getUser(token);\n        // Add the user to the context\n        return {\n            user\n        };\n    },\n});Passing contextOnce the token is extracted, you need to pass the context object to every resolver that will get executed. Now all of your resolver code gets access to the context.In the above example, we can see that the user data is passed to the context.API-wide authorizationThere are a few rules that might need to be enforced at the request level, even before passing it on to the GraphQL resolvers. For example, you could block a user from performing any queries and return a 401, unauthorized error. Again, this involves code logic and the logic could become a lot of boilerplate if there are many rules.Resolver-level authorizationAs the context gets attached to each resolver, it is now possible to authorize the request inside the resolver. This method is only suitable for basic authorization logic when there are only few resolvers and few rules to check and authorize users.For example: You have a users field that returns a list of user names. You will end up writing code, which looks something like this:users: (parent, args, contextValue) => {\n    // In this case, we'll pretend there is no data when\n    // we're not logged in. Another option would be to\n    // throw an error.\n    if (!contextValue.user) return null;\n    return ['bob', 'jake'];\n};The contextValue is now available for parsing and authorizing the user.Note: This is resolver logic for one field. Imagine repeating logic code in every single resolver and field. It is very challenging to scale this. It is also very challenging to update any logic quickly.GraphQL schema-based authorizationIn a large schema with plenty of data based authorization, there are patterns of rules that are applicable for multiple queries. For example: Allow the user to fetch their own data and not any one else.Here\u2019s an example from GraphQL AuthZ library using schema as a data source.// using schema as a data source inside pre-execution rule\nconst CanPublishPost = preExecRule()(async (context, fieldArgs) => {\n    const graphQLResult = await graphql({\n        schema: context.schema,\n        source: `query post($postId: ID!) { post(id: $postId) { author { id } } }`,\n        variableValues: {\n            postId: fieldArgs.postId\n        }\n    })\n    const post = graphQLResult.data?.post\n    return post && post.author.id === context.user?.id\n})If you look at the return statement at the end in the above code snippet, that\u2019s where the logic of checking user ID is written.How does Hasura\u2019s authorization layer work?Hasura has a powerful authorization engine that allows developers to declaratively define fine-grained permissions and policies to restrict access to only particular elements of the data based on the session information in an API call.Implementing proper data access control rules into the handwritten APIs is painstaking work. By some estimates, access control and authorization code can make up to 80% of the business logic in an API layer. Securing GraphQL is even harder because of the flexible nature of the query language. Hasura radically simplifies the effort needed to build authorization logic into APIs.DeclarativeWith Hasura, you can transparently and declaratively define roles, and what each role is allowed to access in the metadata configuration. This can be done either through the Hasura Console or programmatically through the Hasura CLI. This declarative approach to authorization is simpler to create, maintain, evolve, and audit for developers and security teams.Fine-grained access controlHasura supports a role-based access control system. Access control rules can be applied to all the CRUD operations. You define permissions granularly on the schema, sessions, and data (table, row, and column).For every role you create, Hasura automatically publishes a different GraphQL schema that represents the right queries, fields, and mutations that are available to that role. Every operation will use the request context to further apply permissions rules on the data.Authorization rules are conditions that can span any property of the JSON data graph and its methods:Any property of the data spanning relationships. Eg: Allow access if \u201cdocument.collaborators.editors\u201d contains \u201ccurrent_user.id3\u20444Any property of the user accessing the data. Eg: Allow access if accounts.organization.id is equal to current_user.organization_ioRules can mask, tokenize, or encrypt portions of the data model or the data returned by a method and are labeled. These labels are called \"roles.\"Easily integrate with your authentication systemAuthentication is handled outside of Hasura, and you can bring in your own authentication server or integrate any authentication provider that supports JSON Web Token (JWT). If your authentication provider does not support JWT, or you want to handle authentication manually, you can use webhooks.\"By using Hasura, we cut the development time in half and built our product in three months. The built-in role-based authorization system made it easy to secure our data.\"Mark Erdmann, Software Engineer, PulleyPredicate pushdownHasura automatically pushes down the authorization check to the data query itself, which provides a significant performance boost and cost savings by avoiding additional lookups and unnecessary data egress, especially at larger scale. Hasura automates predicate pushdown, since it is essentially a JIT compiler that can dynamically apply the filter in where the clause of a SQL query is based on the user running the query. Most GraphQL server frameworks require you to write plenty of boilerplate code to achieve the predicate pushdown authorization check.Cross-source authorizationHasura integrates authorization rules based on data and entitlements in different sources. Hasura forwards the resolved values as headers to your external services, which makes it easy to apply authorization rules in your external service. Again, this is made possible by Hasura\u2019s declarative authorization system.OWASP Top 10OWASP is most famous for the \u201cTop Ten\u201d framework for structuring secure applications. As the industry expands into a microservice-driven approach, it\u2019s important for organizations to validate all of their dependencies according to the OWASP framework.Hasura\u2019s security-first approach ensures that the Top 10 security features and criteria are fulfilled. Read more: How Hasura addresses the OWASP Top 10 concerns.When you are building your own GraphQL server and writing authorization logic, you will need to ensure that the Top 10 concerns are handled to be secure and compliant.Try out HasuraSign up to start your journey with Hasura Cloud today. If you are an enterprise looking to learn more about how Hasura fits in your app modernization strategy, reach out to us through the Contact Us form and our team will get back to you."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Boosting database interactivity and developer productivity with Hasura Native Queries and Logical Models Description: Hasura's Native Queries feature provides a powerful tool for enhancing your GraphQL API with the flexibility and control of raw SQL queries. Author: David Meleney"}, {"role": "assistant", "content": "API development has seen a significant surge in demand as organizations across the globe strive to harness the power of data. The Hasura GraphQL engine automates API creation from existing database objects like tables, views, functions, and stored procedures. For developers, this automation leads to significant time savings, freeing them from the hassles of manually developing APIs from scratch. It also brings in the benefits of GraphQL, such as type safety, real-time subscriptions, and performance enhancements.Why Use Native Queries and Logical ModelsThe Hasura GraphQL Engine has gained popularity for its ability to automatically generate a GraphQL API around database objects, providing seamless querying, mutating, and subscribing to data changes. However, there are instances when you require more custom or advanced functionality that goes beyond the out-of-the-box capabilities. For example:\u200b\u200bUse the full power of SQL that Hasura might not provide access to through the typical table API, such as GROUP BY, window functions, or scalar functions.Provide custom arguments to the users of your API to greatly expand its flexibility.Encapsulate sophisticated filtering with a query, allowing your users to provide a single argument rather than having to understand how to manipulate the data.Work with the advanced features of your database to improve performance.Write a compatibility layer around tables, making it easier to change your API without breaking existing clients.Reduce duplication by moving common data manipulation into one place.Hasura's Native Queries feature provides a powerful tool for enhancing your GraphQL API with the flexibility and control of raw SQL queries. By leveraging Native Queries, you can create custom and advanced behavior in your Hasura-generated GraphQL schema without the need for additional database objects or DDL privileges. This enables you to unlock the full potential of SQL while building robust and efficient applications with Hasura.How Native Queries workThe Hasura GraphQL Engine integrates seamlessly with Native Queries. Here's an overview of how it works:Defining Native QueriesTo create a Native Query, you write raw SQL statements directly within Hasura. This query like the one shown below can use arguments using the syntax {{argument_name}}. These queries can be as simple or complex as needed, incorporating SQL features such as joins, aggregations, and custom business logic.Binding to GraphQL Schema using Logical ModelsAfter defining a Native Query, you can bind it to your Hasura-generated GraphQL schema. This automatically exposes the Native Query as a GraphQL API endpoint, making it accessible to your application.Executing Native Queries: When a request is made to the Native Query endpoint via GraphQL, the Hasura GraphQL Engine translates the GraphQL query into an SQL query and executes it against the database. The results are then transformed into GraphQL response format and returned to the client.<root field name>(\n[args: {\"<argument name>\": <argument value>, ...},]\n[where: ...,]\n[order_by: ..., distinct_on: ...,]\n[limit: ..., offset: ...]\n) {\n<field 1>\n<field 2>\n...\n}\n}```\nDevelopers can leverage Native Queries using the Hasura GraphQL API and all the features that come with it \u2013 this includes pagination, filtering, sorting, caching, etc.BenefitsThis is where Native Queries come into play, empowering you to leverage the full power of SQL within Hasura while maintaining flexibility and control over your GraphQL schema. Native Queries enable you to automatically generate a GraphQL API around raw SQL queries, offering a range of benefits:Flexibility: By utilizing Native Queries, you can incorporate custom SQL logic into your GraphQL schema, allowing for complex database operations and tailored responses. This flexibility enables you to meet specific requirements that may not be achievable through standard GraphQL queries.Control: Native Queries give you greater control over your Hasura-generated GraphQL schema. Instead of relying solely on the automatic generation of GraphQL API from the database schema, you can shape the GraphQL schema around your SQL queries, providing a more fine-tuned interface for interacting with your data.Avoiding DDL Privileges: With Native Queries, you no longer need to create additional database objects that require Data Definition Language (DDL) privileges. This simplifies the development process by reducing the dependencies and permissions required for deploying and maintaining your application.Abstraction: By defining the data model in Hasura, it allows you to define a data schema that best fits the needs of your GraphQL API, decoupling it from the underlying database schema. This enables independent evolution of the API and the database structure.Cross-Database Compatibility: Modeling data in the middleware layer can facilitate cross-database compatibility. The middleware can act as an abstraction layer, translating GraphQL queries into the appropriate database-specific queries for different target databases. This allows for greater flexibility in choosing and changing the underlying database technology.In the realm of GraphQL development, choosing the right approach for modeling data is crucial for building efficient, scalable, and flexible applications. Traditionally, developers have relied on creating database artifacts like views, user-defined functions (UDFs), and stored procedures. However, solutions like Hasura represent a paradigm shift in the way data is leveraged by app developers and other data consumers across organizations. Native Queries and Logical Models are a game-changer, they empower developers to unlock unprecedented agility and flexibility in their GraphQL solutions.LimitationsNative Queries currently supports read-only query capabilities for Postgres, SQL Server, and BigQuery. Support for more databases and mutations will be coming soon.ConclusionModeling data in Hasura represents a paradigm shift in GraphQL development, enabling developers to embrace agility, flexibility, and efficiency. By leveraging Hasura's automatic API generation, flexibility in schema definition, rapid development capabilities, access control mechanisms, performance optimizations, and integration capabilities, developers can build scalable and resilient applications without being bound by the limitations of traditional database-centric approaches. As the GraphQL ecosystem continues to evolve, embracing tools like Hasura will undoubtedly become the preferred choice for developers seeking to unlock the full potential of GraphQL.\ud83d\udcda Documentation and ResourcesTo help you get started, we've prepared detailed documentation, guides, and examples:Postgres Native Queries DocumentationSQL Server Native Queries DocumentationBigQuery Native Queries\ud83d\ude80 Get Started Today!We can't wait to see the amazing applications you'll build using Hasura and Native Queries. Get started today by signing up for Hasura Cloud and connecting to one of the supported databases.If you have any questions or need assistance, feel free to reach out to our team on Discord or GitHub. Happy building! \ud83c\udf89"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: \u201cWe got documentation feedback on\u2026transferring wealth?\u201d Description: We see the popularity of our docs site as a template as an extension of our philosophy to ensure users find what they need quickly and get back to building with Hasura!  Author: Rob Dominguez"}, {"role": "assistant", "content": "A few weeks ago, I was plugging away at something significant (read: procrastinating by looking for ways of automating some repetitive task in an inane way) when I heard the familiar \"pop-pop-pop\" notification from Slack. Like many of you, Slack sits permanently open on my machine, every ping like a mosquito needing to be swatted away. However, this one got my attention.In our #docs-feedback channel, which we connected to the feedback component at the bottom of every docs page, a new message came through:New feedback on this page: <company_redacted> Transfers | Wealth DocsI'm relatively neurotic, so the spike in cortisol from a near panic attack was significant, as my first thought was that we'd been hacked. How? Rationality is absent in fight-or-flight mode. \ud83e\udd37\u200d\u2642\ufe0fIt quickly became clear this wasn't a malicious attempt at hijacking our component or site \u2013 rather, this was someone so happy with our docs that they decided to copy them verbatim, leading to their docs feedback arriving on our Slack channel. We were flattered, and it quickly became an internal joke.Then, it happened again. With another company.Flattered as we were, we couldn't have other sites' feedback clogging our channel or dragging our average down inflating our KPIs. To mitigate this, we introduced a tiny bit of logic that prevents the input from going through, if not from our domain. Could you bypass this and still hit our API? Of course, but please don't. You surely have better things to do.Our philosophy on docsAs Hasura\u2019s core is open source, so are our docs. We regularly receive contributions from you all (thanks for catching our spelling mistakes \ud83d\ude18). We are constantly improving our documentation to better serve you in building your projects and products. As we're sure you know from regularly visiting the docs wiki as part of your morning ritual while drinking coffee, you're no doubt familiar with the core of the docs team's philosophy around documentation:We hold a strict standard because we want to ensure our users can quickly find what they need, understand it because it's well-written, and get back to building with Hasura. \ud83d\ude80We see the popularity of our docs site as a template as an extension of that philosophy! And we want you to keep using the docs site as your starting point for your next project's documentation.Get your own Hasura docs!We have an easy one-liner below if you want to get started with our docs site. This command will clone the hasura/graphql-engine repo and then remove every folder except the most important. \ud83d\ude09git clone --depth 1 --filter=blob:none --sparse https://github.com/hasura/graphql-engine.git && cd graphql-engine && git sparse-checkout init && git sparse-checkout set docs && find . -type f ! -path \"./docs/*\" -deleteThen, git init and have some fun building on top of our docs!BONUS: Be on the lookout for our new v3-docs site\u2026coming soon! And, yes: It will still be open sourced for your nitpicking pleasure. \ud83e\udd73\u2764\ufe0f Hasura Docs Team"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: What's new in Hasura Caching! Description: We are thrilled to announce a set of exciting improvements to Hasura's caching capabilities. With these latest updates, we aim to provide you with even better performance and observability, ensuring a seamless experience while working with your data.  Author: Sooraj Sanker"}, {"role": "assistant", "content": "We are thrilled to announce a set of exciting improvements to Hasura's caching capabilities. With these latest updates, we aim to provide you with even better performance and observability, ensuring a seamless experience while working with your data. Let's dive into the details of what's new in Hasura Caching.Caching now available on Hasura Cloud FreeWe understand the importance of caching for optimizing your application's performance. To empower developers, we have enabled caching on the Hasura Cloud Free tier. You can now use caching with a generous 100 MB storage limit, free of charge.Redis clusters for caching (Enterprise Edition)For those seeking enterprise-grade caching solutions on your self-hosted setup, we're delighted to announce support for Redis clusters in Hasura's caching system. Redis clusters offer unparalleled scalability and high availability, ensuring your caching infrastructure handles even the most demanding workloads. With this new addition, you can elevate your caching capabilities to new heights.Note: Hasura Cloud is already tuned for variable workloads and extended caching capabilities, and Redis cluster support is a powerful addition to Enterprise Edition self-hosted users.Improved caching on Remote Schemas and ActionsWe understand that caching data with proper authorization should not be limited to just database responses. Now, you can efficiently cache responses from remote schemas and actions even if you have forward_client_headers enabled. Usually, this is the way to pass down the user context to the remote schema or actions server so that the external server can understand user/role context and generate responses accordingly. With the new enablement, now Hasura will securely cache responses for each header configuration and make sure the cached data will only be available to the same header requests.With the ability to cache remote schema responses and Actions, you can reduce the latency associated with fetching data from external sources. This provides faster responses to your clients and also reduces the load on the remote schema or actions server when there is a cacheable data response. Take advantage of this enhancement to deliver a more responsive and seamless user experience. \u00a0Improved observability with Prometheus metricsEnsuring observability is crucial for understanding the performance and behavior of your applications. To assist you in this endeavor, we added more caching-related Prometheus metrics. This provides comprehensive insights into your caching system's efficiency, allowing you to monitor cache hit/miss rates, and other crucial performance indicators. With enhanced observability, you can effectively optimize your caching strategies.More details on individual caching metrics are documented here. We believe these enhancements will significantly improve your experience with Hasura's caching capabilities. Whether you are a developer on the Hasura Cloud Free plan, or an enterprise user leveraging Redis clusters, these updates are designed to enhance your application's performance and provide the necessary tools to monitor and optimize your caching infrastructure.For detailed instructions and more information, be sure to check out our updated documentation.Start exploring the new features today and unlock the true potential of caching in Hasura! \u00a0Sign up to start your journey with Hasura Cloud."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: API Automation in Healthcare with Hasura Description: In this post, we will look at how low-code API approaches like Hasura helps accelerate the modernization journey of applications in the healthcare space. Author: Praveen Durairaju"}, {"role": "assistant", "content": "IntroductionInteroperability, security and privacy are essential parameters in modern healthcare information systems. Records are maintained in multiple data stores and formats. There are legacy systems in different facilities. Building an app for a modern-day healthcare solution requires the complex wiring of these legacy systems, different formats of data and standards and automating them through a data API layer that is secure, highly performant and can be consumed easily by developers across different organizations.Hasura is a Data API Platform that removes the tedious parts of building and operating production data APIs \u2013 reducing cost, complexity, and time to market. Hasura connects to databases (legacy or modern, existing or new) and generates GraphQL APIs instantly.In this post, we will look at how low-code API approaches like Hasura helps accelerate the modernization journey of applications in the healthcare space.Data Standards in Healthcare with Hasura API DemosHealthcare data management involves a variety of standards designed to ensure data interoperability, privacy, and security. Here are some common standards that are widely used by enterprises to manage data.HL7 v2Health Level Seven v2 (HL7v2) is a standard clinical messaging format for data exchange between medical information systems. Introduced in the early 1980s, it is still widely used for healthcare data worldwide.Considering the standard Electronic Medical Record (EMR) in a relational database like PostgreSQL, Hasura can be used to automate the CRUD APIs instantly. Hasura\u2019s eventing system helps with sending HL7 v2 messages between facilities managing patient records.Demo: Try out the HL7 v2 model with Hasura APIsFHIRFast Healthcare Interoperability Resources (FHIR) is a standard introduced by HL7 as an alternative to HL7 v2 and v3 standards. FHIR leverages RESTful standards for APIs.Hasura on top of an FHIR model will give you instant GraphQL CRUD along with a declarative Authorization layer and the benefits of a high-performance, unified data layer.Demo: Try out the FHIR model with Hasura APIsOMOP / OHDSIThe Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) is an open community data standard, designed to standardize the structure and content of observational data and to enable efficient analyses that can produce reliable evidence. OHDSI leverages the OMOP model to bring out the value of health data through large-scale analytics.Hasura when connected to a CDM in a relational database can specifically be beneficial in querying relational data via GraphQL with added advantage of security via the declarative Authorization layer.Demo: Try out the OHDSI model with Hasura APIsSNOMED - CTSNOMED Clinical Terms is a systematically organized collection of medical terms used in clinical documentation and reporting. The RF2 format comes with greater flexibility for config management of SNOMED terms.There are schema models representing SNOMED in popular relational databases such as PostgreSQL, MySQL, MSSQL etc. Connecting Hasura to this data model will take care of API automation, thereby allowing you to work on the literature use cases of SNOMED.Note: You have to import SNOMED - CT to the DB and connect the DB to Hasura to get the APIs instantly.Healthcare App ModernizationModernization initiatives are usually massive cross-org undertakings with significant time, money, and people investment. And, building (or rebuilding) the API layer to wire modern and legacy components together are often the slowest, most arduous part of the process. For example, consider the massive API development lift in a transformation project that decomposes a large monolith SQL Server with 100s of stored procedures to a database-per-service microservices model.In a healthcare app, some concerns around modernization could be:Moving from XML to JSON and other modern standards.Migrating from legacy data systems to a more modern database(s).API-driven app with standards like REST/GraphQL.Maintaining regulatory compliance despite the changes.Hasura supports modern API standards of REST/GraphQL with JSON. It can connect to your legacy databases* and help you incrementally migrate to modern databases tailored for specific workloads. Hasura is also compliant with industry security standards. Let us look at how Hasura helps you automate the data access layer.API AutomationAccording to the Postman 2022 State of API survey, 51% of developers' time is spent writing APIs. For certain types of modernization projects (for example, modernizing legacy APIs or building a federated API service), that percentage could increase.Even in the most tech-savvy organizations, developers are spending an inordinate amount of time building, maintaining, operating, and scaling the APIs. Furthermore, a huge chunk of the typical API development work is tedious, non-differentiated work with a big opportunity cost to it. All that time spent writing boilerplate API code is time not spent on building innovation or adding a competitive edge. Companies in the healthcare space are not necessarily tech-first and even within the ones which are, it is a lot of resources being spent on boilerplate API code.What does Hasura automate?With Hasura, you can go from a new or existing data source to a rich and flexible API in minutes. Once connected to a database, Hasura automatically generates a GraphQL endpoint with all the core GraphQL operations like queries, mutations, and subscriptions to cater for all CRUD use cases. You can instantly start querying with filtering, sorting, pagination, joins, pattern search, and much more. All the information needed to generate the API endpoint is stored in the Hasura Metadata, which lets you declaratively create simple or complex domain API models from underlying physical data models spread across one or more data sources. The Hasura Metadata can be dynamically configured in the Hasura Console or via the Hasura CLI, which lets you easily integrate Hasura into your enterprise CI/CD processes.\u201cIf we had gone the traditional way this process would have taken us 2-4 years. With Hasura we have been able to crunch it to just under a year. Achieving this timeframe in a highly regulated environment like healthcare is phenomenal.\u201d Karthik Srinivasan, Solution Architect, Philips HealthcareAPI Development with and without HasuraREST EndpointsIf you are coming from an existing legacy system and if you would like to maintain the tooling around it for monitoring, caching and other Ops side of things, you could leverage the RESTified endpoints feature that Hasura provides.Hasura can map a GraphQL query to an equivalent REST API query, so you can define the parameters for and slowly use REST APIs before transitioning to GraphQL across the team.Read more about RESTified endpoints with Hasura.\"By using Hasura we cut the development time in half and built our product in 3 months & built-in role-based authorization system made it easy to secure our data.\" Mark Erdmann, Software Engineer, PulleySecurity and ComplianceCompliance with healthcare data regulations is non-negotiable. Hasura Cloud is designed to help companies follow the three tenets of information security: confidentiality, integrity, and availability. As part of our certification process, we have external agencies conduct routine testing to ensure we\u2019re maintaining industry standards. To incentivize the community for an extra layer of scrutiny, we also offer a path for responsible disclosure.SOC 2 Type II compliantHIPAA compliantRegularly penetration testedISO CertifiedGDPR compliantDeclarative and powerful role-based access controlHasura has a native Authorization system and advanced security features essential for compliance with regulations like HIPAA and GDPR. Hasura's granular role-based access control ensures that data is accessed only by authorized users, reducing the risk of data breaches and associated costs.Whenever possible, Hasura can automatically push down the authorization check in the data query itself. This provides an enhanced security benefit by avoiding additional lookups where it can be avoided and not fetching unauthorized or unnecessary data from the data source. There is an added performance boost and cost savings as well.\u201cThe beauty of Hasura is that we can manage the data through Postgres and we can leverage the data through the APIs that are available both internally and externally. We don\u2019t intend to have every piece of data in the clinical platform - Hasura allows us to manage the data from the database as well as from the API.\u201d Nagaraja Nayak, VP Enterprise Clinical Tech, OptumIn clinical platforms, not every data needs to be exposed to the outside world. And there are some which are accessed internally by admins and technicians. Optum was able to make use of Hasura\u2019s access control system to manage data in the database as well as from the API.Audit LogsHealthcare is a highly regulated industry, so it is critical to have an audit trail to analyze who accessed what data and when. You should be able to audit, debug, and analyze logs from all your services, apps, and platforms at scale.Hasura provides first-party integrations with multiple observability platforms and is fully open-telemetry compliant. For all API requests, the logs are emitted with details for both success and error type responses. Learn more about Hasura\u2019s http-log. You get details like user role, user ID, execution time and a few other details to figure out who made a given query, and at what time and create an audit trail for any given user.API Allow Lists and LimitsAllows lists on Hasura can be configured to safely permit a limited number of GraphQL operations (queries/mutations/subscriptions) for your project. You can review and approve operations, and create collections of role-based allow lists if required. By limiting access to a defined set of known and trusted entities, healthcare providers can protect patient data from potential unauthorized access or malicious activities. This also reduces the attack surface and reduces the risk of data breaches. Rate Limiting in Hasura helps in controlling resource usage. When accessing patient data in a healthcare setting, it is important to ensure equitable access without a particular user blocking resources with a series of API requests.OWASP Top 10OWASP is most famous for the \u201cTop Ten\u201d framework for structuring secure applications. As the industry expands into a micro-service-driven approach, it\u2019s important for organisations to validate all of their dependencies according to the OWASP framework.Hasura\u2019s security-first approach ensures that the Top 10 security features and criteria are fulfilled. Read more: How Hasura addresses the OWASP Top 10 concerns. On the other hand, when you are building your own GraphQL server and writing authorization logic, you will need to ensure that the Top 10 concerns are handled to be secure and compliant.Unified Health Records with Data FederationFragmented patient data across different systems can delay various processes in healthcare workflows. Hasura's unified data graph provides an integrated view of patient health records, leading to speedier, more accurate diagnoses.The best part about this unification is that it doesn\u2019t need any code changes both in the upstream service provider and also within Hasura. Data Federation works out of the box and is driven by configuration.Hasura\u2019s approach to GraphQL and Data Federation is: Declarative: You can declaratively add sub-domains (GraphQL, REST, databases, etc.) to Hasura to get a federated API. Unlike alternatives, you do not need to make any proprietary modification to your GraphQL servers to bring them into your \u201csupergraph\u201dMulti-protocol support:  Federation in Hasura is not restricted to just GraphQL. You can federate instantly across databases, REST APIs, and of course GraphQL APIs as well.Schema registry: Schema Registry makes it easy to evolve the scheme in a transparent, controlled, and collaborative mannerHigh performance, in real-timeIn healthcare, there\u2019s often a high volume of data, especially real-time data and a highly performant and reliable API can significantly improve the user experience. Real-time access to data, such as patient records, reports are critical for effective and timely decision-making.Hasura is a high-performance GraphQL engine, which compiles an incoming GraphQL request to a single SQL query, with all the necessary authorization embedded within the query. This way, it is possible to query any level of nested relational data very quickly. Most patient records have a lot of nested data to query and also in real-time. Add in Authorization rules and you know that it will quickly become complicated if you had to do your own API dealing with such sensitive data.Read more on How Hasura scales to 1 million active GraphQL subscriptions (live queries).SummaryHasura has helped many Fortune 500 companies such as Optum and Philips Healthcare to harness the power of our data-access API engine to modernize their extensive clinical platform and improve healthcare systems for millions of customers in just over three months. Hasura can power an enterprise-level healthcare system with modern digital standards, high-performance API, a highly secured data access layer and compliance with industry standards for data regulations.Try out HasuraSign up to start your journey with Hasura Cloud today. If you are an enterprise looking to learn more about how Hasura fits in your app modernization strategy, reach out to us through the Contact Us form and our team will get back to you.Reading ResourcesFundamentally transform data access and API developmentLearn how Hasura helps transform healthcare businesses"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura + Vector DBs Description: Hasura allows you to integrate your vector database with your application database and access it securely to build production-ready applications quickly. Author: Simrat Hanspal"}, {"role": "assistant", "content": "Hasura allows you to integrate your vector database with your application database and access it securely to build production-ready applications quickly. Want to see how? Let us build a demo together \ud83d\udc47Use caseImagine you own an HR management app that lets candidates apply for jobs. This platform allows HR managers to assign candidates to hiring managers and interviewers while tracking progress from a single dashboard.For the sake of simplicity, we'll assume the app has 2 tables:Candidates table - contains information about the candidateApplications table - contains information about the applicationEach time an application is filed, the HR manager selects candidates by screening resumes and interviewers review resumes to understand the profile. What if we could leverage the capability of large language models (LLMs) to help us with these tasks?Leveraging LLMsLLMs are fundamental models which are trained on huge public datasets. As such, they've gained a very good understanding of general concepts, but this capability can\u2019t be applied to our specific use case as the LLM hasn\u2019t seen our application 's data before.To solve this problem we would either need to fine-tune or provide context which LLMs can use to answer the queries.Passing context seems like a reasonable solution, unlike fine-tuning, which can be cost prohibitive.However, passing context has a limitation too: LLMs have a limitation on input size, which means we can\u2019t pass all of available data. We will need to carefully select data and share only what we deem necessary with an LLM.VectorDBsVectorDBs to the rescue! Vector databases store lots of...vectors. Vectors are simply mathematical representations of the input text from a model. This model can be different from the LLM used to answer the question.We also need to be mindful about how we store this information using VectorDB. If we store the whole artifact, we wouldn\u2019t be able to use it in our LLM queries because of limited input size. Hence, we resort to \"chunking\".Chunking is the process of breaking down large text into smaller chunks. These chunks are then vectorized and stored in VectorDB so that we can leverage the power of vector similarity searches. With Chunked vectors stored in VectorDB, we will be able to retrieve more relevant pieces of information. Some of the popular VectorDBs are Weaviate, Pinecone, Milvus, PgVector, etc. For this demonstration, we are going to use Weaviate. You can read more about Weaviate here.Set-upHasura imageThis pre-built Hasura image has the Weaviate driver integration baked into it. You can download the image from https://github.com/hasura/ai-workshop-hasuracon23/tree/main.You need to ensure you have Git's Large File Storage extension installed before cloning the repository.If you don't have git-lfs setup. Please follow these steps after cloning# one time setup\nbrew install git-lfs\ngit lfs install\n\n# pull the docker images after cloning \ngit lfs pullOnce you've cloned it, fire up the Hasura Console:# Set the right ARCH environment variable for your system\nexport ARCH={arm64|amd64}\ndocker load < hasura-image-${ARCH}.tar.gz\ndocker load < weaviate-gdc-image-${ARCH}.tar.gz\ndocker compose upPostgres set-upIn this demo we are going to assume that this app has its application data in a Postgres database.As a first step we need to create the tables which represent our app's data model. To do so we go to scripts/ folder in the repo and open 1_postgres_setup.ipynb where we can see the table definition under the Data Prep section.We'll begin by creating these tables using the Hasura Console.Step 1. Connect your Postgres databaseIn your browser, open http://localhost:8080/console and navigate to Data before clicking Connect Database:Once there, choose Postgres and click Connect Existing Database:Finally, enter the give the database a name of HRTool. Ensure you've selected the Environment variable option and connect your database using the HASURA_GRAPHQL_METADATA_DATABASE_URL environment variable, which is part of your docker-compose file.Finish by clicking Connect Database.Step 2: Create tablesWe'll use the Hasura Console to create our tables. Head to the Data tab and create a new table named candidate. Add the following columns:id, which be of type integer (auto-increment)name, which is of type textemail, which is also textSet the id as the primary key and click Add Table.Repeat the process for the application table using these columns:id, of type textcandidate, of type integercandidate_id, of type integerresume_url, of type texthiring_manager of type textStep 3: Add data to the tablesExecute all the cells in 1_postgres_setup.ipynb. This will update the tables with all the data we need. This is quickly achievable using Jupyter.Vector DB set-upWith Hasura and our data sorted, we need to vectorize things! We're going to use a service called Weaviate.Step 1: Create a free 14-day cluster on WeaviateHead to https://console.weaviate.cloud/ and register for an account. After confirming via email, click + Create cluster and fill in a name before clicking Create. Once Weaviate has provisioned your sandbox cluster, proceed to the next step.Step 2: Explore the resume data and create the Resume schemaIn the repository, follow notebook 2_vectordb_setup.ipynb to explore resume data and create the Resume schema.Step 3: Update the data via batchFollow along the notebook to update the data in Weaviate using the batch capability.Step 4: Add the Weaviate connector to HasuraStep 4a: Build your data connectorYou can find the Weaviate connector repo on this link \u00a0and to build connector for any of the other vector database you can follow these reference links https://hasura.io/docs/latest/databases/data-connectors/https://github.com/hasura/graphql-engine/tree/master/dc-agents/referencehttps://github.com/hasura/graphql-engine/blob/master/dc-agents/DOCUMENTATION.mdStep 4b: Run your connector You can choose to run the connector in your infra or you can use Hasura CLI to host the connector and get the endpoint. Let us review the steps, assuming this is your first time with Hasura CLI.# install hasura-cli\n# if you are on mac\nbrew install hasura-cli\n# Otherwise follow instructions at https://hasura.io/docs/latest/hasura-cli/install-hasura-cli/\n\n# check if you see connector plugin\nhasura plugins list\n# if you can't you can update your cli \nhasura update-cli\n\n# install connector\nhasura plugins install connector\n\n# help section for connector\nhasura connector -h\n# you can also get help section for specific commands\nhasura connector create -h\n\n# create connector using our weaviate repo\nhasura connector create my_weaviate_connector:v1 --github-repo-url https://github.com/hasura/weaviate_gdc/tree/main/\n\n# check deployment status to get the endpoint\nhasura connector status my_weaviate_connector:v1\n# you can also use list command\nhasura connector list\n\n# view logs at any point of time\nhasura connector logs my_weaviate_connector:v1\n\n# for more commands explore help sectionStep 4c: Add connector to Hasura console Back in the Hasura Console, head to Data and add the Weaviate agent:Step 5: Connect tables (schemas) from WeaviateStep 5a: Connect databaseFill in the details of your database. These fields are part of the connector repo which is required for integration. You don't necessarily need openAI key (openAPI is a typo). apiKey is the key for weaviate and host is your cluster ip without protocol (http/https)Step 5b: Track your tablesUnder Data > Resumes, track the Resume table:Step 6: Define a remote relationshipFinally, define a remote relationship from your vector db to your relational database:All in all, if your Console looks like this \u2014 you should be set \ud83c\udf89QueryingHello Hasura!In our first query use case, we'll select a number of properties from across datastores. Hasura makes querying across data stores seamless; this would have been a lot of effort otherwise with a pipeline duplicating all of this data into VectorDB. Run the following query:query MyQuery {\n  Resume {\n    application_id\n    content\n    applicaiton_relationship {\n      hiring_manager\n      resume_url\n      candidate {\n        name\n        email\n      }\n    }\n  }\n}Executing this query should return information similar to what's below:Query with access controlAs we can see from the earlier screenshot, we have x-hasura-manager-id set as Manager2. This simulates the logged-in user having the identity of \u00a0Manager2, but in the previous example, Manager2 can also see Manager1.We can quickly fix this with Hasura's access control: permissions. Head to Data > Resume and configure permissions for a new role called manager. We'll choose the SELECT column and enter the following rule with a custom check: {\"application_relationship\":{\"hiring_manager\":{\"_eq\":\"x-hasura-manager-id\"}}}. You can copy and paste this rule into the first line of the editor, or configure it using the dropdown GUI.Adding this rule does the magic! We'll now only see the applications belonging to the manager id in x-hasura-manager-id \ud83c\udf89Don\u2019t forget to send x-hasura-role as manager and x-hasura-manager-id as your id of choice:Find for me all marketing resumesTo find records by semantic (contextual) search, use the near text capability on your text data. Near text will fire a semantic search query on your VectorDB:Automatically ingest new resume / update in resumeLet's say we want to automatically ingest and update resumes. You can set up an Event Trigger on your Postgres table, such that whenever there is a new record or change in a record, we automatically fetch the data and store the vectorized data in out VectorDB.You can find this code under handlers/event.py and handlers/server.py.Step 1: Define your specific event handler. You can have one for insert, delete, update - this is totally your call on the use case!As an example, we could handle inserts and deletes like this:def handle_insert(row, client):\n    id = row['id']\n    # In reality you would follow the URL from row['url']\n    content = \"dummy content\"\n    gql_query = gql(\"\"\"\n            mutation insertItem($id: String!, $content: text!) {\n                insert_Resume_one(object: { application_id: $id, content: $content }) {\n                    id\n                }\n            }\n        \"\"\")\n    print(client.execute(gql_query, variable_values={\n        'id': id, 'content': content}))\n\ndef handle_delete(row, client):\n    id = row['id']\n    gql_query = gql(\"\"\"\n            mutation deleteItem($id: String!) {\n                delete_Resume(where: {application_id: { _eq: $id } }) {\n                    affected_rows\n                }\n            }\n        \"\"\")\n    print(client.execute(gql_query, variable_values={\n        'id': id}))Step 2: Define an overall handlerAfter defining each event handler we'll need, we need an overall event handler defined that will execute the appropriate handler when an event occurs:def handle_event(event):\n    gql_headers = {'x-hasura-admin-secret': 'secret'}\n    # Create a GraphQL client with the request transport\n    transport = RequestsHTTPTransport(\n        url=GRAPHQL_ENDPOINT, headers=gql_headers)\n    client = Client(transport=transport)\n\n    event = event['event']\n    op = event['op']\n    if op == 'INSERT':\n        row = event['data']['new']\n        handle_insert(row, client)\n    elif op == 'UPDATE':\n        old_row = event['data']['old']\n        new_row = event['data']['new']\n        # TODO: Do something\n    elif op == 'DELETE':\n        old_row = event['data']['old']\n        handle_delete(old_row, client)\n    else:\n        print(str(event))\n    return \"Success\"Step 3: Create an API for Hasura to callEvent Triggers in Hasura need an API to call. As we're using Python, we'll implement a simple, lightweight Flask API. From the repository's directory, you can run python server.py to start the server.Step 4: Configure the Event Trigger in HasuraHead to Events and click Create to configure a new Event Trigger. We'll reference the public shema and application table. Ensure all CRUD operations are checked as triggers and enter the following URL for our Flask API:http://host.docker.internal:8400/handle_eventThe configuration should look like this before clicking, Create Event Trigger:Finally, we can test this by entering a new application:Find for me resumes with most experienceYou can also set up LLM queries as an Action on a remote source. This is great because we can define access control for Actions as well, and any data that we query will pass through our data access control we set up earlier.Step 1: Write the LLM queryWe'll need the code to query for resumes. This can be found in handlers/query_llm.py. You'll need the get_prompt() function from this file.Please note: You'll need the OPENAI_APIKEY environment variable set when starting server.py for this to work!Step 2: Define the APICreate a function called query_llm() as detailed here. Then, include a route in your Flask server to handle rqeuests to the /query_llm endpoint as detailed in handlers/server.py.Step 3: Create an Action on the Hasura ConsoleOn the Actions page, click Create and enter the following Action Definition:type Query {\n  QueryLLM (user_query: string!): string\n}Then, clear out the Type Configuration. Finally, provide the handler via this URL:http://host:docker.internal:8400/query_llmStep 4: Execute the ActionNow you can use QueryLLM as a type in your GraphQL API \ud83c\udf89"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Breeze through collaboration with the Hasura Schema Registry Description: The Hasura Schema Registry is a version control system for your Hasura GraphQL schema. This shows you the difference between two subsequent versions of your GraphQL schema so you can audit and release API changes to production with confidence. Author: Vaishnavi G V S"}, {"role": "assistant", "content": "\u201cPagerduty alert: Rise in user tickets after a release\u201d \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u201cUrgent war room on a user facing breaking change!\u201dNot just as an SRE, but as an Engineering Manager and as a software engineer in one of the several many teams that contributed to the latest release, haven\u2019t we all been in this situation where, something broke suddenly even after rigorous testing and you\u2019re woken up in the middle of the night to escalate and fix it? With Hasura Schema Registry this is exactly what we\u2019re trying to address!As the software product that we\u2019re building becomes more and more complex, and as the number of teams working on the code together becomes larger and larger, there\u2019s only so much you can do to exhaustively test every single change that is to go into the latest release. As a developer on Team A, I want to make sure that none of my changes are breaking those of Team B. But the constraint is I won\u2019t know before all the changes are merged into main, and with the size of code there is, at that level it only becomes more difficult to test. And as a platform manager, I want to ensure my top level is coherent and doesn\u2019t break my consumer applications. Sure, for code we have the GitHubs and GitLabs of the world. But we wish we had a version control system in the API world, too, right?Introducing the Hasura Schema Registry!The Hasura Schema Registry is a version control system for your Hasura GraphQL schema. This shows you the difference between two subsequent versions of your GraphQL schema so you can audit and release API changes to production with confidence. You can also configure alerts on your pre-production environments to catch breaking changes in time.The Schema Registry is available to all Hasura Cloud users starting v2.26. Just head to Settings -> Schema Registry (Beta) on your Cloud project\u2019s console. The Schema Registry shows you changes to your schema classified by \u00a0\u2018Breaking\u2019,, \u2018Dangerous,\u2019 and \u2018Safe\u2019 since the last snapshot. You can look into each change by clicking into each time- snapped entry.Let\u2019s look at an example. I have connected a new database to my Hasura instance and tracked an \u2018order\u2019 table into my schema. The Schema Registry entry then shows all the changes that newly modified my GraphQL schema; that is, fields and queries, mutations and subscription types associated with the \u2018order\u2019 table.As these are new changes that weren't affecting the previous schema, all of these are classified as \u2018Safe\u2019 changes.As a next step, if I remove a table called \u2018region\u2019 from my GraphQL schema, that shows up as \u2018Breaking\u2019 changes in my Schema Registry entry, which means if I had consumer applications that were querying this \u2018region\u2019 table, they would break!The Schema Registry service automatically computes differences between each subsequent schema instantaneously and will be available on the Schema Registry tab for all your Hasura Cloud projects.This means that you do not need to do any additional setup \u00a0on your end \u2013 and can immediately start looking at the schema changes of your desired project and get productive.What about Role-based schema changes?The Hasura Schema Registry also becomes super important for auditing your role-based schemas. In the registry, you\u2019ll be able to see one entry for each role you have set up on your GraphQL schema. In my example, I have two roles - admin and user. The \u2018User\u2019 role has limited permissions to access and insert to fields in certain tables. So if I have changes in my GraphQL schema that impacts only the \u2018user\u2019 role, those can be seen in the role-based entry in my registry.Not getting a good night\u2019s sleep worried something might break? Schema Registry Alerts to the rescue!The Schema Registry becomes an extremely powerful capability when used in conjunction with your staging or pre-production environments.As someone monitoring your top-level API, you now have the capability to get proactively alerted when worrisome schema changes occur in your staging and pre-production environments. \u00a0This will enable \u00a0you to verify them before going live \u00a0so that breaking changes are not inadvertently introduced to your downstream consumer facing applications.. Say I had a user-facing application that was querying my Hasura API for the \u2018region\u2019 information, and accidentally one of my teams deleted the \u2018region\u2019 table. In the Schema Registry of my pre-prod instance, I get alerts on breaking changes in my top-level schema. The registry entry can then be quickly reviewed to look for changes and appropriate fixes can be done as needed before pushing the latest changes to production!You can customize alerts also so that you can be alerted only for the changes that concern you.This also \u00a0enables easier and smoother collaboration between multiple teams, and you can make sure that changes to your unified API are coherent and reliable.SummaryIn this blog post, we discussed how Hasura\u2019s Schema Registry can audit and proactively alert you on changes to your GraphQL schema and enable you to release API changes to production with confidence. To get started now, sign-up for Hasura Cloud, check out our docs here and reach out to us for any questions or feedback."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura on Microsoft Azure Cloud Description: We are thrilled to announce the availability of Hasura on Azure Cloud starting June 2023. This expansion means Hasura is now accessible on all the leading cloud providers, including AWS and GCP. Author: Hasura"}, {"role": "assistant", "content": "We are thrilled to announce the availability of Hasura on Azure Cloud starting June 2023.We have received numerous requests from our customers, prospects, and developers, to make Hasura available on Azure Cloud and are delighted to fulfill this highly anticipated demand. This expansion means Hasura is now accessible on all the leading cloud providers, including AWS and GCP.Here is the list of all supported cloud providers and regions.How to get started?If you haven\u2019t used Hasura Cloud before, sign up at https://cloud.hasura.io/signup and follow the below-mentioned steps to create your first Hasura project (API endpoint) on Azure.If you are an existing user, head over to your Hasura Cloud Dashboard and \u00a0while creating a new Project, select Azure as the cloud option, select a region, and click on \u201cCreate Project.\u201dLeveraging Azure Cloud for HasuraAzure Cloud is one of the most popular cloud platforms, offering a myriad of advantages and a robust ecosystem enabling you to expedite the deployment of your applications and code.Access to a wide range of Azure ecosystem servicesYou now have the opportunity to harness the full potential of Azure's comprehensive range of services, including databases such as MS SQL Server, Azure DB for PostgreSQL, and more.High Performance with Security and ComplianceBy bringing your API development platform closer to your existing Azure databases and other Azure IT infrastructure, you not only address your security and compliance requirements but also unlock the benefits of high performance and low latency, even during peak usage periods, ensuring a great user experience.Wider reach and scalabilityYou gain the advantage of effortless scalability and the ability to reach users across multiple regions enabling you to ship your applications and APIs faster, accelerating your journey to building modern, cloud-native applications.What\u2019s nextIf you would like to run your Hasura project in a region that\u2019s not yet supported, you can request the region of your choice by following these steps:Go to your Hasura DashboardClick the page to create a new project and scroll down to where all the regions are listedTowards the bottom, you will find an option to request a new regionSubmit your request,Our team will review all these requests and prioritize new regions based on the demandTransacting on Azure Marketplace?If you are a commercial Hasura Cloud customer, we plan to allow you to make it easier to transact with us via the Azure marketplace.Additional ResourcesHasura Cloud documentationChanging the region of your Hasura projectConnecting Hasura to an MS SQL on Azure DatabaseList of regions supportedCreate SQL Azure databaseSign up to start your journey with Hasura Cloud today."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing our MongoDB Data Connector (Beta) Description: We're thrilled to announce our first foray into NoSQL and GraphQL, starting with a beta launch of our MongoDB data connector. Author: Hasura"}, {"role": "assistant", "content": "Having trouble querying your data using MongoDB? Wrestling with the setup of your authentication and data authorization layer for your API? Need to query data from multiple sources? We've got your back.Introducing MongoDB Data Connector support for HasuraIntroducing Hasura for NoSQLWe're thrilled to announce our first foray into NoSQL and GraphQL, starting with a beta launch of our MongoDB data connector. This marks a new chapter in our journey to streamline your work and boost developer efficiency.Our initial focus is on performant reads. You can now track Collections as effortlessly as you would with Tables + Views from Postgres. Plus, we allow you to auto-generate your types using your MongoDB validation schema, simplifying your workflow even further.We've even got a request for comment on how we can streamline this even further by using sampling to help you generate your API even quicker, and would love to hear more from you.First-class performanceIn addition to auto-generating your API, we're dedicated to delivering quick and efficient service by embracing the pillars that made Hasura great for other data sources, transferring less unnecessary data and reducing the number of roundtrip requests.This is made possible through field projection, which fetches only necessary fields, permission predicate pushdowns to the database, and we are soon introducing optimizations for cross-Collection relationships. We're talking about $lookups and more \u2013 stay tuned for these exciting updates.Getting started with Hasura and MongoDB\n\nYour browser does not support the video tag.\nConnecting your databaseTo connect your MongoDB database, you can follow the following steps:\nGo to Data > Manage in your application.\nCheck if your MongoDB data connector is pre-connected to your Hasura instance. You can do this by selecting the Data Connector Agents dropdown from the Data Manager.\nIf the MongoDB data connector is not connected and you have used the docker-compose.yml file mentioned above, use the following details to connect to your agent:\n\nAgent Name: mongodb\nURL: http://mongo-data-connector:3000\n\n\n\n\nAfter that, you can click Connect Database to continue to the database connection screen.\nIf you\u2019re using the provided docker-compose.yml file you can connect your database using the following credentials:\n\nDatabase Name: mongodb\nConnection: mongodb://mongouser:mongopassword@mongodb:27017\ndb: demo\n\n\n\nTrack your first CollectionWe \u201cTrack\u201d objects in Hasura in order to add them to your GraphQL API:Once your database is connected, select the database name from the left-hand sidebar.You will see your collections listed here. Choose a collection and click on \"Track Selected.\"Congratulations! Your selected collection is now added to your GraphQL API.Run your first queryYou can also test your GraphQL queries using the Hasura Console:In the header, select \"API\" to go to GraphiQL, our API testing utility.Utilize the explorer located on the left-hand sidebar to assist you in composing your query (the resulting query will be populated in the body area).When you\u2019re ready, click on the play button to execute your GraphQL query.Your query\u2019s result will be populated in the right-hand column of the interface.Congratulations! You have successfully run your first GraphQL query with Hasura and MongoDB.Current featuresOur MongoDB integration introduces several features to enhance your database operations.Here are the initial capabilities currently available:Schema Generation: We support tracking collections containing a validation schema. This schema serves as the backbone for generating your GraphQL schema.Document Filtering: Enjoy the convenience of filtering on top-level fields within your documents. This feature allows you to access and manage only the data you require.Field-Level Permissions: Security is paramount. We've incorporated permissions on the same top-level fields, reinforcing an additional layer of data protection.Federated Relationships: Experience the power of connecting your data sources. We enable federated relationships from MongoDB to other data sources such as PostgreSQL, enhancing the interconnectivity of your data.And, we're not done. There's more coming soon!And we're just getting started. We're committed to a cycle of continuous improvements, and we're excited to give you a sneak peek of what's coming soon:\ud83d\uddc2\ufe0f Enhanced querying and permissions for embedded documents\ud83d\udd17 Improved cross-collection relationships\ud83d\udd17 Improved cross-datasource relationships (inbound joins to MongoDB, currently supporting from MongoDB)\ud83e\udd1d A customizable GraphQL schema, moving beyond the dependency on validation schemasand the freedom to create your own logical models, providing a powerful escape-hatch from any \ud83e\ude84 auto-magicCurious to explore our Mongo support? Visit our docs here to connect your first MongoDB data source."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Data Delivery Network and other big news at HasuraCon 2023 Description: These product advancements are taking leaps forward into the world of development to come. Ready to celebrate some true Hasura goodness? Author: Rishi Divate"}, {"role": "assistant", "content": "HasuraCon 2023 is finally here! And this morning Hasura CEO and Co-Founder Tanmai Gopal announced a ton of big news from the fourth edition of our annual user conference aimed at keeping everyone on the forefront of APIs and data access and management. And I have to say, the future has never felt so near. These product advancements are taking leaps forward into the world of development to come. Ready to celebrate some true Hasura goodness?If you didn\u2019t get a chance to attend the keynote, registration is still open. So long as you register, you\u2019ll have access to an instant replay of the keynote (and every session after they end).And, hey\u2026 there\u2019s still two days left to attend live sessions and take a deeper dive into each one of these new features and other amazing announcements, which are linked below for your convenience. If you haven\u2019t registered yet, click the button below to get yourself access.Register for HasuraCon 2023Okay, let\u2019s get to the main HasuraCon 2023 headlines. Get unparalleled API performance at global scale and iterate fastThe advancements we\u2019re making with Hasura v3 (coming soon) are truly unprecedented, and there are specifically two new features we know everyone has been hoping and waiting for.First, Hasura Data Delivery Network (DDN) is an edge network enabling developers to run low-latency/high-performance data APIs at global scale, with no additional effort or fees. This is made possible by a major architecture change of the Hasura engine, such that there is a zero cold start for the server. As a result, the Hasura runtime can be instantiated on the edge region when the API is invoked, enabling instant auto-scaling to handle any spike in traffic \u2013 globally. All projects deployed on Hasura Cloud will be automatically deployed to an edge network of 100+ global regions, with 99.99% uptime guarantees, delivering the best experience to the end consumer. To further minimize network latency, between Hasura and the underlying data source, Hasura DDN integrates seamlessly with distributed databases (e.g. CockroachDB, Amazon Aurora, Yugabyte, etc.), automatically routing data queries from the user to the nearest database replica.Hasura DDN is also a much more cost-efficient way to get value-based API pricing, instead of infrastructure-based pricing, than alternative approaches like always-on or warmed-up instances. Then, the other thing every developer wants is to iterate at an extremely fast rate. Hasura DDN also enables instant API iteration and rapid testing, by allowing developers to iterate on their APIs \u2013 CI/CD from code to global-scale production (including building, validation, and testing) \u2013 instantly. This is true regardless of the number of data models connected, across all changes and scenarios.Building APIs traditionally requires build/validate/test on the code. This is difficult to do at scale, and previously took anywhere from minutes to hours depending on the type of change and the complexity of the metadata. Metadata changes will now create builds that can be independently tested and applied to projects, enabling superior API version control and more seamless collaboration between developers.Register here for early access to Hasura v3.\n\n\n\nWatch a replay of the session Introducing Hasura version 3 with Senior Product Manager, Rahul Agarwal | June 21, 2023 @ 10:15 AM PST | for more details\nStreamline governance and evolution of federated GraphQLHasura Federation is one of the most efficient ways to compose multiple domain APIs into a single, unified API \u2013 aka the supergraph \u00a0\u2013 to greatly simplify real-time data access and management. Building on these capabilities, the new Hasura Schema Registry makes managing, governing, and collaborating on a federated API even simpler. The Hasura Schema Registry allows developers across multiple teams to easily audit and get proactively alerted on GraphQL schema changes across diverse data sources, which helps them more reliably and confidently push changes in mission-critical production apps.\n\n\n\nWatch a replay of the session Data Federation made easy with Hasura given by Senior Product Manager, Vaishnavi GVSl | June 22, 2023 @ 9:45 AM PST | for more details\nOpen sourcing model-driven API design and data connectivityThere were two major open source innovations we introduced at last year\u2019s HasuraCon that were created to promote true community collaboration on the Hasura Platform: GraphQL Data Specification (GDS), which enables API developers to create enterprise-grade APIs, and the GraphQL Data Connector (GDC) SDK, which enables developers to author custom data agents. We are excited to announce major updates to these foundational blocks.Previously known as GDS, with Open Data Domain Specification (Open DDS) Hasura has implemented a domain model-driven approach to API design and development. The new Hasura metadata is modeled on the Open DDS, which means developers can now confidently author highly performant, secure, and standardized APIs in a declarative manner.\n\n\n\nWatch a replay of the sessions Domain data and API specification with Staff Engineer, Abhinav Gupta | June 21, 2023 @ 11:30 AM PST | for more details\nNative Data Connector (NDC), which was previously known as GDC, now makes it simpler for anyone to create an NDC agent for a new data source. Leverage state in NDC to build advanced features such as elastic connection pooling into their agents. This update gives creators better support for creating non-database agents, and offers new features like parametrized models and commands. New Hasura GraphQL type gives big boost to database interactivityLogical Models along with Native Queries for PostgreSQL, BigQuery, and SQL Server allow developers to seamlessly incorporate the full capacity of a database's query language, such as SQL into the Hasura auto-generated API. This offers users an enriched and extended range of query possibilities and operations, enhancing the developer experience and broadening the capabilities of the Hasura-generated API. By enabling users to employ their database's full query language capabilities, Native Queries enhances query expressivity by bringing SQL into Hasura auto-generated API.Native Queries does away with the need for DDL privileges to build custom mapping, making it far more accessible for users in different settings, particularly within enterprises.Finally, there\u2019s now support for a more comprehensive range of function return types, allowing Hasura to handle functions that don\u2019t necessarily return a table, thus accommodating a more diverse set of databases and function types.\n\n\n\nWatch a replay of the session Unlock the full potential of your database and turbocharge your app with Senior Product Manager, David Meleney | June 22, 2023 @ 11:30 AM PST | for more details\nOur first foray into instant GraphQL on NoSQL, starting with MongoDB Today\u2019s launch of the Hasura MongoDB connector brings the Hasura magic to a whole new data store category \u2013 NoSQL! In minutes, users can now point Hasura to their MongoDB instances and auto-generate a fully featured GraphQL API from their collections and documents. For any developer looking to build modern GraphQL-powered apps on MongoDB, this connector compresses the backend development effort by orders of magnitude and accelerates the time to market.Bringing API automation to NoSQL stores required rethinking how the Hasura JIT compiler automatically translates an arbitrary GraphQL query into a performant, native MongoDB query. It takes into account pushing down permission rules and projecting fields as the first cut on performance, with an eye to several other areas where it is possible to make fewer requests to optimize end-user performance. With this connector, Hasura now supports the top five databases as ranked by DB-Engines.\n\n\n\nWatch a replay of the session It should\u2019ve been Mongo all along! with Director of Product Design, Martin Mark | June 21, 2023 @ 10:45 AM PST | for more details\nNow available: Hasura Cloud on Microsoft AzureIn addition to its existing availability on AWS and GCP, we are happy to announce that Hasura Cloud is now available on Microsoft Azure, a capability that enhances and expedites API development within the Azure ecosystem.Developers can now build low-latency high-performance and secure applications by co-hosting their APIs and databases within the same Azure regions.\n\n\n\nWatch a replay of the session Hasura on Azure Cloud by Senior Product Manager, Amit Tomar | June 21, 2023 @ 11:45 AM PST | for more details\nConclusionThis HasuraCon marks major milestones in our journey, unveiling our pioneering innovations in API design and performance, rapid iteration, governance, database interactivity, and multi-cloud deployments. All of these advancements have come as a result of feedback, insights, and support from our user community.Thank you for being a part of HasuraCon 2023, and for being a part of our community. We look forward to continued innovation and joint success going forward.One final reminder to register for HasuraCon 2023 to catch sessions live, or if you can\u2019t make it, get access to instant replays!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Unlocking advanced API strategies with Hasura Description: We will explore how Hasura revolutionizes the API ecosystem by enabling data aggregation, supporting microservices and federated GraphQL, facilitating serverless architecture, enabling multi-cloud strategies, and empowering real-time updates and event-driven APIs.\n Author: Akshay Bhat"}, {"role": "assistant", "content": "In today's dynamic digital landscape, organizations are constantly exploring advanced API strategies to build robust and scalable applications. Hasura, a powerful GraphQL engine, provides a transformative solution for unlocking the potential of these strategies.In this blog post, we will explore how Hasura revolutionizes the API ecosystem by enabling data aggregation, supporting microservices and federated GraphQL, facilitating serverless architecture, enabling multi-cloud strategies, and empowering real-time updates and event-driven APIs.Hasura as a data aggregatorHasura's ability to automatically generate APIs based on existing databases makes it an exceptional data aggregator. By integrating Hasura with your API infrastructure, you can effortlessly consolidate data from multiple sources into a unified GraphQL schema. This federation capability allows you to access, query, and manipulate data from diverse backends through a single GraphQL endpoint. The result is a cohesive API ecosystem that simplifies data access, accelerates development and enhances data-driven decision-making.Use Case: E-commerce aggregationHasura connects to various product databases, inventory management systems, and payment gateways.Hasura aggregates product data, inventory levels, and payment information into a unified GraphQL endpoint.Clients can efficiently fetch all the necessary information for an e-commerce application from a single API, eliminating the need for multiple round trips to different services.ArchitectureIn this architecture, Hasura acts as the data aggregator for an e-commerce application. Hasura connects to multiple data sources, including product databases, a payment gateway, and an inventory system. It consolidates the data from these sources into a unified GraphQL endpoint.Clients interact with Hasura's GraphQL API to fetch product information, process payments, and manage inventory. Hasura efficiently aggregates the required data from various sources, eliminating the need for clients to make multiple API requests.By using Hasura, the e-commerce application benefits from streamlined data retrieval, reduced latency, and improved performance. Clients can retrieve all the necessary information from a single GraphQL endpoint, providing a seamless and efficient user experience.Microservices architectureMicroservices architecture promotes modular and decoupled services, enabling organizations to efficiently scale and evolve their applications. Hasura plays a pivotal role in such architectures by acting as a federated GraphQL layer that orchestrates communication between microservices. By integrating Hasura with your microservices infrastructure, you can seamlessly aggregate and compose data from different services, simplifying the development process and providing a unified API interface for clients.Use Case: Social media analytics dashboardHasura serves as a federation for multiple microservices, including user management, social media data retrieval, and analytics.Each microservice exposes its own GraphQL endpoint, and Hasura aggregates the data from these services.Hasura applies security policies, rate limiting, and caching to provide a unified and secure API for the analytics dashboard, improving performance and data consistency.ArchitectureIn this architecture, each microservice handles specific functionalities within the application. Hasura serves as the centralized entry point for API requests, routing traffic to the appropriate microservices.Clients interact with Hasura's GraphQL API, which applies security, rate limiting, and caching policies to the requests. Hasura aggregates the responses from the microservices and presents a unified API to the client. This approach simplifies client-side integration and improves performance by reducing the number of network requests.By using Hasura, organizations can enforce consistent security policies, manage API traffic efficiently, and provide a unified interface for clients. Additionally, Hasura's data composition capabilities allow clients to fetch and combine data from multiple microservices, providing a seamless and cohesive user experience.Federated GraphQL with HasuraHasura's support for federated GraphQL enables organizations to build distributed systems that span across multiple data sources. By integrating Hasura into your API ecosystem, you can federate GraphQL schemas from various services and data providers. This approach promotes data virtualization, allowing clients to interact with a unified GraphQL schema while abstracting the complexities of the underlying data sources. Federation architecture should ideally be used when you have access to all your sources and high-performance joins across databases are required. With federated GraphQL and Hasura, you can achieve a seamless, consistent, and flexible data access layer.Use Case: Travel booking platformMultiple GraphQL services, managed by different teams, handle various travel-related functionalities like flights, hotels, and car rentals.Hasura acts as the federation, combining these services into a single unified API.Hasura's data composition capabilities allow clients to fetch data across different services seamlessly, enabling a cohesive travel booking experience.ArchitectureIn this architecture, Hasura acts as the federation for multiple GraphQL services (GraphQL Service 1, GraphQL Service 2, GraphQL Service 3). Each GraphQL service is managed by different teams and handles specific functionalities.Hasura, as the federation, combines these individual GraphQL services into a single, unified API. Clients interact with Hasura's GraphQL API, which orchestrates and delegates requests to the respective services. Hasura's data composition capabilities allow clients to seamlessly fetch data from across the federated services, providing a cohesive and integrated experience.By leveraging Hasura as the federation, organizations can adopt a micro-frontend architecture, where different teams manage their own GraphQL services independently. Hasura handles the composition and aggregation of data, reducing the complexity of client-side integration and enabling a scalable and modular architecture.Serverless architectureServerless architecture enables organizations to focus on writing business logic without the need to manage infrastructure. Hasura seamlessly integrates with serverless platforms, allowing you to leverage its powerful GraphQL capabilities within serverless functions. By utilizing Hasura in serverless architecture, you can build dynamic APIs, handle real-time updates, and trigger event-driven workflows. This combination empowers developers to create scalable and cost-effective serverless applications with ease.Use Case: Serverless chat applicationHasura acts as the entry point for incoming chat messages and user actions.Based on the GraphQL queries or mutations, Hasura invokes serverless functions to handle specific functionalities.Hasura's dynamic invocation of serverless functions enables a scalable and event-driven API architecture for real-time communication in the chat application.ArchitectureIn this architecture, Hasura acts in a serverless environment. The system consists of multiple serverless functions, such as AWS Lambda or Azure Functions, which handle specific API endpoints or functionalities.Clients interact with Hasura's GraphQL API, which acts as the entry point for API requests. Based on the GraphQL queries or mutations, Hasura dynamically invokes the corresponding serverless functions. This approach enables a scalable and event-driven API architecture.Hasura's capabilities allow for centralized management of security, rate limiting, and caching policies. It simplifies the coordination and invocation of serverless functions, reducing the complexity of managing individual API endpoints.By utilizing Hasura in serverless architecture, organizations can achieve agility, scalability, and reduced development effort. Hasura's dynamic invocation of serverless functions enables efficient handling of API requests and provides a seamless experience for clients.Multi-Cloud strategy with HasuraWith the increasing adoption of multi-cloud strategies, organizations are leveraging the strengths of different cloud providers to optimize performance, reliability, and cost. Hasura enables seamless integration with multiple cloud environments, allowing you to deploy and manage your APIs across different cloud providers. By leveraging Hasura's multi-cloud capabilities, you can achieve workload distribution, improve redundancy, and mitigate vendor lock-in, ensuring a resilient and flexible API architecture. You can check out our blog, Multi-Cloud and Multi-Region Deployment Strategy for Distributed Postgres.Use Case: Hybrid cloud e-commerce platformHasura is deployed across different cloud providers, such as AWS, Azure, and Google Cloud.Hasura provides consistent API management and security policies across all cloud environments.Clients interact with a unified API, seamlessly accessing services and data regardless of the underlying cloud infrastructure, enabling a hybrid cloud e-commerce platform.ArchitectureIn this architecture, Hasura serves in a multi-cloud strategy. The infrastructure is deployed across different cloud providers, such as AWS, Azure, and Google Cloud.Clients interact with Hasura's GraphQL API, which acts as the entry point for API requests. Hasura provides consistent API management, security policies, and caching mechanisms across all cloud environments.Hasura shields the clients from the underlying cloud infrastructure, enabling seamless access to services and data regardless of the specific cloud provider. This approach allows organizations to adopt a hybrid cloud strategy and leverage the benefits of different cloud platforms without exposing the complexities to clients.By using Hasura in a multi-cloud architecture, organizations achieve flexibility, scalability, and resilience. Hasura ensures a consistent and secure API experience for clients, regardless of the cloud infrastructure powering the backend services.Note: The architecture diagram is a high-level representation and may vary depending on specific implementation details and infrastructure choices.Real-time updates and event-driven APIsReal-time updates and event-driven APIs are essential for applications that require instant data synchronization and real-time communication. Hasura provides out-of-the-box support for real-time subscriptions, enabling instant updates to connected clients when relevant data changes occur. By integrating Hasura into your API strategy, you can build event-driven architectures, implement real-time collaborative features, and create interactive applications that deliver a superior user experience.Use Case: Real-time collaborative task managementHasura subscribes to changes in the task management database, such as task updates or new task assignments.Hasura pushes real-time updates to connected clients, allowing instant synchronization and collaboration among team members.Clients receive real-time notifications, live task updates, and instant feedback, enhancing productivity and collaboration within the task management application.ArchitectureIn this architecture, Hasura serves for a real-time collaborative task management application. The application utilizes a task management database to store task-related information.Clients interact with Hasura's GraphQL API, which acts as the entry point for API requests. Hasura subscribes to changes in the task management database, such as task updates or new task assignments. When a relevant event occurs, Hasura pushes real-time updates to the connected clients.Clients receive real-time notifications, live task updates, and instant feedback through the GraphQL subscriptions provided by Hasura. This enables seamless collaboration and synchronization among team members in the task management application.By leveraging Hasura with real-time capabilities, organizations can build interactive and collaborative applications that provide timely updates and enhance productivity. \u00a0SummaryHasura, with its advanced capabilities in data aggregation, microservices architecture, federated GraphQL, serverless architecture, multi-cloud strategies, and real-time updates, unlocks a new realm of possibilities for building advanced API strategies. By integrating Hasura into your API ecosystem, you can achieve a highly flexible, scalable, and performant architecture that empowers developers and enables rapid innovation. Embrace the power of Hasura to revolutionize your API Strategy and stay ahead in the ever-evolving digital landscape."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Aggregation Queries for Computed Fields Description: Hasura's GraphQL API on PostgreSQL now supports comparison and numeric aggregation functions (such as min, max, sum, stddev and so on) on computed fields. Author: Hasura"}, {"role": "assistant", "content": "The Hasura team has recently been digging back into GraphQL Engine's query parsers and SQL generators while working on implementing the new logical models feature. While we were there, we also managed to tick off a long-requested community feature: aggregation queries for computed fields. Let's dig into the problem and how we've solved it!What's the problem?Hasura has supported the idea of aggregation queries for a long time. As a quick refresher, consider an articles table, which we'd normally query like so:query {\n  articles {\n    title\n    content\n    rating\n  }\n}With aggregation queries, we can ask questions about the data set as a whole:query {\n  articles_aggregate {\n    aggregate {\n      count # How many articles have been written?\n      avg {\n        rating\n      } # What's the average rating for all articles?\n      min {\n        rating\n      } # What's the lowest rating for any article?\n    }\n  }\n}There are two types of aggregation queries: those that are numeric (such as sum and avg), and those that are comparable (max and min). Up until now, these queries could be applied respectively to any field with a type that is considered numeric or comparable, unless that field happened to be a computed field.Computed fields allow us to add extra fields to a GraphQL schema node using custom SQL functions. These functions receive a result row as input, calculate an output, and then that output is added to the result. For example, consider the following function that computes the word count of a given article:CREATE FUNCTION word_count(article_row articles)\nRETURNS BIGINT AS $$\nSELECT ARRAY_LENGTH(string_to_array(article_row.content, ' '), 1)\n$$ LANGUAGE sql STABLE;We can add this as a computed field to our articles table, and query it just like any other field:query {\n  articles {\n    title\n    content\n    rating\n    word_count\n  }\n}This would all work perfectly well, but things would get complicated when we wanted to know the average word count of all our articles, for example. One way would be to query the word_count for every row in the entire table and compute the average client-side, but that quickly becomes intractable as our tables grow to thousands of rows!Another way to work around this is to add an extra column to the articles table for word_count, and add a database trigger to compute the character count. This has a few drawbacks:It requires a table schema change. This means a migration, potentially having to rework queries throughout your application, updating Hasura permissions, and so on. It may also mean asking a database administrator to add a ticket to their backlog, which all contributes to a slower development cycle.There's quite a lot of room for error. For a simple computation like this one, we just need to remember to recompute the word count whenever we INSERT or UPDATE. However, plenty of factors can make it much more difficult to determine when to recompute the values. What if we require information from multiple tables? What if we compute something based on the current time?We have to start this whole process over again when we're asked for another aggregation! It's easy to end up with a fragile tower of triggers once we start needing several different aggregation queries.If only we could aggregate computed fieldsClearly, it would be better to compute the value of the computed field when we require it with the current state of our application data, and that's what aggregation queries over computed fields give us:query {\n  articles_aggregate {\n    aggregate {\n      avg {\n        word_count\n      } # What's the average word count for all articles?\n      min {\n        word_count\n      } # What's the shortest word count of any article?\n    }\n  }\n}As of Hasura v2.27 for Postgres, computed fields can be aggregated just like any other fields!How does it work?Parsing computed fields in aggregation queriesLet's briefly go through the lifecycle of a GraphQL Engine query. When we make a request, this request is parsed by a parser built specifically for your schema and metadata configuration. This means that, for example, you can only query a table that exists (and you're permitted to see). Otherwise, it's a syntax error - the parser doesn't recognise your requested table as a valid table name!Consequently, the first step to implementing aggregation queries on computed fields is extending the aggregation query parser to recognise computed fields as something we know how to aggregate. Now, as well as looping over a table's columns to determine which columns can be aggregated, we also loop over the computed fields. At this stage, we're not generating any SQL: we're simply declaring what can and cannot be aggregated. In the case of computed fields, we can now aggregate any fields whose types are a comparable (in the case of min and max) or numeric (in the case of avg, sum, and so on) scalar type. Computed fields that return table-like types cannot be aggregated.Intermediate representationWith the parser extended, we can make an aggregation query on a computed field without a syntax error, and we can move on to interpreting the query into something useful. GraphQL Engine has its own intermediate representation (IR) outputted by the schema parser. This is an internal data structure whose core describes the query we want to make in a backend-agnostic language. It also includes relevant user configuration such as permission filters, whether to stringify numbers, and so on.As with the parsers, the first step of the change is to extend the IR to allow computed fields to appear in aggregation operations. In practice, this means adding another constructor to the type of allowed fields in aggregations:data SelectionField -- Simplified for illustration purposes\n\t= SFColumn ColumnName ColumnType\n+   | SFComputedField ComputedFieldName ComputedFieldSelection\n\t| SFExp TextOne of the joys of working with Haskell (or any language with exhaustive pattern matching) is that, at this point, the compiler will find every place where SelectionField values are used that now need to add a case for computed fields, and the rest of the work becomes a matter of fixing compiler errors. After fixing a few dozen of those, all that's left is to generate the SQL!IR to SQLAt this point, we're still just following the compiler's breadcrumbs, and we find our way to the aggregation query generator. Let's say we make a request for the average rating of all articles. The generated query probably ends up looking a bit like this:SELECT \n  -- (3)\n  json_build_object(\n    'aggregate', \n    json_build_object(\n      'avg', \n      json_build_object(\n        'rating', \n        avg(\"rating\")\n      )\n    )\n  ) AS \"root\" \nFROM \n  (\n    SELECT \n      -- (2)\n      \"_root.base\".\"rating\" AS \"rating\" \n    FROM \n      (\n        SELECT \n          -- (1)\n          * \n        FROM \n          \"public\".\"article\" \n        WHERE \n          ('true')\n      ) AS \"_root.base\"\n  ) AS \"_root\"This is the anatomy of an aggregation query:First, we query the table for the rows we wish to aggregate, which we name _root.base. This would also be where filters (such as permissions) are applied.From that query, we select the fields we want to aggregate (internally, we refer to this step as applying the extractors), which we call _root.Finally, we call the aggregation functions with the extracted fields in _root, producing the eventual response object as root.In order to handle computed fields, we need to compute the result of the computed field before we call the aggregation functions in step (3). Because computed fields take the whole row as an input, most of our new complexity sits in step (2): if any of the fields we're querying are computed fields, then we need to extract the entire row, rather than individual fields. If none of the fields we're querying are computed fields, then we preserve the old behaviour.The new behaviour can be seen in a query that uses both a regular column and a computed field:SELECT \n  -- (3)\n  json_build_object(\n    'aggregate', \n    json_build_object(\n      'avg', \n      json_build_object(\n        'rating', \n        avg(\"rating\"), \n        'word_count', \n        avg(\n          \"public\".\"word_count\"(\"_root\")\n        )\n      )\n    )\n  ) AS \"root\" \nFROM \n  (\n    SELECT \n      -- (2)\n      \"_root.base\".* AS \"entire_table_row\" \n    FROM \n      -- (1)\n      (\n        SELECT \n          * \n        FROM \n          \"public\".\"articles\" \n        WHERE \n          ('true')\n      ) AS \"_root.base\"\n  ) AS \"_root\"As we can see, step (2) now extracts the entire row for word_count, allowing us to aggregate computed fields, while also not breaking aggregates over regular fields. With the SQL generation done, it's up to Postgres to execute the query and return some results!Wrapping upThanks to some recent work in the schema parsers and query generators, this ended up being a very satisfying change to make. Aggregation queries on computed fields will be available in v2.27 for Postgres, and we're excited to see what you do with them!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: When Congress moves faster than your CIO Description: Lean into low-code APIs or API automation to reap the benefits of faster time to delivery (months instead of years-long projects), lowered costs, and greater scalability to match new and changing demands.  Author: Zack Alspaugh"}, {"role": "assistant", "content": "4 Key Benefits of a Modernization ProjectWant to hear a dad joke? If 'pro' is the opposite of 'con' what is the opposite of 'progress'? Congress!Get it? The United States legislative body is known for being slow \u2013 filibuster-filled, laggard, sluggishly slow. In fact, they were designed to be that way. But when it came to modernizing their 30-year-old technology stack, the lower chamber of the U.S. Congress moved fast \u2013 shockingly fast. Their modernization project was 10x faster than the modernization project of most enterprises, and I\u2019d be willing to bet it will be faster than yours\u2026That\u2019s because most enterprises manually build their APIs, which eats up developer\u2019s time and energy, bottlenecks the modernization process, and adds to the project\u2019s time and cost. Not great. But development teams manually build APIs because they believe it\u2019s the only way it can be done, like when people traveled by horse and buggy or when offices shared memos by fax. Now we are in an era of self-driving cars and generative AI, so why not modernize the modernization process? Companies that lean into low-code APIs or API Automation are reaping the benefits of faster time to delivery (months instead of years-long projects), lowered costs, and greater scalability to match new and changing demands. How automation changed the HouseAndrew Doyle, the Director of Legislative Applications for the U.S. House of Representatives is responsible for architecture that manages all of the data for the legislative branch of the government. Which means he\u2019s responsible for the integrity and the timeliness of all the data associated with proposed bills, passed bills, provisions, amendments, and floor activities. Their tech stack was very outdated and was making it painful for their developer team to maintain and deliver new features and updates. So they had to make a change \u2013 \u201cAll in favor of modernization, say \u2018aye\u2019\u2026 the ayes have it!\u201dModernization, much like a deadlocked House of Representatives, can be a daunting thing. But when Doyle and team decided to automate the API building process, they completely unlocked a new level of speed. That\u2019s because API building is rife with manual coding and repetitive tasks, and it\u2019s at the heart of every step of every modernization process. By eliminating that manual code writing, you drastically ratchet up developer productivity and free up their time, energy, and resources for other tasks. This was a huge advantage for the House\u2019s modernization project.As Doyle explains, API Automation \u201cgot us out of the business of writing data access APIs and allowed us to focus on our business logic.\u201d It became much easier for them to connect to their databases, get instant access through GraphQL, and create fast, data-fetching APIs.But it wasn\u2019t just about speed \u2013 that increased focus on business logic led to a cascade of other critical benefits.Simplified authentication and authorizationCongress is a government institution, which means they work with sensitive government information. Data security was crucial. Automation allowed them to work quickly while still ensuring their data was secure, reliable, and of the highest integrity. Role-based permissions maintained the right levels of access control so only authorized individuals could access the data. Easy integration with ASP.NET Web APIsThe department used ASP.NET Web APIs for their backend, so they could wrap their business logic around their existing REST API endpoints. This enabled them to map it to the central GraphQL API and make it easier to consolidate and encapsulate their business logic.Undisrupted OperationsIt\u2019s the federal government \u2013 they can\u2019t afford downtime. The department was able to use incremental adoption, which eased the transition to a modern architecture, all without disrupting their present operations. They could gradually modernize their systems while still using their legacy APIs and REST endpoints, ensuring a smooth, timely transition. Scalability and AdaptabilityAs a result of these other benefits \u2013 the speed, reliable authorization, no downtime, and updated business logic wrapped around their existing REST APIs \u2013 the U.S. House of Representatives was not only able to modernize their 30-year legacy system but help future-proof their architecture. They can automatically grow the number of users, applications, and data calls without having to think about the number of instances, cores, memory, thresholds, etc. Over time, this will allow for greater flexibility and scalability, ensuring that their architecture adapts to their evolving needs.Based on the success of their modernization effort, the Legislative Applications department plans to implement similar architectures for other applications they manage. They will continue to optimize their tech stack and meet the demands of their users.If the government can modernize its old monolith \u2013 quickly and on budget \u2013 what are you waiting for?Click here to hear an exclusive Q&A with Andrew Doyle and his engineering team. To learn more about the U.S. House of Representatives modernization project, including challenges, architecture diagram, and additional benefits, click here."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Get GraphQL API on Snowflake with Hasura \u2013 Sample App Description: Learn how to create a sample app using Weather data available on the Snowflake marketplace, that utilizes user-defined functions and provides instant GraphQL API in just a few steps. Author: Smit Shah"}, {"role": "assistant", "content": "Hasura just launched GA support for Snowflake, and you can read the announcement here.Hasura provides a high-performance, instant GraphQL API layer over Snowflake. By integrating GraphQL with Snowflake, you can leverage both technologies\u2019 strengths to create efficient, flexible, and responsive APIs for data-driven applications.Today, we will create a sample app using Weather data available on the Snowflake marketplace, that utilizes user-defined functions and provides instant GraphQL API in just a few steps.Architecture:We would use a simple architecture for this application to expose Snowflake data through Hasura. Hasura becomes the API layer through which we expose the data to our clients.Using GraphQL APIs, we can also showcase charts for analytics purposes.Setup Snowflake with Hasura0. Setup Weather Data on SnowflakeYou can add the sample data needed for the application from Snowflake\u2019s marketplace for free. Please use this link to follow the instructions.1. Signup for Hasura CloudSign up for a free Hasura Cloud account and spin up a new project.2. Connect Your Snowflake to Hasura:Follow our Snowflake database guide to set up a Snowflake Data Source for Hasura. Enter your Snowflake JDBC connection details in Hasura\u2019s console as shown below.3. Track tables from SnowflakeOnce connected, you can see your Snowflake tables in Hasura. You can select all the tables you want to query using Hasura and click on Track Selected.4. Query your Snowflake APIOnce you connect and track tables, Hasura instantly gives you a GraphQL API on top of Snowflake that you can immediately use to start querying the data.At the bottom-right of the GraphQL API tab, you'll see the quick response times Hasura's high-performance GraphQL API provides. In this example, computing the average temperature over a few thousand records takes about ~500 milliseconds.Note: These queries are running directly on Snowflake without replicating the data in a different datastore.5. Cache your Snowflake APIYou can use the @cached directive to your query to decrease your response time. The default TTL is 60 seconds, reducing latency by about ~100 milliseconds.Caching is useful when querying Snowflake, as latencies could be a bottleneck, primarily when serving customers on web/mobile applications. If the same data is frequently fetched, caching can improve latencies and reduce the load on data warehouse resources.6. Tracking User-Defined FunctionsHasura now supports user-defined functions (UDF) in Snowflake. To add a UDF, you need to create it from Snowflake\u2019s worksheet, you can find the details here.In our example, we create a UDF which returns avg temperature grouped by week.We also need to create a view to define the return type in Hasura, for that we execute the following query.Now we can track the UDF in Hasura\u2019s console.After clicking track, we need to select the return type, and select the view you created above.Post that we can query the UDF, just like any normal table and we get the same great performance we expect from Hasura.We can also cache the results and get ~100ms response time as we do with normal tables.7. Query your Snowflake API with ReactNow we can expose these data as charts, to help with that we can make use of graphql2chartjs to reshape your GraphQL data per the ChartJS API. This makes it easy to query a GraphQL API and render the output as a ChartJS chart.You can find the sample code to setup this component here.ConclusionAs we can observe, Hasura makes it easy to expose our Snowflake data using GraphQL/REST APIs in a few mins.Hasura, unlike other solutions, doesn\u2019t require you to maintain a copy of your data, and it generates SQL queries based on your request. Hasura also provides a way to cache and rate limit at the same time while offering authentication and authorization options to safeguard your data.Overall, the combination of Hasura Cloud and Snowflake provides an effective way to build scalable, secure, and performant GraphQL APIs on top of a powerful data warehouse. Check out this blog on Hasura's RBAC systems helping secure Snowflake data to understand how you can ensure that your APIs are secure and compliant while also providing rich, flexible access to data.Try out our Snowflake support on Hasura Cloud."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura integration with Snowflake: Empowering developers with effortless data APIs Description: Together, Hasura and Snowflake accelerate the development cycle for data-intensive applications by drastically reducing the effort required to create a data API that acts as the self-service data access layer for application developers and other data consumers. Author: David Meleney"}, {"role": "assistant", "content": "We are excited to announce the GA release of Hasura support for Snowflake. Together, Hasura and Snowflake accelerate the development cycle for data-intensive applications by drastically reducing the effort required to create a data API that acts as the self-service data access layer for application developers and other data consumers. \u201cHasura enabled us to migrate quickly and easily from PostgreSQL to Snowflake, which is a more performant and scalable datastore for serving the APIs and dashboards in our space utilization and analytics platform. Rebuilding the API layer with Hasura took practically no time, allowing our team to focus the migration efforts on building a better customer experience on our new data backend.\u201d \u2013 Matt MacGillivray, Co-Founder and VP, R&D of InnerspaceOur mission has always been to make data access fast, secure, and scalable for developers, and this new integration allows developers to build applications and services on Snowflake 10x faster.Build APIs on Snowflake 10x fasterWith Hasura support for Snowflake, you can now connect your Snowflake data warehouse to Hasura and instantly get GraphQL and REST APIs for your data. This integration makes it incredibly simple to build applications on top of Snowflake, taking advantage of its elasticity, performance, and scalability. Hasura is a Data API Platform that removes the tedious bits of building and operating production data APIs \u2013 reducing cost, complexity, and time to market.6 Key featuresInstant GraphQL and REST APIs: Connect your Snowflake data warehouse and instantly get GraphQL or REST APIs to query your data.Role-based access control: Secure your data with Hasura's fine-grained, role-based access control system.Unified data access: Combine your Snowflake data with other databases, REST endpoints, and GraphQL services in a single, unified API.High-performance SQL: Hasura's powerful query engine optimizes your GraphQL queries for efficient, high-performance data access.User-defined functions: The Hasura GraphQL API can be configured to track Snowflake user-defined functions, unlocking more use cases that leverage the best features of Snowflake, such as AI/ML workflows in Snowpark.Easy to get started: Use Hasura Cloud to start using Snowflake with Hasura in just a few minutes, without any additional setup or infrastructure changes.What can I build with Hasura and Snowflake?With the Hasura Data Connector for Snowflake, developers can rapidly build multi-tenant mobile/web applications directly on Snowflake Data Cloud, without spending countless hours writing (and securing, tuning, hardening, etc) the APIs required to serve the data to the frontend. This could be for internal or external applications, for example: Real-time analytics dashboards: Build live analytics dashboards to visualize and monitor key business metrics from SnowflakePersonalized customer experiences: Create personalized customer experiences by combining Snowflake's powerful data processing capabilities with Hasura's real-time data access, allowing you to tailor content and recommendations based on customer behavior and preferences.IoT and data streaming: Integrate Snowflake's data ingestion and processing capabilities with Hasura's real-time subscriptions to power IoT applications and data streaming solutions.Omnichannel retail: Provide a seamless, unified shopping experience across multiple channels by connecting Snowflake's centralized data warehouse to Hasura's GraphQL API, allowing you to manage inventory, pricing, and promotions in real-time.Supply chain optimization: Improve supply chain efficiency by connecting Snowflake's data analytics capabilities to Hasura's real-time API, enabling instant insights and adjustments based on real-time data.Data-driven marketing: Empower your marketing teams with real-time access to customer data from Snowflake, enabling personalized marketing campaigns and improved targeting.Harnessing Snowflake UDFs with HasuraOne of the standout features of Snowflake is its support for User-Defined Functions (UDFs), which allow you to create custom functions using SQL expressions and the languages supported by Snowflake.UDFs are incredibly powerful, enabling complex computations that would otherwise be difficult or impossible with standard SQL. UDFs enable you to define custom functions using SQL or JavaScript, encapsulating complex logic and computations that go beyond the capabilities of traditional SQL queries.With Hasura's Snowflake support, you can now expose UDFs via GraphQL APIs. This opens up a whole new world of possibilities. For example, you can create a UDF in Snowflake to calculate the average order value for a customer, and then expose this function via a GraphQL API in Hasura. Learn more about Hasura\u2019s support for Snowflake UDFs here.Lightning-fast APIs with native SQL query optimizationThe Hasura engine uses a JIT compiler approach to translate an inbound GraphQL query into the SQL query that is executed on Snowflake. The Hasura connector for Snowflake uses cutting-edge SQL query optimization techniques that are specifically tailored for Snowflake. This ensures that the Hasura-generated API is blazing fast out of the box \u2013 no SQL expertise needed. We meticulously analyzed the performance bottlenecks that arise when querying large datasets with complex relationships and permissions. By leveraging the unique features of Snowflake, we have fine-tuned the query execution pipeline, resulting in remarkable performance improvements.With our optimized SQL queries, you'll experience significant reductions in query execution time, for large and small datasets. Hasura on Snowflake empowers you to retrieve precisely the data you need, minimizing unnecessary data transfer and accelerating your application's responsiveness. Say goodbye to long waits and hello to lightning-fast data access.ConclusionThe GA release of Hasura support for Snowflake marks a significant milestone in empowering organizations to leverage the power of real-time data and effortlessly generate applications from their Snowflake data. This integration brings together the strengths of Hasura's GraphQL API platform and Snowflake's cloud data platform, unlocking a multitude of business use cases and offering a range of benefits to organizations.By seamlessly integrating Snowflake's powerful data processing capabilities with Hasura's real-time data access and application generation, organizations can accelerate their development cycle and reduce the effort required to create data APIs. This allows developers and other data consumers to focus on building exceptional customer experiences and driving innovation, rather than spending valuable time on API development.To get started with Hasura and Snowflake, comprehensive documentation, guides, examples, and quickstarts are available to support developers in their journey. By signing up for Hasura Cloud and connecting their Snowflake data warehouse, organizations can unlock the full potential of their data and embark on a path of building innovative, data-driven applications.Happy building! \ud83d\ude80\ud83c\udf89\ud83d\udcda Documentation & ResourcesTo help you get started, we've prepared detailed documentation, guides, and examples:Getting Started with Hasura and SnowflakeSnowflake QuickstartSecuring Your Snowflake Data with HasuraSnowflake Integration VideoSnowflake Blog PostSnowflake Sample App\ud83d\ude80 Get Started Today!We can't wait to see the amazing applications you'll build using Hasura and Snowflake. Get started today by signing up for Hasura Cloud and connecting your Snowflake data warehouse.If you have any questions or need assistance, feel free to reach out to our team on Discord or GitHub. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Elevating your API strategy with Hasura Description: Rather than viewing Hasura as a replacement for API gateway tools, we will highlight the incremental adoption approach, showcasing how Hasura seamlessly integrates with established API gateway platforms such as Apigee, MuleSoft, and more.  Author: Akshay Bhat"}, {"role": "assistant", "content": "Leveraging Hasura with API gateway tools.In today's digital landscape, APIs (Application Programming Interfaces) play a crucial role in enabling seamless integration, data exchange, and collaboration across various systems and applications. As organizations strive to build robust API strategies, they often turn to API gateway tools to effectively manage and secure their APIs. \u00a0In this blog post, we will explore the powerful combination of Hasura, a cutting-edge GraphQL engine, with API gateway tools. While Hasura itself offers comprehensive API management capabilities, we will focus on how existing API gateway tools can complement and enhance Hasura, empowering developers and organizations to unlock the full potential of their API strategies.Rather than viewing Hasura as a replacement for API gateway tools, we will highlight the incremental adoption approach, showcasing how Hasura seamlessly integrates with established API gateway platforms such as Apigee, MuleSoft, and more. By leveraging the unique features and strengths of both Hasura and API gateway tools, organizations can optimize their API strategies, improve time to market, enhance security, and drive monetization opportunities.We will delve into different scenarios where Hasura can be utilized alongside API gateway tools to address specific challenges and unlock key benefits. From API monetization and metering to traffic routing, TLS termination, load balancing, preventing bot attacks, and catering to custom routing use cases, we will explore the breadth of possibilities when combining Hasura with API gateway tools.Whether you are a developer, a DevOps engineer, or a technology leader, this post provides valuable insights into how Hasura enhances your existing API gateway infrastructure, enables rapid development, streamlines monetization strategies, and accelerates time to revenue. So let's dive in and discover the immense value of leveraging Hasura alongside API gateway tools to supercharge your API strategy.1. \u00a0API generation for the API gatewayOne of the key advantages of using Hasura in conjunction with other API gateway tools is its ability to directly generate APIs. Hasura acts as a GraphQL engine that sits between your applications and various data sources, allowing you to expose GraphQL and REST APIs that seamlessly connect to your backend services. By leveraging Hasura's automatic schema generation and real-time subscriptions, you can simplify the process of creating APIs for the API gateway.Typically, API gateway tools require explicit configuration and setup for each API endpoint. However, with Hasura, you can generate a comprehensive GraphQL API that encompasses all the required data and functionality from your backend systems. This eliminates the need for manual configuration of individual APIs in the gateway, streamlining the development process and reducing maintenance overhead.By utilizing Hasura's API generation capabilities, you can seamlessly integrate it with the API gateway of your choice. The API gateway can then leverage the generated APIs to manage traffic, apply security policies, and implement additional features specific to your API strategy. This combination allows you to leverage the power of GraphQL and the advanced capabilities of the API gateway, resulting in a more efficient and flexible API ecosystem.Use CaseAPI generation with Hasura for API gatewayArchitectureIn this architecture, the API gateway serves as the entry point for incoming requests. It handles request routing, security, and other gateway-specific features. When a request is received, the API gateway routes it to Hasura.Hasura acts as a GraphQL engine that generates APIs based on the underlying data sources, which can include various backend services, databases, or microservices. It automatically generates the GraphQL schema and exposes a comprehensive API that encompasses the required data and functionality from the backend systems.The API gateway can then leverage these generated APIs provided by Hasura to manage traffic, apply security policies, and implement additional features specific to your API strategy. It serves as the intermediary between client requests and the Hasura-powered APIs.Hasura's ability to generate APIs for the API gateway simplifies the process of API configuration and maintenance. Instead of manually configuring individual APIs in the gateway, you can rely on Hasura to dynamically generate the APIs based on the underlying data sources.By combining Hasura's GraphQL generation capabilities with the features provided by the API gateway, you can create a powerful API ecosystem that seamlessly integrates different backend services while taking advantage of the gateway's advanced functionalities.2. API monetization, metering, and time to revenueWhen considering Hasura in conjunction with other API gateway tools, one of the key advantages is its ability to enhance API monetization, and metering, and significantly reduce the time to revenue. By leveraging Hasura's GraphQL capabilities alongside the monetization features of API gateway tools like Apigee or MuleSoft, you can expedite the process of generating revenue from your APIs.Hasura plays a vital role in this architecture by generating comprehensive GraphQL APIs that seamlessly integrate with your backend services. These APIs serve as the foundation for your monetization strategy, allowing you to effectively monetize your APIs.By combining Hasura's API generation with the monetization features provided by API gateway tools, you can quickly implement pricing models, usage-based billing, subscription plans, and access control mechanisms. This enables you to efficiently monetize your APIs and start rapidly generating revenue.The value of Hasura, from a monetization perspective, lies in its ability to significantly reduce time to revenue. With Hasura, you can build products and services up to 10 times faster compared to traditional approaches. Once your APIs are built using Hasura, you can seamlessly integrate them with API gateway tools like Apigee or MuleSoft, which offer robust monetization features.The API gateway tools provide capabilities such as usage tracking, metering, analytics, and reporting, enabling you to gain deep insights into API usage patterns and trends. These insights are valuable for understanding customer behavior, optimizing pricing strategies, and making data-driven business decisions.By leveraging the combined power of Hasura and API gateway tools, you can accelerate your time to market and revenue. Hasura's rapid API generation capabilities, coupled with the monetization features of the API gateway tools, allow you to roll out your products and services quickly, capitalize on market opportunities, and start generating revenue in a shorter timeframe.Use CaseImplementing monetization and metering capabilities for APIs to generate revenue based on usage. ArchitectureThe existing API gateway tool can handle API monetization and metering, while Hasura serves as the backend GraphQL layer, providing data aggregation and composition. Clients interact with the API gateway, which enforces access control, rate limiting, and metering policies before routing the requests to the Hasura GraphQL API.In this architecture, the existing API gateway acts as the entry point for API requests. It handles the initial request processing, security, and rate-limiting aspects. The API gateway then forwards the requests to Hasura's GraphQL Engine.Hasura serves as the central GraphQL layer, providing a unified API interface for data sources. It integrates with various data sources, such as databases, microservices, or external APIs, to aggregate and compose the necessary data. Hasura retrieves the requested data and returns it to the API gateway, which then responds to the client with the aggregated data.By integrating Hasura with the existing API gateway, organizations can leverage the power of GraphQL and enhance their API strategy. Hasura simplifies data composition, reduces the complexity of handling multiple data sources, and provides a consistent API interface. This architecture enables incremental adoption of Hasura within the existing API strategy, allowing organizations to take advantage of GraphQL capabilities without completely replacing their current API gateway solution.The emphasis on time to revenue underscores the importance of speed and agility in today's competitive landscape. By leveraging Hasura's efficiency and the monetization features of API gateway tools, you can streamline your API strategy, reduce time-to-market barriers, and achieve faster returns on your API investments. \u00a03. API traffic routing via policiesAPI gateway tools excel at managing API traffic and routing requests to the appropriate backend services based on defined policies. However, in some cases, more complex routing requirements may arise, necessitating the need for additional capabilities. Hasura can seamlessly integrate within this architecture to enhance traffic routing.By placing Hasura behind the API gateway, you can leverage the gateway's routing capabilities, such as route matching, traffic splitting, or URL rewriting. Hasura's GraphQL engine becomes one of the backend services that the API gateway routes requests to. This allows you to take advantage of the advanced routing features of the API gateway while benefiting from Hasura's data aggregation and composition capabilities.In this architecture, the API gateway acts as the entry point for incoming requests, applies routing policies based on criteria such as request headers, paths, or query parameters, and directs the requests to the appropriate backend services, including Hasura. Hasura then retrieves and processes the required data from the connected data sources and returns the response to the API gateway, which forwards it to the client.By combining Hasura with an API gateway's powerful routing capabilities, you can achieve more flexible and granular control over API traffic routing, enabling you to implement complex routing scenarios and meet specific business requirements.Use CaseRouting API traffic based on specific policies such as user roles, geographic location, or request attributes. ArchitectureThe API gateway tool handles the traffic routing based on policies, while Hasura acts as the backend GraphQL layer that provides unified access to data sources. Clients communicate with the API gateway, which directs the requests to the appropriate Hasura endpoints based on the defined policies.In this architecture, the existing API gateway is responsible for initial request processing, security enforcement, and other gateway-specific functionalities. The API gateway forwards the requests to Hasura's GraphQL engine.Hasura serves as the GraphQL layer, providing a unified API interface to interact with various data sources. It integrates with data sources, such as databases and microservices, to aggregate and compose the necessary data. Hasura then applies policies defined by the API gateway to perform API traffic routing based on factors like user roles, geographic location, or request attributes.The policies determine how the requests are routed to specific APIs or data sources within Hasura. Hasura evaluates the policies and directs the requests to the appropriate resources, ensuring that the traffic follows the defined routing rules.By integrating Hasura with the existing API gateway, organizations can leverage the powerful routing capabilities of the gateway and the data aggregation capabilities of Hasura. This architecture enables fine-grained control over API traffic and provides flexibility in defining routing rules based on various factors.4. TLS terminationWhen it comes to secure communication between clients and backend services, TLS termination is essential. While API gateway tools excel at handling TLS termination, Hasura can seamlessly integrate within this architecture.Hasura's GraphQL engine can be placed behind the API gateway, where the gateway handles TLS termination and routes the requests to Hasura over an encrypted connection. This ensures secure communication while allowing Hasura to focus on data aggregation and composition.Use CaseHandling TLS termination for secure communication between clients and backend services. ArchitectureThe existing API gateway tool can handle TLS termination, while Hasura focuses on data aggregation and composition. Clients establish secure connections with the API gateway, which terminates TLS and proxies the requests to the Hasura GraphQL API.In this architecture, the existing API gateway is responsible for handling TLS termination and routing for secure communication between clients and backend services. The API gateway acts as the entry point for encrypted requests.Hasura's GraphQL engine sits behind the API gateway and handles data aggregation and composition. It receives requests from the API gateway over an encrypted connection.The API gateway is responsible for terminating the TLS connection, ensuring secure communication between the clients and the gateway. It also handles routing the requests to the appropriate Hasura GraphQL endpoints based on the defined routing rules.By integrating Hasura with the existing API gateway, organizations can leverage the secure communication capabilities provided by the gateway while benefiting from Hasura's powerful GraphQL engine for data aggregation and composition.5. Load balancing / reverse proxy(nginx-like use cases)In high-traffic scenarios, load balancing and reverse proxy capabilities are crucial for distributing requests and achieving scalability. API gateway tools often offer these features, and Hasura can complement them effectively.By placing Hasura behind the load balancer or reverse proxy, you can leverage the load-balancing algorithms and routing capabilities provided by the gateway. The gateway distributes the incoming requests to multiple instances of Hasura, ensuring optimal performance and high availability.Use CaseDistributing incoming API requests across multiple backend instances to ensure scalability and high availability. ArchitectureThe existing API gateway tool performs load balancing and acts as a reverse proxy, distributing requests among backend services. Hasura, as the GraphQL layer, connects to the backend instances managed by the API gateway, ensuring seamless data retrieval and composition for clients.In this architecture, a load balancer is responsible for distributing incoming API requests across multiple instances to ensure scalability and high availability.The load balancer receives the incoming requests and forwards them to a reverse proxy, which acts as an intermediary between the clients and the backend services.Hasura's GraphQL engine sits behind the reverse proxy and serves as the centralized GraphQL layer. It connects to the data sources, such as databases and microservices, to retrieve and aggregate the necessary data.The reverse proxy receives the incoming requests and routes them to the appropriate Hasura GraphQL endpoint based on load-balancing algorithms or other configuration rules. Hasura then handles the data retrieval and composition, returning the aggregated data to the reverse proxy, which in turn responds to the clients.By integrating Hasura with the load balancer and reverse proxy, organizations can achieve scalable and high-performing APIs. The load balancer ensures the efficient distribution of requests, while the reverse proxy handles routing and load balancing at the network level. Hasura focuses on data aggregation and composition, providing a powerful GraphQL engine for seamless interaction with data sources.6. Preventing bot attacks and unusual activityIn the ever-evolving landscape of cybersecurity threats, organizations need robust measures to protect their APIs from malicious activities, including bot attacks and unusual traffic patterns. While API gateways typically offer some security features, additional tools like Imperva are often utilized for specialized bot mitigation capabilities.Hasura, when combined with existing API gateway tools, can augment the security measures in place by providing an additional layer of protection against bot attacks and unusual activity. While it is true that some API gateway tools may not offer comprehensive bot mitigation features, the combination of Hasura and specialized security tools delivers a robust defense strategy.By leveraging Hasura as an intermediate layer, you can implement custom security measures, such as rate limiting, request validation, and IP blocking, to identify and mitigate malicious bot traffic. Hasura also provides security features against these use cases (you can have a look here).By combining the strengths of Hasura and specialized security tools, you can establish a comprehensive defense against bot attacks and unusual activity. Hasura's ability to integrate with various security solutions allows you to customize and strengthen your API security posture, ensuring the integrity and availability of your APIs.It is worth noting that while API gateways may not provide extensive bot mitigation capabilities out of the box, the integration of Hasura with specialized security tools can help you achieve a robust and comprehensive security strategy. By incorporating Hasura in this architecture, you can leverage its flexibility, scalability, and ability to integrate with best-in-class security solutions, enhancing your overall API security posture.Use CaseProtecting APIs from malicious bots, DDoS attacks, and other unusual activities. ArchitectureThe API gateway tool specializes in bot detection, DDoS mitigation, and anomaly detection, providing a robust defense mechanism. Hasura complements this by serving as the GraphQL layer, where clients interact securely with the API gateway's protected endpoints.In this architecture, a bot detection and security system is responsible for identifying and preventing bot attacks and unusual activities.Hasura's GraphQL engine sits behind the bot detection and security system and serves as the centralized GraphQL layer. It connects to the data sources, such as databases and microservices, to retrieve and aggregate the necessary data.The bot detection and security system monitors incoming API requests for suspicious patterns, unusual activity, or known bot signatures. It applies security measures to identify and block potential bot attacks, ensuring the integrity and availability of the API.Once the requests pass through the bot detection and security system, they are forwarded to Hasura. Hasura retrieves the requested data from the data sources, performs necessary data transformations or aggregations, and returns the response to the client.By integrating Hasura with a robust bot detection and security system, organizations can enhance the protection of their APIs against bot attacks and unusual activities. Hasura's GraphQL engine ensures seamless data retrieval and aggregation, while the security system adds an additional layer of protection to safeguard the API infrastructure.7. Custom routing use casesIn some scenarios, you may have specific routing requirements, such as hosting the GraphQL endpoint on a different URL or using custom domains. API gateway tools can facilitate these custom routing needs, and Hasura can seamlessly fit into such architectures.By integrating Hasura with an API gateway that supports custom routing configurations, you can route requests to the appropriate Hasura GraphQL endpoint or define custom domains for different parts of your API. Hasura's GraphQL engine handles data retrieval and composition, while the gateway ensures the requests are directed correctly based on the custom routing rules.Use CaseConfiguring custom routing for specific GraphQL endpoints or utilizing custom domains. ArchitectureThe API gateway tool handles the routing configurations, including mapping custom domains or setting up endpoint URLs. Hasura seamlessly integrates with these custom routes, providing the necessary GraphQL functionality and data aggregation as requested by the clients.In this architecture, a custom routing layer is responsible for directing API requests to specific endpoints or services based on custom routing rules.Hasura's GraphQL engine sits behind the custom routing layer and serves as the centralized GraphQL layer. It connects to the data sources, such as databases and microservices, to retrieve and aggregate the necessary data.The custom routing layer receives the incoming API requests and evaluates custom routing rules. Based on these rules, the requests are directed to the appropriate Hasura GraphQL endpoint or to other services that may be integrated within the routing layer.Once the requests reach Hasura, it handles the data retrieval, composition, and any necessary transformations, returning the response to the custom routing layer.Additionally, the custom routing layer can also handle custom domains, allowing API requests to be routed to specific domains or subdomains based on configuration.By integrating Hasura with a custom routing layer, organizations can achieve fine-grained control over request routing and customize the behavior of their API endpoints. Hasura's GraphQL engine handles the data aggregation and composition, while the custom routing layer adds an additional layer of flexibility and customization to the API architecture.Conclusion Integrating Hasura with existing API gateway tools can enhance their capabilities and provide additional value in various use cases. Whether it's API monetization, traffic routing, TLS termination, load balancing, security, or custom routing, Hasura acts as a powerful GraphQL engine that complements the functionalities of API gateway tools, resulting in a more comprehensive and efficient API management solution.While API gateway tools provide essential functionalities for managing, securing, and optimizing APIs, integrating Hasura within your API strategy can bring added benefits. Whether leveraging Hasura's GraphQL engine for data aggregation, complementing existing gateway features, or accommodating custom routing needs, combining Hasura with other API gateway tools can enhance your overall API architecture and deliver a seamless developer and user experience.By assessing your needs, and evaluating the capabilities of both Hasura and the API gateway tools, you can design an API strategy that maximizes the strengths of each component.Leveraging the strengths of both Hasura and API gateway tools, enterprises can create robust, scalable, and secure APIs that cater to their specific requirements, while enabling incremental adoption and enhancement of their existing API strategies.Sign up now for Hasura Cloud to try these out!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura vs Apollo: Comparing GraphQL Platforms Description: In this blog, we will share our (opinionated, but grounded) perspective on why Hasura is better than Apollo for building and running production GraphQL APIs at scale in most enterprise use cases. Author: Praveen Durairaju"}, {"role": "assistant", "content": "GraphQL has gained prominence for developing modern apps because of its flexible querying, efficient data fetching, strong typing and self-documentation. Hasura is a popular choice for generating GraphQL APIs because it allows developers to instantly go from data source to API, bypassing a lot of the manual effort and time required to build GraphQL backends. When talking to developers evaluating Hasura, we are often asked how Hasura compares with Apollo GraphQL, another popular tool in the GraphQL ecosystem. While the two technologies co-exist in the GraphQL space, the amount of code, time, effort, and skills it takes to go from data to production-grade GraphQL API is very different in Hasura and Apollo. In this blog, we will share our (opinionated, but grounded) perspective on why Hasura is better than Apollo for building and running production GraphQL APIs at scale in most enterprise use cases.What is Hasura?Hasura is a data API automation platform that removes the tedious parts of building and operating production-ready APIs on data. With Hasura, developers don\u2019t need to write GraphQL resolvers or boilerplate CRUD code or spend precious time adding access control, performance optimizations, or security limits into your API to get it operational. What is Apollo GraphQL?Apollo is a GraphQL tooling company that provides a set of open-source and proprietary tools to enable developers to build apps with GraphQL. Apollo Server is the JavaScript implementation for writing a custom GraphQL server. Apollo Client is a front-end SDK to integrate GraphQL in client-side apps. Apollo GraphOS is the platform for building, managing, and scaling a supergraph (a federated graph composed of several subgraphs).Hasura advantages over Apollo GraphQLWe will primarily compare Hasura vs Apollo Server for creating individual GraphQL services or endpoints. While we\u2019ll touch on how Hasura compares with Apollo GraphOS for composing, managing, and evolving a federated graph, a more detailed comparison of the two technologies for GraphQL Federation use cases will be made in a follow-up blog.Hasura\u2019s main value over DIY GraphQL tools like Apollo is time to value. For the data sources it supports (see full list), Hasura eliminates or automates 50-80% effort that goes into building, securing, deploying, and scaling production APIs. By radically compressing the API development timelines, enterprises can shave off months/years of effort from their modernization projects with Hasura.\"If we had gone the traditional way this process would have taken us 2-4 years. With Hasura, we have been able to crunch it to just under a year. Achieving this timeframe in a highly regulated environment like healthcare is phenomenal.\u201d \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Karthik Srinivasan, Solution Architect, Philips HealthcareTime to APIHasura auto-generates a full-featured and customizable GraphQL API endpoint from your database in minutes. With DIY GraphQL using Apollo, you need to write a lot of boilerplate code (resolvers, schema, etc.) to get a basic API running.With Hasura, you can go from a new or existing data source to a fully featured API in minutes. Once connected to a database, Hasura automatically generates a GraphQL endpoint with all the core GraphQL operations like queries, mutations, and subscriptions based on the schemas of the connected data sources. This means you can start querying tables and views with filtering, sorting, pagination, joins, pattern search, and much more in a matter of minutes. If you are using the Apollo Server, you would need to manually write the resolvers to generate the GraphQL APIs. It\u2019s a lot of code and resources to create a boilerplate CRUD API.In contrast, Hasura uses a JIT compiler approach to automatically compile GraphQL queries into a single SQL query to be executed on the database, completely bypassing the need for writing resolvers \u2013 a huge productivity boost.And this is just to get a basic CRUD API. With DIY GraphQL, developers have to spend a lot of time adding security, performance, and scalability to get the API ready for production. Many tasks are also simplified in Hasura through built-in features highlighted in the following sections.PerformanceHasura dynamically compiles your GraphQL query into a single SQL query that is highly optimized for the underlying datastore the data is being fetched from, giving you a fast API out of the box. With DIY GraphQL on Apollo, API performance needs to be manually tuned by the developer building the server and relies on knowledge of GraphQL and SQL performance best practices. Hasura is a metadata engine that dynamically compiles a GraphQL query (of any depth) to a single SQL query in real-time. The compiled GraphQL queries are efficient and highly optimized for the underlying datastore and don't have the typical N+1 problem with GraphQL. With DIY GraphQL approaches like Apollo, a developer needs to spend significant time tweaking their code to make it performant (for example, avoiding the N+1 problem). The problem can become more difficult when multiple teams/developers are collaborating on an API. Hasura also avoids the cartesian product problem while fetching data from underlying systems by performing JSON aggregations in the upstream database itself. This is usually not the case with DIY GraphQL servers like Apollo since the SQL for JSON aggregations are not generated with ORMs.You can further boost API performance with the native query response caching feature in Hasura that works automatically with an embedded Redis instance. (Read more about Hasura Caching). Apollo Server requires a manual setup of a cache backend like Redis.Finally, Hasura uses predicate pushdown to embed authorization rules into the single query run on the databases. Not only is this better from a security perspective, it\u2019s also more performant by avoiding multiple roundtrips to execute authorization logic coded in a separate layer. Embedding authorization rules into the SQL query of the resolver is a lot of work that would amount to building an engine like Hasura.See our recent benchmark tests where Hasura was 3.6x faster than DIY GraphQL APIs on PostgreSQL. The performance difference was more prominent in complex queries where multiple levels of nesting to fetch related data are requested, in line with GraphQL fetch-what-data-you-want principles.Bonus: Cost savings!Most of Hasura\u2019s performance advantages come from the compiler-based approach that allows Hasura to fetch the required data (and nothing else) from the underlying datastore in a single highly-optimized SQL query. This translates to cost savings by minimizing network hops and data egress, which can add up quickly for data-intensive applications. \"Hasura reduced our client data fetching time from 10 seconds to less than 1 second, which greatly improved the user experience.\" Read more about Cajoo\u2019s case study.Authorization, Security, and GovernanceHasura comes with a powerful authorization engine that allows you to define fine-grained access control policies in a simple, declarative manner. \u00a0With a DIY Apollo solution, you need to manually parse the context details of the request and write extra code logic to handle authorization.Hasura comes with a powerful security and rule engine that handles schema, model, and field-level authorization. Hasura\u2019s authorization engine helps model the following types of common authorization requirements:Role-basedAttribute-basedRule-basedInheritance compositionRow-levelField/column levelThe declarative nature of Hasura authorization lets organizations easily enforce, review, and audit security rules, without talking to developers or dealing with code.Hasura's Role Based Access ControlThe authorization model on Apollo is based on forwarding the request \u201ccontext\u201d to the resolvers, where you have to manually parse the context details like the headers to create logic for authorization. For a complex permission system, this is challenging to replicate and maintain in your code base. Moreover, Hasura\u2019s authorization engine uses a predicate push-down model that is secure, and also more performant. Apollo does not do a predicate push-down. The database query generated through ORMs and other forms does not embed the user context to multiple depths of a query; and in cases where you want to, it is not trivial. And since this is code written manually to apply these rules, it is error-prone.\"By using Hasura, we cut the development time in half and built our product in 3 months. The built-in role-based authorization system made it easy to secure our data.\" Read the Pulley's case studyFor runtime security, Hasura supports setting API rate limits based on authorization attributes and user roles. It also provides advanced security for GraphQL subscriptions and WebSockets to prevent DoS attacks, malicious users, and badly configured clients. This is not possible with Apollo Server and usually requires integrating an external library or service and additional custom code.API Limits and Security in HasuraDeveloper ExperienceWith GraphQL and Hasura, frontend developers are empowered to feel like full-stack developers since it gives them the power to quickly create, modify, and iterate on the API layer without getting blocked on other teams. This is made possible by a bunch of features that Hasura provides, but more importantly, it\u2019s just a great developer experience, to begin with. Hasura is declarative and metadata-driven. Apart from the custom logic code that is integrated with Hasura, everything else can be ported to different environments in a few clicks or commands. Hasura Console UI lets you create, manage and test GraphQL APIs and the data sources connected. Hasura CLI tool lets you manage metadata.Apollo Studio is the UI element in the GraphOS stack. It helps you manage your GraphQL schema, analyze usage, and explore APIs. Apollo Studio lets you manage the subgraphs, org-level access control, and GraphQL schema via their Schema Registry interface.Hasura Console, on the other hand, gives a powerful user experience for managing connected data sources via a database manager UI. Here you can create and manage schemas and tables and establish relationships between them. You can also configure the GraphQL Federation API by adding any external GraphQL API and setting up joins between them. Hasura\u2019s Authorization system is declarative and easy to define, thanks to the Console experience.Using the Hasura Console and CLI, you can automate the creation of database migrations that can be applied to different environments. Apollo\u2019s GraphOS platform doesn\u2019t deal with databases in its workflows.With Apollo Server, you are also bound to write a custom GraphQL implementation with JavaScript. If you want to switch to a different language or framework, you need to go outside the Apollo stack and add them as a subgraph. GraphQL implementations are not known to be optimized for performance in all languages and frameworks. As the official GraphQL spec changes, everything gets ported to the native JS implementation and might take a while to reflect in other languages and frameworks, including Apollo. The experience is different as limitations differ in different implementations.Real-time APIs and appsGraphQL Subscriptions in Hasura are automatically generated, highly scalable, and provide usage analytics out of the box which is hard to replicate in a DIY setup.Hasura offers real-time data synchronization out of the box. Hasura uses WebSocket connections to push data changes to clients in real time. This means that clients receive updates as soon as they are available, without needing to refresh the page or make additional requests.Real-time APIs over WebSockets are quite stateful and expensive, making them incredibly difficult to deal with in production. WebSockets need another layer of web security implemented. Hasura\u2019s authorization system ensures that it sends the correct events to the right consumers.Scaling Hasura's GraphQL Subscriptions to 1 million requestsApollo also offers real-time data synchronization, but it requires additional setup and configuration and developers have to write server code to handle WebSocket connections. A custom pub-sub implementation is required with Apollo. It becomes even harder when authorization is involved for a custom-written real-time API since this state needs to be managed somewhere and the results need to be filtered at the source layer, in real-time.Hasura is also able to give insights into usage analytics for GraphQL over WebSockets and GraphQL subscriptions. It is not straightforward to monitor and analyze WebSockets with Apollo\u2019s GraphOS platform. Learn more about how Hasura gives you a scalable subscriptions API on Postgres.GraphQL FederationHasura allows you to declaratively combine multiple sub-graphs such as databases, REST APIs or GraphQL APIs into a supergraph while maintaining the flexibility and modularity of the underlying services.As GraphQL adoption spreads into enterprises, a federated GraphQL approach is needed to tame the sprawl while retaining team autonomy. Apollo has introduced and \u00a0evangelized the \u201csupergraph\u201d \u2013 a federated GraphQL architecture to help enterprises operate GraphQL at a larger scale. Apollo GraphOS is their platform for building, managing, and scaling a supergraph. While Apollo GraphOS does provide convenient features (like Apollo Studio, Schema Registry, Apollo Router, etc.) for composing and managing the supergraph, it has some limitations: Not declarative: You need to insert custom middleware code into your subgraphs to connect them into the Apollo supergraph. Check out the list of compatible libraries that are compliant with Apollo Federation specifications.Heavy lift for non-GraphQL subgraphs: Apollo Federation is optimized by federating compatible GraphQL servers. While you can technically federate REST and non-GraphQL resources with the Apollo, it requires significantly more effort.Does not account for the effort to build the subgraph Apollo GraphOS assumes your subgraphs are built and ready to go. It does not account for the time it takes to build a GraphQL for your sub-domain.By contrast, Hasura's approach to GraphQL Federation is: Declarative: You can declaratively add subgraph resources (GraphQL, REST, databases, etc.) to Hasura to get a federated API. Unlike Apollo, you do not need to make any proprietary modifications to GraphQL subgraph servers.Multi-protocol support:  Federation in Hasura is not restricted to just GraphQL. You can federate instantly across databases, REST APIs, and of course GraphQL APIs as well.Instant GraphQL Subgraphs: Hasura gives instant GraphQL APIs over many popular data sources and can be used as a subgraph without any extra effort.When to use Apollo GraphQL?As discussed above, Apollo does not automate any of the GraphQL API creation, security, or performance optimization. It was not designed to. Apollo GraphQL is a good choice if you really need to manually develop a GraphQL server. The common use cases where we see users choose Apollo over Hasura:You want to write new custom logic code in GraphQL (ideally, not recommended. You can use REST APIs and stitch them to GraphQL as well).Already have an existing GraphQL server manually built using Apollo Server and want to continue for maintenance.Hasura does not support instant GraphQL / API for your database.In all other cases, you are going to be investing a lot of time and resources to build a custom GraphQL backends with Apollo.Already using Apollo?If you are already using Apollo GraphQL, you have several choices to incrementally and gradually bring Hasura into the mix to enjoy some of the benefits mentioned above. Hasura as a subgraph: If you are using Apollo GraphOS for GraphQL federation, you can feed a Hasura-powered GraphQL endpoint as a subgraph into your Apollo supergraph. Hasura will let you build new subgraphs much faster than building them by hand with Apollo or other DIY methods. You can also gradually port existing subgraphs to Hasura if the maintenance burden of those is getting cumbersome.Hasura as a federated graph (supergraph): If you have existing GraphQL endpoints powered by Apollo Server, you can bring that in as a source via a Remote Schema. Hasura acts as a gateway of sorts in this case and lets layer on benefits like declarative authorization layer on your existing Apollo GraphQL endpoint.If you are already using or exploring Apollo and want to learn more about how you can leverage Hasura, reach out to us for a custom consultation / demo.Business Impact of HasuraHasura\u2019s biggest advantage over DIY GraphQL with Apollo is significantly lower time to market, with a smaller backend development team. By automating 50-80% of the time it takes to develop, operate, and scale a high-quality GraphQL API, Hasura allows you to build and ship products and features faster. With Hasura, your developers can focus on innovating and building products vs. spending hours on non-differentiated work like writing CRUD code and resolvers. Hasura also reduces your reliance on developers with deep GraphQL expertise, which significantly broadens the talent pool for hiring, but also lets you do more with the team in place.Hasura\u2019s compiler approach is architecturally more cost-efficient, which results in cost-savings out of the box. Similar efficiency and savings could be achieved with DIY GraphQL but with significant development effort.As with any DIY approach, you likely get slightly more control of the implementation assuming you have GraphQL experts who really know what they are doing. The question to ask yourself is whether you really need that level of customization, and is the resulting increased time to market, costs, and complexity worth the tradeoff?"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Increase performance with Webhook Auth Caching Description: Introducing Webhook Auth Caching \u2013 a big leap forward in optimizing the performance of webhook-based authentication mode, making it an even more powerful and efficient option for your authentication needs. Author: Sooraj Sanker"}, {"role": "assistant", "content": "We're excited to announce an enhancement to Hasura's authentication capability in webhook mode. Introducing Webhook Auth Caching \u2013 a big leap forward in optimizing the performance of webhook-based authentication mode, making it an even more powerful and efficient option for your authentication needs.Authentication is the bedrock of any secure application, and at Hasura, we're constantly striving to enhance the developer experience while ensuring robust security. Webhook auth mode historically involved making a webhook call for every API request. We understand that performance is crucial, and repetitive webhook calls could impact the end-user experience of your applications.With Webhook Auth Caching, you can strike the perfect balance between security and performance. It not only saves precious network round-trip time but also reduces the load on your authentication infrastructure, enabling it to scale effortlessly as your user base grows. Additionally, this feature seamlessly integrates with existing authentication workflows, making it easy to adopt without any significant changes to your codebase.What\u2019s the default behavior?If you enable webhook authentication on Hasura, Hasura will make a webhook call to authenticate each request. Unlike the JWT mode authentication, it requires a network call for each GraphQL query execution, as shown in the following diagram.What's new?The default configuration works well for normal use cases (considering the webhook is fast enough), but what if you need to build even higher performance apps? What if the webhook is processing the same request context over and over again? Can this be reused for some time? With Webhook Auth Caching, the webhook auth response is cached in Hasura to make sure the subsequent requests (with same token) don\u2019t require any additional webhook requests for a given period of time.How does it work?As shown below, the webhook calls can be optimized by enabling Webhook Auth Caching.Here\u2019s how it works:Client makes the GraphQL query with request headers.Hasura receives the request and pass it to the Webhook Auth processor, and:If the caching is configured, Hasura checks if the webhook response has a valid previous cache entry or not. (Configuring Webhook Auth Caching requires a Redis instance to be connected, and the `Cache-Control` headers can be used on the auth webhook response to handle cache expiry \u2013 more details are documented on the docs.)If the caching is not configured, fast forward to to step 4 \u2013 make a webhook request with GraphQL request context.Redis returns a webhook response if the data is cached.If there is a valid cache entry, Hasura skip step 4 and 5 and fast forward to 6 and directly pass the auth context to Permission Rule engine to process the request and make data fetch query.If there is no valid cache response, proceed to step 4.Hasura initiates the webhook API call with the GraphQL request context.If the request is authenticated, webhook returns the authorization headers back to Hasura.Hasura will process the request based on the Permission Rules and data fetch query will be generated and executed to generate the end GraphQL response. Benefits of Webhook Auth CachingThis new functionality benefits your system in a couple of ways, which will help increase performance and ultimately lower the cost to run your applications.Increased GraphQL query performanceA subsequent webhook request no longer requires an authentication check until the cache expires, making the new GraphQLrequests much faster than the first request.The following trace shows how much the query performance can vary with normal conditions. This performance elevation can be even more significant if your auth service is costly.Reduced load on auth serviceConsidering the number of webhook calls are reduced with effective webhook caching, your auth server/service can now relax a bit with a lot less requests. We did an experiment by simulating GraphQL requests with and without webhook auth caching. The following shows the load on the auth server has reduced drastically with a ~1 min caching setup. ConclusionAt Hasura, we're committed to empowering developers with cutting-edge tools that simplify and optimize their application development and help them quickly build production-grade APIs. The introduction of Webhook Auth Caching represents a powerful enhancement in the authentication toolkit in Hasura. Upgrade to the latest version of Hasura today and unlock the full potential of Webhook authentication with enhanced performance and efficiency.Sign up now for Hasura Cloud to try this feature!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: 5 HasuraCon 2023 happenings that\u2019ll make attending another virtual event worth it Description: HasuraCon 2023, happening June 20-22, is just around the corner! If you're a developer, engineer, or tech enthusiast, this event is not to be missed.  Author: Hasura"}, {"role": "assistant", "content": "HasuraCon 2023, happening June 20-22, is just around the corner! If you're a developer, engineer, or tech enthusiast, this event is not to be missed. HasuraCon brings together experts, thought leaders, and professionals from around the globe to explore the latest trends, insights, and innovations in the world of GraphQL, data architecture, and app development. HasuraCon 2023In this blog, we\u2019re highlighting five reasons why attending HasuraCon 2023 is a must for anyone looking to stay ahead in the rapidly evolving technological landscape.Access unparalleled learning opportunitiesHasuraCon offers a diverse, extensive range of sessions, workshops, and keynotes, all delivered by experts. From deep dives into GraphQL and data engineering to hands-on coding workshops and use cases illustrating real-world implementations, HasuraCon's agenda is packed with valuable insights and knowledge-sharing opportunities. Some callouts are Day 1 workshop \u201cBuild an AI application using Hasura,\u201d Day 2 keynote with Hasura CEO and Co-Founder, Tanmai Gopal on \u201cThe future of data delivery via API with Hasura,\u201d and Day 3 product deep dive \u201cData federation made easy with Hasura.\u201dWhether you're a beginner or an experienced practitioner, you'll find content catered to your needs, allowing you to expand your skills and deepen your understanding of these future-forward technologies.Network with tech leaders and peersOne of the advantages of attending HasuraCon is the opportunity to connect and network with like-minded developers, data architects, and more. The event attracts a diversified and highly talented audience, including engineers, developers, architects, and CTOs from leading companies. Engage in conversations, exchange ideas, and forge valuable connections with the folks shaping the future of data access Collaborate, learn from others' experiences, and explore potential partnerships that can accelerate your growth. Our virtual platform of choice to host HasuraCon 2023 \u00a0features topic tables to prompt discussion and a leaderboard with points for interacting with sessions and a chance to win some cool prizes!Stay on top of the latest data trendsHasuraCon is at the forefront of the strategies and solutions for data access, management, and modern app development, making it the ideal platform to gain insights into the latest advancements and emerging practices. Be the first to learn about breakthroughs, innovative tools, and strategies transforming the tech landscape. For HasuraCon 2023, we\u2019ll be highlighting all the most important updates happening in Hasura version 3, and introduction of a new way to define your domain\u2019s data and API specification, as well as plenty of content on GenAI and indispensable tips on unlocking the full potential of your database. Stay ahead of the curve by attending sessions on serverless architectures, federation, AI applications, data engineering best practices, cloud-native development, and more. Hands-on workshops and demosAt HasuraCon 2023, passive listening in sessions is only one way to get educated on all things GraphQL and APIs. The event offers one day of hands-on workshops, like \u201cBuild an AI application using Hasura\u201d and \u201cGraphQL federation with Hasura,\u201d as well as plenty of interactive demos that allow participants to engage directly with experts and learn through practical, real-world examples.Whether you want to explore the depths of GraphQL, understand data modeling and schema design, or master the art of building scalable and performant applications, these workshops provide an immersive experience to enhance your skills and put theory into practice. Gain firsthand experience, ask questions, and receive personalized guidance.Discover new tools and technologiesHasuraCon 2023 is a treasure trove of new tools, technologies, and solutions that can revolutionize how you develop applications and manage data. Discover innovative frameworks, libraries, and development platforms that can supercharge your productivity and enable you to build robust, scalable, and performant applications. Get your hands on beta versions, preview upcoming releases, and learn from the creators. Unleash your creativity and uncover game-changing tools that empower you to build cutting-edge applications.With plenty of learning opportunities, networking, insights into the latest trends, hands-on workshops, and access to new tools and technologies, HasuraCon 2023 is the place to be. See you there!Register today!!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How Protex AI Builds Fast and Saves Lives Description: Ireland-based Protex AI uses computer vision to help enterprise occupational health and safety teams make proactive safety decisions contributing to a safer workplace. They build on the Hasura platform while protecting data privacy with granular, role-based authorization. Author: Cliff Malmborg"}, {"role": "assistant", "content": "Ireland-based Protex AI uses computer vision to help enterprise workplace health and safety teams make proactive safety decisions contributing to a safer workplace. They build on the Hasura platform while protecting data privacy with granular, role-based authorization.The Problem: Protex AI\u2019s ChallengeWorkplace health and safety is a matter of life and death; in the U.S., a worker died every 101 minutes in 2021.Ciar\u00e1n O\u2019Mara, the Co-Founder and CTO of Protex AI, heard of a death on a production line and wondered whether AI and machine learning could help prevent such accidents. O\u2019Mara and long-time friend Dan Hobbs started interviewing health and safety practitioners, speaking with 200 people before writing a single line of code. \u201cCompanies either take health and safety seriously or very seriously,\u201d says O\u2019Mara. \u201cThere's no one that does not take health and safety seriously.\u201d Safety challenges cut across industries, from manufacturing to logistics, meat processing, energy, oil, gas, and more. Despite ongoing investments in best practices, specialized equipment, and training, accidents and fatalities continue. Closed-circuit television (CCTV) cameras monitor production line activity from all angles. What if those video feeds could be analyzed to find predictors or indications that best practices were slipping and risk was increasing?But running machine learning on CCTV video in a way that does not violate privacy laws presents yet another challenge. Any solution must balance safety with privacy, satisfying both health and safety professionals and the IT leaders who scrutinize technology solutions.Protex AI\u2019s solution would need to complement existing technology stacks mainly made of Excel, email, Microsoft Teams, and reporting software. \u201cWe would need to sit on top of that stack and augment the data pool that health and safety people have to make decisions,\u201d said O\u2019Mara.The company\u2019s goal? To deliver value to health and safety teams week after week, providing insights and education that materially improve the health and safety of employees.The Solution: Why Protex AI Chose HasuraO\u2019Mara and another developer set about building a proof of concept (POC) with an Express.js backend, which required writing microservices from the ground up. \u201cThere were two of us building the proof of concept. We knew it wasn't going to scale, or if it was going to scale, it was going to be painful,\u201d said O\u2019Mara. \u201cWe were putting a lot of time into architecting APIs when we wanted to put the time into architecting the database.\u201dAn intern showed them GraphQL, which led them to Hasura, with its instant API capabilities. \u201cIt became obvious straight away that Hasura was going to help us build fast,\u201d said O\u2019Mara. \u201cWith Hasura, the API piece becomes an abstracted afterthought. Hasura writes the resolvers so we don\u2019t have to.\u201dWhen developers had questions, the Hasura Community offered immediate answers. \u201cHaving the ability to chat with Hasura engineers was massive,\u201d said O\u2019Mara.As the Protex AI team continued to build with Hasura, they found even more reasons to make it their development platform of choice.Diverse Development Teams Meet in HasuraProtex AI has several development teams: a product team, an infrastructure and security team, a computer vision and systems team, a data and ML ops team, and a privacy and security team. Hasura is a unified surface where the teams meet. \u201cHasura is the same language to the integrators on the computer vision team and the product team,\u201d said O\u2019Mara. \u201cWhen we think about our database or API system, it's Hasura, even though we have a full database stack behind it.\u201dAccelerating OnboardingHasura cuts the time for onboarding developers. \u201cWhen we onboard a new hire, we give them a couple of projects centered around Hasura,\u201d said O\u2019Mara. \u201cThey're able to pull data and play with it, show it on the front end, and see that it works.\u201d The Hasura API Explorer makes it easy for Protex AI developers to build events and visualize what they're doing. Time to productivity for new developers is impressive: \u201cIt takes us less than two weeks to get new engineers up and running using the API system,\u201d says O\u2019Mara.Satisfying Security RequirementsProtex AI has to satisfy its customer\u2019s IT security leaders, who are concerned with data security, as well as the privacy of the CCTV camera feeds that Protex AI ingests and analyzes.Hasura\u2019s granular authorization enables Protex AI to lock down access to data. Protex AI deployed Hasura Enterprise on-premise to oversee sensitive customer data flowing through their database. Hasura\u2019s security features have helped Protex AI in passing ISO27001 audits as well as penetration tests.The Results: The benefits of using HasuraHasura\u2019s instant API capabilities do the lion\u2019s share of the work in creating APIs. There\u2019s no need to hire API engineers because Hasura solves the problem for them. Without Hasura, O\u2019Mara estimates that they would need two additional experienced API architects to manage the extra workload.\u201cHasura has allowed us to stop thinking about data access and start thinking about data processing,\u201d says O\u2019Mara. \u201cIt's data abstracted for us. We don't have to worry about data connectivity.\u201dRecent penetration tests showed that authorization rules written in Hasura blocked unauthorized access to the data in Protex AI\u2019s database. Pen testers could see front-end components but could not get to the data. Importantly, pen testers attempted to exploit the OWASP Top 10 security vulnerabilities, and Protex AI successfully passed those tests. Building fast helps Protex AI close business. During a POC, a new customer may request several features. Delivering some features immediately strengthens the business case and often wins the deal. \u00a0\u201cBuilding on Hasura has enabled us to ship features that require no database changes and build internal tools in less than a week in some cases,\u201d says O\u2019Mara.Protex AI picks up on signals such as lapsing safety behaviors and procedures that aren\u2019t being followed. \u201cThat is the way that health and safety should be practiced,\u201d says O\u2019Mara. \u201cIt's picking up on things before they lead to accidents as opposed to waiting for accidents to happen.\u201d Hasura frees up developers so that Protex AI can focus on its business in much the same way as they use computer vision tools instead of writing them. The company can then focus on its core mission: Making the world a safer place.Ready to hear more interesting and insightful real-life Hasura use cases? Register today for HasuraCon 2023, happening June 20-23, both virtual and free!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Automating the logistics of developing with Hasura Self-Hosted Description: Explore ways to automate these processes to create a more streamlined and consistent development experience when working with Hasura. Author: Hasura"}, {"role": "assistant", "content": "About EquiEqui helps investors unlock generational wealth by providing access to investments normally reserved for the institutions and the ultra-wealthy. We evaluate thousands of investments to provide our customers with one diversified and managed portfolio of alternative asset investments (things that are not stocks and bonds). Best of all, we are powered by an intuitive and easy-to-use software platform that our customers can use anywhere. Learn more at www.equi.com.Why Equi Chose HasuraThe Equi engineering team prides itself on being nimble and efficient. We\u2019re a small team aiming for top 1% in productivity. That\u2019s why when it came time to architect our core backend service we chose Hasura. Hasura helps us write dramatically fewer backend controllers, enforces row-based security and provides a management UI that makes day to day operations a breeze.Hasura\u2019s self-hosted version (the graphql-engine container) is incredibly powerful. You can run it on any infrastructure and get a powerful API generator that can either be a gateway to other services in your stack or sit alongside your services as part of a larger ecosystem. Updating Hasura\u2019s configuration (they call metadata) is easy with their user interface, and they provide tools to export (and thus add to source control) all the metadata as a set of JSON files. Managing this configuration with multiple developers, even with a small team, can be tricky to avoid accidental omissions or errors.In this blog post, we'll explore ways to automate these processes to create a more streamlined and consistent development experience when working with Hasura.The ScenarioYou have a local developer environment that uses Hasura and at least one remote schema (i.e. an application that provides data or actions to your graph). For the purposes of this walkthrough we assume your using Docker Compose to orchestrate the setup of these containers. This setup allows for a consistent and isolated development environment that's easy to replicate across different stages of the project.The Hasurawatcher Sidecar ContainerThe first automation we'll implement is a sidecar container that imports metadata and reloads remote schemas on boot. This container will run a simple script that waits for Hasura to load, then executes the necessary import commands. By doing so, we'll ensure that metadata and remote schemas are always up-to-date when your local instance starts. \u00a0CODE: watchconfig.sh (note this script assumes that your Hasura metadata files are mounted inside the Docker container at /hasuraconfig/ as in the docker-compose example below).#!/bin/sh\n\n\n# This script is intended to be run inside the 'hasurawatcher' docker container\n\n\n# Give hasura itself some time to load\necho \"waiting for hasura to load...\"\nyarn wait-on -l http-get://hasura:8080/console\n\n\necho \"hasura loaded, beginning initial import\"\n\n\n./node_modules/hasura-cli/hasura metadata apply --admin-secret LOCAL_HASURA_ADMIN_SECRET --skip-update-check --endpoint http://hasura:8080/ --project /hasuraconfig/\nCODE: docker-compose.ymlhasurawatcher:\n  container_name: hasura_watcher\n  stop_signal: SIGKILL\n  depends_on:\n    hasura:\n      condition: service_started\n  build:\n    context: .\n    target: hasurawatcher\n    dockerfile: ./Dockerfile\n  volumes:\n    - './packages/hasura/:/hasuraconfig/:Z'\n  networks:\n    - equi-local-network\n  extra_hosts:\n    - 'host.docker.internal:host-gateway'\n  command: sh watchconfig.sh\n  restart: unless-stoppedCODE: DockerfileFROM node:18 AS hasurawatcher\n\nCOPY packages/hasura/package.json .\nRUN apt update\nRUN apt-get install inotify-tools -y\nRUN yarnpackages/hasura/package.json: (make sure you replace <LOCAL_HASURA_ADMIN_SECRET> with your actual local hasura admin secret){\n \"name\": \"hasura-local-tools\",\n \"version\": \"0.0.1\",\n \"description\": \"\",\n \"author\": \"\",\n \"private\": true,\n \"license\": \"UNLICENSED\",\n \"scripts\": {\n   \"hasura\": \"./node_modules/hasura-cli/hasura --skip-update-check\",\n   \"reload:local\": \"yarn hasura metadata reload --admin-secret <LOCAL_HASURA_ADMIN_SECRET>\",\n   \"export:local\": \"./node_modules/hasura-cli/hasura metadata export --admin-secret <LOCAL_HASURA_ADMIN_SECRET> --skip-update-check\",\n   \"import:local\": \"./node_modules/hasura-cli/hasura metadata apply --admin-secret <LOCAL_HASURA_ADMIN_SECRET> --skip-update-check\",\n \"devDependencies\": {\n   \"hasura-cli\": \"^2.13.0\",\n   \"js-yaml\": \"^4.1.0\",\n   \"wait-on\": \"^6.0.1\"\n }\n}Ensuring metadata changes from remote are importedTo keep your local environment in sync with remote changes, we'll add an inotify script to our sidecar container. This script will continuously monitor the metadata directory for changes and automatically apply them to your local Hasura instance. This ensures that any updates made by a fellow developer are automatically propagated to your local environment after you git pull without any manual intervention.echo \"Initial import complete, watching for local changes to hasura config...\"\n# Watch for changes forever and import\nwhile inotifywait -q -e modify -r /hasuraconfig/; do\n   echo \"Saw a change in hasura config!  Re-applying...\"\n   ls /hasuraconfig/\n   ./node_modules/hasura-cli/hasura metadata apply --admin-secret LOCAL_HASURA_ADMIN_SECRET --skip-update-check --endpoint http://hasura:8080/ --project /hasuraconfig/\ndone\nMaking it easier to export and commit metadata changesTo make it simple for developers to export and commit metadata changes, we'll provide a script with local authentication that exports metadata to the appropriate location. This script can be easily executed whenever a developer makes a change in the Hasura console, ensuring that all changes are saved to version control. This script is also referenced in the package.json file above.\"export:local\": \"./node_modules/hasura-cli/hasura metadata export --admin-secret LOCAL_HASURA_ADMIN_SECRET --skip-update-check\",\nFinally, write up some instructions for your team so they know how this setup works. \u00a0For example:README.MDOur team has implemented a process (called hasurawatcher) that monitors your local file system and automatically imports any changes in hasura metadata into your local dockerized hasura instance.  All you have to do to import a coworker's changes is pull them down with your source control system and they will automatically be imported.\nCommitting Local Hasura Metadata Changes\nWhenever you make changes to Hasura metadata (e.g. permissions, relationships, remote schema configuration) you should then run `yarn export:local` which will export all the metadata as json to your local file system and be ready for a git add / commit.```\n\nConclusionBy implementing these automations in your Hasura development workflow, you'll make it vastly simpler for your developers to keep their Hasura instance in sync with other developers, saving time and frustration on a regular basis. With metadata and remote schemas consistently synchronized across your local instance, other running services, the file system, and higher environments, you can focus on building powerful applications with confidence. Happy developing!About the authorZach Goldberg is a seasonsed tech entrepreneur, innovation enthusiast, and author with a \"founder's mentality\" who believes that engineering software should be more science than art. A passionate learner, leader, and executive, Zach is one of the rare CTOs who has cultivated extensive, nuanced, and proven expertise that spans industries while serving numerous startups and scaled organizations, including Microsoft, Google, Dama Financial, Savvy Insurance, and Lottery.com. He is currently at Equi, an investment firm."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura GraphQL on Snowflake using RBAC: A secure and scalable data access solution Description: Learn what role-based access control (RBAC) is, its advantages, and how Hasura's RBAC system can be used to secure your Snowflake data warehouse. Author: Akshay Bhat"}, {"role": "assistant", "content": "When it comes to managing access control for your databases, role-based access control (RBAC) is a popular choice. In this post, we'll take a look at what RBAC is, its advantages, and how Hasura's RBAC system can be used to secure your Snowflake data warehouse.What's RBAC?RBAC is a security model used to control access to resources based on the roles of the users. In this model, roles are defined based on the responsibilities and permissions of the users. Access control policies are then defined based on these roles, rather than for individual users.In an RBAC system, you might define a \"manager\" role that has access to certain resources and an \"employee\" role that has access to a different set of resources. Users are then assigned to these roles, and their access to resources is determined by the permissions associated with the role.Advantages of an RBAC SystemRBAC provides several advantages over other access control models:Scalability: RBAC is highly scalable and can be used to manage access control for large and complex systems.Simplicity: RBAC is easy to understand and implement, as it is based on the roles and responsibilities of users.Granularity: RBAC provides fine-grained access control, allowing access to be controlled at a very granular level.Flexibility: RBAC allows access control policies to be easily modified as the needs of the organization change.Why Hasura's RBAC?Hasura's RBAC system provides several advantages over Snowflake's native RBAC system:Integrations: Hasura's RBAC system can be easily integrated with other Hasura features, such as row-level security (RLS), to provide even more granular access control. Hasura's RBAC system can also be integrated with other third-party authentication and authorization systems, such as Auth0, to provide even more flexibility and control.Ease of use: Hasura's RBAC system is easy to use, with a simple and intuitive user interface.Automation: Hasura's RBAC system can be automated, allowing access control policies to be easily modified as the needs of the organization change.Performance: Hasura's RBAC system is designed to be performant, with minimal overhead and fast response times. Check out this blog post.RBAC Example in HasuraLet's assume we have a Snowflake data warehouse containing one table \u2013 \"employees.\" We want to build a GraphQL API using Hasura that allows users to query the data in these tables based on their roles.We will set up Hasura to connect to the Snowflake data warehouse using JDBC. We will configure Hasura to use the RBAC roles defined in Snowflake to control access to the data in the GraphQL API.Then, we will define the roles and permissions using Hasura's RBAC system. We will create one more role along with the default \"admin,\" namely \"user.\" The \"admin\" role will have full access to all data in the \"employees\" table, while the \"user\" role will have access to only his profile in the \"employees\" table.The above architecture diagram describes how this row permission works.If the \"admin\" user runs this query, they will receive a list of all employees, including their IDs and names.If the \"user\" user runs this query, they will only receive their own profile id.Here\u2019s the architecture of the different components:Here's how it works:1. \u00a0Connect database from Snowflake: You can connect the database from Snowflake that you would like to use in your APIs and your application.2. \u00a0Define permissions in Hasura: You can define permissions in Hasura for each role that controls access to the data.3. \u00a0Use Hasura to control access to APIs: You can use Hasura's RBAC feature to control access to the APIs based on the roles of the users. 4. \u00a0Use Hasura to generate GraphQL APIs: Hasura will auto-generate GraphQL APIs based on your Snowflake database schema and the permissions defined in Hasura.RBAC in Hasura Demo \u00a0Now let\u2019s move on to an example to demonstrate how you can use RBAC using Hasura and Snowflake. In this tutorial, we will set up Hasura Cloud to use Snowflake as a data source using JDBC and RBAC.Prerequisites:A Hasura Cloud accountA Snowflake account with JDBC driver installed and configuredBasic knowledge of Hasura and GraphQLStep 1: Create a new Hasura Cloud projectLog in to your Hasura Cloud account and create a new project. Once your project is created, you'll be taken to the project dashboard.Step 2: Add a new data sourceClick on the \"Data\" tab on the left-hand side of the dashboard, and then click on \"Add\" to add a new data source. Select \"Snowflake\" as the data source type and provide the required connection details, including the JDBC connection string, username, and password.Step 3: Track the Snowflake tablesClick on the \"Data\" tab on the left-hand side of the dashboard, and then click on Database you just added as a new data source. Go to the \u201cUntracked\u201d tab, select the tables and click \u201cTrack Selected.\u201d You should now see the tables on the \u201cTracked\u201d tab.Step 4: Set up RBACOnce your data source is connected, you can set up RBAC to control access to your data. Click on a table you want to set up the permissions for and then click the \"Permissions\" tab on the left-hand side of the dashboard, and then click on \"Add role\" to add a new role. Give your role a name, such as \"user,\" and then define the permissions for that role.There are primarily two types of permissions you can set up for each of Insert, Select, Update and Delete \u2013 they are row and column permissions.You might allow the user role to read only certain rows from the table. Here\u2019s an example of row permission:{\u201cEMPLOYEE_ID\u201d : {\u201c_eq\u201d:\u201dX-Hasura-User-Id\u201d}}\nThis only allows the user to select rows from the table where the \u201cEMPLOYEE_ID\u201d equals the Hasura user id.You can also define permissions at the column level, allowing a role to read data from certain columns but not others. You might also want the user to be able to read the data but not modify or delete it.Here\u2019s an example of setting the column permissions to allow the user to access only the \u201cEMPLOYEE _ID\u201d column.Step 5: Test your GraphQL APINow, you can test your GraphQL API using the Hasura console or by sending requests to your API using a GraphQL client. For example, you might send a query to retrieve data from a specific table:query MyQuery { EMPLOYEES { EMPLOYEE_ID NAME } }\nIf your role has permission to read data from the \"EMPLOYEES\" table, you should receive a response containing the requested data.You can notice that since \u201cX-Hasura-Role\u201d isn\u2019t selected, it uses the default \u201cadmin\u201d role, which can access all rows and columns in the table.Now, if we select the \u201cuser\u201d role in the headers, it can only access specific rows and columns as specified in the permissions before. Otherwise, it throws an error.In this specific example, the \u201cuser\u201d role can only see and select rows with \u201cEMPLOYEE_ID\u201d equal to 2 and can only see the EMPLOYEE_ID column.ConclusionIn conclusion, Hasura Cloud provides an easy-to-use and powerful platform for creating GraphQL APIs on top of Snowflake using JDBC, with added security through RBAC. By leveraging Hasura's RBAC system, developers can ensure that the right users have access to the right data, while also being able to enforce fine-grained access control down to the row and column level.The ability to set RBAC rules and policies in Hasura Cloud provides a powerful way to secure and control access to your data, making it ideal for building APIs for sensitive or complex data sources like Snowflake. With Hasura's intuitive UI and seamless integration with Snowflake, developers can easily build and deploy powerful APIs with minimal configuration and maximum security.Overall, the combination of Hasura Cloud, Snowflake, and JDBC provides an effective way to build scalable, secure, and performant GraphQL APIs on top of a powerful data warehouse. With Hasura's RBAC systems, developers can ensure that their APIs are secure and compliant while also providing rich, flexible access to data.Sign up now for Hasura Cloud to try this feature!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Get GraphQL APIs on WordPress and MySQL using Hasura Description: With Hasura\u2019s support for MySQL, we can now point Hasura to existing or new MySQL databases that are configured to work with WordPress installations. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Hasura just announced support for Instant GraphQL APIs on top of MySQL, MariaDB and Oracle. Read the announcement post.With Hasura\u2019s support for MySQL, we can now point Hasura to existing or new MySQL databases that are configured to work with WordPress installations.But wait a minute, WordPress? Yes, WordPress is still a dominant player in the world of online publishing. While the estimates of WP powering 43% of all websites certainly include a number of abandoned blogs and ill-fated high-school newspapers, it does power a surprising amount of platforms for household names. Al Jazeera, Bloomberg, and CNN just get us through the ABCs of their logo wall with top-shelf brands spanning travel, social media, finance, news, and more. WordPress + MySQL GraphQL APIs using HasuraNeedless to say, enabling GraphQL for data models built on WP lets development teams leverage familiar editorial workflow tools while accessing the latest in software development trends.What does this mean for you? In case you have a WordPress setup, usually configured on MySQL, you can now:Get Instant GraphQL and REST APIs for querying your blog posts and any properties in WordPress.Run WordPress as a Headless CMS.Replace plugin-style installations like WPGraphQL with Hasura as Hasura just connects to DB and doesn\u2019t modify WordPress instances.Declaratively configure granular access control rules for fetching blog posts, pages, tags, categories, and plugin data with Hasura\u2019s Authorization layer.Join this WordPress data with other databases like Postgres or other MySQL sources to get a federated API for your client.Customize the API tailored to developer needs.Instant GraphQL APIs for querying WordPressHasura, when connected to the MySQL database, can give an instant GraphQL API for CRUD operations. WordPress is typically connected to MySQL or its variants for performance, which opens up the ability to add an API on top of the connected DB.WordPress does not come with GraphQL support by default and the REST APIs can get verbose quickly with each added custom Post Type. There are plugins and tooling, which add a GraphQL API on top. What Hasura offers on top of providing a GraphQL API is the ability to configure role-based permissions giving the APIs a declarative Authorization layer.Hasura GraphQL APIs on top of WordPress/MySQL setupHeadless CMS with WordPress and Hasura APIsOnce you separate out the API layer from the WordPress CMS, content creators can focus on using WordPress purely from a CMS perspective and developers can use the Hasura APIs on top to build rich frontend websites.A while ago, we posted about exploring Hasura as a Headless CMS.Now the frontend teams can build the website with the framework of choice. Popular frameworks like React (Next.js, Gatsby), Vue.js (Gridsome), Svelte, and more (like static site builders) can now be used to generate fast-performing pages, while still getting the benefits of the CMS solution in WordPress.Connect WordPress-configured MySQL DB with HasuraLet\u2019s create a new project on Hasura Cloud. This will create a project and give a GraphQL endpoint on Hasura Cloud. Next up, we need to connect the database. Now, let\u2019s delve into how to do this. For the MySQL DB connected to WordPress, you need the following details:MySQL HostMySQL Port (usually 3306)MySQL Database nameMySQL userMySQL passwordOnce you have these details, you need to form the JDBC connection string which is of the format:jdbc:mysql://host:post/database?user=user&password=passOn the Hasura Console, head to the Data Tab, click on Connect Database, and choose the MySQL option in the Data Source Driver dropdown. Enter the JDBC connection URL that you just created. Give the database a name, let\u2019s say \u201cwordpress.\u201dConnect Existing DatabaseQuerying WordPress Posts, Tags, and Author via GraphQLThe tables in MySQL DB are all in the format of wp_posts, wp_users, wp_terms (for categories), etc. If you are interested in the entire DB structure, refer to their documentation on Database Description. The schema data model looks like this, as cited in their docs.WordPress DB SchemaAssuming we added the MySQL DB with the name wordpress, the query for fetching Posts will look something like this in the default settings.query fetchPosts {\n  wordpress_wp_posts {\n    ID\n    post_title\n    post_date\n  }\n}\nSince the table name for posts, is wp_posts and the DB prefix is wordpress, Hasura exposes GraphQL on this db/table combination. The column names of the table include ID, post_title, post_date and similar for other fields.From a GraphQL API point of view, this query is not particularly elegant. So let\u2019s customize the API to make it more natural and concise.Let\u2019s update the GraphQL schema for the table name and column names. Head to the wp_posts table and navigate to the Modify tab.Under Custom Field Names, click on Edit field names and enter the value posts for the custom table name.Similarly, for the column names, click on Edit and add the custom name.Customize GraphQL Field Types WordPress using HasuraAfter doing this for post_title and post_date to be updated to title and date, we can now make the following GraphQL query:query fetchPosts {\n  posts {\n    id\n    title\n    date\n  }\n}The above query is now more readable and elegant for the clients to consume.RESTified endpoints for WordPressHasura supports RESTifying a GraphQL query to an equivalent REST API. This is useful in case the clients need a REST API over GraphQL, or if there is existing tooling in place for REST APIs across your services. On the GraphiQL interface of Hasura Console, click on the REST button for the query that you just executed.GraphQL query to fetch posts from MySQL using HasuraYou can define the endpoint which starts with /api/rest/ and give it a name. You can select if this request is going to be a GET or POST etc. Here we can select GET since we are just fetching posts with 3 columns and no filters.Create REST Endpoint for WordPress Posts using HasuraAfter creating this, Now you can start making requests to fetch posts on the REST endpoint, which will look like below:http://<your-hasura-base-endpoint>/api/rest/postsYou will be able to customize the entire API workflow by using both GraphQL and REST and make your frontend client make authorized calls.Notes on SupportHasura currently supports MySQL versions v8.0 and above and if your WordPress installation is on an older version of MySQL, you need to upgrade it to the newer version to be able to use the APIs."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Dynamic Routing on Hasura Description: If you have data distributed/replicated over different databases, or planning to improve performance of your data layer by introducing different database topologies, or if your use-case requires different queries to use database user credentials dynamically \u2013 you will find this blog useful. Author: Sooraj Sanker"}, {"role": "assistant", "content": "Have you ever wondered how flexible Hasura makes querying selective data on the frontend? What if you get that level of flexibility in the database connection selection process? Isn\u2019t it amazing to serve data with different database connections or even different databases for different types of requests? Wouldn\u2019t that be opening up a lot of possibilities? With read replicas, Elastic connection pooling, and the auto-scaling capabilities of Hasura, you can achieve more balanced database queries and a more reliable system. One thing that was missing, however, is the ability to route these requests dynamically to different databases or database connections based on the request context.Well! We are excited to announce a new feature \u2013 Hasura Dynamic Routing for data sources! This feature is designed to incorporate different database topologies in Hasura and use them effectively. This new feature helps with many use cases including having multiple (database) users, using a per-database, multi-tenant model and advanced routing logic. If you have data distributed/replicated over different databases, or planning to improve performance of your data layer by introducing different database topologies, or if your use-case requires different queries to use database user credentials dynamically \u2013 you will find this blog useful.What is Hasura Dynamic Routing?With Dynamic Routing, you can define multiple database connection configurations (connection sets) and routing logic (connection template), and when a GraphQL query hits Hasura, Hasura checks the routing logic and chooses the correct database connection configuration to execute the database query. Dynamic Routing ensures that databases are used efficiently, which delivers better performance and helps reduce infrastructure cost.Let's look at some of the basic examples that I feel will help you understand the feature in a better way. (Of course, you can do a lot more with this!)Connect with different database connection configurationsIf you are integrating with an IAM vendor or wish to use database RLS, then you\u2019ll want to use different connection credentials per request. For example, in Hasura you may use a session-variable like x-hasura-role to use a specific connection string for that role.The connection template for the same would be:{{ if ($.request.session.x-hasura-role == \"manager\")}}\n    {{$.connection_set.manager_connection}}\n{{ elif ($.request.session.x-hasura-role == \"employee\")}}\n    {{$.connection_set.employee_connection}}\n{{ elif ($.request.session.x-hasura-role == \"X\")}}\n    {{$.connection_set.DB2}}\n{{ else }}\n    {{$.default}}\n{{ end }}\n\nNote: $.connection_set used above is explained later in this document.No stale reads when read replicas are configuredCurrently, if read replicas are configured, all query operations are routed to the read replicas, which sometimes leads to stale reads due to replication lag. You can force certain query operations to the primary connection by using some operation context variable like operation name or a special request header.The connection template for the same would be:{{ if (($.request.query.operation_type == \"query\") \n|| ($.request.query.operation_type == \"subscription\")) \n&& ($.request.headers.x-query-read-no-stale == \"true\") }}\n    {{$.primary}}\n{{ else }}\n    {{$.default}}\n{{ end }}\n\nThe above example will route the read queries to the primary database when x-query-read-no-stale header is passed. Even if a read replica is configured, this query hits the primary DB with the above routing logic.Route to a specific shard or node-group in a distributed databaseYou can now route the queries to a specific node in a distributed database system like Yugabyte/CockroachDB by specifying different nodes as connection sets. And then, define the routing logic by defining a Kriti template based on your business logic. The connection template for the same would be:{{ if ($.request.session.x-hasura-tenant-id == \"my_tenant_1\")}}\n    {{$.connection_set.my_tenant_1_connection}}\n{{ elif ($.request.session.x-hasura-tenant-id == \"my_tenant_2\")}}\n    {{$.connection_set.my_tenant_2_connection}}\n{{ else }}\n    {{$.default}}\n{{ end }}\n\nA sample logic can be something like Route database query to Node A if the GraphQL request contains session variable \u201cx-hasura-tenant-id\u201d is equals to \u201ctenant_1\u201dEnhanced security benefits of using Dynamic RoutingDynamic database connection routing in Hasura provides enhanced security for your applications by allowing you to control access to your databases on a per-request basis.By configuring connection sets and connection templates, you can route requests to specific databases or connections based on session variables or headers, which helps in isolating data and providing role-based access control. Let\u2019s walk through some of the key security benefits.Fine-grained access controlDynamic connection routing allows you to route requests based on user roles, enabling you to implement role-based access control at the database connection level. This ensures that users can only access the databases and connections they are authorized to use, reducing the risk of unauthorized access.Isolation of sensitive dataBy routing requests to specific databases or connections, you can isolate sensitive data in separate databases, and ensure that only authorized users can access it.Integration with third-party IAM solutionsDynamic connection routing can be integrated with third-party identity and access management (IAM) solutions, providing additional security for your databases. How does Dynamic Routing work?You can now provide a connection template to the source configuration which will be resolved during runtime for any non-admin graphql query and the query will be executed against the resolved connection. A \"resolved connection\" can be any one of:A reference to existing primary connection (connection_info) or read-replicasA connection set member (a connection configuration that can be added to a datasource and can be used in template as $.connection_set.<connection_set_name>)A direction to fallback to current behaviorThe \"connection set\" is a list of connections predefined in the metadata so that:it's easy to refer to it in the template andadditional properties like connection pool settings can also be supplied> Note: The \"primary\" connection url (connection_info) is still required with a source configuration to run system queries like catalog introspection, metadata queries, event triggers, etc.Overall, the GraphQL request resolution would look like the following: As mentioned above, Dynamic Routing will now decide which database connection configuration needs to be used to resolve that particular GraphQL query instead of using the primary configuration.ConclusionDynamic Routing for databases helps you to implement more complex database topologies, such as multi-tenancy models, without sacrificing performance or introducing another infrastructure to handle the database query routing. With this, you can use a per-database multi-tenant model with minimal metadata management. This can lead to cost savings and increased efficiency, as well as a delightful user experience for your customers.More technical details on the Dynamic Routing lives here. We are actively looking for more use cases and how this feature helps your organization to scale, so reach out to us in our Discord channel if you find it helpful or if you have any questions around this.Sign up now for Hasura Cloud to try this feature!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Instant APIs for MySQL, MariaDB, and Oracle Description: Today, we're announcing Hasura has achieved a breakthrough \u2013 instant APIs (GraphQL and REST) on top of MySQL, MariaDB, and Oracle with built-in production-ready features allowing you to get an API server running within days.  Author: Rishi Divate"}, {"role": "assistant", "content": "MySQL, MariaDB, and Oracle are undoubtedly some of the most widely utilized databases in the market today. Over the past several years, I\u2019ve worked closely with numerous software development teams building applications on top of these databases and have personally experienced the effort it takes to build a data access layer that provides an interface between the application and the database.This data access layer is generally built by using an ORM (object-relational mapping) library or framework that maps objects in the application to relational database tables. It takes a lot of custom coding to create an API layer that provides a standardized way for business logic services or other applications to access and manipulate the data.These efforts often take months since \u2013 in addition to building the API server from scratch \u2013 to productionize it, you often need to secure the APIs, optimize performance, and ensure reliability. Today, I\u2019m delighted to share that Hasura has achieved a breakthrough \u2013 instant APIs (GraphQL and REST) on top of MySQL, MariaDB, and Oracle with built-in production-ready features allowing you to get an API server running within days. We\u2019ve achieved this by releasing data connectors (currently in beta) for these databases. Over the past few months, we\u2019ve learned that our customers and users see these connectors being useful for rapidly: Modernizing legacy applications to newer frontend frameworks like React, NodeJS, etc. with no change to the databaseBuilding new applications on top of existing dataCombining data from MySQL, Oracle, and MariaDB with newer data sources (such as PostgreSQL and existing GraphQL APIs) through powerful remote relationship capabilitiesHow do these new data connectors work?To get a sneak peek, check out these videos illustrating how Hasura connects to MySQL and Oracle databases and generates GraphQL APIs to perform reads, writes, and joins across \u00a0databases and other GraphQL APIs. Note: You can create a REST endpoint from a Hasura GraphQL API in less than 30 seconds \u2013 check out our docs for more info.Getting Started with MySQL and MariaDB\n\nGetting Started with Oracle\n\nProduction readinessWe\u2019ve built a number of features with Hasura to ensure that your API server can be deployed in your environment and is reliable, secure, and scalable. Here are some key considerations:Support for a number of cloud databasesOur MySQL, MariaDB, and Oracle database connectors have been tested and validated on a number of cloud databases, including:Amazon Relational Database Service (RDS) (MySQL, MariaDB, and Oracle)Amazon Aurora (MySQL)Google Cloud SQL (MySQL and Oracle)Digital Ocean (MySQL)SkySQL (MariaDB)Flexible deployment optionsTo use our data connectors, you can either use:Our fully managed service \u2013 Hasura Cloud is deployed in AWS or GCP and automatically scales with your applicationOur self-hosted offering (Hasura Enterprise Edition) allows you to run Hasura in your infrastructure in the container environment of your choice (Kubernetes, Docker, AWS ECS, Google CloudRun, Azure Containers, etc.) For more information, check out our \u201cgetting started\u201d guides for MySQL, MariaDB, and Oracle.PerformancePerformance is critical for API servers as it directly impacts the user experience and the success of your application and its related services. We\u2019ve done performance benchmarking of our data connectors to ensure that our API server responds quickly to client requests, with sub-second response times, and scales to thousands of requests per second. This is done by:Implementing predicate push-downs that handle role-based authorization through the database query as opposed to filtering or transforming at the API layerImplementing query field projection that ensures only the requested fields are being queried via SQL, which optimizes for as few fields as possible being sent over the wireEfficiently solving for the (N+1*) query anti-pattern that can occur with naive implementations that join between tables*The N+1 query anti-pattern is where you query \u201ctable B\u201d for every row returned by the query to \u201ctable A,\u201d which is very inefficient. In Hasura, join queries are optimized to query each table once, limiting the number of separate queries issued to the smallest possible number.We\u2019ll publish a follow-up blog with full details on our performance benchmark test soon. Plus, query response times can be brought down even further with our query-response caching feature.SecurityHasura has a number of robust features to ensure the security of your MySQL, MariaDB, and Oracle applications in production. With Hasura, you can easily manage access to your data and APIs by defining roles and permissions. Hasura's fine-grained access control system allows you to specify which users can access which parts of your data, ensuring your application is secure and robust. For more information, check out our docs on authentication and authorization.Leverage API limits to prevent API performance issues caused by malicious or poorly implemented queries.Use role-based allow-lists to restrict queries to be accessed by only certain roles.Restrict access to the GraphQL schema on a per-role basis.Implement multiple JWT secrets to support multiple authentication providers and roles in your Hasura-based application and multiple admin secrets to implement security mechanisms like rotating secrets and enable lifecycle management for individual secrets.ReliabilityGiven that Hasura forms an integral part of your application stack, you want to ensure reliability for your apps in production by monitoring, diagnosing, and troubleshooting all aspects of your Hasura deployment along with all other application components and infrastructure.Hasura provides application telemetry data in the form of logs, metrics, and traces that can be exported to your observability tool/APM of choice. For more information, check out our docs on observability integrations in Hasura Cloud and how we provide logs, metrics, and traces in Hasura Enterprise.ConclusionHasura's support for the MySQL, MariaDB, and Oracle data connectors offer developers a powerful way to quickly build data APIs and ensure they are secure, scalable, and reliable in production. By simplifying the process of building and deploying APIs, Hasura\u2019s data connectors help you focus on quickly delivering value to your end-users, without getting bogged down in the details of building a data API yourself.Join our launch event webinar on May 18We have an exciting launch event planned! Don't miss out on our virtual product launch event, where we\u2019ll introduce these three new data connectors for MySQL, MariaDB, and Oracle that simplify code writing and SQL queries. The event will feature a live demo, a keynote from Hasura Co-Founder and CEO Tanmai Gopal, and a customer panel discussion with two leading development consultancies. Register now to reserve your spot.Connect with usA reminder, this is the beta release, and we\u2019re moving quickly to add new functionality for MySQL, MariaDB, and Oracle.If you\u2019d like to discuss our connectors, request new features, or just ask questions about Hasura and these databases, connect with us on Github or join our Discord. To get started right away, sign up for a free Hasura Cloud account."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura & CockroachDB Easily Handle Thousands of Requests Per Second with Low CPU Utilization \u2013 Part 1 Description: CockroachDB Serverless is a great choice of database, as it auto-scales based on load, making it an ideal Hasura partner for highly concurrent workloads.  Author: Smit Shah"}, {"role": "assistant", "content": "Hasura's core value is decimating the time to build a data API for GraphQL and REST and getting it production-ready. It does so by automating 50-80% of the manual effort into making production-ready APIs so that developers can focus on building differentiated products, not writing APIs.Developers and architects are impressed with the productivity gains from using Hasura and how quickly one can spin up an API. But we still hear the common question, \u201cIs a Hasura-generated API as fast, correct, and reliable as a handwritten API?\u201d The answer is YES! In fact, because of the optimizations Hasura does under the hood, Hasura-generated APIs are several times more performant than a handwritten API. In this blog series, you\u2019ll learn about all the technical ways Hasura achieves blazing-fast APIs.We\u2019ll also share performance benchmarks for Hasura-generated APIs for CockroachDB. We benchmarked two sets of performance metrics: Using standard benchmark datasets, we benchmark requests per second (RPS) and latencies (p50, p95) of Hasura-generated GraphQL APIs on CockroachDB.How does a Hasura-generated API compare to a hand-crafted GraphQL API?We will cover the RPS and latencies benchmark in this post. In Part 2, we will cover Hasura vs. DIY GraphQL APIs.The following benchmarks show Hasura easily handles thousands of sustained requests per second on a single Kubernetes pod with considerable CPU to spare. The CockroachDB serverless cluster also plays a pivotal role in managing high concurrent traffic.Requests per second benchmark on CockroachDBWe ran Hasura v2.20.1 on Kubernetes on a single pod for this benchmark. The pod configuration is 8 vCPU and 16 GB of RAM on the GCP asia-south1 (Mumbai, India) region, running against a serverless CockroachDB cluster on the cloud in asia-southeast1 (Singapore).We used the open source load testing tool Locust for generating the benchmark load. The benchmark was done on the Chinook dataset. The Chinook dataset is widely used for its real-world relational data.Query/WorkloadHandled Requests/sMedian latency (ms)(@ 100 concurrent users)95th percentile latency (ms)CPU utilizationAvg. response size (bytes)Primary Key query lookup120085ms100ms8%89 bytesLimit query fetch100089ms120ms10%18KBComplex join query fetch302160ms200ms10%33.5KBInsert mutation297124ms38ms5%105 bytesUpdate mutation281328ms46ms5%105 bytesThanks to CockroachDB\u2019s powerful index recommendation functionality, it recommended the creation of some missing indexes, which saw a 95% improvement of query performance.As long as the underlying tables have the proper indexes and you are fetching only the required data, which Hasura helps with, you can handle a significant load on CockroachDB using just a single pod of Hasura.Applied in the real worldFor a big sale event, an e-commerce company wants to cater to 100k requests per second. How many instances of Hasura would it take to achieve that request volume?Hasura achieves, on average, 1,658 requests per second at ~10% CPU utilization on mixed workloads. If we want to utilize ~80%, we can scale up to ~13,000 requests per pod. Thus, you can handle 100k requests per second in 8 pods or less, which can easily be auto-scaled using Kubernetes or any other container orchestrator.CockroachDB Serverless is also a great choice of database, as it auto-scales based on load, which makes it an ideal partner for Hasura for highly concurrent workloads. The serverless nature of both Hasura and CockroachDB saves time in both scaling systems and cost; instead of over-provisioning for peak load from the get-go, you can let CockroachDB/Hasura scale up only when required, and it scales back once the load subsides. 1. Primary Key Query Lookupquery {\n  Album_by_pk(AlbumId: 1) {\n    Title\n    AlbumId\n  }\n}\n\nA single Kubernetes pod achieved 1200 requests/s with median latencies of 85ms and 95th percentile of 100ms with 100 concurrent users. The CPU utilization was around ~8% at the peak, and the average response size was 89 bytes.2. Limit Query Fetchquery {\n  Album(limit: 100) {\n    AlbumId\n    Title\n    ArtistId\n  }\n}\n\nA single Kubernetes pod achieved 1000 requests/s with median latencies of 89ms and 95th percentile of 120ms with 100 concurrent users. The CPU utilization was around ~10% at the peak, and the average response size was 18 KB.3. Join Query Lookupquery MyQuery {\n  Album(limit: 30) {\n    Title\n    Album_Artist {\n      Artist_Albums {\n        Title\n      }\n    }\n    Album_Tracks {\n      AlbumId\n      Composer\n      Name\n    }\n  }\n}\n\nA single Kubernetes pod achieved 302 requests/s with median latencies of 160ms and 95th percentile of 200ms with 100 concurrent users. The CPU utilization was around ~12% at the peak, and the average response size was 33.5 KB.The reason for higher latencies is that the query joins with two additional tables and fetching ~33.5 KB data per query, which resulted in a drop in RPS for this benchmark.4. Insert Mutationmutation {\n  insert_Album_one(object: { Title: \"Trying now\", ArtistId: 1 }) {\n    Title\n  }\n}\n\nA single Kubernetes pod achieved 2971 requests/s with median latencies of 24ms and 95th percentile of 38ms with 100 concurrent users. The CPU utilization was around ~5% at the peak, and the average response size was 105 bytes.5. Update Mutationmutation {\n  update_Album_by_pk(\n    pk_columns: { AlbumId: 1000 }\n    _set: { Title: \"Trying now 1\" }\n  ) {\n    Title\n    AlbumId\n  }\n}\n\nA single Kubernetes pod achieved 2813 requests/s with median latencies of 28ms and 95th percentile of 46ms with 100 concurrent users. The CPU utilization was around ~5% at the peak, and the average response size was 105 bytes.ConclusionAs observed in this post, Hasura easily handles thousands of sustained requests per second with very low CPU utilization and latency. Also, all the underlying queries were generated by Hasura based on GraphQL requests, making the results even more impressive.Got questions or want to see the API performance in action? Reach out to the team to schedule some time, and stay tuned for Part 2 of this series."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Apigee + Hasura: Data APIs at Enterprise Scale \u2013 Part 1 Description: When working in parallel, solutions like Apigee and Hasura form the backbone of a complete enterprise API strategy. Author: Adam Gold"}, {"role": "assistant", "content": "Enterprises typically are well aware that API Management platforms like Apigee are key to the success of their application modernization strategies. What we find is less known is that API Management is only one piece of the puzzle when beginning your \u201cAPI-first\u201d and cloud-native development journey. These platforms address many of the challenges with operationalizing APIs for consumption both inside and outside of your organization, but solving for a lack of skills on the development side and an ever evolving set of API specifications to work with data is something that industry leading API Management vendors generally do not address. How do organizations scale their ability to both deliver and develop better APIs and digital products?Data API platforms are commonly deployed in enterprise architectures within the same data flow as an API management platform in order to scale the development and integration side of an enterprise\u2019s overall API strategy. While API management solutions like Apigee are primarily focused on managing the life cycle of APIs, with an established history of working with REST and SOAP/XML services, Data API platforms such as Hasura are leading efforts on generating APIs in both GraphQL and REST specification from underlying databases and data sources.When working in parallel, solutions like Apigee and Hasura form the backbone of a complete enterprise API strategy.Let\u2019s quickly have a look at a high-level breakdown of each solution\u2019s responsibilities in an enterprise-ready production environment:ApigeeApigee is an API Management Platform that provides a layer of governance, security, engagement, and analytics for your existing APIs. The key feature areas for Apigee are publishing your API products to Apigee\u2019s developer portal, controlling API traffic with routing policies, detecting security threats with machine learning, and analyzing your API performance and adoption. Apigee also tracks API usage and billing, enabling enterprises to monetize their API products, and critically supports GraphQL API routing and TLS termination.HasuraHasura is a Data API Platform that automatically generates GraphQL or REST APIs from an underlying database\u2019s schema and can merge existing GraphQL and REST APIs into a single, unified GraphQL endpoint. Federating data in this way dramatically improves the frontend developer experience and productivity without placing a burden on backend engineering teams. Hasura provides a simple and fast way to create APIs that are tightly coupled with your data models on popular supported databases like PostgreSQL, MySQL, Snowflake, and Oracle.Hasura + Apigee: Tying Together Your Organization\u2019s API Architecture Let\u2019s go a bit deeper into a typical enterprise architecture, and how Hasura and Apigee work together to bring velocity, governance, and scalability to an organization\u2019s API architecture:TLS Termination and RoutingApigee can serve as the TLS termination point and also act as an intelligent routing layer for Hasura generated APIs. It's worth noting that Hasura does not terminate its own TLS, and a load balancer is typically included in a standard Hasura reference architecture implementation. However, using API Management solutions such as Apigee efficiently perform these tasks.API Auto-Generation and Granular AuthorizationHasura offers Apigee customers the advantage of GraphQL's effective, developer-friendly query language without the burden of creating and maintaining GraphQL APIs by hand. In most cases, where Apigee is the chosen API Management platform for an organization, Hasura sits below the Apigee layer, handling API generation, authorization, and data federation tasks, while Apigee manages the resulting Hasura generated APIs. This architecture allows for efficient, seamless integration between your data producers and your data consumers (often developers and other external APIs), streamlining the API development process, and enabling faster time-to-market for digital products and services.Developer PortalJoint customers can take advantage of Apigee's developer portal to easily discover and access these new Hasura-generated APIs within their organization. Since GraphQL APIs are self-documenting, developers can quickly understand what each endpoint is capable of by simply examining the API itself in the portal. This streamlined approach improves overall developer experience for large enterprises, increases adoption of new Hasura-generated API endpoints, and ensures that all APIs are properly governed and secured to organizational standards.Apigee Advanced API Security (AAS): Apigee's Advanced API Security (AAS) functionality, offered as an add-on for current customers, assists in mitigating the common attack vectors that leave enterprises vulnerable to fraud and abuse across their API architecture.API misconfigurations are one of the leading causes of security incidents and data breaches. Apigee AAS comes with the ability to identify and resolve misconfigurations for REST APIs. REST APIs are often connected to Hasura federated data layers in a joint architecture. These APIs may include external data sources that provide stock market transaction data, weather data, or data from SaaS services like Salesforce or Zendesk. Apigee AAS enables large enterprises to detect misconfigurations and reduce security risks associated with the sensitive information transmitted through the API architecture.Apigee's AAS also provides configurable rules to identify and block malicious bot attacks, enabling API teams to quickly detect unusual traffic patterns from a single IP address and categorize it as bot traffic. This helps reduce the risk of identity theft and automated denial-of-service (DoS) attacks. In addition, AAS speeds up the identification of data breaches by identifying bots that have successfully obtained HTTP 200 OK success status response codes.BOLA (Broken Object Level Authorization) Attack ProtectionAccording to OWASP, broken object level authorization (AuthZ) or BOLA, topped their ubiquitous \u201cTop 10\u201d list as the number one application security risk in 2021. To prevent BOLA attacks on GraphQL and REST APIs, model-based AuthZ \u2013 most often embedded in custom business logic of backend codebase \u2013 is the most secure and flexible method for AuthZ. But it can be challenging to implement this in a performant manner. Hasura provides one of the most mature and optimized ways to implement model-based AuthZ on your APIs in the entire API ecosystem. The engine enables AuthZ to extend across multiple pieces of an application\u2019s architecture, with AuthZ models automatically extended to things like locally cached resources (common API queries that Hasura caches in our local Redis or a customer\u2019s managed Redis instance). Hasura's model-based Authz also extends to GraphQL queries, mutations, and subscriptions, and can be extended to existing GraphQL endpoints that organizations wrote by hand prior to plugging Hasura into their architecture. The easy-to-use authorization engine in Hasura enables developers to add new AuthZ rules as needed, and in a way that extends beyond what is possible leveraging Apigee alone.MonetizationOrganizations are actually creating revenue streams by metering and monetizing the utilization of their APIs. Apigee is well-known as one of the leading solutions to streamline and manage API monetization. Using Apigee and Hasura together enables you to easily monetize and adjust billing for your API consumers on Hasura generated API endpoints. Apigee allows you to create policies to record and define transactions on all of your organization\u2019s API products. These policies define what Apigee should look for in an API transaction to calculate billing and any revenue sharing necessary. The monetization engine in Apigee also allows you to define what the gross price of a particular transaction type should be. Finally, Apigee enables API product managers to actually group and package multiple API endpoints into different product packages, and create rate plans around these packages.You can choose to monetize any APIs in your organization, whether written by hand or generated by Hasura, in any number of ways with Apigee sitting in front of your API architecture. Possible ways include something as straightforward as charging a flat monthly rate for API access, a more nuanced, metered per transaction fee, fees based on both the size and number of transactions, or a combination of all of these.ConclusionUltimately, the combined use of Apigee and Hasura streamlines an organizations API design, development, and operations processes, enabling \u00a0faster time-to-market for digital products and services. Monetization and billing for Hasura generated API utilization is easily achieved using Apigee, and the addition of Apigee's developer portal makes discovery and access of Hasura APIs more accessible to developers. By leveraging both solutions, enterprises can build a business around a more performant and secure API architecture than DIY solutions can provide.To take Apigee for a test drive, Google offers a free trial of the platform from its pricing page here. Hasura Cloud can be tested and developed on our free Cloud tier here. Upgrading for production use is a seamless process once Hasura sales and engineering are involved, so please reach out to us with questions and/or to simply take things forward with your Hasura Cloud initiative. To test Hasura Enterprise in your own environment, download Hasura OSS, and follow the instructions found here for a 30 day upgrade to Hasura Enterprise Edition."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Convert REST endpoints from OpenAPI spec to GraphQL in 2 clicks Description: The new import OpenAPI feature helps users in Creating an action from OpenAPI 10x faster than before - with just 2 clicks. Author: Sooraj Sanker"}, {"role": "assistant", "content": "Modernizing REST endpoints to GraphQL is a very common process that we have seen from the community and enterprise users. This process can be simple or hard based on the complexity and how well the REST APIs are documented. OpenAPI is a way to document REST API endpoints in a machine readable format and is widely adopted in the software industry.REST to GraphQL in 2 ClicksIn Hasura terms, connecting a REST endpoint to GraphQL schema means creating a Hasura Actions, and one of the main challenge with the Action creation workflow is that it used to take some time when developers want to import multiple REST endpoints as they need to fill up the create action form for all the endpoints, but with the new import OpenAPI feature, creating an action from OpenAPI is 10x faster than before - with just 2 clicks.Writing a GraphQL resolver yourself can be a cumbersome experience, developers are often reluctant to modernize their REST APIs with GraphQL, Hasura saves significant time in development and enables quick plug and play with REST API sources.OpenAPI spec and GraphQL spec are fundamentally different, and hence getting a full feature parity may not be possible, with this feature we aim to make the developer experience of converting REST to GraphQL more easy as possible.Why REST to GraphQL?There are several reasons why you might consider moving from REST to GraphQL:More efficient data retrieval: With REST, you often have to make multiple API calls to retrieve all the data you need. GraphQL allows you to retrieve all the required data in a single API call, which can be more efficient.Better with brownfield projects: By adopting GraphQL as the gateway in a brownfield project already using REST APIs, developers can unify the data fetching process and get all the benefits of GraphQL without a complete re-architecture.Better client control: In REST, the server determines what data is returned to the client. With GraphQL, the client has more control over the data it receives, and can request only the data it needs.Easier to evolve APIs: With REST, evolving APIs can be difficult because changes can break existing clients. With GraphQL, you can add new fields to the schema without breaking existing clients, as clients only request the data they need.Increased flexibility: GraphQL allows for more flexibility in the type of data you can retrieve, and can handle complex data relationships more easily than REST.Better developer experience: GraphQL has a strong type system, which can help catch errors early in the development process. Additionally, GraphQL provides powerful developer tools, such as GraphiQL, which can help with testing and debugging.How to convert REST to GraphQL?In this blog we will see How to convert REST endpoints from a popular open API spec - Petstore OpenAPI into Hasura GraphQL Engine with access control in 2 clicks. You can also read more about the feature in our documentation.Step 1 : Preparing the OpenAPI specification JSON/YAMLOpenAPI specification is generally represented in JSON/YAML format, and Hasura accepts this directly as an input on the console. Usually the OpenAPI spec will be maintained on a repository (like twilio) or some may have a link to JSON on swagger UI (like petstore), getting this spec file is all that you need to connect REST endpoints to Hasura GraphQL schema.Step 2 : Load the spec on Hasura console UITo load the JSON/ YAML OpenAPI spec on Hasura console, you can click on the Actions Tab -> Import from OpenAPI.Import from OpenAPIAnd paste the JSON/YAML file on the UI (you can also upload the file using file picker).OpenAPI SpecificationOnce your specification is pasted/ uploaded you will now see the endpoints listed on the right hand side of the console.In this example we will use the spec file from petstore openAPI .Step 3 : Importing Endpoints to GraphQL Schema Make sure you have a valid Base URL if this is not automatically populated. Some OpenAPI specifications may not have the server URL part of the specification, so when there is no base URL automatically populated, make sure you provide a valid base URL. In this example we have the base URL auto populated from the specification so we can move to the next step.Once the above steps are completed, You can click on each Create button of each endpoint to import them to your GraphQL Schema.Create GraphQL from OpenAPI SpecBy clicking on the Create button Hasura creates a Hasura Action under the hood and uses the OpenAPI spec to figure out the input argument types and available output data types. You will now see the newly created endpoints listed as Hasura Actions on the left hand side so that you can add permissions or modify any configuration if needed.Whoo! We have now connected a REST endpoint to GraphQL schema successfully \ud83c\udf89.Step 4 : Testing the endpointTo test the new endpoint, you can head to the API tab and try out the new query. The GraphQL query type would be the same as in the OpenAPI specification. You will also notice the input payload arguments and URL template arguments are now part of the GraphQL input type so that you can make GraphQL queries as dynamic as the REST endpoint.Testing the endpointExtras \ud83e\udd73Adding Action Permissions : Once a REST endpoint is added to Hasura, you will be able to add access permissions to these endpoints. This helps you to make this REST endpoint accessible to only a set of users who access the GraphQL API.Additional configurations : Imported actions from OpenAPI spec can be modified like any other Hasura Actions - This means you could,Hide sensitive information like Access Keys by setting it as env variables.Customize REST Connector transformations: Provide default values, merge or transform graphql input values to REST API payload/query parameters/ URL itselfWhat is supported?\nSupported HTTP Request Types\n\nGET\nPOST\nPUT\nDELETE\n\n\n\nQuery Parameters : While converting a REST endpoint to GraphQL, Hasura will parameterise the query parameters into GraphQL input types.Header Parameters : Hasura can even take inputs from GraphQL input type and convert that into HTTP request headers and make HTTP requests.Request URL Template/ Path Parameters : When there is a templated HTTP URL or path parameter specified in the OpenAPI specification, Hasura will convert them to GraphQL input types to make that available on the GraphQL query.Request Body Type : JSON/ form-url-encoded are supported to be used while adding a REST API to GraphQL schema.Note: The Import OpenAPI is a feature that helps you to configure Hasura Actions from OpenAPI spec, if there are any incompatibilities you could configure an Actions manually from Hasura Console.SummaryIf you have a large OpenAPI spec that you want to modernize and power up with GraphQL. Hasura makes it easier and faster by providing plug and play configuring experience, this can save significant time in development \u00a0and helps you to focus on what you build, instead of worrying about writing - long resolvers, access control handlers, data transformers, etc. You can try this feature on all Hasura offerings - Cloud, Enterprise and open source community edition.Try creating a Hasura project and connect your first REST endpoint to get a GraphQL endpoint all in under 5 minutes. Sign up to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Try Hasura Enterprise Edition Free for 30 Days Description: Hasura Enterprise Edition has a host of upgraded capabilities, including security, observability, and performance enhancements, along with support for every enterprise database. Author: Vaishnavi G V S"}, {"role": "assistant", "content": "Ever looked at your favorite product and thought, \u201cI\u2019d love to try these super cool advanced features!\u201d only to find out that you\u2019d need to first talk to a sales representative to even get to them? Fret not, we\u2019ve been there too, fellow developers. We know that for us, it\u2019s all about experiencing that product value before we know we can commit to a sales conversation. Sticking to our developer-first ethos here at Hasura, in this blog post, we introduce to you the Hasura Enterprise Edition trial. So grab your latest Hasura instance, signup, and get rolling with all the enterprise features we offer at your convenience free for 30 days, no credit card required. It\u2019s literally that simple. (PS. our team is on standby for when you need help, so we got your back)Hasura Enterprise Edition Free TrialWhat\u2019s included in the Hasura Enterprise Edition?In addition to everything that\u2019s loved about Hasura in the Community Edition, Hasura Enterprise Edition has a host of upgraded capabilities, including security, observability, and performance enhancements, along with support for every enterprise database.Highlights:Support for instant GraphQL APIs on databases like Snowflake, Amazon Athena, MySQL and OracleSecurity features like API Limiting, multiple Admin and JWT secrets, role based allow lists to conform to your organization\u2019s security requirementsPerformance features like caching and read replicas to provide the best experience to your API consumers/ end usersObservability features like Prometheus metrics and OpenTelemetry traces to ensure the reliability of your Hasura-based applications, allowing you to monitor, diagnose and troubleshoot issues in productionRead more about our Enterprise features here.Getting started with the Enterprise Edition TrialWith the Enterprise Edition trial, you can now try all Hasura features yourself and completely free for 30 days. Just follow this quick start guide, to get started in under a minute. Here\u2019s a detailed guide.Step 1: Run Hasura/ Update your Hasura instance to the latest versionEnterprise Edition Trial is supported starting version v2.23.0 onwards. Spin up your instance and open the Hasura Console.Step 2: Register for Enterprise Edition TrialClick on the Enterprise button on the top right and fill out some basic information.Activate your Trial License30 day trial of Hasura Enterprise has been activatedStep 3: Restart your instance and explore Hasura Enterprise Edition!Just restart your instance and you should now be able to try all the Hasura Enterprise Edition features.Click the EE button in the top right to look into all the Enterprise Edition features that you could try.Benefits of Hasura Enterprise EditionHaving trouble getting started? Please refer to our FAQs.Ready to purchase an Enterprise Edition plan? Meet with a Hasura expert."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Instant data activation with Hasura on GCP: Cloud SQL, AlloyDB, BigQuery Description: In this blog, we will explore how to use GCP Cloud SQL, AlloyDB, and BigQuery with Hasura GraphQL Engine to build an efficient and robust data management system. Author: David Meleney"}, {"role": "assistant", "content": "In today's world, data is the backbone of any application, and handling it effectively is essential. With the emergence of cloud technologies, managing data has become even more comfortable and accessible. Google Cloud Platform (GCP) offers a suite of cloud-based services that can help developers build scalable and secure applications.In this blog, we will explore how to use GCP Cloud SQL, AlloyDB, and BigQuery with Hasura GraphQL Engine to build an efficient and robust data management system.Hasura GraphQL EngineHasura GraphQL Engine is an open source engine that enables developers to build GraphQL APIs quickly. It provides a fast and secure way to access data from multiple sources and combine them into a single endpoint. Hasura GraphQL Engine offers real-time subscriptions, queries, and mutations, making it an ideal choice for building modern applications.Hasura automates 50-80% of your API dev work, instantly providing a production ready API backed by the following features:Instant CRUD APIs on your domain modelsGraphQL and RESTGranular row- and column-based authorizationAPI limits (rate, depth, node, timeout)CachingEvent triggers and subscriptionsInput checking and validationGCP Cloud SQLGCP Cloud SQL is a fully managed relational database service that offers MySQL, PostgreSQL, and SQL Server databases. It provides automatic backups, replication, and failover, making it an ideal choice for building scalable and high-availability applications. With Cloud SQL, developers can easily create, manage, and scale relational databases in the cloud.AlloyDBAlloyDB is a distributed SQL database that offers high performance and scalability. It provides a PostgreSQL-compatible interface and supports distributed transactions, making it an ideal choice for building cloud-native applications. With AlloyDB, developers can store and retrieve large amounts of data quickly and efficiently.BigQueryBigQuery is a fully managed, serverless data warehouse that offers fast and scalable analytics. It provides SQL-like queries and can handle petabyte-scale datasets, making it an ideal choice for building data-driven applications. With BigQuery, developers can easily store and analyze data in the cloud.Using Hasura GraphQL Engine with GCP Cloud SQL, AlloyDB, and BigQueryHasura GraphQL Engine integrates with GCP Cloud SQL, AlloyDB, and BigQuery, enabling developers to build scalable and efficient data management systems. Here's how to get started:Create a project in GCP and enable the required APIs (Cloud SQL, AlloyDB, and BigQuery).Create a Cloud SQL instance or an AlloyDB cluster and create a database.Connect Hasura GraphQL Engine to Cloud SQL or AlloyDB using the Postgres database connector. Hasura GraphQL Engine can automatically generate GraphQL schemas from the database schema.Create tables and insert data into the database using SQL commands.Create a BigQuery dataset and table and load data from Cloud SQL or AlloyDB into BigQuery.Use Hasura GraphQL Engine to query data from Cloud SQL, AlloyDB, and BigQuery.Use Hasura GraphQL Engine subscriptions to receive real-time updates from Cloud SQL, AlloyDB, and BigQuery.Step by step guide for connecting AlloyDBHasura Cloud Enterprise on Google Cloud MarketplaceGoogle Cloud customers can also now procure and pay for Hasura Cloud Enterprise via the Google Cloud Marketplace. Please contact your account manager or sales@hasura.io for more details.ConclusionBy using Hasura GraphQL Engine with GCP Cloud SQL, AlloyDB, and BigQuery, developers can easily build scalable and efficient data management systems. With the combination of these powerful tools, developers are empowered to create high-performance and reliable applications that handle large amounts of data with ease.Other ResourcesExploring faster reads with Google AlloyDB PostgreSQL and Hasura GraphQL APIReady to get started for free with Hasura Cloud? In just a few clicks and no credit card required, create or connect your own database today."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building Powerful and Scalable Applications with Hasura and AlloyDB Description: Explore the benefits of using AlloyDB and Hasura together and how they can help you build efficient and scalable applications. Author: David Meleney"}, {"role": "assistant", "content": "AlloyDB and Hasura are two technologies that can be used together to build powerful and scalable applications. AlloyDB is a distributed SQL database that is designed to be highly available and fault-tolerant, while Hasura is an open source engine that provides instant GraphQL APIs for existing databases.In this blog, we will explore the benefits of using AlloyDB and Hasura together and how they can help you build efficient and scalable applications.AlloyDB OverviewAlloyDB is a distributed SQL database that is designed for high availability and fault tolerance. It is built on top of PostgreSQL and provides additional features such as automatic sharding and replication, distributed transactions, and automatic failover. With AlloyDB, you can scale your database horizontally by adding more nodes to the cluster, making it highly scalable.One of the key features of AlloyDB is its ability to automatically shard tables across nodes, which allows you to distribute data across multiple machines for faster and more efficient queries. AlloyDB also supports distributed transactions, which means that you can run transactions across multiple nodes in the cluster, ensuring data consistency and integrity.Hasura OverviewHasura is an open source engine that provides instant GraphQL APIs for existing databases. With Hasura, you can connect to any database and instantly get a GraphQL API that can be used by your frontend applications. Hasura supports a wide range of databases and data warehouses, including PostgreSQL, BigQuery, MySQL, SQL Server, and Oracle.Hasura provides a set of tools for managing your API, including a schema explorer, a real-time API explorer, and a GraphQL playground for testing your queries. Hasura also supports authorization and authentication, allowing you to control access to your API and manage user roles and permissions.Using AlloyDB and Hasura TogetherAlloyDB and Hasura can be used together to build highly scalable and efficient applications. By using AlloyDB as the underlying database, you can take advantage of its distributed architecture to scale your application horizontally. With Hasura, you can easily create a GraphQL API for your database, which can be used by your frontend applications.One of the benefits of using AlloyDB and Hasura together is that you can take advantage of AlloyDB's automatic sharding and replication capabilities to distribute data across multiple nodes. This allows you to handle more traffic and queries without having to worry about performance issues.Another benefit of using AlloyDB and Hasura together is that you can easily manage your API using Hasura's set of tools. Hasura provides a schema explorer that allows you to visualize your data model and relationships, making it easy to understand and manage your API. Hasura also provides a real-time API explorer that allows you to test your queries and see the results in real-time.What can you build with Hasura on AlloyDBHasura and AlloyDB are both powerful tools for building modern web applications. Here are some ideas of what you can build with them:Real-time chat application You can use Hasura's real-time GraphQL API to build a chat application that allows users to send and receive messages in real-time. AlloyDB can be used as the database for storing chat messages and user data.E-commerce platform You can use Hasura to build a GraphQL API for your e-commerce platform, and use AlloyDB to store product data, user data, and order information. With Hasura's real-time capabilities, you can create a real-time notification system for order updates and shipping status.Social media platform You can use Hasura to build a GraphQL API for your social media platform, and use AlloyDB to store user data, posts, and comments. With Hasura's real-time capabilities, you can create a real-time notification system for new posts, comments, and likes.IoT dashboardYou can use Hasura to build a real-time dashboard that displays data from IoT devices, such as temperature sensors or motion detectors. AlloyDB can be used as the database for storing historical data and device information.Content management system (CMS) You can use Hasura to build a GraphQL API for your CMS, and use AlloyDB to store content data, user data, and access control information. With Hasura's real-time capabilities, you can create a real-time notification system for content updates and user activity.These are just a few ideas to get you started. With Hasura and AlloyDB, the possibilities are endless!ConclusionAlloyDB and Hasura are two powerful technologies that can be used together to build highly scalable and efficient applications. By using AlloyDB as the underlying database and Hasura to create a GraphQL API, you can take advantage of the distributed architecture of AlloyDB and the ease of use of Hasura to build powerful and scalable applications. Documentation: Connecting Hasura to a Google AlloyDB Postgres DatabaseWhether you are building a new application or migrating an existing one, AlloyDB and Hasura can help you achieve your goals faster and more efficiently.Ready to get started for free with Hasura Cloud? In just a few clicks and no credit card required, create or connect your own database today."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Power of Women in the Workplace: Insights and Inspiration from Hasura Female Leaders Description: Women\u2019s History Month coming to an end, and we\u2019re wrapping up our own celebration at Hasura by interviewing women leaders within the org, touching on the ups and downs and in-betweens that go along with navigating being a female in a male-dominated industry.  Author: Hasura"}, {"role": "assistant", "content": "With Women\u2019s History Month coming to an end, we\u2019re wrapping up our own celebration at Hasura by interviewing women leaders within the org, touching on the ups and downs and in-betweens that go along with navigating being a female in a male-dominated industry. We encourage reading all the way to the end because there is a whole lot of wisdom to be gleaned throughout this post.And if we ever wonder why we continue to go through these exercises still at this point, let\u2019s turn toward a Maya Angelou quote: \u201cThere is no greater agony than bearing an untold story inside you.\u201dWe all have our stories, and these are some of ours. The importance of women\u2019s empowerment in the workplaceLet\u2019s kick this off with our fearless female leader, Co-Founder and COO, Rajoshi Ghosh. \u201cWe have such incredible and strong women at Hasura, and I continue to be impressed with each new addition to the team. The perspective that they bring to their colleagues, teams, and the product, has shaped the company in such a meaningful way. I\u2019m encouraged by how our team is committed to building a diverse and inclusive workplace, not just because it\u2019s our responsibility, but because we have seen and felt the positive impact this kind of environment has on our business outcomes and team member experience.\u201dWhen asked about defining women\u2019s empowerment, and the benefits of having empowered women in the org, Andrea Graham, Senior Sales Engineer, said, \u201cWomen's empowerment can be so many things, but mainly it prompts women's sense of self-worth, their independence to make their own choices, and the strength to make social change for themselves and others. Having empowered women in an organization allows them to set social change in motion by pulling in and lifting up others to feel enabled to take up space.\u201dManushi Khanna, Senior Manager, Engineering, focused a bit more on the unencumbered aspects of empowerment, and said, \u201cTo me, women's empowerment is the power to express myself freely and have equal opportunities in society. It's the ability to follow my dreams without being bound by any rules.\u201d What do females bring to the table in a male-dominated industry?Maria F. Almeida, Operations Assistant, said, \u201cWhenever I participate in a conversation, I try to add value and include others. Because I am privileged to be named alongside an incredible team, I go all out to correspond with them and deliver high-quality results to the Developer Success team.\u201dAnd Jocelyn Hoppa, Director of Content, who more or less takes the approach of a good old-fashioned wear down, said, \u201cI\u2019m not trying to defy stereotypes women are cast with or approach with boss-lady energy. That\u2019s not a goal. I merely bring with me to the table, just by showing up and doing good work, the possibility that \u201cwomen in tech\u201d is a phrase that eventually becomes redundant.\u201dSome big benefits of having women in leadership\u201cWe\u2019ve all seen substantial backing evidence on the financial implications of having women in the workforce or in boardrooms, right from profits being higher to increase in per capita incomes,\u201d Vaishnavi G V S, Product Manager, said. \u201cI want to emphasize the slightly nuanced aspects of this. I believe that having women in leadership positions brings diversity in thought processes, more like a completion of perspective with respect to various decisions. It increases the overall empathy as a group, trust in the organization and its people, and ups the collaboration quotient, which directly impacts the health of an organization, its people, and its outcomes.\u201dVaishnavi also addressed women\u2019s increasing awareness of an organizational culture that uplifts women and provides them with equal opportunities. \u201cOn many occasions, I\u2019ve had women reach out to me as a woman leader/entrepreneur/engineer, when starting out in similar circumstances to understand the challenges they could face in my position,\u201d she said.Andrea said of the benefits of women in leadership, \u201cUnlocking gender diversity is a huge business opportunity at all levels, whether it be social or economic. Female leaders not only promote social fairness but they have also been statistically proven to increase financial metrics and innovation. Female leaders set the stage for an effective organization with high emotional intelligence, self-awareness, and integrity.\u201dAs for Hasura, Manushi said, \u201cWomen hold key leadership positions at Hasura. It\u2019s lovely to see other women not only pursue their careers but also lead large mandates for the organization. Seeing a woman lead a team encourages other young women and gives them the confidence that they too can follow their dreams and need not settle!\u201dA tip on how women can support other women in their orgs\u201cAs women leaders in the industry, I think it becomes our responsibility to foster a culture of growth and collaboration where other young women feel comfortable expressing themselves and contributing to the organization,\u201d said Vaishnavi. \u201cEncouraging women to take ownership of impactful projects, driving initiatives that fuel them, giving them speaking opportunities, and even tiny things like asking for their thoughts in an otherwise hard-to-interrupt discussion, all count toward building that foundation of women\u2019s participation. Supporting women who come back to the workforce after a break, who needed this push all the more, and sensitizing both men and women in the organization to create an equitable workplace becomes a priority for growth!\u201dOn building career confidence and resilienceSometimes resilience and confidence are borne of frustration. Recalling one particular past example, Jocelyn said, \u201cOnce I was passed up for a promotion given to a male with fewer years at the company and less overall experience. The reason I was given: my \u2018female energy\u2019 would not work in leadership. So yeah, that\u2019s been a key to my resilience. Tell me I can\u2019t do something. I\u2019ll find a way to do it and prove you wrong.\u201dSometimes resilience and confidence are found in female role models. Maria said, \u201cStaying curious and having a growth mindset has been foundational for building resilience and confidence. One of my biggest inspirations/role models is Serena Williams. And like her, I learned from a young age that hard work does pay off.\u201d(Big-time love for Serena <3)And sometimes it\u2019s a mixture of both! Vaishnavi shares her experience: \u201cIf I look back, there were a million instances, small and big, where I faced discrimination, overt or unintended. Where people had bewildering questions along the lines of, \u2018Oh, you're the decision maker? Why don't you call your male boss? Or let me talk to a technical person,\u2019 when I was the go-to person in all these scenarios. The bottom line comes down to this: Gain an exceptional command over your subject! If you know what you're talking about, no matter how uncomfortable the situation, your confidence comes from within, and for good reason! It is okay to step out of your comfort zone, as that's what builds you as a person and helps you learn and grow. Ask for help when you need it, talk to your peers and mentors, all the while being true to who you are and holding your ethics in the highest regard. I'd add one more dimension to this, your support system can be men and women! I have had the good fortune of having worked with some amazing men in my career, right from my professors to colleagues to mentors, managers, friends, and family. They've encouraged me, guided me, recognized my caliber, and pushed me to do better. I think all of this contributed to building my confidence and resilience over the years.\u201dA bit more about some career challenges we\u2019ve faced\u201cAs a woman in a technical role, I am often the only woman in the room. This used to be extremely intimidating but has instead become a superpower of mine,\u201d Andrea said. \u201cMy perspectives are completely different from many folks in the room, so my thoughts and opinions help to fundamentally shift the thinking. Having the guts to speak up in a room full of men can be one of the most powerful traits a woman can possess. By sharing your perspective, you\u2019d be surprised how much people will resonate with your thoughts and how you\u2019re not alone in your thinking.\u201dBritt Harris, Head of People, shares her perspective: \u201cNot only as a woman, but as a woman of color, I have had to navigate the existing cultures, dynamics, and politics in place at various organizations and balance being my authentic self in the workplace, while actively managing against different stereotypes and preconceived notions that individuals may have. Worrying about how to convey authority without being perceived as bossy, being passionate without being discounted as emotional, and expressing dissatisfaction without being seen as angry sometimes feels like it takes effort and energy I could otherwise be putting into other areas. Over time, I think I\u2019ve found a balance that works for me, and I\u2019ve also prioritized working at companies with a culture and leadership that enable me to contribute effectively in a way that is also natural for me.\u201dDo we believe women can truly \u201chave it all\u201d balancing career, personal life, and passions?In a word: yes! Manushi said, \u201cIt does not have to be this or that! You can have it all but maybe not at the same time. I think it\u2019s important to understand what your priorities really are and give all aspects of your life equal attention. It also helps to have a solid support system at home and at your workplace. I think the trick is to surround yourself with the right people.\u201dBritt said, \u201cYes! We can have it all because we can handle it all! As long as we keep our cups filled by doing things that restore and replenish us, we\u2019ll have unlimited resources to invest in the things and people that matter.\u201dImposter syndrome: feeling it and navigating it\u201cEvery time I grow in my career, I feel \u2018imposter syndrome,\u2019\u201d said Andrea. \u201cIt\u2019s hard to overcome, but you can certainly navigate it. It\u2019s important to find comfort in the discomfort and build personal resiliency in an intimidating environment. No one expects you to step into a role and already know everything there is to know about the responsibilities, so drop the perfectionism and pick up a learning mindset. Restructuring your mindset is important, but so is an encouraging culture that welcomes space for growth and fun challenges.\u201dBritt concurs, and says, \u201cI definitely still experience this. I don\u2019t know that I\u2019ll ever not feel this way, but what has helped me is talking with some of the most impressive, educated, and highly effective women I know, and hearing that they are feeling the same way. From those conversations, it seems that the feelings at least in part, come from our constant introspection to ensure that we\u2019re adding value and meeting expectations, and while in extreme cases these feelings can be crippling, often I think it fuels \u00a0us/me to work harder to provide the best ROI for any organization that has entrusted me with the responsibility.\u201dBest personal and/or professional advice received from another woman?\u201cThe best professional advice I can share is to remember to always give yourself credit,\u201d said Maria. \u201cA woman once told me that even if we don\u2019t have an impressive title or designation, our work speaks for itself. You may have been performing a manager\u2019s role without even realizing it.\u201dFor Britt, it\u2019s community-fueled ongoing support and advice. She said, \u201cThe best advice I\u2019ve gotten is to stop second-guessing myself, but what\u2019s been more beneficial than any advice is just the support that I\u2019ve received from other women. Being able to pick up the phone and call my mentor or others I\u2019ve built relationships with over the years, talk to them about anything I may be going through, and know that they will listen and be understanding and supportive (even if it\u2019s with some tough love) is invaluable and something I\u2019m extremely grateful for.\u201dAnd while the gender pay gap is shrinking as of 2023, unequal pay for women is still very real. Jocelyn said, \u201cThe advice I got was in the form of some tough love from a mentor, to know my worth, and she very much meant that in a financial sense. I\u2019ve been particularly bad about this in the past, way too modest and passive, like \u2018oh you know, whatever, just happy to be here!\u2019 But that\u2019s nonsense. Being prepared to have the money conversation and standing up for yourself on your own behalf when asking for a raise or when offered a new job or promotion is essential.\u201dA nugget of advice for women who want to do more in their careersBritt is all about getting started today. She said, \u201cDon\u2019t wait until you feel comfortable doing something to try it. If I only applied for the jobs I thought I would get or only done the thing I knew I would succeed at, my personal life and professional career would have looked very different.\u201d In closing, a bit about Hasura\u2019s culture and what we\u2019re most excited about\u201cAt Hasura, we are fundamentally changing the way organizations and developer teams build and maintain their software,\u201d said Andrea. \u201cI am so excited to see us change how companies develop today. I am also excited about the culture and diverse backgrounds of all of my colleagues. As an organization, we are welcoming, encouraging, supportive of healthy challenges, and always empowering one another.\u201dVaishnavi wraps it all up quite nicely: \u201cOne thing I\u2019m super excited about is the sheer diversity of people at Hasura, not just in terms of gender but life experiences across the world. I love how open and welcoming our team is here. We have rock musicians, polyglots, clothing designers, and firefighters\u2026 all exceptional in their professional fields also. I think this is a great culture we\u2019ve fostered, where growth in who you are personally and professionally goes hand in hand. I think the five Hasura values we live by, which I think are great personal values to imbibe also, speak for the culture of the company.\u201dSo there you have it. As we bring March and Women\u2019s History Month to a close, we give one last salute to all those who paved the way for us, all those who propped us up, and all those who continue to inspire us along the path to true workplace equality.\u201cWe\u2019re strongest when we all cheer each other on.\u201d \u2013 \u00a0Serena Williams"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Why You Need a Multi-Cloud and Multi-Region Deployment Strategy for Distributed Postgres Description: Learn about multi-region, multi-cloud deployments and how Hasura lets you hide the nuances of these deployments behind its generated API abstractions. Author: Jesse Martin"}, {"role": "assistant", "content": "When I sat down to write this article and create the associated material needed for the videos, I ran into some technical challenges. I had provisioned three cloud providers in three separate regions to simulate multi-cloud and multi-region deploys.But I couldn\u2019t get one of the regions to behave as I expected when provisioning the environment. As it turned out, the region (and by association, cloud) was down. Worse than that, the provider was experiencing issues related to an undersea cable cut and estimated multiples of weeks until the outage was mitigated.Ouch.Fortunately, for demo content, there are plenty of clouds in the sky to choose from, but the importance of multi-cloud redundancy highlighted was particularly poignant. If this had been a production environment and my only deploy region, I wouldn\u2019t be wanting to talk about it right now.In this post, we\u2019re going to look at some of the reasons you choose multi-region and multi-cloud deployments, whether for resiliency or regulation, and see how Hasura lets you hide all the nuances of these deployments behind its generated API abstractions.Resilient reasons to choose multi-cloud vs multi-region architecturesThe first reason to choose either multi-cloud or multi-region environments, to begin with, is for data resilience. When our data is in a single region or on a single cluster, there is simply enough anecdotal evidence at this point that you will experience data failure. If you want to keep the data lights on 24-7, you need a mitigation strategy. These strategies fall into two buckets: multi-region and multi-cloud.We first need to define the differences between multi-cloud vs multi-region. The architectures are simply the implementation details of how we plan for one vs the other. However, depending on your database provider, those details can represent a significant time investment, which we\u2019ll look at later on.Multi-RegionWith multi-region, we deploy our applications to (typically) geographically distributed regions around the world. However, regional deploys are subjected to physical faults. Inclement weather, political unrest, acts of foreign or domestic terrorism, and other physical variables have all led to regional outages in the past.In practice, regardless of cloud, the regions tend to have exposure to similar physical faults. Instead of simply losing your data, we can switch over to another region to handle the workload. For as much as our industry hates latency, it is alway preferred over loss.Deploying to multiple regions allows for failover handling. If the Sydney region goes down, the Singapore region may still be online. It would be a globally bad day if the same physical faults impacted Frankfurt, Sydney, and Seattle at the same time.Multi-CloudPhysical faults are only one of the types of problems that public and private clouds encounter. User error, poorly configured networks, and other more banal errors can have substantial consequences that impact entire clouds, either from the provider or the user itself. In these circumstances, regional failover won\u2019t help you. An expired certificate, accidental deletion, or disabling the wrong port will take you offline from Seattle to Sydney. If that happens, someone, somewhere, is most definitely awake, and noticed, and will likely not be very happy.Choosing a multi-cloud configuration protects you from these types of faults so that if Google is having interruptions, AWS is still online, or IBM, Deutsche Telekom, OVH, or Hetzner.The right answer: have both!Multi-region is a go-to method to mitigate against mostly physical faults. Multi-node clusters in each of those regions handle general data corruption or failures on a more granular level.Multi-cloud is a go-to method to mitigate configuration or software-level faults, which are not that uncommon.The right answer is to have a blend of the two, working within operational cost constraints and general survivable requirements from both regulatory and business objective use cases. I do want access to my heart-rate data from two years ago. The order of browsing across Amazon shopping is of less importance.While the blend is the right answer, this still doesn\u2019t address the logistical overhead of managing multiple cloud providers' APIs (Terraforms and Pulumis of the world try to solve this use case) or the ability to automate roll-backs and health checks.We\u2019ll look into that in a later section.Regulatory reasons for multi-region architectureApart from data resiliency, most companies serving a global market are subjected to a number of geopolitical influences as well. Most users will be familiar with the GDPR regulations ensuring European data stays on European soil. Further regulations require the ability for government oversight to request data at will from service providers, which requires its own set of reasonable isolation from those not in those jurisdictions.Using a multi-region database provider allows companies to ship single binaries of software without creating isolated applications for different countries. A practice that was the norm not too many years ago.Performant reasons for multi-region architectureRetention and regulation aside, in many cases, you simply want a better experience for your users. If you watched the video above, you\u2019ll see just how much latency region separation can introduce. It goes without saying, but the less copper your data has to traverse, the faster the experience will be for your users. Co-locating data and access provides the optimal pathway for getting a user\u2019s data to the end destination.And it\u2019s not just the individual user\u2019s data. Imagine a sales enablement platform where an account executive needs to manage all APAC or all of EMEA. If these accounts are optimally stored for reads on geo boundaries, you can often remove seconds of delay between page navigations for your account manager. And we all know how much account executives like to click.Choosing a DB provider with multi-region and multi-cloudAt this point, you should be well-convinced of the benefits for multi-region and multi-cloud deployments. But at what cost? Creating replication, failover, logging, and synchronization mechanisms is a non-trivial ask. Even with deployment tooling, it is still a major operational lift to coordinate all the pieces for just multi-region deploys, let alone multi-cloud where variables in APIs come into play.While shipping horizontally scaling playbooks or helm charts with Pulumi provisioned hardware is a strategy, many of today\u2019s distributed Postgres providers simply come with all these functionalities baked in.Choosing a single provider that deploys across regions and clouds means you get to benefit from their collective experience while reducing a major burden.For some quick how-tos on connecting Hasura to Yugabyte or Cockroach, check out our documentation or the following videos.Multi-region and multi-cloud with multi points-of-presence on Hasura CloudOr, why you might not need Kubernetes.Having a database provider offer on-click multi-region deploys is the way to go, but you still need to handle the data access layer. The role-based access, federation, and other settings that enable your developers still need to be distributed along with the underlying data sources.Using the Hasura metadata approach, it\u2019s actually quite simple to deploy the same configuration for horizontally scaled Hasura instances, leaving you to pick your geo router of choice to distribute the request to the nearest point of presence.ClosingHopefully, you\u2019ve seen just how approachable a resilient, distributed data-access layer is by using the right primitives and full-powered providers. If you have any questions on how to activate these patterns in your own setup, we have solutions engineers that are happy to help.Thanks for reading.Ready to get started for free with Hasura Cloud? In just a few clicks and no credit card required, create or connect your own database today."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Performs 3.6x Faster than DIY GraphQL APIs on Postgres Description: This post will benchmark Hasura against a handwritten Node.js GraphQL server. The end goal is to show how Hasura performs similarly despite being significantly easier to develop. Author: Arjun Yelamanchili"}, {"role": "assistant", "content": "GraphQL Performance Benchmarks: Hasura vs DIY Node.js with Dataloader\nIn this blog post, we explore the performance of GraphQL APIs built using Hasura versus those built using a DIY approach with Node.js and DataLoader. We conduct a series of benchmark tests to measure the query response times, query execution times, and memory usage of both approaches to a Postgres database.\nThe results show that Hasura consistently outperforms the DIY approach in terms of query response times and memory usage. Moreover, Hasura requires significantly less setup and maintenance effort, making it an excellent choice for developers who prioritize productivity and ease of use.\n\nWant to get started with Hasura now? Sign up for a Free Hasura Cloud account or contact our sales team for a custom demo.\n\nWhat is Hasura?\nHasura enables you to instantly create a production-grade GraphQL API with authorization and other essential features, such as event triggers, real-time capabilities, and much more for your data without writing code.\nOnce you connect it to your database, Hasura automatically generates an API with the following:\n\na schema\nqueries\nmutations\nsubscriptions\n\nAs a result, you save significant time and effort since you don\u2019t need to write your logic for basic CRUD operations manually.\nThe first question people have about the automagic is - how does Hasura perform? This article aims to answer the question by benchmarking Hasura against a handwritten Node.js GraphQL server that uses DataLoader.\nThe comparison proves that Hasura performs better (in some cases significantly) while drastically reducing the effort and resources required to build an API.\nBenchmark setup\nWe took these benchmarks on Google Cloud Run configured with 4 CPU cores and 16 GB of RAM. For the database we used the Chinook dataset on Neon serverless Postgres 15. We used GraphQL Bench and k6 with Hasura 2.18.0 and Node.js 18.\nThree tests were ran:\n\n\nFor every artist, get their albums and the tracks in those albums with two thousand requests per second (RPS) for ten seconds.\nquery GetAllArtistsAlbumsAndTracks {\n  Artists {\n    ArtistId\n    Name\n    Albums {\n      AlbumId\n      Title\n      Tracks {\n        TrackId\n        Name\n      }\n    }\n  }\n}\n\n\n\nGet an album by primary key with ten thousand requests.\nquery AlbumByPK {\n  albums_by_pk(id: 1) {\n    id\n    title\n  }\n}\n\n\n\nGet an album again by primary key, except with 100 RPS for 5 seconds, then 1000 RPS for 5 seconds.\nquery AlbumByPK {\n  albums_by_pk(id: 1) {\n    id\n    title\n  }\n}\n\n\n\nResults\nFetch All Artists With Their Albums and Tracks\nAverage Hasura requests per second (RPS) was 29.17 while Node.js was 8.05 RPS.\n\n\n\n\n\n\n\n\nHasura\n\n\n\n\n\n\n\n\n\n\n\nNode.js\n\n\n\nFetch Album by Primary Key Fixed Requests\nAverage Hasura latency was 64.85 while Node.js was 68.65.\n\n\n\n\n\n\n\n\nHasura\n\n\n\n\n\n\n\n\n\n\n\nNode.js\n\n\n\nFetch Album by Primary Key Fixed Multistage\nAverage Hasura latency was 60.68 while Node.js was 63.21.\n\n\n\n\n\n\n\n\nHasura\n\n\n\n\n\n\n\n\n\n\n\nNode.js\n\n\n\nTakeaways\nN + 1 Problem\nWhen writing GraphQL APIs, one of the most significant performance issues is N + 1 queries. For our artist benchmark, each artist has to fetch the related albums. If implemented in a naive way, we would have to make N number of SQL requests, one for each artist. 100 artists would result in 100 requests for album data! To solve this in Node.js, we use the Dataloader library. Dataloader allows us to batch requests, so instead of 100 requests, one request can get all the associated albums. The tradeoff is that you must manually add these dataloaders everywhere that needs them, which can sometimes be unclear. This is on top of the work you already have to do creating your server.\nDataloader is primarily there to solve batching, but if you want it also to do caching, the complexity increases further. Let\u2019s say a query fetches a track by trackID and later fetches the same track by albumID. By default, these are two separate Dataloader caches resulting in two seperate requests for the same data. To fix this, you must manually prime the trackID and albumID cache, depending on which one you just fetched. So as different ways to fetch the data grows, so does your code if you want to stay performant.\nPlease visit the source code to see how this code can exponentially grow.\nHere is a sample of the Album resolver:\nAlbum: async (_parent, _args, context: Context, _info) => {\n    const albums = await context.getAllAlbums.load(\"1\");\n    for (const album of albums) {\n        context.getAlbumsById.prime(album.AlbumId.toString(), album);\n    }\n    const albumsByArtistId = keyByArray(albums, \"ArtistId\");\n    for (const [ArtistId, albums] of Object.entries(albumsByArtistId)) {\n        context.getAlbumsByArtistId.prime(ArtistId, albums);\n    }\n    return albums;\n},\n\nHasura acts as a SQL compiler. Because it knows how your data is structured, queries can be compiled into a single SQL statement instead of multiple. See this blog post for more information on how Hasura solves the N+1 problem. We can see the benefit of this in the \"Fetch All Artists With Their Albums and Tracks\" benchmark, Hasura was 3.6x faster!\nRead more:\n\nArchitecture of a high performance GraphQL to SQL engine\nBlazing fast GraphQL execution with query caching and Postgres prepared statements\n\nAutomagic generation\nThe Node.js sample GraphQL server demonstrates the amount of code required just for the basic functionalities such as queries. Adding more features such as mutations, subscriptions, or authorization requires even more code and time.\nIn your own applications you may spend up to 50 - 80% of development on CRUD. Hasura saves you a significant amount of time from writing the basics of a production server and comes with features such as authorization, security, event triggers, and API joins built-it.\nRead more:\n\n\nHow Hasura Works Docs\n\n\nHasura Design Philosophy\n\n\nConclusion\nBenchmarks are challenging, and these results are not scientific. We showed that in a previous blog post things like CPU power management could play a significant role.\nAs part of this benchmarking series we discovered possible places we can speed up Hasura's HTTP stack. So overtime, your APIs will get even faster with no user change needed.\nWe hope we demonstrated that automatically generated Hasura GraphQL API performed very similarly and better to a handmade GraphQL server with no development time and with added features like authorization on top for free.\nReady to get blazing fast GraphQL APIs out of the box?\nSign up for Hasura Cloud or talk to a Hasura expert.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How to Build Data APIs on Snowflake Description: In this post, we will learn how to use Hasura to rapidly build APIs on top of the Snowflake data to power embedded analytics in mobile/web apps. Author: Smit Shah"}, {"role": "assistant", "content": "In this post, we will learn how to use Hasura to rapidly build APIs on top of the Snowflake data to power embedded analytics in mobile/web apps. Using Hasura, we can quickly create secure and performant GraphQL/REST APIs on Snowflake, and then query the API from a React frontend.Why APIs?Snowflake is typically a central data source for the organization that aggregates data from different data silos, internally and externally. With all this rich and valuable information stored in a single place, businesses are looking for ways to directly use this data to power more intelligent and personalized experiences. For example, it is an in-product analytics widget showing users their usage patterns. Embedding analytics and dashboards powered by your data warehouse directly into user-facing applications requires a secure and performant data serving layer with low latency and high concurrency. Low latency is critical for delivering a snappy user experience. Fine-grained access control is crucial when exposing a data warehouse with sensitive information to user-facing apps. Analytical APIs are the perfect tool to meet these requirements because they provide a decoupled layer to implement your security and performance needs. Besides surfacing real-time analytics in apps, data APIs are an efficient way of sharing specific controlled views on the data warehouse with internal and external consumers. Employees, customers, and partners can hit a particular API endpoint to get the data they need in a self-serve way without being blocked on manual data-sharing workflows. While APIs are an excellent method to implement this secure and low-latency serving layer, building APIs the traditional way can be tedious, time-consuming, and require substantial backend service logic. This post shows how to create a performant data API on Snowflake in minutes with Hasura.Does Snowflake have an API?Snowflake's SQL API allows users to execute SQL statements against their data warehouse through a REST API. However, this API doesn't provide additional optimization, abstraction, or security features to support the development of performant, low-latency applications that require complex authorization use cases.How do I build an API on top of Snowflake?Follow these 5 steps to build your first Snowflake API and use it from a React frontend app. 0. Deploy HasuraSign up for a free Hasura Cloud account and spin up a new project. 1. Connect your Snowflake Data Source:To set up a Snowflake Data Source for Hasura, follow our Snowflake database guide. Enter your Snowflake JDBC connection details in Hasura\u2019s console as shown below,Make sure to use CLIENT_SESSION_KEEP_ALIVE=TRUE param in JDBC connection URL. Snowflake closes JDBC connections after a while (max: 4 hours). This param will ensure the connection is kept alive and bypass the restriction.2. Track tables from SnowflakeOnce connected, you can see your Snowflake tables in Hasura. You can select all the tables you want to be able to query using Hasura and click on Track Selected.3. Query your Snowflake APIOnce you connect and track tables, Hasura instantly gives you a GraphQL API on top of Snowflake that you can immediately use to start querying the data.At the bottom-right of the GraphQL API tab, you'll see the quick response times Hasura' 's high-performance GraphQL API provides. In this example, computing the average temperature over a few thousand records takes about 530 milliseconds.Note these queries are running on Snowflake directly without replicating the data in a different datastore.4. (Optional) Cache your Snowflake APIYou can use the @cached directive to your query to decrease your response time. The default TTL is 60 seconds, reducing latency by about 200 milliseconds.Caching is useful when querying Snowflake, as latencies could be a bottleneck, primarily when serving customers on web/mobile applications. If the same data is frequently fetched, caching can improve latencies and reduce the load on data warehouse resources.5. Query your Snowflake API with ReactWe will make a chart using graphql2chartjs to reshape your GraphQL data per the ChartJS API. This makes it easy to query a GraphQL API and render the output as a ChartJS chart.You can install it via:npm install --save graphql2chartjs\nPost that, you can create a bar chart component as an example,import {Query} from 'react-apollo';\nimport gql from 'graphql-tag';\nimport graphql2chartjs from 'graphql2chartjs';\nimport {Bar} from 'react-chartjs-2';\nconst Chart = () => (\n  <Query\n    query={gql`\n      query {\n        WEEKLY_TEMPRATURE(order_by: {week: asc}) {\n          label: week\n          data: TEMPRATURE\n        }\n      }`}\n    }>\n    {({data} => {\n if (data) {\n        const g2c = new graphql2chartjs(data, 'bar');\n        return (<Bar data={g2c.data} />);\n      }\n      return null;\n    }\n  </Query>\n);\n\nYou can use this component in your react application to render charts like the one below; it uses NYC weather data over the last six years.ConclusionThis post taught us five easy steps to create an API over Snowflake and then call that API from a React frontend. In a follow-up post, we will learn how easy it is to add access control to your APIs using Hasura.Hasura, unlike other solutions, doesn\u2019t require you to maintain a copy of data, and it generates SQL queries based on your request; it also provides a way to cache and rate limit same time while providing authentication and authorization options to safeguard your data.You can try out our Snowflake (beta) support on Hasura Cloud."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using OpenTelemetry Traces to observe your Hasura-powered applications Description: In this blog post, we\u2019ll talk more about how you can use traces exported in the OpenTelemetry format to the observability tool of your choice from Hasura Cloud and Hasura Enterprise. Author: Manas Agarwal"}, {"role": "assistant", "content": "In our previous post, we talked about the importance of observability, the three pillars of observability, and an overview of how to use logs, metrics, and traces emitted by Hasura to monitor, troubleshoot, and diagnose your application. In this blog post, we\u2019ll talk more about how you can use traces exported in the OpenTelemetry format to the observability tool of your choice from Hasura Cloud and Hasura Enterprise.What are traces and why are they important?To recap, traces are used to find where the problem is in your application and provide end-to-end visibility into a system\u2019s operations by tracing requests through all the different services. Traces can show how a system is functioning, identify bottlenecks, and improve performance. A span is an operation that represents a part of the workflow.For example, in an e-commerce application, we have three services \u2013 Auth, Store, and Purchase \u2013 that help users of mobile or web clients make a purchase.A trace shows the end-to-end workflow and is typically represented by a flame graph as shown below:Each individual operation (e.g., on the Auth/Store services) represents a span.Since Hasura acts as a central gateway for your application stack, it's imperative that we provide traces to analyze the performance of GraphQL queries, mutations, and subscriptions and their interactions with data sources.For example: If the database span (the leaf node of the trace) of the trace takes too long, we can consider adding an index to the table or cache the response using the @cached annotation. If the second to last span is large, there is high latency between Hasura and the database, so bringing them close geographically is a good option.Traces and OpenTelemetryOpenTelemetry is a CNCF incubating project and provides a free, open source, and vendor-neutral collection of tools, APIs, and SDKs for telemetry data. OpenTelemetry is adopted and supported by industry leaders in the observability space.To use tracing in your observability stack, you need to instrument your applications via OpenTelemetry libraries and use observability tools like DataDog, New Relic, Dynatrace, and Sumo Logic to analyze and monitor traces. Hasura is committed to open source and open standards, and therefore we have instrumented code for API queries, mutations, and subscriptions with traces for our Cloud and Enterprise products in the OpenTelemetry format that you can then export to the observability tool of your choice. How do I get started? To get started, you should first determine how your observability tool expects OpenTelemetry traces. It may either provide an out of the box endpoint (e.g., Dynatrace, New Relic, Sumo Logic), or you may have to set up the OpenTelemetry collector with the appropriate exporter. Once you have determined how you are going to ingest traces, the next step is to simply configure Hasura to send traces to the OTLP endpoint.For example, if you would like to export traces from Hasura Enterprise, in the Hasura console, you can navigate to your Settings \u2192 OpenTelemetry Exporter and then configure how to set up your OpenTelemetry compliant receiver endpoint.The key fields in this form are:The Endpoint URL: The endpoint exposed by the OpenTelemetry collector or the OTLP endpoint of your observability tool.Connection Protocol: HTTP is supported for Hasura Enterprise, and both gRPC and HTTP are supported for Hasura Cloud.Data Type: We support only traces for now.Batch Size: This is the number of data points or spans that are included in every export, which is an integer value that goes up to 512.Headers: Optional header values are added to every export request to your APM tool. These are typically authorization or API keys that may be required by your observability tool.Attributes: These custom tags are added to telemetry data can identify sources (e.g., a dev/test/stage/prod environment) and will help you filter specific traces in your observability tool.For more details, see the docs for Hasura Enterprise and Hasura Cloud. Once the endpoint is set up, Hasura begins exporting traces to your endpoint.Use your observability tool's UI to visualize and perform further analytics on trace data to monitor how your application is performing and to diagnose and troubleshoot issues with your application. Traces and their spans are listed out and selecting a trace shows a flame graph with spans that show where each operation spent its execution time.In the picture, we see the flame graph for trace of a GraphQL query that took 5.5 seconds, out of which 4.41 ms was spent in executing a query in Postgres. The bulk of the time was spent in executing the POST request (the last span of the trace) which in this case is a GraphQL API request to a remote schema.SummaryIn this blog post, we discussed how traces can help diagnose and troubleshoot application issues in production and how Hasura has adopted the OpenTelemetry standard to enable you to \u00a0collect and analyze your traces in the observability tool of your choice.To get started now, sign up for Hasura Cloud or contact us to trial Hasura Enterprise."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing HasuraCon 2023 Call for Proposals Description: Want to speak at HasuraCon 2023? Read this blog to get brainstorming on your talk and find out how to submit it for consideration. Author: Praveen Durairaju"}, {"role": "assistant", "content": "HasuraCon is back this year for a free three-day virtual event, June 20-22, which brings together thousands of developers from around the globe. Well, the Call for Proposals is open now through Friday, March 10 and we want to hear from you!We know the Hasura dev community is involved in all sorts of interesting projects. And so, while we offer up some guidelines below for the types of talks we\u2019re interested in showcasing at HasuraCon 2023, we\u2019re really just prompting you to think about your project and ways to build a talk around it. What makes your project unique or exciting, specific Hasura features you can\u2019t live without, the takeaways, tips, tricks, or recommendations for those just getting started\u2026 whatever it is, we\u2019d love to hear from you!If you\u2019d like a little more context, check out some talks from last year\u2019s event.Some ideas to get you started:Hasura in ProductionWhat problem are you solving with Hasura? What does your architecture look like, and why? What was the before/after, and where is the project headed next?Feature Deep DivesShare a deep dive on how you\u2019re using a specific Hasura feature, such as streaming subscriptions, or event triggers to solve a specific task.Best PracticesHelp the Hasura community learn from your journey so they walk the same path a little quicker and with less bruises.Architecture Show-And-TellIllustrate how Hasura fits into your technical stack, what drove your design decisions, and the pros and cons of those choices.Hacked on HasuraUsing Hasura in unconventional and interesting ways? Tell the community!Ready to learn more and save the date for HasuraCon 2023?>>Sign me up"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Achieve high availability and scalability on Hasura Cloud with Elastic Connection Pooling Description: Discover how elastic connection pooling helps you auto-scale Hasura Cloud to manage large amounts of API requests. Author: Rahul Agarwal"}, {"role": "assistant", "content": "Hasura gives developers the tools they need to create instant APIs on databases that are ready to be consumed by a large number of clients serving millions of users. For example, we have e-commerce customers building large-scale SaaS cloud platforms with massive customer request volumes and financial institutions looking to serve data to extensive partner and developer communities.To scale Hasura Cloud to manage large amounts of API requests, usually, multiple instances of Hasura GraphQL Engine are used. The number of instances are scaled up and down automatically depending on the load. This auto-scaling is done quietly without any disruption to \u00a0the API consumer.Challenges due to your database connection limitsAuto-scaling solves the problem of load at the Hasura GraphQL layer. However, databases only have limited connections available to them. In PostgreSQL, for example, every client process connects to exactly one backend process. That means every query or operation you make to the database requires a connection. When the client makes a request, Hasura will translate that API request to SQL and create a connection to the database. The query or the operation will be executed in the database and returned to Hasura. Hasura will receive the result and close the connection. So if the connection limit of the database is fifty, that means there can only be fifty queries or operations at the same time. Any attempt to create more connections will throw a database error.At high-scale workloads, client applications connect and disconnect so often the database cluster response time slows and the cluster is said to be experiencing connection churn. Each new connection to the database cluster endpoint consumes resources, thus reducing the resources that can be used to process the actual workload.Exceeding the database connection limit will make your application or API throw an error, slow down, or even break some requests. This is not a good scenario if you are the API provider, as it can lead to significant business disruption. On top of service disruption, there is also the danger of overloading the database and bringing it out of commission which could lead to inconsistent or corrupt data. Resolving such database errors is typically resource-intensive and increases the cost of ownership for your solution.Third-party and OSS options for connection poolingThere are a few options available in the market that help with the issues mentioned above. Following is an overview of those options:Use a PostgreSQL\u2013compatible connection pooler, such as PgBouncer or PgPool. For more information on how to use such PostgreSQL-compatible connection pooling middleware, check out this post by the Stack Overflow engineering team.Use a connection pooler, such as RDS Proxy. A connection pooler provides a cache of ready-to-use connections for clients. Almost all versions of Aurora PostgreSQL support RDS Proxy. For more information, see Amazon RDS Proxy with Aurora PostgreSQL.Use PgBouncer with layer 4/layer 7 load balancer such as HAProxy. Go here for more details on how AWS recommends implementing this kind of architecture. Image Source All of the approaches above require you to deploy and maintain third-party software such as HAProxy and PgBouncer. This adds to the total cost of ownership for your API system, and at the same time introduces new network interfaces that could lead to more complexity and troubleshooting. Connection pooling on Hasura with zero administrationWith the release of elastic connection pooling on Hasura Cloud in v2.15, for most of your use cases, you no longer need any external network software except Hasura to execute such high availability and performance use cases. You will still need to determine and specify the limits per database, but then the remaining implementation details around auto-scaling instances, connection pooling, and load balancing between different connections will be handled by Hasura Cloud in a seamless manner. What that means, is that you can safely eliminate the following components (highlighted in the red box) from your architecture. \u00a0Image Source While adding a new database, under Connection Settings, all you need to do is set the desired Total Max Connections for primary and read replica pools. Once Total Max Connections is set, Hasura will take care of respecting this limit irrespective of the load on the GraphQL API. Also, there are other connection pool parameters (with defaults) that can further help in better performance. To set or update total_max_connections for existing sources, head to the Data > Databases > Manage > [database-name] > Edit page and scroll down to the Connection Settings section.For more information on how this feature works and how to configure it via the CLI or the API, check out the official documentation for it here.Below you can see how simplified your architecture becomes, and the result is the ability to scale easily with zero to minimal administration.HGE = Hasura GraphQL EngineThe following are some pain points from existing Hasura Cloud customers around scaling their API workloads that were resolved using elastic connection pooling:\u201cI am easily able to send around 1000 simultaneous mutations/queries to our Hasura Cloud server, without getting errors and increasing the max connections on PostgreSQL.\u201d\u201cWe stopped facing issues with postgres-max-connections-error when we utilized the new total-max-connections setting on Hasura Cloud.\u201dYou can further augment this approach to meet tighter uptime requirements and SLAs by configuring read replicas at scale. More information on read replica support in Hasura Cloud lives here. Another option to consider is using a high-scale Postgres SaaS provider like CockroachDB, Yugabyte, or AlloyDB, which don\u2019t have the overhead of connection memory. Go here to see the latest PostgreSQL databases that Hasura supports.SummaryIt is easy to get started on Hasura Cloud, and as you implement this feature, we would like to hear your thoughts on any enhancements or improvements. Elastic connection pooling is currently only supported for PostgreSQL and Microsoft SQL Server data sources. We\u2019re actively looking to expand support to other databases as well as for self-hosted Hasura instances, so reach out to in our Discord channel if this feature would be helpful for you."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using Hasura saved our Beta launch Description: Hasura has proven to be an invaluable tool for our small team at Forwheel as we work to build and deploy scalable GraphQL APIs for our food truck vendor-facing application. Author: Hasura"}, {"role": "assistant", "content": "This post was originally posted on Medium.com by Brenden BishopI\u2019m not typically one for blogging, but I felt compelled to share this\u2026There I was \u2014 frustrated, tired, confused \u2014 all of a sudden our database hookup to Heroku was just FUBAR, gone, never to be seen again \ud83d\udc4b. Smack in the middle of crunch-time, 10 days out from our Beta launch target date, and we just lost our whole database\u2026 \ud83d\ude31What is Hasura?Hasura is a popular open source GraphQL engine that allows developers to build and deploy scalable GraphQL APIs quickly and easily. They can use their existing database and create a GraphQL API with just a few clicks, making it an ideal solution for teams looking to automate and scale their GraphQL layer.Don\u2019t know about GraphQL?via GIPHYFor those who don\u2019t know, GraphQL is a query language for your API that was developed by Facebook as an alternative to REST APIs. It allows you to create flexible and scalable APIs where you can easily request exactly the data you need for a particular use case, rather than getting more data or having to make multiple API calls to get all the data. Also, it\u2019s easy to update and evolve without breaking existing client applications. Since GraphQL is focused on the data rather than the endpoint, you can always add new fields or modify existing ones without affecting the way the client application accesses the data. It is particularly useful for applications that require frequent updates or for teams working on large, complex projects where the data requirements may change over time.Now, back to Hasura\u2026 one of the standout features is its ability to allow you to define GraphQL schemas, tables, metadata, and data using code. This means that if anything goes wrong with the database, you can easily recover by simply applying the metadata, migrations, and seeds using Hasura\u2019s command-line tools.***Update*** This process is now made even simpler by Hasura via their \u201cone-click\u201d cloud deploymentsIntroducing the \"One Click Deploy to Hasura Cloud\" feature \ud83e\ude84You can now deploy your Hasura projects to Hasura Cloud with 1 click. That means you can seamlessly share your backends with other users. \ud83d\udd25 pic.twitter.com/21U9RA5qmM\u2014 Hasura (@HasuraHQ) February 8, 2023\n\nWhat happened?At Forwheel, we were using Hasura to host our dev instance and connect to a Heroku Postgres database. Then Heroku goes off and removes their free-tier model, which then appeared to delete our database!We had just lost our Dev DB and all of its data! This was still a critical time period for feature development and bug fixes, and I was completely devastated. With my prior experience setting up DBs and the works, I was so sure that we now had just lost two weeks minimum to try and restore this (only me and one other dev and we have full-time jobs, this is on the side).via GIPHYAfter much shock and anger, I briefly remembered that with Hasura they had all these concepts of metadata, seeds, and migrations. Luckily it\u2019s made so easy that even when you manipulate the database through the Hasura console, you can generate commit changes with their CLI. So, with just a few commands, we were able to apply our metadata, migrations, and seeds to a new database (using https://neon.tech/) and get our dev instance up and running again in a matter of seconds (once I realized this was possible of course \ud83d\ude44).How to set up Hasura with a \u201cdatabase as code\u201d implementationSetting up your Hasura cloud instance locally is pretty straightforward and is done in a few simple steps:First, install the CLI by running npm install -g hasura-cli or yarn global add hasura-cli.Create a new Hasura project by running hasura init. This will create a new directory with the required files and dependencies for your Hasura project.Once your project is set up, you can apply your metadata by running hasura metadata apply. This will create your GraphQL schema and tables based on your metadata file.After your metadata is applied, you can apply your migrations by running hasura migrate apply. This will update your database with any changes you\u2019ve made to your schema.Finally, you can apply your seeds by running hasura seed apply. This will populate your database with the test and user data you\u2019ve defined in your seeds file.Thats it!We run these commands as our database grows and evolves to easily keep all our database changes in code. I don\u2019t know about you, but especially not having too much backend or DB experience, this was a lifesaver for me. Previously, I only had a handful of times dealing with database as code and I just remember it being a nightmare with migrations, versioning, permissions, etc. \u2013 it never quite \u201cworked\u201d for me. I always had conflicts and issues. Hasura just works.SummaryHasura has proven to be an invaluable tool for our small team at Forwheel as we work to build and deploy scalable GraphQL APIs for our food truck vendor-facing application. It has saved us time and headaches, and we are grateful to the Hasura team for their hard work in creating such a powerful solution. And yet, this is only scratching the surface of what it has to offer. For just two developers focused on the Mobile App UIs, not having to deal with a middle layer has increased our development time tenfold.Yes, we signed up for Hasura Cloud, but our usage is still early and slow at the moment and so it\u2019s all been free! Honestly, it\u2019s an incredible product/service, and I am genuinely excited for when we increase our usage to start actually paying for this \u2013 it will be worth every penny \ud83e\udd11.Oh, I almost forgot\u2026 the ONE time we had an issue with the Hasura Cloud console UI, we got an immediate email back and an opportunity to walk through our issue over Google Meet with their support team and some devs within 48 hours. Again, we have not paid anything yet for this software. I know that type of service won\u2019t last forever as I can see them continue to grow and scale massively, but just\u2026BRAVO Hasura, Bravo! \ud83d\udc4f\ud83d\udc4f\ud83d\udc4fP.S. Shame on you Heroku, not cool\u2026About the AuthorBrenden is a software engineer and serial entrepreneur with a passion for using technology to solve problems. With almost 8 years of experience working in a large financial institution, Brenden has also started three companies in his career. Despite the past two being unsuccessful, he gained valuable insights and applied them to his current venture, Forwheel. Forwheel aims to revolutionize the mobile food industry by creating an unforgettable and universally accessible experience. When not working, Brenden enjoys coding, personal finance and trading, cooking, skiing, and golfing."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing One Click Deploy to Hasura Cloud Description: We are happy to introduce One Click Deploy to Hasura Cloud, helping all our users quickly create a new project on Hasura Cloud from an existing sample app. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Today, we are happy to introduce One Click Deploy to Hasura Cloud, helping all our users quickly create, share and explore a new project on Hasura Cloud from a sample app. Existing Hasura developers can now share their backends quickly for anyone else to try out.As a new user to Hasura (or GraphQL), you will be able to explore GraphQL backends quickly. The example backends will feature a range of use cases from database schemas with sample models and apps, external GraphQL APIs that can be connected to Hasura via Remote Schemas, external REST APIs that can be added to Hasura via Action Transforms and more.Try it outWe have created sample projects you can deploy to Hasura Cloud to try this out quickly.Chinook Music LibrarySlack BackendHow does it work?When you click a Deploy to Hasura Cloud button, you will be taken to the Hasura Cloud dashboard, where a new project is automatically created for you and the migrations, metadata and seeds are applied from the GitHub repo of the sample app connected to the button.\n\n  Your browser does not support the video tag.\nYou can dynamically enter any environment variables required to run the Hasura project. This way, you avoid cloning a Git repo, setting up Hasura CLI and applying the Hasura deploy command pointing to a Hasura Cloud project.Hasura CLI is used extensively by users running Hasura in a production environment. A CLI-generated Hasura project is at the core of collaboration between various teams within an organisation, migrating from local development -> staging -> production, among many other workflows. Extending the power of Hasura CLI, every publicly available Hasura project generated by the CLI is now a sample app that can be deployed as a Hasura Cloud project.Discovering examples and sample appsWe have launched the Hasura Hub page, where you can now browse a list of sample apps that can be deployed quickly using the Deploy to Hasura Cloud button workflow.Browse through a curated list of sample apps and example projects to deploy using one clickWant to contribute to this list? Feel free to submit your sample Hasura project here and we will get back to you.Creating your own one-click to deploy appAn example Deploy button that can be used to embed inside your Hasura project repos.If you have an existing Hasura CLI project on GitHub, it can be instantly deployed to Hasura Cloud. You will have to just format the URL to point to your project and Hasura will take care of applying migrations and metadata automatically.https://cloud.hasura.io/deploy?github_repo=<LINK-TO-GIT-REPO>&hasura_dir=<PATH-TO-HASURA-DIRECTORY-IN-REPO>&branch=<BRANCH-NAME>Replace the github repo, path to Hasura directory and the branch appropriately.Docs and resourcesOne Click Deploy to Hasura Cloud on Hasura DocsCreate a One Click Deploy AppHasura Hub"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Updates to Hasura Pricing and Plans Description: We\u2019re updating our pricing and plans to better align with the value that customers get from Hasura. In this blog, we share the motivation behind this update and outline the changes, timeline, and rollout plans.  Author: Tanmai Gopal"}, {"role": "assistant", "content": "We\u2019re updating our pricing and plans to better align with the value that customers get from Hasura, as well as give customers more flexibility and cost predictability when scaling their usage. This is the first major change to our pricing model since our founding and one we are making after extensive thought. I wanted to share the motivation behind this update, as well as outline the changes, timeline, and rollout plans. Before we get into the details, we would like to clearly state that all existing users \u2013 on both free and paid subscriptions \u2013 can continue to run their current projects at the legacy pricing and entitlements. How Hasura has EvolvedWe founded Hasura in 2017 with a simple idea \u2013 to give developers a radically simpler and faster way to unlock their data via APIs. This simple idea evolved into one of the fastest-growing open source GraphQL projects that now powers billions of API requests per day. Hasura is enabling thousands of organizations to bring their ideas to market faster by dramatically simplifying the data access and delivery piece. Over the last five years, we have made massive improvements to the product, such as: enabling event-based architectures with subscriptions and event triggers, powering advanced federation scenarios via joins across heterogeneous sources, and bringing instant GraphQL magic to more databases including SQL Server, Google BigQuery, Snowflake, CockroachDB, Google AlloyDB, and many more.We are updating our pricing plans to better reflect the value of these product innovations and ensure we keep investing in Hasura to continually create an exceptional experience for every user. We are also moving to a usage-based pricing model that gives customers more flexibility to get started with Hasura and more cost predictability as they scale usage.This is a significant change to our model and one we are not making lightly. Over the last several months, we have surveyed hundreds of our customers and studied the broader market to better understand the Hasura price-value calculus. We have also reviewed and iterated on these new plans and feel confident about their design and ability to better serve the needs of our customers.However, we recognize changes like these are never easy and welcome your feedback as we roll out these adjustments. You can reach out to our team via the Contact Us page or connect with us on our Discord (use the #general channel). What\u2019s Changing?Full details for the new plans can be found on our pricing page, but here\u2019s a quick summary of what\u2019s changing. \nWe are deprecating the current $99/month Standard plan on Hasura Cloud and introducing a new Professional plan with usage-based pricing starting at $1.50/hour.\nWe are updating some of the usage limits and features in our plans to better align with each plan\u2019s objectives:\n\nFree/Community plans for early exploration and learning\nProfessional plan for low-scale production APIs\nEnterprise plans for high-scale and mission-critical workloads\n\n\nWe will continue to offer a generous Free plan on Hasura Cloud but are tweaking what\u2019s included to correspond with its intended goal as a frictionless plan to get started, learn, and evaluate Hasura. For example, we are removing the single region limit in the Free plan and allowing users to deploy their free Hasura projects on any available cloud region.\n\nHow Does This Impact Customers?Hasura Cloud Free and Standard Plan Users For existing Hasura Cloud users, your current Free and Standard projects remain unimpacted \u2013 you\u2019ll continue to run your existing projects indefinitely at the legacy pricing and entitlements until you decide to upgrade your project to a new plan.The new plans will be visible in your account in the next few weeks as we gradually roll them out to all users. Once the new plans are rolled out in the product, all new Free plan projects will automatically be created with the new free entitlements. However, until April 30, 2023, existing Hasura Cloud users will have the option to create and upgrade new paid projects to either the old Standard plan or the new Professional plan. After April 30, 2023, you will only be able to create new paid projects using one of our new pricing plans.\nUpdated Feb 2, 2023: Based on feedback from the community, we have decided to forever grandfather in existing Hasura Cloud users to the legacy Standard plan for both existing and new projects. Any Hasura Cloud user that signed up on or before Jan 31, 2023 can keep their existing projects and continue creating new projects on the legacy Standard plan. \u00a0Legacy Free and Standard plans will not be available to new users signing up for Hasura Cloud once these changes take effect later this month. Enterprise Plan CustomersExisting customers on annual Enterprise plans across Hasura Cloud and self-hosted deployments are not impacted by this change right now. Your account team will discuss the new pricing options with you at contract renewal. Feel free to reach out to your account team if you have any questions before that. If you are a new user evaluating our Enterprise plans, please reach out to our sales team via sales@hasura.io or our website to learn more about our new usage-based pricing. Self-Hosted Community Plan UsersOur self-hosted Community plan will remain open source and free to download. However, going forward, we will be deprecating support for databases that are primarily designed for enterprise use cases, including: Google BigQuery, Amazon Athena, Snowflake, and Microsoft SQL Server, as well as advanced PostgreSQL connectors such as Azure Cosmos DB/Citus and CockroachDB. The plumbing to add support for these databases using GraphQL Data Connector is already available in the Community plan, which will allow anyone to build connectors to any data source from Hasura.We will continue to contribute critical bug fixes to these connectors, but we will not be adding new features or enhancements to these databases in the self-hosted Community plan. New features for these databases will be available in Hasura Cloud as well as self-hosted Enterprise plans. We are committed to our mission of enabling teams to unlock their data via data APIs. We will continue investing in product features to bring the Hasura magic to more databases and use cases. We believe these pricing changes allow us to double down on our commitment to serve our customers and community as they deserve.If you have questions or feedback, we encourage you to connect with us via the Contact Us page or on Discord (use the #general channel)."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Save Time and Stop Writing GraphQL Resolvers Description: Building GraphQL APIs manually requires a lot of effort and resources. This post talks about how Hasura helps build GraphQL APIs eliminating the challenges and time consumed. Author: Catalin Pit"}, {"role": "assistant", "content": "The conventional way of building a Data API through GraphQL involves building everything from scratch. That includes tasks such as:\n\nwriting boilerplate code for basic CRUD functionalities\nimplementing and maintaining GraphQL resolvers\noptimizing the resolvers for performance\nadding authentication and authorization\nimplementing data validation and error checking mechanisms\n\nOne of the most complex and time-consuming tasks is the implementation of GraphQL resolvers, though. Since the resolvers are the ones that generate a response for a GraphQL operation, it's critical to implement them properly.\nHowever, as the complexity of the API increases, so does the effort and difficulty of coding the resolvers. Manually implementing the resolvers can be difficult for several reasons:\n\nBoilerplate code: It requires a significant amount of boilerplate code for the CRUD operations\nComplexity: The resolvers become complex once you add filtering, aggregations, authorization, and custom business logic\nPerformance: Inefficient implementation of resolvers can lead to an underperforming API\nMaintenance: Maintaining the resolvers and keeping them up-to-date with changes in the schema and data sources is time-consuming\n\nMoreover, you must ensure that the API is secure and can scale appropriately. All this work requires lots of effort and resources. What if you could eliminate all these challenges and time-consuming work? This article presents an alternative way of building GraphQL Data APIs without the difficulties of manually-built APIs.\nWriting the GraphQL Resolvers manually\nIn this section, we'll go over a custom-built GraphQL API that represents a digital media store. The API is connected to the Chinook database, which includes tables for artists, albums, and tracks.\nThe figure below illustrates the GraphQL API.\n\nSince the GraphQL resolvers are the main focus of this article, we'll only focus on them. We'll skip the other details that are not relevant to this article. However, you can see the complete application code on GitHub.\nThe schema of the GraphQL API looks as follows:\nschema {\n  query: Query\n}\n\ntype Query {\n  Album(id: Int!): Album\n  Albums: [Album!]!\n  Artist(id: Int!): Artist\n  Artists: [Artist!]!\n  Track(id: Int!): Track\n  Tracks: [Track!]!\n}\n\ntype Album {\n  AlbumId: Int!\n  Artist: Artist\n  ArtistId: Int!\n  Title: String!\n  Tracks: [Track!]\n}\n\ntype Artist {\n  ArtistId: Int!\n  Name: String\n  Albums: [Album!]\n}\n\ntype Track {\n  Album: Album\n  AlbumId: Int\n  Bytes: Int\n  Composer: String\n  TrackId: Int!\n  MediaTypeId: Int!\n  Milliseconds: Int!\n  Name: String!\n}\n\nLooking at the schema, you can see that the API only allows querying the database for artists, albums, and tracks. It doesn't enable mutating data, such as inserting, updating, and deleting records.\nAs a result, this GraphQL API has a couple of resolvers. A resolver for retrieving:\n\na specific album\nall albums\na specific artist\nall artists\na specific track\nall tracks\n\nThe \"resolvers\" code looks as follows:\nimport { readFileSync } from \"node:fs\";\nimport { createServer } from \"node:http\";\nimport { createSchema, createYoga } from \"graphql-yoga\";\nimport { GraphQLError } from \"graphql\";\nimport DataLoader from \"dataloader\";\nimport { useDataLoader } from \"@envelop/dataloader\";\nimport type { Album, Artist, Resolvers, Track } from \"./generated/graphql\";\nimport sql from \"./db\";\nimport { genericBatchFunction } from \"./dataloader\";\nimport { keyByArray } from \"./utils\";\n\nconst typeDefs = readFileSync(\"../schema.graphql\", \"utf8\");\n\nconst resolvers: Resolvers = {\n    Query: {\n        Album: async (parent, args, context, info) => {\n            return (context.getAlbumsById as DataLoader<string, Album>).load(args.id.toString());\n        },\n        Albums: async (parent, args, context, info) => {\n            const albums = await (\n                context.getAllAlbums as DataLoader<string, Album[]>\n            ).load('1');\n            for (const album of albums) {\n                (context.getAlbumsById as DataLoader<string, Album>).prime(\n                    album.AlbumId.toString(),\n                    album\n                );\n            }\n            const albumsByArtistId = keyByArray(albums, 'ArtistId');\n            for (const [ArtistId, albums] of Object.entries(albumsByArtistId)) {\n                (context.getAlbumsByArtistId as DataLoader<string, Album[]>).prime(\n                    ArtistId,\n                    albums\n                );\n            }\n            return albums;\n        },\n        Artist: async (parent, args, context, info) => {\n            return (context.getArtistsById as DataLoader<string, Artist>).load(\n                args.id.toString()\n            );\n        },\n        Artists: async (parent, args, context, info) => {\n            const artists = await (\n                context.getAllArtists as DataLoader<string, Artist[]>\n            ).load('1');\n            if (!artists) {\n                throw new GraphQLError(`Albums not found.`);\n            }\n            for (const artist of artists) {\n                (context.getArtistsById as DataLoader<string, Artist>).prime(\n                    artist.ArtistId.toString(),\n                    artist\n                );\n            }\n            return artists;\n        },\n        Track: async (parent, args, context, info) => {\n            return (context.getTracksById as DataLoader<string, Track>).load(args.id.toString());\n        },\n        Tracks: async (parent, args, context, info) => {\n            const tracks = await (\n                context.getAllTracks as DataLoader<string, Track[]>\n            ).load('1');\n            for (const track of tracks) {\n                (context.getTracksById as DataLoader<string, Track>).prime(\n                    track.TrackId.toString(),\n                    track\n                );\n            }\n            const tracksByAlbumId = keyByArray(tracks, 'AlbumId');\n            for (const [AlbumId, tracks] of Object.entries(tracksByAlbumId)) {\n                (context.getTracksByAlbumId as DataLoader<string, Track[]>).prime(\n                    AlbumId,\n                    tracks\n                );\n            }\n            return tracks;\n        },\n    },\n    Album: {\n        async Artist(parent, args, context, info) {\n            return (context.getArtistsById as DataLoader<string, Artist>).load(\n                parent.ArtistId.toString()\n            );\n        },\n        async Tracks(parent, args, context, info) {\n            const tracks = await (context.getTracksByAlbumId as DataLoader<string, Track[]>).load(\n                parent.AlbumId.toString()\n            );\n            for (const track of tracks) {\n                (context.getTracksById as DataLoader<string, Track>).prime(\n                    track.TrackId.toString(),\n                    track\n                );\n            }\n            return tracks\n        },\n    },\n    Artist: {\n        async Albums(parent, args, context, info) {\n            const albums = await (context.getAlbumsByArtistId as DataLoader<string, Album[]>).load(\n                parent.ArtistId.toString()\n            );\n            if (Array.isArray(albums)) {\n                for (const album of albums) {\n                    (context.getAlbumsById as DataLoader<string, Album>).prime(\n                        album.AlbumId.toString(),\n                        album\n                    );\n                }\n\n            }\n            return albums || [];\n        },\n    },\n    Track: {\n        async Album(parent, args, context, info) {\n            return (context.getAlbumsById as DataLoader<string, Album>).load(\n                parent.AlbumId!.toString()\n            );\n        },\n    }\n};\n\nexport const schema = createSchema({\n    typeDefs,\n    resolvers,\n});\n\nconst server = createServer(\n    createYoga({\n        schema,\n        plugins: [\n            useDataLoader(\n                \"getAlbumsById\",\n                (context) =>\n                    new DataLoader((keys: Readonly<string[]>) =>\n                        genericBatchFunction(keys, { name: \"Album\", id: \"AlbumId\" })\n                    )\n            ),\n            useDataLoader(\n                \"getAllAlbums\",\n                (context) =>\n                    new DataLoader(async (keys: Readonly<string[]>) => {\n                        const albums = await sql`SELECT * FROM ${sql(\"Album\")}`;\n                        return keys.map((key) => albums);\n                    })\n            ),\n            useDataLoader(\n                \"getAlbumsByArtistId\",\n                (context) =>\n                    new DataLoader((keys: Readonly<string[]>) =>\n                        genericBatchFunction(keys, { name: \"Album\", id: \"ArtistId\" }, true)\n                    )\n            ),\n            useDataLoader(\n                \"getArtistsById\",\n                (context) =>\n                    new DataLoader((keys: Readonly<string[]>) =>\n                        genericBatchFunction(keys, { name: \"Artist\", id: \"ArtistId\" })\n                    )\n            ),\n            useDataLoader(\n                \"getAllArtists\",\n                (context) =>\n                    new DataLoader(async (keys: Readonly<string[]>) => {\n                        const artists = await sql`SELECT * FROM ${sql(\"Artist\")}`;\n                        return keys.map((key) => artists);\n                    })\n            ),\n            useDataLoader(\n                \"getTracksById\",\n                (context) =>\n                    new DataLoader((keys: Readonly<string[]>) =>\n                        genericBatchFunction(keys, { name: \"Track\", id: \"TrackId\" })\n                    )\n            ),\n            useDataLoader(\n                \"getTracksByAlbumId\",\n                (context) =>\n                    new DataLoader((keys: Readonly<string[]>) =>\n                        genericBatchFunction(keys, { name: \"Track\", id: \"AlbumId\" }, true)\n                    )\n            ),\n            useDataLoader(\n                \"getAllTracks\",\n                (context) =>\n                    new DataLoader(async (keys: Readonly<string[]>) => {\n                        const tracks = await sql`SELECT * FROM ${sql(\"Track\")}`;\n                        return keys.map((key) => tracks);\n                    })\n            ),\n        ],\n    })\n);\n\nserver.listen(4000, () => {\n    console.info(\"Server is running on http://localhost:4000/graphql\");\n});\n\nThis GraphQL API has a single purpose - to read the data from the database. Although the API is quite simple, it requires a considerable amount of code. Now imagine a complex enterprise application and the amount of code you would have to write.\nThe amount of code you have to write is one of many concerns. There are other things you need to think about, such as:\n\nthe N+1 problem\nextending security rules per field in the schema\nadding gateway features (caching, rate limiting, etc.)\n\nAll of the above is difficult and time-consuming to implement. Hasura solves issues such as the N+1 problem and provides features such as caching, rate limiting, and authorization by default.\nN+1 problem\nGraphQL query execution usually involves executing a resolver for each field. Let's say you are fetching artists and their tracks. Each \"artist\" record in the database would invoke a function to fetch their N \"tracks\". As a result, the total round trips become N+1, which could become a huge performance bottleneck. The depth of the query makes the number of queries grow exponentially.\nHasura mitigates the N+1 problem because, at its core, the server is a compiler. That means it compiles all GraphQL queries to a single SQL query, thus reducing the number of hits to the database and improving the performance.\nRead more about how Hasura solves the GraphQL N+1 Problem.\nSecurity Rules\nMost applications require an authentication system to answer questions such as \"Does this (valid) user have permission to access this resource or perform this action?\". It specifies what data a particular user can access and what action the user can perform. Implementing such a critical system is a challenging task.\nHasura allows you to define granular role-based access control rules for every field in your GraphQL schema. It's granular enough to control access to any row or column in your database.\nWith row-level access control, users can access tables without having access to all rows on that table. This is particularly useful for protecting sensitive personal data, which is part of the table. This way, you can allow all users to access a table but only a specific number of rows in that table.\nRow Level Permissions\nColumn-level access control lets you restrict access to specific columns in the table. This is useful to hide data that are not relevant, sensitive, or used for internal purposes.\nColumn Level Permissions\nCombining both these rules gives a flexible and powerful way to control data access to different stakeholders involved.\nHow does it work? Hasura uses the role/user information from the session variables and the actual request to validate the request against the rules defined by you. If the request/operation is allowed, it generates a SQL query, which includes the row/column-level constraints from the access control rules. It then sends it to the database to perform the required operation (fetch the necessary rows for queries, insert/edit rows for mutations, etc).\nRead more about Authorization and Access Control in the documentation.\nAdding gateway features\nA production-ready API requires features such as caching, rate limiting, monitoring, and observability. When you build the API manually, you must consider and implement all these features.\nThere can be latency issues and slow response times for GraphQL queries because of the response size, the location of the server, and the number of concurrent requests. Hasura has metadata about the data models across data sources and the authorization rules at the application level. This enables Hasura to provide end-to-end application caching. Cached responses are stored for a period of time in an LRU (least recently used) cache and removed from the cache as needed based on usage.\nRead more about caching in the documentation.\nSave time and resources\nFor enterprise-level Data APIs, the above features are a must. The API must be secure and performant. That results in massive work to implement just the basic CRUD functionalities. Once you add authorization, caching, rate limiting, and other features, it requires even more effort and resources.\nWhat if you could skip all these tedious and time-consuming tasks? In the next section, you'll see how you can have a production-ready GraphQL API within minutes without writing code, plus all the above features (and more).\nBuilding GraphQL APIs without writing resolvers\nHasura is a GraphQL Engine that makes your data instantly accessible over a real-time GraphQL API. It connects to your data source and automatically generates the GraphQL schema, queries, mutations, subscriptions, CRUD operations, and authorization.\nIt doesn't require writing any code unless you want to add custom business logic. You get everything out of the box.\nLet's continue by creating a Data API with Hasura to demonstrate the power of Hasura. There are two ways to use Hasura:\n\nusing Hasura Cloud, which is the easiest way to use and build Hasura applications\nusing Docker to self-host it\n\nThis article uses Hasura Cloud. Navigate to your Hasura Cloud dashboard and create a new project by clicking the \"New Project\" button.\n\nIn the next step, click the \"Create Free Project\" button, and you should have a Hasura project up and running within seconds. You should see the console after launching the project.\n\nThe next step involves connecting the Hasura app to your database. Navigate to the \"DATA\" tab, then to \"Create New Database\", and click the \"Create Neon Database\" button.\n\nThe database should be created & connected within seconds.\n\nNow click the \"Create Table\" button to create a new table. For this example, create a users table with the following columns:\n\nid of type UUID (Primary Key)\nusername of type Text\ncountry of type Text\n\nSelect the id as the Primary Key and save the table.\n\nIf you navigate back to the \"API\" tab, you should see all the available GraphQL operations. You get all the operations needed to access and mutate data without writing code (or resolvers). On top of that, you also get real-time capabilities.\n\nThis simple example demonstrates how Hasura improves the process of building and shipping Data APIs to production. It unblocks and allows developers to move extremely fast since they automatically get all the critical features.\nHow does that work\nA GraphQL Resolver is a function that specifies how to process a specific GraphQL operation and turn it into data. A conventional GraphQL API can't exist without resolvers because they are an essential part of it.\nHasura takes a different approach, though. Instead of using the conventional GraphQL resolvers, it uses a GraphQL Engine. The engine, which is actually a compiler, compiles the GraphQL operation into an efficient SQL query.\nLet's take the Chinook database as an example again. The Chinook data model represents a digital media store, including tables for artists, albums, media tracks, invoices, and customers. The example below illustrates a couple of untracked tables.\n\nWhen the tables are untracked, they are not exposed over the GraphQL API. To expose them to the public, you need to track them. When you track the tables, the GraphQL Engine automatically generates a bunch of things, such as:\n\na GraphQL type definition\nqueries\nmutations\nsubscriptions\n\nCheck the docs for the complete list of things it generates.\nOnce the engine tracks the tables, it knows how to respond when users request data from the database to which these tables belong. You can also see the available operations in the \"Explorer\" tab.\n\nAs mentioned earlier, Hasura compiles the GraphQL operation into a SQL query. Consider the following query from the above image:\nquery {\n  Album {\n    Title\n  }\n}\n\nClick on the Analyze button to see what happens under the hood when you run the query. Clicking the button opens a new pop-up with the \"Generated SQL\" and the \"Execution Plan\", as shown in the image below.\n\nThe generated SQL statement for the query is as follows:\nSELECT\n  coalesce(json_agg(\"root\"), '[]') AS \"root\"\nFROM\n  (\n    SELECT\n      row_to_json(\n        (\n          SELECT\n            \"_e\"\n          FROM\n            (\n              SELECT\n                \"_root.base\".\"Title\" AS \"Title\"\n            ) AS \"_e\"\n        )\n      ) AS \"root\"\n    FROM\n      (\n        SELECT\n          *\n        FROM\n          \"public\".\"Album\"\n        WHERE\n          ('true')\n      ) AS \"_root.base\"\n  ) AS \"_root\"\n\nThat's how Hasura works. Instead of using GraphQL resolvers, it generates SQL statements at runtime and then runs them against your database.\nThe video illustrates how Hasura doesn't need to use resolvers for data interop and what the key differences are with other systems.\nAdditional resources to learn more:\n\nArchitecture of a high performance GraphQL to SQL engine\nBlazing fast GraphQL execution with query caching and Postgres prepared statements\n\n\nAdding custom business logic\nEven though Hasura doesn't use resolvers, you can join your existing GraphQL APIs with your Hasura application. That's possible with the help of Remote Schemas.\nRemote Schemas\nHasura can merge remote GraphQL schemas and provide a unified GraphQL API. Think of it like automated schema stitching. All you need to do is build your GraphQL service and provide the HTTP endpoint to Hasura. Your GraphQL service can be written in any language or framework.\n\nThat enables users to connect their existing GraphQL API to their Hasura application. For instance, if you have a GraphQL payment API, you can join it to your Hasura application with the help of Remote Schemas.\nRead more about Remote Schemas in the documentation.\nAdditionally, there are other ways of adding custom business logic, such as Actions and Event Triggers.\nHasura Actions\nActions are a way to extend Hasura's schema with custom business logic using custom queries and mutations. Actions can be added to Hasura to handle various use cases such as data validation, data enrichment from external sources, and any other complex business logic.\n\nAdditionally, Actions in Hasura allow for integrating new and existing REST APIs with your Hasura application. For example, if you have a Node.js REST API for sending emails, you can plug it into Hasura without making any changes to the existing API.\nRead more about Actions in the documentation.\nEvent Triggers\nHasura can be used to create Event Triggers on tables in the database. Event Triggers reliably capture events (such as insert, update, and delete) on specified tables and invoke HTTP webhooks to carry out any custom logic.\n\nA simple example would be triggering an API endpoint to send a welcome email once a user is added to the database.\nRead more about Event Triggers in the documentation.\nSummary\nExposing your data via a GraphQL API doesn't mean you need to spend resources and effort building it manually. You can let Hasura do all the heavy lifting for you. In the use cases where you require custom business logic, you can add it with the help of Actions, Remote Schemas, and Event Triggers.\nAdditional material:\n\nDevelop backend applications in various languages and integrate them with Hasura\nHasura GraphQL Engine Documentation\n\nData federation in Hasura\nHow Hasura GraphQL Engine Works\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Pod42, the Hasura ChatGPT Bot Description: We are happy to announce the launch of Pod42, a bot that answers questions related to Hasura. It uses the GPT AI model, trained on resources like Hasura\u2019s docs, learn and blog portals. Author: Praveen Durairaju"}, {"role": "assistant", "content": "We are happy to announce the launch of Pod42, a bot that answers questions related to Hasura. It uses the GPT AI model, trained on resources like Hasura\u2019s docs, learn and blog portals.Note: It\u2019s in alpha, so expect some bugs or issues with answers and the sources it gives.Getting started with Pod42The bot has been integrated into our discord server. In case you haven\u2019t joined yet, head over to https://hasura.io/discord to join HasuraHQ discord server.Once inside the server, head to #pod42-support channel and tag the @Pod42 bot to ask a question. The format for asking a question looks something like:@Pod42 <question>\n\n// example\n@Pod42 What is Hasura?Testing for funHere are some of the answers we have been getting from the bot after training it with our data sources.Why use Hasura?A fun attempt by David below:Another one about read replicas:DisclaimerThe bot is in alpha, which means it might give inaccurate or incomplete answers sometimes. Use the source/s to verify the information, and when in doubt, reach out to help-forum. Also, we're constantly training Pod42 to improve it. If you have any feedback, please let us know. Any feedback is appreciated!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Security in Production with Automated Allow Lists Description: Learn how to generate allow lists in Hasura automatically from your GraphQL Queries. Author: Catalin Pit"}, {"role": "assistant", "content": "In some cases, you might want to restrict your Hasura application to a limited number of GraphQL operations (queries/mutations/subscriptions). The reason for doing it might be to avoid excessive data exposure, API scraping and to protect your application & data.\nHasura allows you to restrict the number and type of available operations with the \"Allow List\" feature. The \"Allow List\" represents a list of allowed GraphQL Operations that can be executed in your application. So the Hasura GraphQL Engine only runs the operations specified in the \"Allow List\".\nMoreover, there is a way to generate an Allow List based on your front-end code automatically. This way, the allow list and your front-end code are always in sync!\nThe article presents 3 ways to generate allow lists in Hasura, with the last one being fully automated.\nEnable \"Allow List\"\nThe \"Allow List\" feature is disabled by default. You need to navigate to your project settings and enable it.\n\nSet the HASURA_GRAPHQL_ENABLE_ALLOWLIST environment variable to true, as shown in the figure.\nAllow List of operations in Hasura\nLet's use the following Hasura application as an example. It's a simple application that stores users. Each user has an id and a name.\n\nNavigate to the API tab and click on the Allow List at the top of the page (right-hand side).\nThat opens the \"Allow List\" manager. This is where you can add, modify and remove operations in your \"Allow List\".\n\nClicking on the Add Operation button opens a new modal where you can add an allowed operation. There are 3 ways to add an operation:\n\nwrite it manually\nupload a file with the operation/operations\npick it from the Quick Add dropdown (it displays the latest operation executed)\n\n\nAfter you add the operation, you can see it on the \"Allow List\" page.\n\nAnother way to create an AllowList\nThere's another way you can manage your \"Allow List\". Navigate to the MONITORING tab and click on the Allow Lists option.\nIf you navigate to the NEW OPERATIONS page, you can see the operations you executed that are not in the \"Allow List\" yet. That means Hasura Cloud allows you to choose from a list of operations you executed previously and add the ones you want to the \"Allow List\".\n\nInstead of manually writing the operations, you can select the ones you want to add to the \"Allow List\" and click the Add to Allowlist button.\n\nAfter that, you should see them on the ALLOWED OPERATIONS page.\nAutomatically Generate an AllowList\nCreating an AllowList manually is fine, especially when the application is not large. However, once the application grows, it can become time-consuming to manually add operations to the AllowList. Keeping the list in sync with your front-end code can also be tedious.\nThankfully, there is a way to automate that with the GraphQL Code Generator library. The GraphQL Code Generator automatically generates code from your GraphQL schema and operations. It also has a plugin @graphql-codegen/hasura-allow-list that automates the creation of the \"AllowList\" based on the GraphQL Queries found in your front-end code.\nAfter you set up @graphql-codegen in your project, add the following to your codegen file:\nimport type { CodegenConfig } from '@graphql-codegen/cli';\n\nconst config: CodegenConfig = {\n generates: {\n  'path/to/metadata/allow_list.yaml': {\n    plugins: ['hasura-allow-list']\n  }\n }\n}\n\nexport default config;\n\nNow that the process of creating the \"AllowList\" is fully automated, the \"AllowList\" and your front-end code are always in sync.\nNote: The plugin only works in the JavaScript ecosystem.\n\nIt's important to mention that this is a community plugin, and not the official Hasura solution.\n\nAdditional resources:\n\nLearn how to declaratively protect your GraphQL APIs with Hasura Cloud - best practices to configure authz rules, allow lists, rate limiting & schema restrictions\nLearn how to optimize your Hasura application for security\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The rise of GraphQL APIs on data warehouses Description: In this article, we will talk about the drivers behind the rise of data APIs in data warehouses, challenges with current approaches to building, operating, and scaling data APIs, and how Hasura addresses those challenges for our customers. Author: Tanmai Gopal"}, {"role": "assistant", "content": "TL;DRData warehouses are used to store valuable enterprise data. Data APIs \u2013 both GraphQL and REST \u2013 are emerging as a tool to power frictionless and high-quality integrations between applications/services and data in enterprise data warehouses. In this article, we will talk about drivers behind the rise of data APIs on data warehouses, challenges with current approaches to building, operating, and scaling data APIs, and how Hasura is addressing those challenges for our customers.Table of Contents\n\nThe rise of data warehouse integrations\nTop use cases for data warehouse integration\nData APIs are the solution to data warehouse integration\nBuilding APIs the traditional way can be slow and expensive\nBuilding and scaling GraphQL and REST APIs faster with Hasura\nGet started with instant API on your data warehouse\n\nThe rise of data warehouse integrationsCloud data warehouses such as Snowflake, Google BigQuery, Amazon Redshift, etc. have seen a dramatic increase in adoption over the past few years. The maturity of the solutions themselves and the tooling (ETL) around them has improved a lot as well, making their adoption and usage a lot more tractable.With the broader movement to become more data-driven as well as to get more from their data investments, enterprises are constantly looking for ways to bring data into more initiatives, decisions, and experiences. How can the data be shared more widely (but in a secure and compliant way) to enable more internal and external stakeholders to make better decisions? How can data be incorporated into applications to build more delightful user experiences \u2013 for customers, partners, and employees.Top use cases for data warehouse integrationLet's take a look at the top 3 data-first use casesData-as-a-product to other teams in the orgTeams that own data want to publish it to other teams in their organization or externally to other partners. Increasingly they want to think of their data as a product.Example: \u200b\u200bBuyers with very high returns because of fraudulent reasonsEmbedded data features in existing products (apps or services)End users of digital products are increasingly expecting their product experience to be \"smarter\". For offline products, users benefit from a digital management and analytics experience. At the same time, product and business teams know that intelligent features can drive increased stickiness, retention and usage of their products. Example: Show \"predicted\" end of month bills on a consumption-based billing service. Or, surface a customer 360 type of view for internal marketing and sales teams by aggregating data across multiple tables and data sources. Integrations to enrich data in other business toolsEnterprise tools and SaaS products (like Marketo, Salesforce, etc.) that are the operational foundation for the business can become a lot more valuable when they are integrated with more data and context, which usually resides in a data warehouse. Example: Visualising data using Google's data studio.Data warehouses themselves are evolving to keep up with the demand for extracting more value from data. For example, Google Cloud has been investing heavily in BigQuery ML that allows teams to leverage their existing SQL knowledge to easily create and use ML models.The desire for integrating transactional and analytical use-cases has led to Snowflake announcing Snowflake Unistore \u2013 their unified platform that brings product use-cases to the core CDW value-proposition that they are known for. However, customers are still looking for better ways to integrate their warehouse data into these use cases. Many are coming to the conclusion that APIs (and more specifically GraphQL APIs) are the most flexible, secure, and performant way to enable these use cases on their data warehouses. Let\u2019s look at why data APIs are well-positioned to address these use cases.GraphQL Data APIs are the solution to data warehouse integrationBefore we dive into why APIs are the best way to power this new breed of integration needs, let's look at how organizations primarily access data in their warehouses today, and why/how those current approaches fall short for these newer use cases. The 2 primary ways of connecting to data warehouses is either via JDBC / ODBC connectors in 3rd party applications or by writing SQL (or derivative) queries directly against the data. Both of these approaches are great for traditional analytics and visualization needs. For example visualizing data in a BI or visualization tools like Tableau or Looker; or querying and analyzing data for analytics and reporting use cases. But this type of direct access to data doesn\u2019t work for these new integration use cases because it tightly couples the consumption layer with the data layer in a way that is hard to maintain, operate, and secure. A data API access layer becomes necessary. Let\u2019s look a few scenariosMultiple applications/services need access to the same dataMultiple tenants connected to the same database makes it hard to evolve the data and schema of the database, or to provide performance QoS or tiers to different consumers. Building an API layer between the producer and consumer allows decoupling, and provides an intermediate layer that can absorb those changes in a more elegant way.Enforcing authentication & authorization without an API is painfulIAM in data warehouses are geared towards providing secure access to individuals and analysts, but are not ideal for application/service integrations.Sharing database credentials with multiple applications becomes a management headache and a potential security nightmare. Applications probably have a centralized authentication mechanism that should be leveraged, but databases typically don't integrate with that directly.Furthermore, entitlements (especially fine-grained policies) that govern access to certain rows or columns of data based on properties of the end-user or the application are hard to manage and enforce in the data warehouses because they don't provide those capabilities or because maintaining those policies in the data warehouse\u2019s proprietary setup makes it hard to scale reviewing & auditing these security policies.GraphQL APIs allow data consumers to fetch just the data they needDirect access is fine when the consumer is trying to pull data that lives in a single table / view. But, can be inefficient because of multiple request roundtrips if you need to pull and join data from multiple tables/views/databases, etc. This inefficiency can have a non-trivial cost and performance impact depending on level of overfetching and query volume. REST APIs can also suffer from this limitation. GraphQL was created to solve exactly this problem for transactional workloads, and is gaining popularity for addressing this issue for analytical / OLAP data sources as well.All these reasons make APIs, and more specifically GraphQL APIs, a more elegant way to address the data integration use cases for data warehouses.Building APIs the traditional way can be slow and expensiveWhile GraphQL/REST APIs are great fit for these data integration use cases, building, scaling, and operating APIs can be a time-consuming task, which can in turn cause organizations to shy away from API-driven approach to data warehouse integrations.Here are just a few of the many things that add time and effort to build and operate APIs. Creating a single API end point requires developer to build and maintain these:Boilerplate code for CRUD APIMapping client types and schemaBaking authentication and authorization is tedious and riskyValidation and error checking code in the API layerBuilding GraphQL API brings additional costs. Developers need to write and maintain resolvers to map the GraphQL query to the underlying SQL queries. The effort to write resolvers increases significantly with the richness and complexity of the GraphQL queries, for e.g. if you have filtering, aggregations, complex and/or logic, etc. Furthermore, making sure that the resolvers are written in a way that the resulting GraphQL query is performant takes more planning and optimization. Operating APIs efficiently brings its own set of challenges. Making sure that the APIs are performant, reliable, and can scale smoothly with load can become a heavy lift, especially as this expertise is not typical in teams managing the central data warehouse. Updating the API to keep up with changes at the database level needs some discipline as well to make sure the consumers have a smooth experience.Hasura completely eliminates these challenges with traditional API creation and operation workflows. It allows organizations to become API-driven, but without all the effort that goes into API creation, scaling, security, and operations.Building and scaling GraphQL and REST APIs faster with HasuraHasura provides a GraphQL API over transactional sources of data, and over the last year we've been seeing first-hand how developers and teams increasingly need to access data from a data warehouse. Hasura allows rapid creation of GraphQL APIs (or REST) by just configuring the data sources and the mapping of underlying data models into API models, and then Hasura provides an API that has joining, filtering, pagination & aggregation capabilities built-in. Hasura also has a session-aware caching layer that makes integrations with realtime applications easier with OLAP and CDW systems.Imagine the following the eCommerce schema in a Data WarehouseAn API call that fetches users based on number of orders they've made, is a normal query to make, but is challenging to represent as a flexible REST API call:An API call that fetches orders based on what region the users are in, without fetching unnecessary user data:Streaming SubscriptionsIf you have a large amount of data, or \"fast moving\" data in Postgres, Hasura now allows you to instantly create an API for clients to fetch that data as a continuous stream. This API can be safely exposed to internal or external HTTP clients.For example, in the users placing orders scenario, during holiday season, there would be plenty of orders coming in every second. We can make use of streaming subscriptions to async load the data on the UI and not bombard the client to fetch orders which are still pending payments.subscription getPendingOrders {\n  orders_stream(cursor: {initial_value: {created_at: \"2023-01-01\"}, ordering: ASC}, batch_size: 2) {\n    id\n    amount\n    created_at\n    user_id\n  }\n}GraphQL Streaming Subscription with HasuraPerformanceHasura is highly optimized for performance, scalability and reliability. Hasura also provides caching and query optimization services, which further help improve performance.CachingNow, some queries are more frequently accessed than others. Typically, there could be latency and slow response times due to the size of the response, location of the server (in a distributed multi-region app), number of concurrent API calls.Hasura has metadata about the data models across data sources, and the authorization rules at the application level. This helps Hasura to provide end to end application caching.High performance GraphQL to SQL EngineInternally Hasura parses a GraphQL query, gets an internal representation of the GraphQL AST. This is then converted to a SQL AST and with necessary transformations and variables the final SQL is formed.GraphQL Parser -> GraphQL AST -> SQL AST -> SQLThis compiler based approach allows Hasura to form a single SQL query for a GraphQL query of any depth.Read moreArchitecture of a high performance GraphQL to SQL engineBlazing fast GraphQL execution with query caching and Postgres prepared statementsGraphQL helps make one API call to fetch related data and a precise subset of data, instead of making multiple REST API calls that might return potentially redundant data.This has a cost implication because only the right subset of data is fetched and a single query is executed to fetch related pieces of data (as opposed to multiple REST API calls that fetch more data than is necessary)SecurityHasura has a fine-grained authorization engine that allows creating policies to restrict access to only particular elements of the data based on the session information in an API call.Hasura Authorization and AuthenticationRow level and column level permissionsHasura supports role-based access control system. Access control rules can be applied to all the CRUD operations (Create, Read, Update and Delete). Some operations can be completely restricted to not allow the user perform the operation.Row Level PermissionsColumn Level PermissionsThe rules can be setup easily using Hasura Console, where you define a role and setup rules for access.For example, for row level permissions, we can set up filters like{\"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}The above rule is equivalent to saying: allow this request to execute on the current row if the value of the user_id column equals the value of the session variable X-Hasura-User-Id.Column Level Access RulesRead more:Hasura Authorization System through ExamplesImplementing Authorization for Organization-based, Team-based, or Tenant-based Apps in HasuraGet started with instant API on your data warehouseHasura supports BigQuery, Snowflake, Amazon Athena, and Postgres flavors like AlloyDB and Hydra (geared for OLAP). If you and your team are looking at exposing data warehouse data over an API, Hasura can significantly reduce the time and effort to do so, in turn freeing up your developers to do other high-value work with direct ties to business outcomes.To get started follow our guides to get instant APIs on Snowflake or BigQuery.Get started with instant APIs on BigQueryGet started with instant APIs on SnowflakeYou can see other guides here.Or, reach out to the Hasura team for a custom demo to get all your questions answered about how Hasura can accelerate your path to APIs on data warehouses."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Best Practices Guide for GraphQL Observability with Hasura [Part 1] Description: Hasura Cloud and Enterprise come out of the box with logs, tracing, and a bunch of other metrics. We will look at best practices to make use of that data for GraphQL Observability Author: Arjun Yelamanchili"}, {"role": "assistant", "content": "Hasura is an open-source product that accelerates API development by 10x by instantly giving you GraphQL or REST APIs on your data. Hasura plays a central role in your application stack as it can call out to your other services and databases while adding authorization and caching on top. Your development teams can quickly access critical data that powers their applications. Please see our website for more information.\nNote: This is a multi-part series. The current post delves into the various Observability metrics relevant for apps built with GraphQL / REST APIs. In the next parts, we will talk about the use cases and how Hasura specifically solves them.\nObservability\nAs your app grows and becomes more complex, it becomes increasingly difficult to understand how it is used and performs. This is where observability comes in. Observability is the ability to gauge a system\u2019s internal conditions by looking at its outputs, such as metrics or logs. Observability means you can:\n\nGain insights into the functionality and health of your systems, collect data, visualize them, and set up alerts for potential problems.\nHave distributed tracing provide end-to-end visibility into actual requests and code, helping you improve your application\u2019s performance.\nAudit, debug, and analyze logs from all your services, apps, and platforms at scale.\n\nBecause Hasura can act as a central gateway for your application stack, we take observability very seriously.\nSLOs, SLIs, and the Three Pillars\nThe path to thoroughly observing any app is to ensure you have your SLOs defined, SLIs identified, and telemetry data that enable the collection of those SLIs. In production, your SRE team would define service level objectives (SLO) that are intended to measure the reliability of your app. Each SLO is a target value or range of values for a service level measured by a service level indicator (SLI). For example, a service level indicator could be the number of requests per second, and an SLO could be that our app needs to be able to serve a minimum of ten thousand requests per second.\nOnce our SLOs and SLIs are defined, we use the three pillars of observability to ensure we are meeting our objectives.\nThe Three Pillars of Observability\nObservability relies on three key pillars: logs, metrics, and traces. Access to these components does not automatically make systems more visible, but when used together, they can provide powerful tools for understanding a system\u2019s internal state.\n\n\nMetrics are used to determine if there\u2019s a problem to begin with. They provide snapshots of a system\u2019s metadata, such as server resources used or requests per second. Metrics can monitor performance, identify trends, and set alerts for potential troubles.\n\n\nTraces, used to find where the problem is, provide end-to-end visibility into a system\u2019s operations by tracing requests through all the different services. Traces can show how a system is functioning, identify bottlenecks, and improve performance.\n\n\nLogs, used to determine what the problem is, provide timestamped messages of events in a system. You can use them to debug issues.\n\n\nHasura Cloud and Enterprise come out of the box with logs, tracing, and the following metrics:\n\nRequest rate\nError rate\nAverage request execution time\nActive subscriptions\nNumber of WebSockets open\n\nAll observability data is available via the Hasura Console, one of our APM integrations such as Datadog and New Relic, or any APM receiver that supports the OpenTelemetry specification.\nWith the traditional way of rolling your own GraphQL server, you have to manually set up all your resolver logic, observability, and authorization code.\nHasura Cloud and Enterprise come with support out of the box.\nMetrics\n\nRequest Rate\nThe request rate represents how many requests a service receives per unit of time, such as 100 requests per minute (RPM). This is the most fundamental metric, as you must design your entire architecture around your expected request rate. As this metric grows, you must scale up your system with techniques such as database sharding or risk your system being overwhelmed and unreliable.\nHow to Use\nRequest rate tracks the overall usage of a system. It becomes too performance-intensive at high request rates to monitor every request. Instead, you would sample a percentage. Some other best practices:\nScale our services up and down dynamically based on the request rate. An example is Hasura Cloud\u2019s automatic horizontal scaling.\nIf there are trends in RPM, such as more requests in the morning, you can pre-provision infrastructure.\nWhen RPM spikes above expected usage, an alert should warn of a possible DOS attack.\nHigh RPM from specific users may warrant setting up IP or auth-based rate limiting.\nError Rate\nThe error rate represents how many errors a service receives in a period of time. Most HTTP APIs signal errors via HTTP status codes, 400 for client errors and 500 for server errors. GraphQL differs because most errors are in the body\u2019s errors key. Hasura automatically parses both forms of errors for you.\nErrors are an inherent part of distributed systems. Network communication is complex, and a lot can go wrong.\nHow to Use\nError rates are a good indicator of the overall health of your systems and are often the quickest way to spot problems. We can distinguish apparent disruptions over background noise by viewing the error rate over time and looking for patterns.\nA couple of ways you can use the error rate are monitoring to quickly roll back deployments when deploying service updates using techniques like canary or blue-green deployments or triggering an alert anytime the error rate is over a baseline amount.\nViewing Individual Errors\nFor REST HTTP APIs, the HTTP status code tells us general information about the error. For example, HTTP 429 \u201cindicates the user has sent too many requests in a given amount of time\u201d. Based on the code, we can begin troubleshooting.\nGraphQL does not use HTTP status codes unless it\u2019s a server error. Instead, the response object has an errors key that you need to check for errors.\nAverage Request Execution Time\nThe average request execution time is calculated by the total time taken by all requests during time range / time range in minutes.\nHow to Use\nLatency is a huge factor in user experience, so as developers, we should try as hard as possible to reduce this metric. If your data allows it, caching built into Hasura is a significant first step in lowering execution time. Using tracing, which we learn about in an upcoming section, we can see remote APIs that need to be optimized or queries for which we may need to add an index.\nSubscriptions\nWith Hasura GraphQL subscriptions, there are two main components: database subscriptions and the WebSocket connection to the client.\nHow to Use\nPrimarily useful to monitor performance, the subscription metrics allow you to see the usage of GraphQL subscriptions. You can see a list of clients connected via WebSocket. The database subscriptions feature helps you diagnose any database performance issue related to subscriptions. To understand the mapping between a WebSocket client and the underlying database subscription, please check out this article on how we scaled to one million GraphQL subscriptions.\nLogs\nOnce metrics have identified an incident, you can drill into individual logs to find the root cause. Hasura has many logging options, such as choosing what layers emit logs, so please read the documentation. Here are a few ways we can use the Hasura logs to monitor our system:\nAggregating by Query Name\nWith typical REST HTTP APIs, we aggregate our statistics by the endpoint. Since GraphQL uses one endpoint for all requests, we should combine the security feature of allow lists with aggregating by operation name. This reduces our monitoring surface drastically because all queries are determined ahead of time.\nThis view is built-in to Hasura Console but can be done with any APM system using the data in the query-log layer.\nDiagnosing Errors\nRequests to Hasura record success or error in the http-log layer. Once your metrics alert on an abnormal error rate, search your logs for the level: error. The response objects have an error key with the GraphQL error.\nDistributed Tracing\nA request can go through many services; therefore, it can be difficult to see where it failed or slowed down. Distributed tracing via the B3 spec combined with observability tooling allows you to follow a request through its life and pinpoint issues.\nHasura, acting as the API gateway, can trace external APIs and database queries.\nExternal API Tracing\nWhen a request comes into Hasura, it generates a B3 header if one doesn\u2019t already exist. Whenever an API encounters a B3 header, it will report its request metrics to an APM system. The APM system then aggregates all the information to build a trace for each request, allowing you to visualize them efficiently.\nDatabase Tracing\nThere is no standard method for trace propagation in Postgres, so Hasura injects a transaction-local JSON variable.\nTo get the trace information into database logs and native database monitoring tools, query tags are used. Query tags are SQL comments appended to the generated SQL statements with tracing metadata. Tools like pganalyze can use them to help you optimize your queries.\nQuickly and easily set up a GraphQL API and explore Hasura\u2019s observability features for free on Hasura Cloud.\n\nSummary\nIn conclusion, observability is a critical aspect of modern systems. By collecting and analyzing data from logs, metrics, and traces, you can gain valuable insights into the health and performance of your services. With the right tools and techniques offered out of the box with Hasura Cloud and Enterprise, you can improve the observability of your systems and ensure that they are functioning optimally.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: December 2022 Newsletter Description: Welcome to the final edition of the Hasura Newsletter for 2022!\nThis month, we introduced the Hasura GraphQL Data Connector for Snowflake, which lets teams build GraphQL and REST APIs on their Snowflake data within minutes. Learn more, and give it a try yourself on Hasura Cloud. Author: Starr Stephenson"}, {"role": "assistant", "content": "Welcome to the final edition of the Hasura Newsletter for 2022!This month, we introduced the Hasura GraphQL Data Connector for Snowflake, which lets teams build GraphQL and REST APIs on their Snowflake data within minutes. Learn more, and give it a try yourself on Hasura Cloud.Join us for our Snowflake+Hasura product deep dive on January 11 @ 10 AM PST. Register \u2192HighlightsWe\u2019ve launched the Hasura GraphQL Data Connector for Snowflake! Read our blog or join us on January 11 for a deep dive webinar. Register \u2192The Tokyo Hasura User Group has planned its 2nd meetup on January 19. Register \u2192Product UpdatesSnowflake Integration: This month, we released the beta version of the Hasura GraphQL Data Connector for Snowflake! This connector lets you instantly generate read-only GraphQL and REST APIs to your Snowflake data. For more information, check out our blog post on the release.Elastic Connection Pooling on Hasura Cloud: In Hasura Cloud, you can now configure the maximum size of the connection pool for your entire project. Apart from the general performance benefits of using pooled connections, this also ensures that your database remains reliable even during periods of massive load. Read more about this here.Alloy DB: On December 13, 2022, Google Cloud announced the GA release of AlloyDB, with Hasura listed as one of the technology solution launch partners. This ongoing partnership ensures the best experience and support when you use Hasura with AlloyDB. Coming soon in v2.17, AlloyDB will appear as an option under the data sources dropdown in the Hasura UI. When using the AlloyDB data source, Hasura read-only queries leverage the bulk and concurrent read capabilities of AlloyDB, improving performance. For more information, see Google Cloud\u2019s blog on partner solutions for AlloyDB and our guide to getting started with AlloyDB here.Database Connection Latency Checks for Hasura Cloud: You can now proactively identify API performance issues by checking the connection latency of databases connected to your Hasura Cloud projects via the \u2018Data Manager\u2019 tab of the Hasura Cloud console. Elevated latency conditions generally occur when databases and projects are in geographically distant regions, so this information can be used to determine if you need to move your Hasura project to another region.Events[Virtual] Instant GraphQL APIs on Snowflake with Hasura | Jan 11 @ 10 AM PTJoin this launch webinar with Hasura engineers and product managers to get a deep dive into Hasura\u2019s GraphQL Data Connector for Snowflake. This data connector will let teams build a unified GraphQL API for all their Snowflake data sources within minutes and allows for data to be queried and joined from multiple sources outside of Snowflake. Register here.Hasura User Group Tokyo Meetup | Jan 19 @ 6:45 PM JSTThe Tokyo HUG is back for its second in-person meetup in January. The event will feature two talks along with some snacks and soft drinks. The agenda is being finalized, but you can register for the meetup here.[Community Call] Our first call of 2023 | Jan 26 @ 9 AM PTHasura is kicking off 2023 with some fun! Join Tanmai Gopal, co-founder & CEO, and hear what\u2019s in store for Hasura in the new year. Plus, you could win some exclusive swag from our recently launched swag store! Register here.[Virtual] GraphQL Galaxy | Now Available On-DemandIf you missed GraphQL Galaxy earlier this month, you catch all the sessions on-demand by signing up for the multipass. Hasura hosted two sessions: Exploring the Data Mesh Powered by GraphQL with Tanmai Gopal and GraphQL Everywhere with Jesse Martin.New ContentBlogpostsHasura Actions using Netlify Functions for Custom LogicNext.js 13 Nested Layouts, Streaming SSR with Realtime GraphQLIntroducing Instant GraphQL APIs for Snowflake Cloud Data PlatformRow Level Security with Postgres (via Hasura AuthZ)Improving Hasura\u2019s Internal PR Review processBuilding a Digital Twin with MQTT and Hasura Streaming Subscriptions[Community] GraphQL federation with Hasura GraphQL Engine[Community] How to Query a Database from Postman with Hasura[Community] Using Rails with Hasura (Japanese)[Community] Switching from Heroku to Neon (Japanese)[Community] Deploying a GraphQL API in minutes with Hasura and Docker (Spanish)Featured VideoOther VideosHasura Quick Tip: Resolvers[Community] Hasura GraphQL and REST API Generator[Community] Revisiting Hasura GraphQLCommunity UpdatesTooling from the CommunityCrudio automatically creates test data. It does virtually everything it takes to get a data platform up and running, so you can quickly prototype, build or demonstrate your system, product, or service. Built by Chris Nurse (vistalert#3519 on Discord), it recently got a bunch of new features, including the ability to create data streams, like IoT weather station data, health monitors, etc. Try it out and chat about the project with Chris on the Hasura Discord.Monthly RecommendationThis month\u2019s recommendation comes from Praveen Durairaju, a Developer Advocacy Content Manager at Hasura. He recommends StoptheMadness browser extension for Mac users. He likes this extension because it re-enables all the default functions on Mac. The browsing experience has improved multi-fold as there are no more annoying restrictions for contextual menus, enables auto-completion for input fields by default, and much more.We\u2019re hiring for some exciting roles at Hasura! If you or someone else is interested, please apply. A few of the key roles we are hiring for:Cloud Infrastructure Engineer - Hasura CloudSenior Product DesignerSite Reliability Engineer (SRE) - Hasura CloudSenior Haskell EngineerYou can apply for these roles and others here.That\u2019s a wrap on December\u2019s newsletter. If you're interested in receiving the monthly updates in your inbox, subscribe to our newsletter.See you next year,Hasura"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top 3 Reasons why Enterprises Choose Hasura Description: This article showcases the main 3 reasons why enterprises choose Hasura for their applications. Author: Catalin Pit"}, {"role": "assistant", "content": "GraphQL is a popular choice for enterprises looking to improve the performance, efficiency, and flexibility of their APIs. Functionality like being able to fetch very specific data (no over or under-fetching), having a strongly typed system, and schema introspection all make GraphQL a great choice.\nHowever, building a GraphQL API manually requires considerable time, effort, and investment. Engineering organizations need to write a significant amount of code by hand just for the basic CRUD functionalities of your GraphQL API. Adding authorization, multiple data sources to your end-point, and custom business logic requires even more work and code. This work will likely take months or even years depending on complexity and a team of developers.\nWe find more and more enterprises asking themselves whether their business is a GraphQL business or not. Why spend time and resources on tedious tasks that are not actually core to your IP when you can get the most heavily leveraged GraphQL functionality out of the box?\nThis is where Hasura comes in.\nHasura is an open-source product that auto-generates GraphQL or REST APIs with a built-in authorization layer for your data. Point Hasura to the databases, REST & GraphQL endpoints, and third-party APIs, and it provides a unified, connected, real-time, and secured GraphQL API for all of your data.\nHasura speeds up the development process, and in this article, we will explore the top three reasons why enterprises choose Hasura for their GraphQL needs.\nData Access\nData in the modern world most likely comes from multiple sources. In an enterprise environment, you might have data from sources such as your database(s), internal APIs, and third-party services.\nThat means there are time-consuming and repetitive tasks, such as connecting those data sources to your applications. After you connect them, the work continues by ensuring they function properly on their own and with the other existing data sources.\nSo each time a new data source needs to be integrated with an existing application, the process repeats:\n\nIntegrate the data source -> configure it -> test it -> maintain it\n\nThat can quickly become a bottleneck that slows down the development team and the application. So the question is - is there a better alternative? The alternative is Hasura.\n\nAs the diagram shows, Hasura abstracts all the tedious, boring, and time-consuming work. You connect all the data sources (databases, REST and GraphQL APIs) to Hasura, and it automatically generates a unified, secure, and real-time GraphQL API.\nIn the case of databases, the Hasura GraphQL Engine automatically generates the GraphQL Schema and operations such as queries, subscriptions, and mutations. It also enables you to connect existing REST and GraphQL APIs to your Hasura application.\n\nHasura unifies all data sources and enables the client to access unified data through the GraphQL API provided by Hasura. Creating a unified data-access API layer also allows developers to access and manipulate data in their database without having to write complex SQL queries. To access the data, developers can use the GraphQL API endpoint provided by Hasura to send queries and mutations that specify the data that should be retrieved or updated.\nHasura provides several tools and features to help developers work with data more effectively. For example, the Hasura Console provides an intuitive interface for exploring and testing the GraphQL API, and the Hasura CLI allows developers to manage their data from the command line.\nOverall, Hasura\u2019s approach to data access makes it easy for developers to work with their data and build robust and scalable applications on top of enterprise data.\nAPI Production Readiness\nIn addition to improving data access, another primary business value Hasura provides is reducing the time to market by offering you critical features out of the box such as:\n\ncaching\nreal-time capabilities\nauthorization\nobservability\n\n... and much more.\nAs mentioned previously, building a GraphQL API manually requires effort just for the basic functionalities. Implementing complex operational features like caching, real-time capabilities, or authorization requires even more effort.\n\nTraditionally, building an API manually takes weeks or months and a team of developers. In this process, the developers also need to optimize the API to be production ready. That includes security, authorization, availability, scalability, and observability.\n\nHasura transforms the process of building the API into a service that comes with features such as authorization, real-time capabilities, observability, and availability out of the box. And everything highlighted in the above diagram.\nHasura and the developer tooling around it are designed to help teams build products faster and more efficiently.\nIncreased Developer Productivity\nHasura cuts down development time by 50% to 80%. You can find more from our case studies here. Hasura does this by automating key GraphQL development tasks such as defining schemas, writing resolvers, and setting up a server. Secondly, it automates and abstracts tedious tasks such as:\n\ndefining the GraphQL schema\ndefining and implementing the GraphQL resolvers\ncreating and configuring the GraphQL server\n\n... plus other mundane work.\nSuch features are critical for an enterprise and  tricky to implement. They require experienced API developers and a considerable amount of effort and time. Once they are implemented, someone needs to maintain them. Letting Hasura take care of them improves the developers\u2019 productivity as they can focus on other critical tasks, like delivering core IP back to the business.\n\nAnother thing that increases developer productivity is Hasura\u2019s metadata. The metadata is the brain of a Hasura application that captures vital information such as the following:\n\nconnections to the data sources\nmapping the model from the data sources into an API\nrelationships between the models\nauthorization rules\n\nHasura uses this metadata information to generate a JSON API schema and exposes it through a GraphQL API. GraphQL is ideal for working with JSON data, as it is native to the JSON format. As an API specification, GraphQL allows developers to create flexible and powerful APIs that can operate on a wide range of data models and support a variety of methods. At the same time, it maintains the controlled boundaries that are necessary for a web service.\nDevelopers can then use the metadata to configure and manage their applications. As a result, it improves the developer experience and productivity since developers don\u2019t have to write code to configure or enhance their applications.\nSummary\nMost of the work involved in building a Data API via GraphQL is either tedious and repetitive or difficult and time-consuming. It\u2019ll require lots of effort and resources.\nBut it doesn\u2019t have to be that way. Hasura enables enterprises to build a production-ready Data API, with all the critical features, without having to write any code. That makes it easy for developers to quickly and easily build scalable and reliable applications without spending time on tedious and complex tasks.\nOverall, Hasura offers a comprehensive and robust platform for enterprise application development.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Critical Vulnerability in Hasura GraphQL Engine v2.10.0-v2.15 Description: Security note about a critical security vulnerability discovered in Hasura GraphQL Engine v2.10.0-v2.15 on Nov 21, 2021 Author: Timothy Cline"}, {"role": "assistant", "content": "IntroductionOn November 21st, Hasura became aware of a critical security vulnerability within our GraphQL Engine \u201cUpdate Many\u201d API, which was introduced in version 2.10.0. \u00a0The vulnerability was identified by Morten Hillbom and Issaaf Kattan from Nhost's customer company Celsia.io. There was a Missing Authorization vulnerability which allowed some expansion of update capabilities of a user. This vulnerability impacted row level authorization on Update Many calls to Postgres datastores. DetailsIn the implementation of the new Update Many API for Postgres backends, there was a bug in the code which would enforce the row level authorization mechanisms. \u00a0Since this was within the Update Many API, a user could perform an update on any column they had update permissions to on any row within the target table, and subsequently retrieve any columns they had select permissions on from any impacted rows. In order for this to be exploited, the Update Many API would need to be called against a Postgres datastore, the API would need to not have been disabled using the configuration added in 2.14.0, the caller of the API would need to have update permissions on the target table, and the API would need to not be restricted by Allow Lists.TimelineAs noted in the introduction, we became aware of the vulnerability on November 21st, however, this is not a complete representation of the issues encountered. \u00a0Morten Hillbom and Issaaf Kattan identified an issue around row level matching and permissions around November 16th which they tracked down to the GraphQL Engine Update Many API. \u00a0They reached out following our documented process of emailing build@hasura.io and provided details about the estimated scope and impact of the vulnerability, however, the email alias listed in the documentation had become misconfigured and was no longer sending its emails to the support ticketing tool. This resulted in our team not seeing the email. Fortunately, they persisted and reached out to Johan Eliasson at Nhost, who was able to use direct contacts to bring the vulnerability to our attention on November 21st.Recognizing the dangers inherent in missing authorization and authorization bypass, we quickly investigated and began patching the vulnerability. By the end of the day on November 21st, we had produced a patch and deployed it to our cloud hosted environments. During the night and into the next day, November 22nd, we completed backports of the patch and released builds of all affected versions back to 2.10. Once patches were available, a security advisory was published on GitHub and notifications were sent out on our security mailing list and to our Cloud and Enterprise customers with limited details and ~10 business days lead to this notification, in order to give users who self-manage Hasura deployments the time needed to apply the patches.Starting on November 22nd, we created queries to try to track adoption so we would know when it was safest to release these details.Remediating ActivitiesIn response to this vulnerability, Hasura has released patches for all impacted versions starting from 2.10 when the vulnerability was introduced and removed all vulnerable versions from docker hub. Hasura continues to monitor usage of vulnerable versions and strongly encourages users to update as soon as possible. For users on 2.14.0 or later, they may turn off the impacted API using --experimental-features=hide_update_many_fields. In addition to supplying patched versions, Hasura is doing the following:Updating all documentation to use email distribution lists/aliases in order to ensure a human is receiving all messages, even if they are also going into an automated message boxCompleting an audit of existing mailboxes to ensure all reports have been properly tracked and addressedUpdating testing procedures and automated tests in order to prevent regressionRedesigning the authorization implementation in order to better ensure all APIs enforce every type of authorizationReviewing Dynamic Application Security Testing (DAST) tool coverage and processesImproving Vulnerability Management processes to ensure layered defensesBuilding and managing customer notification lists, so we can more easily and readily communicate vulnerabilities in particular types of deploys (Cloud, CE, EE, Postgres, Snowflake, etc.)Updating Security Best Practices for using Hasura to include layered defenses such as Allow Lists and access reviews.Identifying ImpactsYou can look into calls to the Update Many API within your services and review any tables which have been the target of these calls. If you have disabled API and telemetry logging or are not certain you keep all of these records long enough, then you should do a consistency check across all of your Postgres datastores behind Hasura 2.10.0 to 2.15.0.We take these issues very seriously and will be making numerous changes to prevent and respond better in the future. If you have questions, or need assistance, please reach out to support@hasura.io."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Instant GraphQL APIs for Snowflake Cloud Data Platform Description: We're excited to announce the beta release of our new GraphQL Data Connector for Snowflake. This powerful new database integration will allow Hasura Enterprise users to instantly create a unified GraphQL API that runs read-only queries across all your Snowflake data tables. Author: Adron Hall"}, {"role": "assistant", "content": "Real-time access to data is essential for building modern applications. However, as the data footprint in an organization grows and spreads, it creates unique challenges for developers who need to aggregate data across multiple sources and develop the necessary APIs to securely and efficiently access and serve the data required to power their applications.To help development teams with these challenges, we're excited to announce the beta release of our new GraphQL Data Connector for Snowflake. This powerful new database integration will allow Hasura users to instantly create a unified GraphQL API that runs read-only queries across all your Snowflake data. Hasura drastically reduces the time and effort required to build and secure data APIs, allowing teams to quickly power their analytical applications with data aggregated in Snowflake.Important - This beta release of the Hasura GraphQL Data Connector for Snowflake supports read-only queries. During the beta period, the connector is available for Hasura Enterprise Edition (EE) and all Hasura Cloud customers. Once this integration moves to General Availability (GA), it will only be available in the Hasura Enterprise plans. For more information, read our docs.What is Snowflake?Snowflake, often referred to as a Cloud Data Platform, provides a way to bring together and easily access data siloed across multiple sources. Snowflake enables elaborate application development, collaboration, and more across a wide spectrum of data sets, all in the cloud. This provides massive horizontal scalability along with other capabilities that meet a wide variety of application, reporting, and other data consumption needs.Building Applications with Snowflake and HasuraHasura customers across many industries, such as construction, energy, financial services, healthcare, and human resources, use Snowflake to house critical data needed to derive insights and make data-driven business decisions. Many of these customers also use or want to use Snowflake data to directly power analytics in internal and external data applications. Or they want to open their Snowflake data \u2013 in a safe and controlled way \u2013 to internal or external stakeholders to make data-driven decisions. However, this journey has some challenges. The resource-intensive nature of building an API, especially if they intend to build multiple apps and services that use data from Snowflake.Customers have to maintain and manage secure connections to Snowflake so only the authorized individuals and applications have access to this business critical data.To address these challenges, our new integration with Snowflake helps by:\nProviding an instant GraphQL API on Snowflake to access data securely.\nProviding a unified GraphQL endpoint across Snowflake and other data sources such as PostgreSQL, SQL Server, BigQuery, Amazon Athena and CockroachDB. GraphQL is an extremely flexible query language for APIs and a runtime for fulfilling those queries with your existing data.\nBringing Hasura's out-of-the-box capabilities to Snowflake, such as:\na. Query response caching allows you to cache responses in Hasura for frequently executed queries as opposed to retrieving the data from Snowflake every time. This greatly improves application performance and consequently your end-user experience.\nb. Declarative authentication and authorization\nc. API Security\nd. Integration with your custom REST API application workflows via Actions and Event Triggers\ne. Observability to ensure the reliability of your application stack\n\nThe new data connector for Snowflake introduces new ways to query, correlate and aggregate data, enabling Snowflake customers to do more with their data and increase the ROI on their Snowflake investments across the board. What Can I Build with Hasura and Snowflake?We\u2019ve seen some really creative use cases that are only possible by integrating Hasura with our new Snowflake database connector. Below are a few examples to explore with your team.Surface Analytics Data in Customer-facing and Internal AppsEmbedding analytics directly into applications is not only creating great user experiences, but it\u2019s also helping teams build a competitive advantage. Quick and efficient data access is essential for building applications like an internal customer service dashboard that surfaces real-time analytics and helps deliver world-class customer support. With Hasura GraphQL, development teams can quickly build a unified API for Snowflake that efficiently serves up data from multiple tables, all within a single query, instead of making multiple fetches to individual tables. This saves developers countless hours of writing additional APIs and queries required to surface data from Snowflake.Share Snowflake Data with Internal and External Consumers via APIs In an effort to be more data-driven, organizations want to open their Snowflake data to more internal and external stakeholders, giving them the data they need to make better decisions. But given the critical and sensitive nature of the data often stored in Snowflake, organizations are often hesitant to do this through typical built-in connectors (eg. JDBC) and prefer to share just the data they want via APIs. API services allow them to provide data access in a secure, controlled, and limited way. With the Hasura connector for Snowflake, customers can now generate those APIs in a fraction of the time it would have otherwise taken to build. With the built-in Hasura authorization engine, customers are also able to easily implement access privileges into the API layer.Combining Data from Snowflake with Other Sources for a Unified APIWhile Snowflake is often used as a central data warehouse, large organizations can still have critical data stored in other locations for a variety of reasons. Architecture or security designs, legacy databases that have yet to be migrated, or siloed business units can all lead to data being stored in locations outside of Snowflake. This poses a challenge when building applications that require data across those multiple sources. Hasura GraphQL creates an easy and efficient way to combine Snowflake data sources with additional data sources. For example, maybe a company needs to map order data and shipment data during order fulfillment. Combining these data source silos drives better product insights for things like returns, quality issues, and buyer profiling. all through a singular unified API.What\u2019s Included in the Snowflake Connector Beta?The Hasura connector for Snowflake currently provides query support only, along with a host of other standard Hasura features called out above. For example, connecting two tables is a standard Hasura feature that really stands out when working with denormalized reporting data that is often used in Snowflake. For a full list of capabilities, visit the documentation. How Can I Get Started?To get started, you will needAn existing Snowflake account instance and data to connect Hasura to query against.A Hasura Cloud or Hasura Enterprise Edition self-hosted setup - version 2.16.0 or later.While the Snowflake connector is in beta, it is behind a feature flag. Please follow the steps outlined in the documentation to learn how to enable it.Navigate to the Data tab as shown and click on Connect Database. The following dialog requires four steps:Turn on the experimental features.Select Snowflake (beta) from the dropdown. This will then shape the rest of the connection form elements for the appropriate data needed for the Snowflake connection.Set the name you\u2019d like the connection to be listed as.Add the JDBC connection string to your Snowflake instance. The JDBC connection string will need the URI path with credentials, warehouse, database, role, and schema added to the string. It would look something like this.jdbc:snowflake://[URI instance to connect to]?user=[theUser]&password=[superSecretPassword]&warehouse=[THE_WAREHOUSE]&db=[the_db]&role=[role_like_PUBLIC]&schema=[schema_to_use]Once those steps are done, click on Connect Database.From there, you can select tables and query against those tables just like you would in any Hasura GraphQL API connected database. Let\u2019s say, for example, you want to query a band along with the band\u2019s albums. That query would look something like this, with the inferred nested object of albums returning as an array.query artistAlbums {\n  CHINOOK_PUBLIC_ARTIST(where: {NAME: {_eq: \"Metallica\"}}) {\n    NAME\n    artistAlbums {\n      TITLE\n    }\n  }\n}The result would then come back with the array of album titles for the particular artist Metallica.{\n  \"data\": {\n    \"CHINOOK_PUBLIC_ARTIST\": [\n      {\n        \"NAME\": \"Metallica\",\n        \"artistAlbums\": [\n          {\n            \"TITLE\": \"Garage Inc. (Disc 1)\"\n          },\n          {\n            \"TITLE\": \"Black Album\"\n          },\n          {\n            \"TITLE\": \"Garage Inc. (Disc 2)\"\n          },\n          {\n            \"TITLE\": \"Kill 'Em All\"\n          },\n          {\n            \"TITLE\": \"Load\"\n          },\n          {\n            \"TITLE\": \"Master Of Puppets\"\n          },\n          {\n            \"TITLE\": \"ReLoad\"\n          },\n          {\n            \"TITLE\": \"Ride The Lightning\"\n          },\n          {\n            \"TITLE\": \"St. Anger\"\n          },\n          {\n            \"TITLE\": \"...And Justice For All\"\n          }\n        ]\n      }\n    ]\n  }\n}Correlative data across disparate tables within reporting tables are difficult requests to process. With Hasura connected to Snowflake, users can easily add relationships between tables to bring data together into a single GraphQL Query. A tactical example can be seen with the relationship shown in the console here. In this example, we have a music library database where we are trying to define a relationship between the Artist and Albums table.Hasura connects the Artist table to the Album table so that a relationship can traverse either direction when querying.The following query shows how a nested query works based on that relationship, adding power to queries when working with Snowflake. Users now have multiple queries issued from a singular GraphQL query. Thereby reducing your query complexity, keeping the number of client calls minimal, and improving the readability of your code base.Adding caching is extremely easy. For example, using the query listed above, we can add the directive as shown.query artistAlbums @cached {\n  CHINOOK_PUBLIC_ARTIST(where: {NAME: {_eq: \"Metallica\"}}) {\n    NAME\n    artistAlbums {\n      TITLE\n    }\n  }\n}Join our webinar on Jan 11We are hosting a live webinar on this topic on Jan 11, 2023, where we will do a technical deep dive on the connector, cover use cases, and more with a live demo. It\u2019s a great opportunity to connect live with the technical team behind this capability. Register here. Want to see a custom demo sooner? Get in touch with our team by signing up here. Put in \u201cSnowflake demo\u201d in the notes, along with more context of your use case for Hasura+Snowflake.What\u2019s Coming NextA reminder, this is the beta release, and we\u2019re moving quickly to add new functionality for Snowflake as well as support for additional connectors. In the coming months, we\u2019ll be building features like GraphQL mutation support for our API, and Snowflake will be one of the first connectors to receive that update. If you\u2019d like to discuss Snowflake, request new features, or just ask questions about Hasura and Snowflake, join our Discord."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a Digital Twin with MQTT and Hasura Streaming Subscriptions Description: In this blog we will be talking about how to build a digital twin of a toy car with an android phone (to leverage sensors),  MQTT broker and Hasura GraphQL Engine.  Author: Sooraj Sanker"}, {"role": "assistant", "content": "In this blog we will be talking about how to build a digital twin of a toy car with an android phone (to leverage sensors), \u00a0MQTT broker and Hasura GraphQL Engine. We will go over,Building a digital twin of a toy car attached with a gyroscope sensor through an android deviceSetting up the device to stream real time sensor data to an MQTT Broker.Receiving data from the MQTT broker and sending it as a GraphQL mutation to Hasura GraphQL engine.Consume live data on the ReactJS application with Hasura GraphQL subscriptionsRendering the real time gyroscope sensor data as a 3D car model using ThreeJSWhat is a Digital Twin?A digital twin is a virtual representation of a real-world physical system or product (a physical twin) that serves as the digital counterpart of it for practical purposes, such as system simulation, integration, testing, monitoring, and maintenance.Digital Twins are created in a way that it can receive sensor data in real time from the physical counterpart. This allows the twin to simulate the physical object in real time to learn about the characteristics of the physical twin. There are systems that also have controllers to remotely manage and maintain the system in real time. It is also used to remotely debug large machinery, with the help of mixed reality gears- this ease out the need of skilled technicians to travel across the globe and save time and cost.Digital Twins are mainly used inManufacturingAutomobilesPower GenerationHealthcareIndustry 4.0Town planningRemote DebuggingBuilding a Digital TwinThe basic requirements to build a digital twin arePhysical object (a toy car in this case)Sensors to collect data (will be using an android device) & ControllersConnectivity to the central system (Internet)Server that manages the data flow and control events (Hasura GraphQL Engine & MQTT Broker)Virtual Representation (a static web application, using ThreeJS to render 3D in this case)Note : The repository is available here and the commands mentioned in the steps are configured in the repository.Setup a Physical ThingAs shown above, we can have a toy car and an android phone attached to it.The android device attached to the device will be able to collect real time sensor data (Gyroscope in this case) and send it to the cloud.Streaming data to CloudConsidering most of the real world IoT systems use low footprint transfer protocols to send data from device to server communication, we will be using MQTT as the preferred protocol here. We can also use GraphQL over HTTP from this point itself, but considering the current systems and integrations this demo will talk about consuming and ingesting data from end devices to MQTT Broker and then to a Postgres DB through a Hasura instance.To simplify the end device implementation, we will be using the android application \u00a0MQTT Simulator to collect and send rotational data to an MQTT broker.Receiving DataMQTT follows a topic based pub-sub pattern, and it requires a broker to handle this. For the demo purpose, let\u2019s use HiveMQ public MQTT broker so that we don\u2019t need any setup here.If you use the above application, the data will be streaming to digital_twin/android/<provided_device_id> by default and this can be listened easily by subscribing to the same topic or digital_twin/android/#. # here will make sure any topic name that starts with the pattern will be subscribed.At this point, we have successfully received the data from the device to the internet/network, we now need a way to store this data and stream the data to the clients who require live data from the device.Now we need a place to store and access the data in a secure way, to do this we will be using Hasura GraphQL Engine backed with a postgres DB.Setting up Hasura and Postgres DBDeploy to HasuraOpen Hasura Console, head to Data tab and connect a Postgres DB if not done already.Head to Data->SQL tab to create a table.CREATE TABLE \"public\".\"device_data\"(\"id\" serial NOT NULL, \"data\" jsonb NOT NULL, \"timestamp\" timestamptz NOT NULL, \"device_id\" text NOT NULL, PRIMARY KEY (\"id\") );Track the table device_data so that it can be accessed through GraphQL APIs.Consuming MQTT data and sending to HasuraWe need a NodeJS runtime to consume data from the MQTT runtime and send it to Hasura. I have a small NodeJS script that does the following,Subscribe to the MQTT broker with the same channel name used by the Android MQTT SimulatorParse the MQTT payload and transform the dataclient.on(\"message\", function (topic, message) {\n // message is Buffer, convert this to generate GraphQL mutation from the message\n const decoded = getMutation(message.toString(), topic);\n sendToHasura(decoded, CONSTANTS.HASURA_HOST);\n});GraphQL mutations (as shown below) on Hasura GraphQL Engine to store it on the DB.mutation AddDeviceData($data: jsonb!, $device_id: String!, $timestamp: timestamptz!) {\n insert_device_data_one(object: {data: $data, device_id: $device_id, timestamp: $timestamp}) {\n   id\n }\n}\nAt this point we successfully managed to get the data to the postgres instance and the data flow will looks like the followingTo start this service,clone the project from https://github.com/soorajshankar/digital-twin-graphqlChange .env file with your project configurationRun the command `yarn connect` \u00a0or `npm run connect` This command will connect to MQTT broker and start subscribing to the channel mentioned in the env file. You could also use the public MQTT broker (HiveMQ) used in the repository for testing purpose.Streaming Live Data to the DashboardFrom this part we will be discussing the ReactJS application and the main functionalities.When you connect a postgres table on Hasura, you automatically get a GraphQL Subscription API to listen to the live changes. We will be using the latest streaming subscription feature from Hasura to stream the live data.In this case, the query looks like the following:subscription GetDeviceData($timestamp: timestamptz) {\n   device_data_stream(\n     batch_size: 1\n     cursor: { initial_value: { timestamp: $timestamp }, ordering: ASC }\n     where: { device_id: { _eq: \"android\" } }\n   ) {\n     data\n     device_id\n     id\n     timestamp\n   }\n }\nHere is how the streaming subscription work in general:\nVisualization Visualizing data is very important in the connected world as it can save a lot of time to understand, make decisions and act on it. The concept of Digital Twin plays a great role in simplifying this task to an extent. Both human and simulated environments can process the data much faster when it is virtualized in a digital environment. To do this we will be using ThreeJS model to visualize the toy car, the car model orientation will refresh frequently with the latest data stream.Demo\n\n  Your browser does not support the video tag.\nRepo: https://github.com/soorajshankar/digital-twin-graphqlSummaryEven though we built a digital twin of a toy car, this can be made useful for variety of use cases likeConnected carsRemote Monitoring : Windmills, Smart Factories, Industry 4.0 and other Industrial IoT systemsRemote Debugging with AR goggles and connected data. Also if you have an existing data stream/ queue, it can easily be connected to Hasura GraphQL to set up a secure and easy way to store and access realtime data until the end consumer."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Improving Hasura's Internal PR Review process Description: During 2022, we realized our Pull Requests stayed open, on average, for seven days. An internal Working Group identified the root causes and the solutions. Author: Stefano Magni"}, {"role": "assistant", "content": "During 2022, thanks to Haystack, we realized our Pull Requests stayed open, on average, for seven days.The Haystack reportAs you can imagine, this is troublesome because:It slows down the Software Development Life Cycle, preventing us from merging PRs and releasing new features for our users.It requires a long back and forth between the PR author and the reviewers, sometimes resulting in the need to reload the context in people\u2019s minds again and again.The PR\u2019s code becomes outdated, often requiring repeated merges/rebases and resolving conflicts.We all see the problem, but what causes it? Inspired by GitLab\u2019s model, we created a dedicated internal Working Group (inspired by GitLab\u2019s Working Groups) to dig into the human aspects of this problem.How the Working Group proceededDespite this being only the second internal Working Group, we already have a standard way to proceed:State the problem and explain why you think it\u2019s importantLook for a sponsor among all the engineering managersDefine the exit criteriaCreate a dedicated Slack channelGet people involvedWhen speaking about the specific \u201cPR Review Working Group\u201d, we decided to:Schedule some early meetings: people interact the most during sync meetings compared to async documentsElaborate on the problem and the possible solutionsRequest feedbackInterview some internal engineers: we opted for interviews instead of sending out a surveyFind some best practicesPresent the best practices internallySo, Rob Dominguez, Daniel Chambers, and I (Stefano Magni) started working on the Exit Criteria, drafted the first notes and the first opinionated solutions, asked for feedback, etc.The exit criteriaDefining the exit criteria of the Working Group is crucial to work toward a goal. We determined the following indicators:Identifying some best practices to follow to get PRs merged as soon as possible.Identifying the best practices in newly-opened PRs.Significantly shortening the average lifecycle of a PR.Improving the relationship between author and reviewer on both ends as measured by a monthly survey.Over the next several months, we\u2019ll monitor the progress and adoption of points 2 through 4. For the remainder of this article, we\u2019ll focus on how we identified these best practices.The root causes the Working Group found outWe all know that a slow PR review process is terrible, but what are the root causes for this slowness? What happens between a developer creating the PR and getting the PR merged? Well, let\u2019s list them in order of when they occur during the end-to-end PR creation and review process:Missing or not enough detailed PR description: Reviewing a PR is mostly about understanding the author\u2019s context and validating their changes against this context. When a description is missing or too short, it deprives the reviewers of context, making it more difficult to understand and adding a lot of drag to the whole process.Big PRs: PR size directly impacts the review time. Working for a full sprint and then opening a single PR guarantees the PR will not reviewed promptly.Code Owners: GitHub Code Owners help to define the specific people or groups that own part of the codebase. Until recently, PR reviews from code owners were mandatory, which prevents merging a PR quickly.Lack of ownership: When GitHub assigns reviews to a code owners group instead of an individual, nobody feels the review is their responsibility because they believe someone else will pick it up. An author won\u2019t waste time waiting for someone to review; they\u2019ll naturally move on to their next task, which, as we\u2019ll see later, causes them to lose their context on their work.Notifications missing: The modern developer has so many services competing for their attention, and, in the deluge of notifications, it can be difficult to parse what is essential and what is simply noise. We determined that important messages notifying us of pending reviews or feedback on PRs went unnoticed too often.PR review time not explicitly allocated: Reviewing PRs takes time, primarily because of the complexity of Hasura\u2019s system. When planning the sprints, estimating features, etc., we must never forget to dedicate part of the day to reviewing each other people\u2019s PRs.Timezone overlaps: As a fully-distributed, remote-only company, Hasura\u2019s engineers work worldwide. Short overlap adds some delays when you need to interact with people on the opposite side of the world.CI issues: CI is a gift and a curse, and it can make our review process more streamlined and give reviewers more confidence when used correctly. However, automated steps exceeded the scope of our WG; another group recently began work on this task in conjunction with our Infrastructure team to improve the developer experience.The best practicesOne by one, let\u2019s check the best practices we identified for every mentioned problem.PR description missing or insufficiently detailedPR authors must try to walk in the reviewer\u2019s shoes. And the reviewers need to know a lot of things likeWhat has changed?Why did you choose this solution? More importantly, why did you not select other solutions?What is the dependency graph of the changes? Where should I start reviewing the changes?How should I test/verify?In one word: context! The PR author must describe the PR context in detail, adding images, videos, Code Tours, diagrams, pseudo-code; whatever helps reviewers understand the PR changes.It\u2019s important to understand that this activity could take one hour or more, but every minute dedicated to describing the PR saves every reviewer two minutes. Also, the description acts as documentation for posterity and could help the PR authors when they have to return to the PR after some time.Thanks to a detailed description, reviewers can dedicate their time by checking that the developers\u2019 code reflects their intentions instead of guessing what was in the developers\u2019 minds when they wrote the code.Tests are also an essential part of the PR; they should be present and reflect the changes introduced by the PR. Tests serve as a quick summary answering the question, \u201cWhat should the code do?\u201dBig PRsAuthors need to create reasonably-sized PRs. All projects - or even subsets of codebases - are different, but the size of a PR exponentially impacts the time required to review them. The best way to create easily-parsed PRs is\u2026 writing a detailed description! Why? Because if you are describing your PR in detail, and you realize that you are spending hours describing it, chances are the PR is too big.Code OwnersBefore August 2022, we organized Hasura into stack-oriented teams (the Server team, the Console team, etc.), and code-owners groups reflected this internal structure. Since this created too many cross-team dependencies (resulting in slowing down E2E features implementation), we reorganized into feature-focused teams that include all the necessary people to implement and release features independently.As a result, if GitHub automatically assigns your PR to the Console code-owners group, there\u2019s a high probability that you have at least a couple of Console developers on your team! And since they participate in your standup and align with the team goals, they are the best candidates to review your Console-related PR.The suggestion is always to use Code Owners as a hint to understand what knowledge people should have to review your PR, then find the people with that knowledge in your team and assign the PR to them.In the future, code owners at Hasura will reflect the new teams\u2019 structure, but this is only partially possible at the time of writing.Lack of ownershipLack of ownership happens in two different moments:When no one reviews a PRWhen the PR authors forget about their PRWe can automatically resolve the former case with what we shared in the previous point via assigning a specific person to review a PR. In contrast, the latter happens when the PR authors become tired of waiting for a review and start working on new tasks.Whenever possible - aka, when getting the PR merged is a matter of hours - the PR authors should not start working on new tasks, but they should opt for getting the existing PR merged. Working on new tasks means forgetting and de-prioritizing the current PR, while striving to get it merged to ensure it will bring to completion.What happens when a review takes too long? Simply put: authors get stressed. However, this response is critical to get them to become self-advocates and draw attention to the issue:Raise their hands.Send messages.Ask for help.Report the problem to the managers and in the retros.Escalate if the problem persists.We know this process could be better, but it also works towards resolving the issue. Working on new tasks, instead, buries the problem under the rug.Notifications missingWe solved this problem by:Firstly, enabling GitHub real-time alerts since it\u2019s the best way to receive only the important notifications in SlackGitHub real-time alerts for SlackSecondly, we enabled Haystack daily reports in Slack, so that teams never forget their waiting, stale or large PRs.GitHub real-time alerts for SlackPR review time not explicitly allocatedWe suggested that:Individuals should enable GitHub real-time alerts, since it is the best way to receive only important notifications in Slack.Teams should prioritize reviewing PRs (who cares if a team creates a lot of PRs without getting them merged?)Teams should raise the problem of lack of review time during retrosTeams should set correct expectations in terms of workload the team can afford, also considering the code reviewsTeam managers always remember that soft code reviews often result in bugs hitting our users in productionTimezone overlapsWe reduced this problem almost to zero naturally by assigning PR reviews to peers from the same team instead of code owners. We built feature teams with timezone compatibility in mind.What happens if we do not follow the above best practices?Simply put:We start working on many features, but we release only a few.We release features at a slower pace.We end up with a backlog of stale PRs that haven\u2019t seen activity in months and wither away, neglected on GitHub\u2019s servers.The presentationInstead of opting for the standard slide deck, we made a more engaging presentation focused on a story: a Hasura developer opening a PR and facing all the PR review-related problems. We wanted attendees to garner empathy - a company core value - for devs on either side of this process.To make the story more engaging, we relied heavily on gifs and exaggerated - only slightly! - some of the impediments we all face in this industry.We created some vertical slides to have the slides side-by-side with the presenter (using a virtual camera and OBS, you can find all the instructions in this tweet about how to do the same).Here is a picture of the presentation layout:A picture taken before the presentationWhat\u2019s next?Once we identified the best practices, we immediately put the \u201cStarting to review PRs\u201d topic in our onboarding docs to ensure new hires understand the importance of these best practices within their first few weeks.Over the next few months, we\u2019ll continue analyzing metrics via Haystack to understand if the average number of days a PR stays open decreases. If not, we will try to understand the developer\u2019s difficulties applying the best practices mentioned.Special thanksI (Stefano Magni) volunteered to create and drive the Working Group, but I want to thank Rob Dominguez for raising the quality of our work by doing a lot of under-the-hood things. His presence and energy always motivated me too!Thanks a lot also to Daniel Chambers. Daniel got involved in all the discussions, provided a lot of feedback, and volunteered to interview some of our developers. Thanks, Daniel!Finally, thanks to all the people we interviewed and those who participated in every async and sync discussion \u2764\ufe0f.External resourcesStefano Magni\u2019s Support the Reviewers with detailed Pull Request descriptionsStefano Magni\u2019s Re-building a branch and telling a story to ease the Code ReviewStefano Magni\u2019s A Case History: Analysing Hasura Console\u2019s code review processThierry de Pauw\u2019s The Practices That Make Continuous Integration - Team Working for Continuous IntegrationThierry de Pauw\u2019s The Practices That Make Continuous Integration - Coding for Continuous IntegrationThierry de Pauw\u2019s The Practices That Make Continuous Integration - Building for Continuous IntegrationGoogle\u2019s Code Review Developer GuideConventional Comments"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Row Level Security with Postgres (via Hasura AuthZ) Description: This blog covers the basics of Postgres row-level security and the Hasura way of solving these permission rules. Author: Arjun Yelamanchili"}, {"role": "assistant", "content": "Security is one of the most critical topics when managing a database. Thankfully, in 2016 developers got help with this challenging task when Postgres 9.5 was released with Row-level Security (RLS). RLS allows application developers to control access to rows in a database table based on rules evaluated during query execution.\nThis blog will cover the basics of Postgres RLS and the Hasura way of solving these permission rules.\nPostgres Row-level Security Basics\nTo enable RLS on a table, we use the ALTER TABLE ... ENABLE ROW LEVEL SECURITY command. Once run, only the table owner or admin can CRUD rows on the table. To give permissions to other users or roles, we create policies.\nPolicies\nOnce RLS is enabled, we use policies to create permission rules on the table. A policy is a per-table SQL expression that returns a boolean representing if the permission is satisfied.\nThe two types of policy expressions are USING, which checks existing rows, and WITH CHECK, which covers new rows. You can combine policies with OR or AND clauses.\nAn example from the Postgres documentation that allows all users to view all rows in the users table but only modify their own:\nCREATE POLICY user_sel_policy ON users\n    FOR SELECT\n    USING (true);\n\nCREATE POLICY user_mod_policy ON users\n    USING (user_name = current_user);\n\nPassing data to the policy\nIn the above example, the users table user_name column is compared to the current_user value. The current_user is the currently active Postgres role. Roles can be thought of as either a database user or a group of database users, depending on how the role is set up. Your role is set initially by how you connect to Postgres and can be changed later using the SET ROLE command.\nHowever, we usually handle roles at the application layer instead of the database layer. For example, usually, the front end has its own login system. They don\u2019t create a Postgres role on every signup. Instead, we can set session variables in Postgres and read them in our policies via current_setting(). Check out this excellent article by Tomas Vondra for an introduction to this topic. Here is an excerpted example:\nSET my.username = 'tomas';\n\nCREATE POLICY chat_policy ON chat\n    USING (current_setting('my.username') IN (message_from, message_to))\n    WITH CHECK (message_from = current_setting('my.username'));\n\nPostgres row-level security is a deep topic with a lot of nuances. Please make sure to read the official documentation before implementing.\nHasura RLS Implementation\n\nInstead of using Postgres's RLS, Hasura has its own authorization system (AuthZ) for several reasons:\n\n\nHasura federates data from multiple sources such as different databases like BigQuery and external APIs like Remote Schemas. Now you can use the same AuthZ solution everywhere.\n\n\nWith Postgres RLS, every user query would execute the policies separately, which can affect performance. With Hasura\u2019s implementation, similar queries are batched, allowing for million+ user performance.\n\n\nAuthentication (AuthN)\nBefore setting up the AuthZ rules, Hasura\u2019s authorization system must get a user\u2019s role and session variables. These can come from a JWT or a webhook. Session variables are prefixed with X-HASURA-. You can also configure what role an unauthenticated user would use.\nAuthorization (AuthZ)\nFor Authorization or Access Control, Hasura helps you define granular role-based access control rules for every field in your GraphQL schema (granular enough to control access to any row or column in your database).\nHasura uses the role/user information from the AuthN layer and the request itself to validate the operation against your rules. If the request is allowed, it generates a SQL query, which includes the row/column-level constraints from the access control rules. It then sends it to the database to perform the required operation (fetch the rows needed for queries, insert/edit rows for mutations, etc.).\nEarlier, we looked at an example of Postgres RLS restricting users to only seeing their rows. Here\u2019s how that looks in Hasura:\n\nThis permission rule reads: \u201cFor the role user, table authors, and operation select/query, allow access to those rows where the value in the id column is the same as the value in the X-Hasura-User-ID session variable\u201d.\nLike Postgres RLS, the row-level permissions are boolean expressions that help you restrict access to rows depending on the operation. You can create powerful rules based on session variables, the types of columns, static values, and relationships. For more details, please see the Hasura row-level security documentation.\nAuthZ in the application layer gives us benefits like using the permissions system for other databases and APIs. You may still need to add permissions to your database if other applications have access.\nSummary\nThis article covered Postgres row-level security basics and how Hasura approaches the same problem. These basics should help you get started building secure applications that users love. Try out what you learned on a free Hasura Cloud instance!\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: November 2022 Newsletter Description: \n\nWelcome to the November edition of the Hasura Newsletter!\n\nThis month we introduced integrations for 2 new databases, CockroachDB and\nAmazon Athena. Both integrations are available as beta features in Hasura v2.15.\nYou can try them out on Hasura Cloud [https://cloud.hasura.io/signup] or give\nthem a spin on self-managed Hasura instances. \n\nAlso, support for Snowflake is coming in a few weeks. To be notified about the\nlaunch, sign up here. [https://hasura.io/graphql/database/snowflake]\n\n\n\n\n----- Author: Bela Gandhi"}, {"role": "assistant", "content": "Welcome to the November edition of the Hasura Newsletter!This month we introduced integrations for 2 new databases, CockroachDB and Amazon Athena. Both integrations are available as beta features in Hasura v2.15. You can try them out on Hasura Cloud or give them a spin on self-managed Hasura instances. Also, support for Snowflake is coming in a few weeks. To be notified about the launch, sign up here.HighlightsEnterprise GraphQL Conference was this month, where LinkedIn, Optum, and Goldman Sachs gave great talks on how they are tackling enterprise data access and management challenges using GraphQL. Request the sessions on-demand.We launched a new video tutorial series on how to build a full-stack food ordering app with Hasura, Next.js, and Vercel in 20 bite-sized videos! Check it out here.Hasura is heading to AWS re:Invent in Las Vegas. See all the ways you can connect with us in Las Vegas here!Product UpdatesNative database integration for CockroachDB: We are pleased to announce the beta release for CockroachDB datasource on the Hasura GraphQL Engine v2.15. In this beta release, Hasura supports Queries and Mutations with plans to add support for subscriptions before the GA release, which will coincide with the release of CockroachDB v22.2 (tentatively scheduled for early December). Together with Hasura, CockroachDB now supports: Instant GraphQL & REST APIs, Declarative Role Based Authorization, Advanced Security and Performance with rate limiting, allow lists, and caching. Follow these instructions to connect your CockroachDB to HasuraHasura GraphQL Data Connector for Amazon Athena: We are excited to release the beta integration for Amazon Athena to Hasura v2.15. This release provides read-only query support for Enterprise and Cloud Enterprise users. For a limited time while in beta, it is also available to standard and free-tier cloud users! Be sure to check it out and let us know what you\u2019d like to see for further Athena or other databases supported.Two new regions on GCP: Hasura Cloud now supports two new regions, europe-west1 in Belgium and us-east4 in Ashburn, US on GCP.Enhanced API Security with Batch Limits (Enterprise Edition Only): We have introduced a new batch limit in the API Security section to set the maximum number of operations that can be sent in a batch request. For more information, see the docs here.  Action Response Transforms on Console: Response transformation for Actions has been available in the API. Now you can configure response transforms on the console. See the docs for more details.GraphQL Type Generator in Actions: While creating a new Action, now you can use a type generator on the Console to easily create GraphQL types from JSON sample requests. See the docs for more details.Query Parameters are now optional for Actions: We addressed a user problem around defining optional query parameters for rest endpoints. Now, actions and event trigger REST connector query parameters are optional. For more information please see docs here.ODBC Driver 18 for SQL Server: The ODBC Driver 18 for SQL Server is now installed. This allows users to provide ODBC Driver 18 connection strings. Because this version has native support for arm64/aarch64, Hasura GraphQL Engine can now connect to an MS SQL Server database when running on macOS with an aarch64 chip (M1 or M2).Updates to JSON payload transformation templates: Transforming payloads for Actions and Event Triggers in Hasura is achieved using the in-house developed templating language - kriti-lang. In this release, we have updated it to version 0.3.3 with the following changes: 1) Adds elif syntax to if expressions 2) Improved error messaging and error codes, 3) Allow arbitrary expressions as range iterators, 4) Kriti.CustomFunctions.basicFuncMap functions are added to the kriti executable, and 5) KritiError type is added to exports from Kriti.Events[Virtual] Enterprise GraphQL Conference - Sessions Available On-DemandDid you miss the conference last week? Find out how some of the largest enterprises use GraphQL to solve data access challenges. Request the recordings and we\u2019ll send you a link to them once they\u2019re uploaded.[Virtual] Weekly Demo Series| Every Tuesday @ 10 AM PT or 10 AM CETLooking to get your Hasura questions answered? Join our weekly demo every Tuesday, alternating at 10 AM PT and 10 AM CET to answer any questions about getting started. We\u2019ll cover deployment options, tracking tables and relationships, enabling access, and trying out APIs. This week\u2019s demo is today, 11/15, at 10 AM PT. Register here to join us.AWS re:Invent 2022 | Las Vegas | Nov. 28th - Dec. 2ndAre you heading to Las Vegas for AWS re:Invent? Here\u2019s everything Hasura has happening https://hasura.io/events/reinvent/. Be sure to stop by booth #2251 and say hi!Hasura User Group Meetup | Hyderabad | Dec. 2nd @ 6:30PM ISTIf you are in or around Hyderabad join us for their first meetup on December 2nd. [Community Call] Hasura\u2019s Final Call of 2022 | Dec. 15th @ 9AM PTHasura is wrapping up 2022 in style. At our final Community Call we\u2019ll recap on some exciting features we launched in 2022. Plus some exclusive swag from our recently launched swag store! Register here.New ContentBlogsDiving into the key insights and latest GraphQL trends identified in the first State of GraphQL 2022 Survey.Top psql commands and flags you need to knowTop features and changes to know about PostgreSQL 15 releaseLooking for free solutions to host your Postgres database? Here are the best Postgres providers you can use for free[Community] GraphQL vs. gRPC: How do gRPC work and its benefits[Community] https://www.estuary.dev/how-to-connect-kafka-to-hasura-graphql/[Community] Strategies to Build Your Backend with Hasura and PostgreSQL[Community] (Japanese) Hasura makes it easy for frontend developers to get started with GraphQL[Community] (Japanese) Separate query namespace from remote schema[Community An Enterprise 3-Factor Tech Stack on Autopilot. Deploying Hasura with a Flutter Frontend on GCP.VideosHow to create a full-stack food ordering app with Hasura, Next.js, and Vercel in 20 bite-sized videos![DAPI] Upserting with TimeScale & streaming from Hyper Tables with Hasura[DAPI] Introducing the Neon Postgres integration in Hasura[Community] Build a Serverless full stack app with Nhost using Hasura, GraphQL, Vanilla JavaScript and ViteCommunity UpdatesCommunity Member of the MonthHiroaki Karasawa is our community member of the month. Hiroaki is the lead organizer of the Tokyo Hasura User Group and coordinated the very first meetup a few months ago. If you are interested in speaking at one of the future Tokyo HUG meetups or want to attend them, please reach out to him on Twitter. Hiroaki is an engineering Lead at Dinii inc. He started using Hasura in production in April 2020 and currently focuses on developing multiple client applications with a single Hasura backend. He is especially interested in combing through the GraphQL subscriptions that Hasura generates to optimize them.Monthly RecommendationThis month's recommendation comes from Raju Sunny, a Technical Account Manager / Solutions Architect at Hasura. He recommends Notion as his go-to tool for productivity because it is a combination of a markdown editor, excel sheets, Kanban boards and a lot more, all integrated with each other. At Hasura, the customer success team uses it extensively for note-taking, task tracking, knowledge management, and data management.We\u2019re hiring for some exciting roles at Hasura! If you or someone else is interested please send us your resume here. Few of the key roles we are hiring for: Cloud Infrastructure Engineer - Hasura CloudSenior Data Engineer - Product AnalyticsSenior Haskell EngineerTake a look at the rest of our listings here.That\u2019s a wrap on November\u2019s newsletter. If you're interested in receiving the monthly updates in your inbox, subscribe to our newsletter."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Next.js 13 Nested Layouts, Streaming SSR with Realtime GraphQL Description: In this post, we will demonstrate all the new Next.js 13 features by building a mini e-commerce store with Hasura GraphQL Author: Arjun Yelamanchili"}, {"role": "assistant", "content": "Next.js, the open-source React framework by Vercel, has released version 13, laying the foundations to be dynamic without limits. Please read their release blog post for an excellent overview.\nIn this post, we will build a mini e-commerce store where you can view the products on sale, order one, and see a real-time feed of orders. We are covering new Next.js features such as:\n\nNested Layouts\nData Fetching\nStreaming and Suspense\nClient and Server Components\n\nWe also see how to integrate with Hasura GraphQL for realtime features such as Streaming Subscriptions.\nLet\u2019s dive in with the e-commerce schema from the Hasura Super App. See our Data Hub for instructions on how to import the schema into Hasura.\nThe source code is available on GitHub.\nNext.js Setup\nCreate a Next.js TypeScript application. As of this blog post, we will use the experimental version because that allows us to use beta features.\nnpx create-next-app@latest --experimental-app\n\nInstall the dependencies\nnpm install @graphql-codegen/cli @graphql-codegen/client-preset @graphql-typed-document-node/core dotenv graphql graphql-request graphql-ws\n\nReplace the contents of next.config.js\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  experimental: {\n    appDir: true,\n    runtime: \"experimental-edge\",\n  },\n  images: {\n    remotePatterns: [\n      {\n        protocol: \"http\",\n        hostname: \"img6a.flixcart.com\",\n      },\n      {\n        protocol: \"http\",\n        hostname: \"img5a.flixcart.com\",\n      },\n    ],\n  },\n};\n\nmodule.exports = nextConfig;\n\nThe experimental-edge runtime lets us use Vercel's edge runtime. The term \u201cEdge\u201d refers to the orientation toward instant serverless compute environments and not a specific set of locations. The image section configures Next.js's image component to use pictures from our e-commerce store.\nOur last step is adding two entries to .env.local\nNEXT_PUBLIC_HASURA_GRAPHQL_URL=<Hasura GraphQL endpoint>\nNEXT_PUBLIC_HASURA_GRAPHQL_WS_URL=<Hasura websocket endpoint>\n\nIf your Hasura instance has an admin secret, you can also set HASURA_GRAPHQL_ADMIN_SECRET for use in the code generator.\nGraphQL Code Generator Setup\nWhen making TypeScript applications with GraphQL we highly recommend using GraphQL Code Generator. Combined with the graphql-request library, we have fully typed queries and mutations by the magic of code generation!\nCreate a file codegen.ts\nimport type { CodegenConfig } from \"@graphql-codegen/cli\";\n\nconst config: CodegenConfig = {\n  overwrite: true,\n  schema: [\n    {\n      [process.env.NEXT_PUBLIC_HASURA_GRAPHQL_URL!]: {\n        headers: {\n          // If you have an admin secret set\n          \"x-hasura-admin-secret\": process.env.HASURA_GRAPHQL_ADMIN_SECRET!,\n        },\n      },\n    },\n  ],\n  config: {\n    skipTypename: true,\n    enumsAsTypes: true,\n    scalars: {\n      numeric: \"number\",\n    },\n  },\n  documents: \"lib/service/queries.graphql\",\n  generates: {\n    \"lib/gql/\": {\n      preset: \"client\",\n      config: {},\n      plugins: [],\n    },\n  },\n};\n\nexport default config;\n\nThis config connects to our Hasura GraphQL instance and generates typings based on the client preset.\nIn package.json add a script to load environment variables and run the code generator \"codegen\": \"graphql-codegen --require dotenv/config --config codegen.ts dotenv_config_path=./.env.local\"\nNext, define our queries/mutations in lib/service/queries.graphql\nquery GetProducts {\n  product(limit: 10) {\n    id\n    name\n  }\n}\n\nquery GetProduct($id: Int!) {\n  product_by_pk(id: $id) {\n    id\n    description\n    name\n    price\n    image_urls(path: \"$[0]\")\n  }\n}\n\nmutation PlaceOrder($products: order_product_arr_rel_insert_input!) {\n  insert_order_one(\n    object: {\n      products: $products\n      billing_address_id: 222\n      user_id: 225\n      shipping_address_id: 222\n    }\n  ) {\n    id\n  }\n}\n\nFinally, run the generator:\nnpm run codegen\n\ngraphql-request Setup\nCreate a graphql-request client in lib/service/client.ts\nimport { GraphQLClient } from \"graphql-request\";\n\nexport const gqlClient = new GraphQLClient(\n  process.env.NEXT_PUBLIC_HASURA_GRAPHQL_URL!,\n  { fetch }\n);\n\nWe pass in the environment\u2019s global fetch, so our Next.js will work anywhere, regular Node.js or an edge runtime.\nNested Layouts\nOne of the new Next.js features is nested layouts. They allow us to have a shared parent component, and the router can render in child routes.\nTo demonstrate this, we will display ten products on our home page. When you click on one, it will navigate the right-hand side to the product details page.\nFetch Home Page Data\nNext.js 13 uses React\u2019s new way of fetching data. Read more about the RFC on Github. We will use it to get a list of products.\nIn app/layout.tsx, we add an async function to get data from Hasura:\nimport Link from \"next/link\";\nimport { GetProductsDocument } from \"../lib/gql/graphql\";\nimport { gqlClient } from \"../lib/service/client\";\nimport \"./globals.css\";\n\nasync function getProducts() {\n  const { product: products } = await gqlClient.request(\n    GetProductsDocument,\n    {}\n  );\n  return products;\n}\n\nWe call it in our component using async/await, then create an unordered list of product links:\nexport default async function RootLayout({\n  children,\n}: {\n  children: React.ReactNode;\n}) {\n  const products = await getProducts();\n  return (\n    <html lang=\"en\">\n      <head />\n      <body>\n        <main style={{ display: \"flex\" }}>\n          <section style={{ minWidth: \"400px\", width: \"400px\" }}>\n            <ul>\n              {products.map((product) => (\n                <li key={product.id}>\n                  <Link href={`/${product.id}`}>{product.name}</Link>\n                </li>\n              ))}\n            </ul>\n          </section>\n          <section style={{ flexGrow: 1 }}>{children}</section>\n        </main>\n      </body>\n    </html>\n  );\n}\n\nWith nested layouts and a layout.tsx file, the URL will control the component output in {children}.\nProduct Details Page\nWe will now add a Next.js page that gets the product ID from the URL and fetches/displays the details.\nCreate app/[id]/page.tsx. The [id] means we will pass down data from the URL as a prop named id:\nimport Image from \"next/image\";\nimport { GetProductDocument } from \"../../lib/gql/graphql\";\nimport { gqlClient } from \"../../lib/service/client\";\n\nasync function getProduct(id: number) {\n  const { product_by_pk } = await gqlClient.request(GetProductDocument, { id });\n  return product_by_pk!;\n}\n\nexport default async function Page({\n  params: { id },\n}: {\n  params: { id: string };\n}) {\n  const product = await getProduct(Number(id));\n  return (\n    <>\n      <h1>{product.name}</h1>\n      <Image\n        src={product.image_urls}\n        alt={`${product.name} Picture`}\n        width={200}\n        height={200}\n      ></Image>\n      <div>{`$${product.price.toFixed(2)}`}</div>\n      <p>{product.description}</p>\n    </>\n  );\n}\n\nNow when you click a link on the left, you should see the product details page appear on the right!\nStreaming and Suspense\nStreaming and Suspense are new React features that allow us to instantly display a loading UI, then stream in UI once data fetching is done.\nAs an example of this, create app/loading.tsx:\nexport default function Loading() {\n  // You can add any UI inside Loading, including a Skeleton.\n  return <div>loading</div>;\n}\n\nNow when you try navigating between products you should see the loading UI before the details are loaded.\nClient Components\nAll the stuff we've built so far uses Server Components which means they render on the server and don't need to ship JS to the browser.\nHowever, sometimes we need to create Client Components that have logic on the browser. For our example, we will have a button that orders a product and a Hasura Streaming Subscription that shows us a real-time list of orders.\nFor our real-time subscription, we'll use the graphql-ws library. For the mutation we will continue to use graphql-request.\nWe create a new client component in app/components/orders.tsx using the syntax we are used to in React. The only new thing part is the \"use client\"; at the top which tells Next.js this is a client component.\n\"use client\";\n\nimport { useEffect, useState } from \"react\";\nimport { PlaceOrderDocument } from \"../../lib/gql/graphql\";\nimport { createClient } from \"graphql-ws\";\nimport { gqlClient } from \"../../lib/service/client\";\n\nconst wsClient = createClient({\n  url: process.env.NEXT_PUBLIC_HASURA_GRAPHQL_WS_URL!,\n});\n\nexport default function Orders({ id }: { id: number }) {\n  const [orders, setOrders] = useState<\n    Array<{ id: number; created_at: string; quantity: number }>\n  >([]);\n\n  useEffect(() => {\n    const unsubscribe = wsClient.subscribe(\n      {\n        query: `\n          subscription ProductOrders($id: Int!) {\n            order_product_stream(\n              batch_size: 10\n              cursor: { initial_value: { created_at: \"2021-02-22T18:16:12.95499+00:00\" } }\n              where: { product_id: { _eq: $id } }\n            ) {\n              id\n              quantity\n              created_at\n            }\n          }`,\n        variables: {\n          id,\n        },\n      },\n      {\n        next: ({ data }) => {\n          setOrders((orders) => [\n            ...orders,\n            ...(data?.order_product_stream as any),\n          ]);\n        },\n        error: (error) => {\n          console.log(error);\n        },\n        complete: () => {\n          console.log(\"complete\");\n        },\n      }\n    );\n    return () => {\n      unsubscribe();\n    };\n  }, [id]);\n\n  return (\n    <>\n      <button\n        onClick={async () =>\n          await gqlClient.request(PlaceOrderDocument, {\n            products: { data: [{ product_id: id, quantity: 1 }] },\n          })\n        }\n      >\n        Order\n      </button>\n      <li>\n        {orders.map((order) => (\n          <li key={order.id}>\n            {`Order ${order.id} created at ${new Date(order.created_at)}`}\n          </li>\n        ))}\n      </li>\n    </>\n  );\n}\n\nWhen a new order comes from the subscription we update useState and render out the list of orders. When we click on the button, we trigger a GraphQL mutation to add a new order.\nNow back in app/[id]/page.tsx we import the component and add it to the end of the page\nimport Orders from \"../components/orders\";\n\nexport default async function Page({\n  params: { id },\n}: {\n  params: { id: string };\n}) {\n  const product = await getProduct(Number(id));\n  return (\n    <>\n      ...\n      <Orders id={Number(id)}></Orders>\n    </>\n  );\n}\n\nWhen we navigate to the product detail page you should now be able to order a product and see new orders pop up in real-time!\nConclusion\nNext.js is packed with new features and deep integration with cutting-edge React techniques. Combined with Hasura, we can easily make great applications for our users!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: 7 Things You Can\u2019t Miss at AWS re:Invent Description: Hasura is back in Las Vegas for AWS re:Invent in a few short weeks, from Nov.\n28th to Dec. 2nd. With over 2,000+ sessions, 30+ receptions, and countless other\nactivities happening, we wanted to highlight what Hasura is up to at AWS\nre:Invent. \n\nHere are 7 things you just can\u2019t miss:\n\n 1. Hasura Challenge at the AWS Jam Lounge: Want to be a GraphQL Rockstar or at\n    least appreciate a few good musical puns? Check out our AWS Jam challenge\n    for building a federated GraphQL API at AWS re:Invent Author: Bela Gandhi"}, {"role": "assistant", "content": "Hasura is back in Las Vegas for AWS re:Invent in a few short weeks, from Nov. 28th to Dec. 2nd. With over 2,000+ sessions, 30+ receptions, and countless other activities happening, we wanted to highlight what Hasura is up to at AWS re:Invent. Here are 7 things you just can\u2019t miss:Hasura Challenge at the AWS Jam Lounge: Want to be a GraphQL Rockstar or at least appreciate a few good musical puns? Check out our AWS Jam challenge for building a federated GraphQL API at AWS re:Invent. Come to the Expo to sign up, but then work on the challenges anywhere. If you complete Hasura\u2019s challenge, there are chances to win prizes and swag!Happy Hour at Mercato Della Pescheria: Join us Tuesday, Nov. 29th, 6 - 8 PM PST, for a night out with our friends Cockroach Labs and Launch Darkly. There will be an open bar, food, a photo booth, and networking. Let us know if you\u2019re joining us.Dr. Werner Vogels Keynote: Join Dr. Werner Vogels, Amazon.com VP, and CTO, on Thursday, Dec 1st, 8:30 - 10:00 AM PST, as he shows how customers and AWS use novel architectural patterns to build scalable, resilient, and fault-tolerant applications. He highlights innovations and emerging technologies that enable builders to create systems that would have been previously unimaginable and describes how the cloud is at the center of this new era of innovation.Book a Demo with Hasura: We\u2019d love to show you how Hasura makes teams super productive by cutting out the manual work of creating APIs by giving you instant GraphQL APIs out of the box. You can schedule a 1:1 demo with an Hasura expert here. Breakout Session - Are you integrating or building distributed applications?: In this session, Wednesday, Nov. 30th, 1:45 PM - 2:45 PM, reflect on two decades of building connected systems, ponder the nuances of messages versus events, learn that loose coupling isn\u2019t always better, and discover how serverless cloud automation can change the way you think about integration.Workshop - Build an iOS-native app with a cloud-based backend: In this workshop on Thursday, Dec. 1st, 2:45 PM - 4:45 PM PST, you\u2019ll create a data-driven iOS-native app integrated with a cloud-based backend. Learn how to use Amazon Cognito to manage user authentication. Discover how to design a data-first strategy with GraphQL APIs to store data in Amazon DynamoDB.Chalk Talk - APIs: Critical for data transfer, but how do you keep them secure?: In this chalk talk on Thursday, Dec. 1, 11 - 12 PM, you\u2019ll review best practices for building a resilient architecture specifically for API-driven applications. Explore practical tips for defending against DDoS events and securing APIs with OAuth JWT authentication. We look forward to seeing you and hope you\u2019ll find time to stop by and meet the Hasura team at our booth (#2251) and see how Hasura\u2019s modern data-access engine can enable your teams to accelerate API and application development. You don\u2019t want to miss the exclusive swag we\u2019re rolling out! *Hint: It might be the cutest swag we\u2019ve ever made! Safe travels, and see you there!The Hasura Team"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Actions using Netlify Functions for Custom Logic Description: In this guide, we will show how to write your own Hasura actions using Netlify functions (with your own custom business logic or a third-party API integration). Author: Hasura"}, {"role": "assistant", "content": "In this guide, we will show how to write your own Hasura actions using Netlify functions (with your own custom business logic or a third-party API integration). But first, definitions.What are Hasura actions?Hasura provides support for integrating our own business logic or third-party APIs (usually REST APIs) with the help of \u201cHasura Actions\u201d. Learn more about Hasura Actions.What are Netlify functions?Netlify functions are AWS\u2019s serverless lambda functions that allow you to define your business logic and functions as an API and expose them with an endpoint. Click here to learn more about netlify functions.How to write your own Hasura Actions using Netlify functions?This guide will go through the process in three parts, feel free to skip to the part that is more interesting for you.Part I: Create a Function in an appPart II: Define the functions with logicPart III: Add the function to Hasura using ActionsNote: Hasura Actions can also be used to add REST endpoints to Hasura\u2019s GraphQL schema.Part I: Create a Function in an appTo create a function in the app, install the netlify-cli package from npm (preferably as global).npm install -g netlify-cliOnce the netlify-cli is installed, go to the project\u2019s root folder, and link your Netlify site to the folder. Click here to learn more about linking sites in Netlify.netlify linkOnce linked, type the following command to create a function.netlify functions:createYou will see a list of predefined templates. Choose the \u201chello-world\u201d template.The function will be created in <project-root>/netlify/functions/hello-world/hello-world.jsPart II: Define the functions with logicHere is an example of a Netlify function for Hasura's Actions, which checks if an email is present in the DB without exposing all the users to the client.const axios = require(\"axios\");\n\nconst hgeEndpoint = \"https://example.hasura.app/v1/graphql\";\nconst adminSecret = \"adminSecret\";\n\nconst handler = async (event) => {\n  let request;\n  try {\n    request = JSON.parse(event.body);\n  } catch (error) {\n    // Make sure you add code and message to errors. These will be shown in the hasura console errors.\n    let response = {\n      status: false,\n      code: \"api/parse-error\",\n      message: \"error\",\n      error: { message: \"cannot parse input\" },\n    };\n    return { statusCode: 400, body: JSON.stringify(response) };\n  }\n\n  if (!request.input.email) {\n    // Make sure you add code and message to errors. These will be shown in the hasura console errors.\n    let response = {\n      status: false,\n      message: \"Please send email\",\n      code: \"input/undefined\",\n      error: { message: \"Please send email\" },\n    };\n    return { statusCode: 400, body: JSON.stringify(response) };\n  }\n\n  let query = {\n    query: `\n      query check_user($email: String) {\n        users(where: {email: {_eq: $email}}) {\n          id\n          email_id\n        }\n      }\n    `,\n    variables: { email: request.input.email },\n  };\n\n  try {\n    let result = await axios.post(hgeEndpoint, query, {\n      headers: { \"x-hasura-admin-secret\": adminSecret },\n    });\n    if (result.data.users.length > 0) {\n      let response = {\n        status: true,\n        message: \"success\",\n        user: data.data.users,\n      };\n      return { statusCode: 200, body: JSON.stringify(response) };\n    } else {\n      // Make sure you add code and message to errors. These will be shown in the hasura console errors.\n      let response = {\n        status: false,\n        code: \"user/not-found\",\n        message: \"No user found\",\n        error: { message: \"No user found\" },\n      };\n      return { statusCode: 400, body: JSON.stringify(response) };\n    }\n  } catch (error) {\n    let response = {\n      status: false,\n      code: \"api/fetch-error\",\n      message: error.message,\n      error: { message: error.message, detail: error.toString() },\n    };\n    return { statusCode: 500, body: JSON.stringify(response) };\n  }\n};\n\nmodule.exports = { handler };NOTE: Add the code and message to the response body. These are displayed in Hasura\u2019s GraphQL errors (whenever the API is returning an error in Hasura, they follow the GraphQL error standard).The error in Hasura is displayed like this:Error state in Hasura GraphQLPart III: Add the function to Hasura using ActionsOnce the custom business logic is created as an API using Netlify functions, add it to Hasura using Actions as follows:Login to your Hasura console and go to the \u201cActions\u201d tabClick on the \u201cCreate\u201d buttonDefine the inputs and outputs of your API as shown below. (Learn more about the GraphQL inputs & outputs on Hasura)Add the endpoint of your Netlify function under the Handler.Creating an ActionNote: You can also modify the permission of the actions if you\u2019re using role-based permissions.Try out the Actions using Hasura\u2019s GraphQL API ExplorerThe success state on Hasura\u2019s API Explorer from the Netlify functions will look like this:Success state of an actionThe error state on Hasura\u2019s API Explorer from the Netlify functions will look like this:There are two error statesWhen the user is not found with the requested email, you\u2019ll see the following (User not found in DB error)When the email is not sent along with the request, you\u2019ll see the following (Validation Error)Input not given / undefined errorAbout the AuthorMatheswaaran S is a \u00a0full stack developer & Tech Enthusiast. You can learn more about what he does on his website, and connect with him on Twitter or Linkedin. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top Serverless PostgreSQL Providers Description: This post shows the top serverless PostgreSQL providers to host your database that fits in well with Hasura Cloud. Author: Catalin Pit"}, {"role": "assistant", "content": "Postgres (or PostgreSQL) is open-source, free, and one of the most popular relational databases. As a result, there's a lot of community tooling built on top of Postgres, or that's fully compatible with the Postgres spec, which allows for advanced features, unique use cases, and scaling.\nThis post will show some of the top serverless Postgres database providers. The providers are in no particular order. If you know other providers that could be added to the article, feel free to mention @HasuraHQ on Twitter, and we will try to add them.\nNeon\nNeon offers a fully-managed serverless Postgres service. Their aim is to separate storage from compute, which results in benefits such as auto-scaling, branching, and bottomless storage.\nThat enables you to do things such as scaling CPU and I/O resources independently, running multiple compute Postgres instances without having multiple data copies, or quickly starting and shutting down your instance.\nNeon has both paid and free tiers. The free tier comes with the following:\n\n10GB project size\n1 vCPU and 256MB of RAM\n3 Postgres databases\n\nGet started with Hasura and Neon here.\nWe recently launched the native Neon Integration inside Hasura Cloud. Read more\nAmazon Aurora\nAmazon Aurora is another fully-managed serverless database service that is compatible with Postgres. Aurora claims to offer 3X more throughput than the base PostgreSQL.\nIt comes with low latency read replicas, automated backups that enable point-in-time recovery, automatic software patching, and in-depth metrics.\nAmazon Aurora has both free and paid tiers, with the free tier offering the following:\n\n20GB storage\n750 hours of compute time\n20GB of storage for backups and DB snapshots\n\nGet started with Hasura and Amazon Aurora here.\nCrunchy\nCrunchy gives you a high-availability Postgres database with their Crunchy Postgres solution.\nCrunchy Postgres comes with disaster recovery, auto-scaling to tens of thousands of connections, comprehensive monitoring, high availability, and automatic failover.\nGet started with Hasura and CrunchyDB here.\nCitusDB\nCitus extends and transforms the base PostgreSQL into a distributed database. You get the power of distributed tables on top of the powerful Postgres capabilities.\nCitus comes with features such as parallelism, which allows you to speed up queries by 20x to 300x. You can also distribute data and queries by adding more nodes as you grow. Citus Postgres is an open-source project, which means that you can download and run it for free.\nGet started with Hasura and CitusDB here.\nBit.io\nBit.io claims to be the fastest way to get a serverless Postgres database. It allows you to get a database in 1 click.\nBit.io Postgres offers automatic scaling to match your usage, high availability, and security features such as TLS/SSL and disk encryption.\nIt has a paid and a free tier too. The free tier includes the following:\n\n3 Postgres databases with 3GB storage each and 1 billion rows queries\n\nGet started with Bit.io here.\nFurther reading\n\nHasura and Postgres\nPostgreSQL basics\nPostgres 15 release: Top things to know\nLearn Postgres for free\nTop PostgreSQL database free tier solutions\npsql commands and flags you need to know\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing native database integrations for CockroachDB and Amazon Athena Description: We are thrilled to announce that Hasura is adding integrations for 2 new databases, CockroachDB and Amazon Athena, with support for Snowflake coming in a few weeks.  Author: Asawari Samant"}, {"role": "assistant", "content": "We are thrilled to announce that Hasura is adding integrations for 2 new databases, CockroachDB and Amazon Athena, with support for Snowflake coming in a few weeks. Hasura\u2019s mission since the early days has been to accelerate application & API development by radically simplifying the data access and management part of the app dev workflow. By connecting to a new or existing database, data source or service, and instantly auto-generating a GraphQL or REST API on top of it, Hasura shrinks the time required to build APIs, secure access, deploy and scale the app server, and optimize performance from weeks or months to minutes. With the launch of these new integrations, we are expanding that instant GraphQL / REST API magic to more data sources, and helping developers and teams using these databases to build and ship faster. These new integrations build on existing native integrations with PostgreSQL (including many variants and flavors), BigQuery, and Microsoft SQL Server.The CockroachDB and Athena integration are available as beta in Hasura v2.15.0. We welcome you to try these out on Hasura Cloud, or give them a spin on self-managed Hasura instances.\nThis beta release of the Amazon Athena integration is available for Hasura Enterprise Edition (EE) and all Hasura Cloud customers. Once this integration moves to General Availability (GA), it will only be available in Enterprise Edition and Hasura Cloud Enterprise plans. The CockroachDB integration will be available in all Hasura plans.\n\nCockroachDB integrationCockroachDB is a distributed SQL database designed for speed, scale, and resilience. Given the popularity of CockroachDB in powering modern applications, it is no surprise that it was one of the most requested database integrations on the Hasura GitHub repo. With the new CockroachDB integration in Hasura, existing as well as new users of CockroachDB can instantly generate data APIs \u2013 both GraphQL and REST APIs \u2013 to enable fast and secure access to their CockroachDB data. We are excited to partner with the Cockroach Labs team on bringing this capability to market. Stay tuned for more collaborative content and sessions with their team.Connecting CockroachDB on Hasura ConsoleThe Hasura-CockroachDB integration works with CockroachDB version 22.2, which is currently in beta and not available on the CockroachDB managed service yet. We expect this integration to seamlessly work with CockroachDB version 22.2 GA when it launches in the near future. Visit the deep dive blog to learn more and take this new integration for a spin.Amazon Athena integration Amazon Athena is a serverless query service that makes it easy to query and analyze data in Amazon S3 using standard SQL. Hasura integration for Amazon Athena was a request from large enterprises that have standardized on the Amazon ecosystem, and want to create a cleaner and secure federated access layer to their S3 data.Connecting Athena to Hasura ConsoleThe Athena connector is currently in beta, the auto-generated APIs only supports read-only queries capabilities. We will be adding support for other GraphQL features, such as mutations and subscriptions, in future releases of the connector. Visit the deep dive blog to learn more and try out this new integration.Snowflake integrationWe are seeing a growing interest in the need for GraphQL / REST APIs on top of data warehouses, as a way to enable API driven, self-service data access for analytical and other workloads in a quick, yet secure and controlled way. Snowflake is a leader in the Cloud Data Warehouse space, and hence no surprise about the inbound interest in Hasura integration to Snowflake. Just like the Hasura-BigQuery integration that launched earlier this year, this will be primarily powering data access for analytical applications. The Snowflake integration for Hasura is expected to launch before the end of the year. Sign up here to be notified when it launches, and early access. Got questions?We invite you to give both these new connectors a try on Hasura Cloud. Got questions? Have feedback? Join Hasura engineers and community on our Discord or GitHub Discussions. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Instant GraphQL for CockroachDB Description: We are pleased to announce the beta release for CockroachDB on Hasura in v2.15.0. Author: David Meleney"}, {"role": "assistant", "content": "We are pleased to announce the beta release for CockroachDB on Hasura in v2.15.0, which will be available in the open source and cloud free versions of Hasura. Together, Hasura and CockroachDB drastically reduce the time and effort required to build or migrate to a high availability distributed database architecture that is leveraged using an enterprise grade data access layer that includes GraphQL & REST APIs, Declarative Role Based Authorization, Advanced Security and Performance with rate limiting, allow lists, and caching. The combined solution of our platforms cut what could be years of experimentation and development down to weeks.What is CockroachDB?CockroachDB is a distributed SQL database built using a strongly-consistent key-value store. It scales horizontally; survives hardware failures with minimal disruption or manual intervention; supports strongly-consistent ACID transactions; and supports the PostgreSQL wire protocol and the majority of PostgreSQL syntax.When is CockroachDB a good choice?CockroachDB is a good choice for applications that require high availability and reliability across multiple regions. Many of the traditional drawbacks associated with distributed databases such as complexity that increases operational overhead are addressed with simple configurations that allow users to automatically replicate, rebalance, and recover from failures. The ability to scale up and down makes CockroachDB a cost effective solution for a wide range of operational requirements.Why Hasura?Hasura\u2019s intuitive tools and documentation make it easy to set up and connect to CockroachDB, instantly generating GraphQL schemas and resolvers based on the tables and views in the database. With little to no effort organizations use Hasura to create a powerful data access layer that includes GraphQL & REST APIs, declarative role-based authorization, advanced security and performance with rate limiting, allow lists, and caching with little to no effort.Auto-Generated GraphQL Schemas and ResolversHasura\u2019s ability to instantly generate GraphQL schemas and resolvers based on the tables and views in CockroachDB saves time spent on development and continued maintenance because developers no longer have to author more data access APIs or modify the code every time there is a database change. Hasura provides \u201cGraphQL out of the box\u201d with its ability to auto-generate GraphQL schemas and resolvers from CockroachDB, saving months of development time - especially in the early stages of greenfield projects or migrations when changes to features and the underlying database happened frequently. Many organizations use Hasura for nearly all queries and updates for hundreds of database tables.Ref: https://hasura.io/case-studies/pulley/Security, Authorization & AuditingHasura\u2019s granular authorization features can be applied down to the row and column level to address a wide range of data authorization use cases and Hasura\u2019s built-in auditing capabilities can be used to track changes in user permissions, providing a history of changes.A mature and hardened data authorization layer is essential to protecting sensitive financial and personal data. To ensure the security of the system, Hasura regularly performs penetration testing to make sure Hasura is hardened from external attack.Data MigrationsHasura\u2019s migrations features track CockroachDB database schema changes in metadata, allowing users to keep their database consistent between development and production.Management ConsoleHasura\u2019s console also increased the developer\u2019s productivity by making it easy to perform GraphQL schema generation tasks, perform GraphQL queries, and perform traces to identify any performance bottlenecks.BeforeWith HasuraWhy Hasura and CockroachDB?Together, Hasura and CockroachDB drastically reduce the time and effort required to build or migrate to a high availability distributed database architecture that is leveraged using an enterprise grade data access layer that includes GraphQL & REST APIs, Declarative Role Based Authorization, Advanced Security and Performance with rate limiting, allow lists, and caching. The combined solution of our platforms cut what could be years of experimentation and development down to weeks. Providing our solutions as a service allows organizations, both big and small, to treat their data as mission critical to their business ensuring data is reliable, available, correct, with response times measured in milliseconds, regardless of scale 100% of the time.What is next?Hasura will add support for Subscriptions in an upcoming release. Hasura support for the CockroachDB managed services will be available following the GA release of CockroachDB version 22.2.Try it outYou can follow the instructions found in the documentation here to try Hasura's beta support for CockroachDB using docker today."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Instant GraphQL on Amazon Athena Description: The beta integration for Amazon Athena to Hasura v2.15 is now available!\n\nThe beta release comes with read-only query support for Enterprise and Cloud Enterprise users.  Author: Adron Hall"}, {"role": "assistant", "content": "Today marks the beta release of the Amazon Athena GraphQL Data Connector for Hasura. Amazon Athena is a service integrated with AWS Glue Data Catalog providing a unified metadata repository across various services, allowing queries to crawl data sources and discover schema to populate your catalog. Now, with the release of the Hasura GraphQL Data Connector for Athena all of that power comes to Hasura for you to query against!Important - This initial release of the Hasura GraphQL Data Connector for Amazon Athena is in beta, for read-only queries, and available for Hasura Enterprise Edition (EE) and all Hasura Cloud customers. Once this connector moves to General Availability (GA) it will only be available via Enterprise Edition and Hasura Cloud Enterprise plans.Hasura & Amazon Athena Combined!The combination of these two services can be utilized either through serverless architecture at AWS and Hasura Cloud or with Hasura GraphQL Engine deployed to your own data center or AWS VPC! To set up the connection to Amazon Athena takes the same familiar steps as connecting any data source to Hasura.Once you\u2019ve gotten a project up and running in Hasura Cloud or Hasura EE, enable the Athena Connector by navigating to Settings -> Feature Flags on the Hasura Console. Enable the Experimental features for GDC.Navigate to the Data tab of the console. Then click on the Connect Database button.Next up select athena(beta) as the Data Source Driver.Next enter the Database URL and schema for use with Athena.Now click on Connect Database. Once Hasura is connected to the database, navigate to select tables to track by clicking on the connection just created for Amazon Athena in the left side pane. You\u2019ll then see the tables that can be tracked for query access.From here you can start querying your Amazon Athena data with GraphQL!What Scenarios Work Well for Athena and Hasura GraphQL Engine?To figure out where and what Athena and Hasura give you, let\u2019s quickly recap what each of these two services can provide.Athena is set up to help analyze unstructured, semi-structured, and structured data stored in S3. This could be all sorts of data structures such as CSV, JSON, Apache Parquet columnar data, and Apache ORC. Athena provides access to all of this by running ANSI SQL queries against this underlying data. To manage all of this and the respective queries Athena integrates with the AWS Glue Catalog, which offers a persistent metadata store for the data in S3. This allows for tables to be created and data to be queried in Athena.Shifting to Hasura, we provide a way to connect disparate data sources via direct access - like Athena - along with actions, remote schema to other GraphQL APIs, and other mechanisms to provide a singular unified GraphQL API (or REST if desired!). Bringing all of this data together into a singular unified API provides a dramatically faster way to develop web applications and web APIs for consumers, enterprise, and other organizations.Combining Amazon Athena query capabilities into Hasura with the Hasura GraphQL Data Connector provides a significant increase in capabilities against AWS data stores. By providing Athena a way to integrate, or stand alone, but provide a seamless GraphQL API extends the abilities of Athena to your entire data tier.SummaryWith this release the value of Athena and Hasura extended dramatically. The ability to more quickly integrate this data into applications around a GraphQL API that unifies their respective data sources is very powerful. While it\u2019s in beta be sure to give it a test drive and try it out for yourselves.Want More Athena?Curious about getting even more out of your Athena connection? Be sure to reach out via Discord or send a request directly via Contact Us form \u2013 we would love to chat about what other capabilities you\u2019d like to get connected to with Athena and Hasura!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top psql commands and flags you need to know | PostgreSQL Description: This post will teach you the top psql commands and flags you need to know when working with PostgreSQL. Author: Catalin Pit"}, {"role": "assistant", "content": "PostgreSQL, or Postgres, is an object-relational database management system that uses the SQL language. It's free, open-source, reliable, robust, and performant. PostgreSQL is also one of the most popular & used relational databases.\npsql is an interface you can access through the terminal to interact with Postgres databases. You can use it to connect to a database, add & read & modify data, check the available databases & fields, run commands from a file, and so on.\nIf you want to learn about Postgres commands or refresh your memory, you are in the right place! This article will teach you the top psql commands and flags you need to know when working with PostgreSQL.\n1. Connect to a database - psql -d\nThe first step involves learning how to connect to a database. There are two ways to connect to a PostgreSQL database, depending on where the database resides.\nSame host database\nIf the database is on the same host as your machine, you can use the following command:\npsql -d <db-name> -U <username> -W\n\n// example\npsql -d tutorials_db -U admin -W\n\nThe above command includes three flags:\n\n-d - specifies the name of the database to connect to\n-U - specifies the name of the user to connect as\n-W - forces psql to ask for the user password before connecting to the database\n\nIn this example, the command connects you to the tutorials_db under the admin user.\nDifferent host database\nIn the cases where your database is hosted somewhere else, you can connect as follows:\npsql -h <db-address> -d <db-name> -U <username> -W\n\n//example\npsql -h my-psql-db.cloud.neon.tech -d tutorials_db -U admin -W\n\nThe -h flag specifies the host address of the database.\nSSL mode\nThere might be cases where you want to use SSL for the connection.\npsql \"sslmode=require host=<db-address> dbname=<db-name> user=<username>\"\n\n//example\npsql \"sslmode=require host=my-psql-db.cloud.neon.tech dbname=tutorials_db user=admin\"\n\nThe above command opens an SSL connection to the specified database.\n2. List all databases - \\l\nIn many cases, you will work with more than one database. You can list all the available databases with the following command:\n\\l\n\n\nThe above image illustrates what happens when you run the command. You get a table with all databases and their name, owner, access privileges, and other information.\n3. Switch to another database - \\c\nYou can also switch to another database with the following command:\n\\c <db-name>\n\n// example\n\\c tutorials_db\n\nThe below image illustrates the result after running the command.\n\nThe command switches to the specified database under the user you logged in previously.\n4. List database tables - \\dt\nLet's consider you want to see all the tables from the database. You can list all database tables as follows:\n\\dt\n\n\nThe \\dt psql command returns the tables alongside:\n\nthe schema they belong to\ntheir type\ntheir owner\n\n5. Describe a table - \\d\npsql also has a command that lets you see the table's structure.\n\\d <table-name>\n\n// example\n\\d tutorials\n\n\nThe \\d command returns all the columns, their types, collection, whether they are nullable or not, and their configured default value.\nIf you want more information about a table, you can use the command:\n\\d+ <table-name>\n\n\nNow, you get extra information such as storage, compression, stats target, and a description.\n6. List all schemas - \\dn\nThe \\dn psql command lists all the database schemas.\n\nIt returns the name of the schemas and their owners.\n7. List users and their roles - \\du\nSometimes, you might need to change the user. Postgres has a command that lists all the users and their roles.\n\\du\n\n\nAs the image shows, the command returns all the users.\n8. Retrieve a specific user - \\du\nYou can also retrieve information about a specific user with the following command:\n\\du <username>\n\n//example\n/du postgres\n\n\nNow, you can see the roles of the specified user, and whether the user is a member of a group or not.\n9. List all functions - \\df\nYou can list all the functions from your database with the \\df command.\n\nThe command returns all functions and the:\n\nschema they belong to\nnames\nresult data type\nargument data types\ntype\n\n10. List all views - \\dv\nThe psql interface enables you to list all the database views with the \\dv command.\n11. Save query results to a file - \\o\nThere might be cases where you want to analyze the result of a query at a later time. Or two compare two query results. The psql interface allows you to do that.\nYou can save query results in a file as follows:\n\\o <file-name>\n\n// example\n\\o query_results\n...run the psql commands...\n\\o - stop the process and output the results to the terminal again\n\nLet's save the following query results in a file:\n\nthe available database tables\nthe result of describing a database table\nthe list of all users\nthe details of the user \"postgres\"\n\n\nNote: To stop saving results to the file, you need to run the \\o command again without the file name.\n\nThe above image illustrates the file containing all the query results.\n12. Run commands from a file - \\i\nIt's also possible to run commands from a file. For simple commands, it might not be the best solution. But when you want to run multiple commands and complex SQL statements, it helps a lot.\nCreate a txt file with the following content:\n\\l\n\\dt\n\\du\n\nWhen you run the file, it should return a list of all:\n\ndatabases\ndatabase tables\nusers\n\nYou can run commands from a file with the following psql command:\n\\i <file-name>\n\n// example\n\\i psql_commands.txt\n\n\nThe command returned all the databases, tables, and users as expected.\nQuit psql - \\q\nYou quit the psql interface with the \\q command.\nSummary\nThe psql interface is powerful and allows you to do many things, including running SQL statements. If you want to get started with PostgreSQL or deepen your knowledge, check out our PostgreSQL Tutorial. You can also check out the Postgre articles on our blog.\n\nIf you are looking for options to host your Postgres database, check these PostgreSQL hosting options.\n\nFurther reading\n\nHasura and Postgres\nPostgreSQL basics\nPostgres 15 release: Top things to know\nLearn Postgres for free\nTop PostgreSQL database free tier solutions\nTop serverless PostgreSQL providers\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: State of GraphQL 2022 Survey Report Summary and Insights Description: In this post, we will look at the key insights and latest GraphQL trends identified in this survey. Author: Catalin Pit"}, {"role": "assistant", "content": "The first \"State of GraphQL\" survey was released recently, with over 3000 developers participating.\nIn this post, we will look at the key insights and latest GraphQL trends identified in this survey.\nGraphQL Feature Usage and Popularity\nLet's start by looking at the GraphQL features and their usage. We'll see which features are the most and least popular & used by developers.\nThe most used features are: fragments, interfaces, unions, and custom scalars. On the other hand, the least used features are directives such as: @defer, @specifiedBy, and @stream.\n\nYou can learn about some of these GraphQL concepts in this free GraphQL course.\n\n\nWhen it comes to the GraphQL directives, the @deprecated, @include, and @skip directives are the most used.\nIt's interesting to see the @defer and @stream directives being amongst the least used/popular features considering that they help improve the app performance by reducing latency. These two directives are great features, so you might want to try them out. You can read more about @defer and @stream here.\nIf you are using Hasura to build your GraphQL API, you don't need to use these directives for the use case of streaming real-time data. Hasura comes with streaming over GraphQL subscriptions.\n\nGraphQL Subscriptions seem to be the popular choice among developers when it comes to real-time data. Meanwhile, most developers are not even aware of Live Queries. That means they are less popular and used compared to GraphQL Subscriptions.\nIt's important to mention that features such as @defer, @stream, and live queries are not specified in the official GraphQL specification. Perhaps developers are less inclined to use features that are not included in the GraphQL spec.\n\nMoreover, most developers were only aware of a small subset of GraphQL features. If you search for features such as live queries, for example, you won't find a lot of material on the subject. So it makes sense when you see that most developers didn't know about many features. Maybe we, the GraphQL community, could do better to raise awareness of the different GraphQL features.\n\nIn Hasura, Live Queries is a pattern of querying real-time data where a response is returned every time the data changes. Read more about Hasura's Live Query architecture.\n\nGraphQL Libraries\nIn this section, we'll look at the available libraries in the GraphQL ecosystem. We'll go over the following:\n\nservers\nclients\nAPI generators\nschema builders\n\nGraphQL Servers\nLet's start by looking at the GraphQL servers developers use to power their APIs.\n\nAccording to the survey, the top 5 popular choices are Apollo Server, Express-GraphQL, GraphQL.js, Yoga, and GraphQL-Go.\n\nIf we flip the chart and look at the retention rate, we can see a significant change in the rankings. A higher retention rate means developers would use the technology again to build GraphQL servers. The top 5 are as follows:\n\nDGS\nAbsinthe\nGraphQL.js\nMercurius\nApollo Servers\n\nIt would be interesting to be able to look into why those technologies have a high retention rate. Is it because they have excellent developer experience? Is it because they are complete solutions? Is it because they have lots of features? It would be interesting to know.\n\nLooking at other tools, we can see Hasura in second place. That shows that Hasura is quite a popular choice for developing GraphQL servers.\n\nOverall, there are plenty of choices for building GraphQL servers. Looking at the data, we can see that the community is happy with the current state of GraphQL technologies for building servers. Do note that these are libraries or frameworks you use to write a GraphQL server from scratch completely.\nGraphQL Clients\nNow let's look at the GraphQL clients developers use to query their APIs.\n\nWhen it comes to GraphQL clients, the top 5 most known technologies are as follows:\n\nApollo Client\nRelay\nNo client\nReact Query\nURQL\n\nLooking at the retention rate (whether the developers would use the technology again or not), the chart changes quite a bit.\n\nThe chart shows that the top 5 clients with the highest retention rate are: GraphQL Hooks, graphql-request, React Query, URQL, and Apollo Kotlin. Like with the GraphQL servers, it would be nice to understand why they have a high retention rate.\nOne thing to observe is that GraphQL extends outside the JavaScript ecosystem. We can see clients for Kotlin, Swift, and iOS.\n\nEven though there are only a handful of GraphQL clients, most developers are happy with the pool of choices.\nIf you are interested in exploring GraphQL clients, check this article.\nAPI Generators\nThis section will cover the ORMs, CMSes, and other tools that help developers create a GraphQL API. This category is where Hasura fits in too.\n\nHasura is leading the way when it comes to awareness of such tools. Hasura is then followed by:\n\nStrapi\nContentful\nAWS AppSync\nFauna\nGraphene\n\nand others. The chart includes tools such as API generators, content management systems (CMSes), libraries, and plugins.\n\nIf we look at the usage of these tools, the chart changes a bit, but Hasura is close to the top spot. It seems that developers prefer to use Hasura.\n\nThe above chart illustrates whether developers want to learn/would use a technology again or are not interested/wouldn't use it again. According to the chart, the majority of developers would like to learn how to use Hasura, or they would use it again in their projects.\nLooking at the chart, we can see that there are many tools available to build or support GraphQL APIs. Also, Hasura seems to be relatively popular when it comes to API generators.\nSchema Builders\nIt would be interesting to look at libraries and utilities developers use to build their GraphQL Schema.\nThe \"GraphQL Code Generator\" technology is the most popular and used technology. It's a tool that automatically generates code from your GraphQL schema and operations.\nOther notable technologies are:\n\nType-GraphQL - It enables you to create GraphQL schema and resolvers with TypeScript, using classes and decorators\ngqlgen - Go library for building GraphQL servers\nPothos - another TypeScript schema builder\nNexus\nSangria\n\nTo conclude, there are some excellent technologies to help you kickstart your GraphQL API development.\nOther GraphQL Tools\nThis section is all the tools that help with GraphQL development but don't fit in the previous categories. That includes tools that allow you to combine schemas, data sources, GraphQL IDEs, etc.\n\nThere are cases when you might have multiple GraphQL APIs you want to join together. There are a couple of tools to help you with that, one being Hasura Data Federation.\nAt the moment, Apollo Federation and Schema Stitching are leading the way, but it's worth keeping an eye on to see how things evolve.\n\nWhen it comes to the database used in conjunction with GraphQL, PostgreSQL leads the way, followed by MySQL, MongoDB, Redis, and others. We can also observe that developers prefer SQL databases when working with GraphQL.\nBy the way, if you wish to get started with PostgreSQL, check out the introduction to PostgreSQL tutorial.\n\nLastly, if we look at the chart, we can see that GraphiQL and GraphQL Playground are the most popular & used tools to query GraphQL APIs. It's expected since they are both designed specifically for GraphQL and do a great job!\nCheck out the public version of GraphiQL that you can use to share GraphQL queries - GraphiQL.\nSummary\nThe \"State of GraphQL\" survey gives us valuable insight into the GraphQL ecosystem! Check out the full survey results here.\nThe survey shows that GraphQL has matured over recent years and is also gaining ground outside the JavaScript ecosystem. Hopefully, in the next release of the GraphQL specification, we will also see features such as live queries and the @stream and @defer directives added.\nAll in all, the future looks great!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing a native Postgres integration to Hasura Cloud in partnership with Neon Description: We are excited to partner with Neon to launch a new native Postgres DB integration to Hasura Cloud. Author: Praveen Durairaju"}, {"role": "assistant", "content": "We are excited to partner with Neon to launch a new native Postgres DB integration to Hasura Cloud. With this new integration, users can go from signing up for Hasura Cloud to their first GraphQL API in under 2 minutes. In just one click, users can create a free Postgres database and automatically add it as a data source to their Hasura Cloud project.\n\nA simpler, faster getting started experienceSince the early days of Hasura, improving developer productivity has been core to our mission. With this native database integration, we have now removed friction in a key step of the getting started process \u2013 creating and connecting a database to your Hasura project. The new Neon Postgres integration replaces the current Heroku connection in Hasura Cloud. Before, you\u2019d have to go to Heroku to sign up and create a database to get a new database to try Hasura with. With the one-click Neon integration, you don\u2019t have to leave the Hasura dashboard to create and connect a new database. We expect this to be a much better experience for new users trying Hasura for the first time. More broadly, this integrated database + Hasura setup lets any developer create a new GraphQL backend in a seamless and quick way.Try it out on Hasura CloudStep 1: Create a new project on Hasura Cloud\nStep 2: Connect Neon Postgres DB in the Data tab to attach it as a Postgres data source.Head to the DATA tab in your Hasura Cloud project console and click on the Create New Database (Free) tab.Click on the Connect Neon Database button, and you will be prompted to Sign In to Neon. Select the Continue with Hasura option \u2014 this will automatically create an account for you and log you into Neon.Once you log in and accept the prompts, the serverless database will be provisioned quickly, and it will be attached as a default data source in your project. The Postgres connection string of the Neon DB will be available in the Hasura Cloud dashboard under project details in the PG_DATABASE_URL environment variable.Step 3: Create a table via the SQL tab (the table gets tracked automatically)CREATE TABLE public.message (\n    id integer NOT NULL,\n    username text NOT NULL,\n    text text NOT NULL,\n    \"timestamp\" timestamp with time zone DEFAULT now() NOT NULL\n);Step 4: Head to API explorer and start playing around with the GraphQL queries and mutations.query {\n  message {\n    id\n    username\n  }\n}Check out Hasura Docs on GraphQL mutations to learn how to insert data and play around with queries.A quick primer on NeonNeon is a new fully-managed serverless Postgres service with modern cloud-native architecture. Neon\u2019s architecture is built fundamentally with the notion of separating storage and compute for Postgres to bring numerous benefits like branching, auto-scaling and bottomless storage. For example, you can now scale CPU and I/O resources independently and run multiple compute instances of Postgres without copies of data. Learn more about Neon Postgres Architecture. Neon is open-source, written in Rust and built to be 100% compatible with Postgres. The serverless design lets the unused compute resources be shut down with a quick cold start of 2 seconds, thus saving on costs and scaling up when required.Neon offers a generous free tier for up to 3 Postgres projects. Neon is currently in closed preview with early access to limited users. We are excited to partner with them to bring the Neon serverless Postgres capabilities to the Hasura Cloud via this native integration.Why Hasura + Neon, from the creatorsCheck out the Hasura Hosts Neon session where Nikita Shamgunov, Anna Stepanyan, Heikki Linnakangas, and Tanmai Gopal talk about how serverless Postgres combined with automated GraphQL APIs enhance developer workflows and increase efficiency while building and scaling. This will also give a sneak peek into Neon\u2019s branching feature that opens up new possibilities to developers building apps requiring preview environments for databases.Help & resourcesIf you are facing any issues with the integration, file a support ticket by navigating to the Help & Support tab on the Hasura Cloud dashboard. Or join the Hasura Discord channel for community support and discussions. \u00a0For more on Neon, check out the docs on Neon integrationLooking to learn more about integration? We have an episode coming up on The Data and API show. So stay tuned!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: PostgreSQL 15 released: Top things to know Description: Postgres just announced the stable release of their latest major version, 15. Learn what has changed and try it out with Hasura today! Author: Praveen Durairaju"}, {"role": "assistant", "content": "Postgres just announced the stable release of their latest major version 15. I initially wanted to write how we added support in Hasura for Postgres 15 but that could have been a boring one since everything worked out of the box in our initial tests, although we are looking to leverage some of the Postgres 15 features soon.Try out Postgres 15 with Hasura Cloud and get Instant GraphQL APIs or check the docker compose setup in the post below to try it out locally.Let us look at the top features and changes in this release and how it will impact usage going forward.SQL MERGEThe MERGE command is one of the newest additions to the Postgres DB with the v15 release. It has been a standard feature on other SQL variants like SQL Server before. Take a look at the RFC on graphql-engine to see how the various similar operations are supported through the Hasura GraphQL API on SQL Server.Merge statements are a combination of INSERT, UPDATE and DELETE.Merge statement allows bringing in data from a source table into a target table.Let's understand MERGE use case with an example:SQL MERGE use caseIn the target table, there are 3 rows and the source table has 3 rows. The data from source table should be merged into the target table. There are a couple of conditions:MATCHEDNOT MATCHEDIn both the cases, the next execution is based on WHEN clause. For the above example, the operations for the first row can be DO NOTHING when matched since the data hasn't changed. For the second row, the product_quantity has changed and hence this will be an UPDATE operation. For the third row on the source table, it will be an INSERT operation since the row doesn't exist on the target table. The existing INSERT \u2026 ON CONFLICT feature on Postgres should cover most use cases. Upsert on Postgres is achieved using on conflict feature of the insert statement while Upsert behaviour on SQL Server is achived through a more general MERGE statement. With this v15 release, the same could now be used via Postgres since merge has more capabilities than INSERT ON CONFLICT feature.Instead of executing multiple INSERT / UPDATE / DELETE statements, the MERGE command lets you configure what to do when something matches and when something doesn't match, conditionally.CREATE Permission Revoked for public schemaPostgres 15 removes the global write privilege from the public schema. For example, if you are using a user which doesn't have superuser privileges, the CREATE statements on public will no longer work.# this doesn't work anymore\n\nCREATE table author (id integer, name text);You will get a permission denied error going forward. This is good from a security pov as the public schema is ab(used) for dumping everything but is also exposed on the search_path by default.Hasura supports read-only Postgres, so you if want to give access to public schema for just reads, it will continue to work as intended.Structured JSON LoggingPostgreSQL supports several methods for logging server messages, including stderr (default), csvlog, and syslog. With this release, jsonlog is also supported which is convenient for exporting logs and debugging. In order to enable this, add the jsonlog under the log_destination inside the postgresql.conf file.This will be a nice feature to have with exports to logging and monitoring tools like HoneyComb and DataDog etc.Performance ImprovementsThis release brings in a few performance improvements with managing workloads in both local and distributed deployments. There are improvements to the in-memory and on-disk sorting algorithms that show a performance jump of 25% to 400% depending on what type of data is being sorted.Try out Postgres 15 locally with docker composeIn case you want to run Postgres locally using docker, here is a quick docker compose that spins up latest Postgres and adds Hasura GraphQL APIs on top, all in a few seconds.Note that we are currently using the rc2 tag since the final stable release was not yet up on postgres docker at the time of writing this post.version: '3.6'\nservices:\n  postgres:\n    image: postgres:15rc2\n    restart: always\n    volumes:\n    - db_data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_PASSWORD: postgrespassword\n    ports:\n    - \"5432:5432\"\n  graphql-engine:\n    image: hasura/graphql-engine:v2.13.0\n    ports:\n    - \"8080:8080\"\n    depends_on:\n    - \"postgres\"\n    restart: always\n    environment:\n      ## postgres database to store Hasura metadata\n      HASURA_GRAPHQL_METADATA_DATABASE_URL: postgres://postgres:postgrespassword@postgres:5432/postgres\n      ## this env var can be used to add the above postgres database to Hasura as a data source. this can be removed/updated based on your needs\n      PG_DATABASE_URL: postgres://postgres:postgrespassword@postgres:5432/postgres\n      ## enable the console served by server\n      HASURA_GRAPHQL_ENABLE_CONSOLE: \"true\" # set to \"false\" to disable console\n      ## enable debugging mode. It is recommended to disable this in production\n      HASURA_GRAPHQL_DEV_MODE: \"true\"\n      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log\n      ## uncomment next line to set an admin secret\n      HASURA_GRAPHQL_ADMIN_SECRET: myadminsecretkey\nvolumes:\n  db_data:Next steps and resourcesIt is going to take a few days till official support for Postgres 15 comes up on Hosted Postgres vendors. Most vendors currently support the latest Postgres v14.4 stable release.Watch out for our recommendations on Top PostgreSQL database free tier solutions to see if any of them supports Postgres 15 (we will update the notes as and when they start supporting).Do check out our PostgreSQL Basics learn tutorial that goes over the core concepts around Postgres DB and how to make use of some of those with Hasura.Should you upgrade now? If you are already running production workloads on earlier versions of Postgres it is worth waiting for one more minor release to avoid any migration errors and data corruption that can potentially happen. On the other hand, if you are trying out a new Postgres instance, then definitely go for it!Further readingHasura and PostgresPostgreSQL basicsLearn Postgres for freeTop PostgreSQL database free tier solutionsTop serverless PostgreSQL providersTop psql commands and flags you need to know"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top PostgreSQL Database Free Tier Solutions Description: If you are looking for free solutions to host your Postgres database, you are in the right place! We did a bit of research for the best Postgres providers you can use for free. Author: Catalin Pit"}, {"role": "assistant", "content": "If you are looking for free solutions to host your Postgres database, you are in the right place! We did a bit of research for the best Postgres providers you can use for free. Everything works with Hasura Cloud too :)\nThese free tiers are great for side projects, MVPs, experimenting with Postgres, or learning Postgres. The providers were not added in any particular order so take your time to go through all of them!\nNote that this list is in no particular order and of course not comprehensive. Do let us know in comments if you know any other free tier Postgres vendor.\nNeon.tech\nNeon is a company that provides fully managed serverless Postgres databases. Being serverless, it automatically scales your database based on the project workload and only charges you for the amount of resources used.\nSince it's fully managed, Neon offers high availability without you having to worry about maintaining and scaling your database. It also enables you to create different branches of your database seamlessly. For instance, you can create test, staging, and dev branches.\nNeon comes with a free tier that offers you the following:\n\n10GB of storage\ncompute up to 1 vCPU and 256 MB\n3 projects (databases)\n\n\n\nCheck the free tier here.\n\nTry the Neon offering on Hasura by following the introduction guide.\nFly.io\nFly is a global distribution platform that allows you to deploy and run applications close to the users. You can also use Fly to deploy a Postgres database.\nAll Fly applications, including Postgres, rely on building blocks such as:\n\nflyctl - the CLI that allows you to interact with the Fly apps from the terminal\nprivate networking\nhealth checks\nvolumes\nlogs and metrics\n\nYou can have a Postgres cluster up and running within minutes using the flyctl CLI. Fly automates the cluster creation process and has some extensions to simplify cluster management.\nScaling your Postgres cluster is effortless. You can scale vertically (add more VM resources) or horizontally (add more replicas) with a single flyctl command.\nThe free Postgres tier on Fly comes with:\n\nup to 3 shared-cpu-1x 256MB VMs\n3GB of storage\n160GB outbound data transfer\n\n\n\nCheck the free tier here.\n\nElephantSQL\nElephantSQL is a service that offers PostgreSQL database hosting. It removes the tedious work by automating the setup and running of Postgres clusters. It also automates the tasks such as upgrades and backups.\nAll plans, including the free tier, come with 24/7 monitoring for availability and performance, automated backups, global distribution, and the ElephantSQL Web Browser for simple inspection and test queries.\nThe free tier on ElephantSQL comes with:\n\nshared high-performance server\n20MB of data\n5 concurrent connections\n\n\n\nCheck the free tier here.\n\nAmazon RDS\nRDS stands for Relational Database Service and is Amazon's managed SQL database service. You can use Amazon RDS to deploy and run a Postgres database.\nThe Amazon RDS comes with automatic backups and point-in-time recovery. That allows you to restore the database instance to any point in time (up to 35 days) if anything bad happens to your database. It also updates the database automatically.\nYou have complete visibility into metrics such as CPU, disk, and memory utilization. Lastly, you can scale the cluster with a few clicks to meet your demands and requirements.\nThe Amazon RDS free tier includes the following:\n\n20GB storage\nextra 20GB storage for backups and snapshots\n750 hours of DB usage\n\n\n\nCheck the free tier here.\n\nRender.com\nRender is a cloud company that offers fully managed PostgreSQL databases. The Postgres databases come with encryption by default, automated backups, and expandable storage.\nWith Render, you can have multiple databases in a single Postgre instance. Each database is backed up daily, and the backups are for at least 7 days.\nThe Render free tier includes the following:\n\n1GB SSD storage\n97 connections\nshared CPU\n256MB RAM\n\n\n\nCheck the free tier here.\n\nSummary\nThis article presented 5 free solutions to host your Postgres database:\n\nNeon\nFly\nElephantSQL\nAmazon RDS\nRender\n\nThese free tiers are better suited for hobby projects, MVPs, and learning/experimentations with PostgreSQL.\nIf you know other providers with free tiers, feel free to drop them in the comments. We're keen to learn about them!\nResources\nIn case you want to learn, experiment, or deepen your knowledge about PostgreSQL, we curated these resources for you:\n\nHasura and Postgres\nPostgreSQL basics\nPostgres 15 release: Top things to know\nLearn Postgres for free\nTop serverless PostgreSQL providers\npsql commands and flags you need to know\n\n\nNote: Heroku is deprecating their free Postgres tier on Nov 28, 2022. If you want to move from Heroku to one of these Postgres database vendors, check this article to learn how to migrate your Postgres database.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using Azure Cosmos DB for PostgreSQL & Hasura GraphQL Description: Microsoft announced GA for Azure Cosmos DB for PostgreSQL. This brings significant features to PostgresSQL on Azure by leveraging the power of Azure Cosmos DB. Because Azure Cosmos DB for Postgres is True PostgresSQL, this means you can now connect Hasura to Cosmos DB and get instant GraphQL APIs!  Author: Jonathan Marbutt"}, {"role": "assistant", "content": "Today Microsoft announced General Availability for Azure Cosmos DB for PostgreSQL. This brings significant features to PostgreSQL on Azure by leveraging the power of Azure Cosmos DB. Because Azure Cosmos DB for Postgres is True PostgreSQL, this means you can now connect Hasura to Cosmos DB and get instant GraphQL APIs! This blog post will provide a high-level overview of how to integrate Hasura with the new Azure Cosmos DB for Postgres.How to get started:It is easy to get started with both Hasura and Azure Cosmos DB for Postgres. Sign up for a Hasura Cloud account Create a project in the Hasura Cloud Account Sign up for the Azure Cosmos DB free trial and select Azure Cosmos DB for PostgreSQL Once you have completed the sign up for the trial on Azure, this will bring you to the Azure portalAfter you have created your trial account on Azure, you will need to reset the password to initialize the trial. You can do this by clicking the Important message in the Azure portal. Connecting Azure Cosmos DB to HasuraNow let's connect this new database to our Hasura project.In the menu on the left in the Azure Portal, select Connection StringsCopy the PostgreSQL connection URL, we will use this to place in our Hasura Project.In your Hasura cloud account, edit your project settings and select Env vars.Add a new Env var called DATABASE_URL and paste your connection URL into the value. Replace the {your_password} with the password you created when resetting the database.Save your env variable. Now we can launch our Hasura Console by clicking Launch Console in the top right.Go to the Data tab in the Hasura Console. Click Connect DatabaseGive the database a name and select Data Source Driver of Citus.Select Environment Variable for the Connect Database Via setting.Enter the Environment Variable of DATABASE_URLBuilding your Database Now that you have a Hasura Cloud instance and a database using Azure Cosmos DB for Postgres, you may begin by adding tables or following any of our other getting started guides just as you would with any PostgreSQL DB. Why Azure Cosmos DB for PostgreSQL?There are some great benefits to using Azure Cosmos DB for PostgreSQL, including the following:True PostgreSQL - Microsoft maintains this product with the latest version of PostgreSQL within two weeks of each release.Managed Cloud Database - Being built on Azure's managed services, you can take the worry out of managing databases. Microsoft Azure provides high availability configurations across availability zones. Powered by Citus - You can simply start with a small Free Trial and grow it into a highly scalable database with distributed tables. This will handle building a relational database at scale with distributed transactions, deadlocks, foreign keys, and more. Global Availability - Because Azure Cosmos DB for PostgreSQL is powered by Cosmos DB, you can choose to distribute your database globally with just a few clicks of a button. Conclusion Thanks to the outstanding work Microsoft has provided a product that lets you leverage Hasura on a massive global scale. The power of Cosmos DB meets PostgreSQL is an exciting combination that will allow applications scale from a few users to millions of users and adding Hasura, you now have a complete solution for any application you develop. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building real-time chat apps with GraphQL streaming subscriptions Description: With Hasura, you can create chat apps INSTANTLY straight from your data models without the need of any other infrastructure using streaming subscriptions. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Chat applications on the web or mobile are very common today. But, building a chat app usually \u00a0requires a tremendous amount of effort. On the one hand, you have many infrastructure concerns like requiring a communications protocol, websocket management, a message queue, persistence layer and on the other hand, you need to bake in custom logic to handle the security, performance and reliability of your messages.With Hasura, you can create chat apps INSTANTLY straight from your data models without the need of any other infrastructure using streaming subscriptions. The advantages of using streaming subscriptions on Hasura are:No additional infrastructure requirements apart from Hasura and Postgres. Works with any GraphQL client.Model-level authz that enables sending right events to right clients based on session and data properties.Scale reads the way you would scale Postgres reads, and scale writes the way you would scale Postgres ingestion.Easily scale Hasura to handle horizontal scaling of websocket clients. Benchmarked to handle 1M concurrent GraphQL clients streaming from 10000 channels with 1M events ingested every minute, on a single instance of Postgres RDS.Let\u2019s look at how streaming subscription works with a sample app. This is what our architecture will look like:Architecture of Realtime Streaming Chat AppCreate Data Models for Chat AppThe first step is to create data models that are required for the chat application. Let\u2019s go with a simple chat app where there are some channels and some users. A user needs to be in a particular channel to send and get messages in that channel.For this, we are creating two data models:messages: This stores the chat messages.Add Messages Tablechannel_users: This stores the mapping of channels and users. This is what we will use to model the permissions of our app.We can also create a relationship between messages and channel_users using channel_id as the linking field. We can do this either via foreign keys or manually - we will use manual relationships below and call this relationship recipients.Add a monotonic id to the messages modelOur messages table needs a monotonic id to identify the order of messages. Why? Streaming data depends on having a \u201cpointer\u201d or cursor to the last processed value. The next batch that is sent to the client would be based on moving ahead on the cursor. If we do not have a monotonic id then we may process a higher id before a lower id and miss streaming some event.Achieving monotonic ids is tricky to achieve in Postgres because of complex concurrency control. Even in serializable transaction mode, it is not necessary that two transactions that commit in the order of t1 then t2, will necessarily have ids that also follow the same order - serializability only means that there is some serial order not necessarily the commit order. Hence, we need to make an id that is exactly monotonic with the commit timestamp. We can do this in Postgres via advisory locks. Here is a trigger that you can add (via Run SQL in console) to your table so that it generates monotonic ids:CREATE sequence serial_chats;\n\nCREATE FUNCTION add_monotonic_id()\nRETURNS trigger AS $$\nDECLARE\nnextval bigint;\nBEGIN\n  PERFORM pg_advisory_xact_lock(1);\n  select nextval('serial_chats') into nextval;\n  NEW.id := nextval;\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER add_monotonic_id_trigger\nBEFORE INSERT ON messages\nFOR EACH ROW\n  EXECUTE FUNCTION add_monotonic_id();\nThere are more ways to model a monotonic id and depending on your application you may choose to use a different mechanism. One common performance improvement is to take the advisory lock on a partition value instead of a static value like above. This will work only if you need monotonicity over a partition rather than the entire table.Add permission rules to ensure messages can only be read by channel membersWe need to ensure that users can only read and write to channels that they are members of. We can add Hasura permission to the channels model by creating a role and then adding a row filter that checks that the recipients relationship must have the x-hasura-user-id from the incoming query.Setup Permissions for messages tablexTrying out streaming subscriptionsWith the data models and permissions now configured, we can try out a few queries to see that everything is working properly.In one tab, open a streaming subscription with the following query and role user and some x-hasura-user-id:subscription {\n  messages_stream(cursor: {initial_value: {id: 1} }, batch_size: 5) {\n    id\n    message\n    author_id\n  }\n}\nIn another tab, let's insert some messages for various channels. You can now see that the stream subscription only returns messages for the channels that the user-id is authorized for. Checkout the source code for the above data model here.Try out the Full Sample App with Live DemoWe have built out a realtime chat app that uses the Streaming Subscriptions feature in Hasura.It looks something like this:Realtime Chat App with Streaming Subscriptions Live DemoWhat the app does is, it fetches the last 10 messages from the messages table and then streams all new messages using the newly added Streaming API.A GraphQL mutation keeps running in the background to populate online users. A typing indicator is shown on the UI for which a GraphQL mutation is run and a subscription is run to fetch the latest user typing.The data model for this uses user and message table.Check out the Live DemoSource Code on GitHubLet us know in comments what other use cases you would like to see with Streaming Subscriptions. We have more example apps in the pipeline which covers both the frontend and backend parts of the app for building realtime apps."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: With Hasura, celebrate open-source in style with Hacktoberfest! Description: We are incredibly delighted to be a part of Hacktoberfest and are particularly pleased with how the open source community has united to create quality submissions and to thank the maintainers. Author: Estefani Moreno"}, {"role": "assistant", "content": "IntroHello October! My favorite spooky month. Continuing our tradition, we are back in 2022 for the fifth time! Hasura takes part in the Digital Ocean Hacktoberfest. We are incredibly delighted to be a part of this event and are particularly pleased with how the open source community has united to create quality submissions and to thank the maintainers.Getting startedHow to get involvedSubmit a PR for one or more of our issues mentioned below.Your PR will be reviewed within 2 weeks.If it is valid, we will either merge it or label it as \u201chacktoberfest-accepted\u201d.Our open source projectHere we present to you our open source repo that you can contribute to as part of this exciting year\u2019s challenge:The Hasura GraphQL Engine: The Hasura GraphQL Engine makes your data instantly accessible over a real-time GraphQL API, so you can build and ship modern, performant apps and APIs 10x faster. Hasura connects to your databases, REST and GraphQL endpoints, and third party APIs to provide a unified, connected, real-time, secured GraphQL API for all of your data.Valid PRs \ud83d\ude42If your PR properly addresses and solves an issue labelled as \u201chacktoberfest'', it's a valid PR. All valid PRs will get the label \u201chacktoberfest-accepted\u201d.Invalid PRs \u2639\ufe0fPRs that don't address and solve an issue labelled \"hacktoberfest\" are generally invalid. However, if you submit a PR that we consider valuable, we might still consider it for Hacktoberfest by adding the \"hacktoberfest-accepted label\". There are no guarantees for this though.DeadlineFor your PR to qualify for Hacktoberfest, you must submit it on or before October 31st.We will be reviewing PR submissions for the open issues until November 11th.Contributing guidesBefore you get started, please read through the contributing guide of the component you wish to work on:Docs (Markdown, Docusaurus)Community (multiple technologies)Learn (multiple technologies)Server (Haskell)Note: We don't encourage contributions to the Console code because of its complex architecture. GuidelinesWe've put together a few guidelines to make Hacktoberfest fun for everyone!Code of ConductA big part of open source is learning from and contributing to the community. It is vital that we create and maintain an inclusive environment so that everyone can grow and learn together.We ask that you read and adhere to our Code of Conduct thoroughly. Violations of it will not be tolerated and will result in disqualification from the challenge.Quality, not quantity, is our measureWe believe that you can create better results when you focus on doing fewer things but put more thought into them. While reviewing PRs, we also adhere to the official Hacktoberfest quality standards, as we trust quality trumps quantity. PRs that fail to meet these quality standards will not be eligible for completion.Winning swagA t-shirt and a sticker pack is sent to everyone who submits a valid contribution.We\u2019ll reply with a comment to your PR asking you to fill out an order form.Keep calm and win!The Hasura Community is excited to have you all participate in yet another celebration of open-source!Follow us on Twitter to keep an eye out for updates & join us on our Discord to come to hang out with the rest of the Hasura community!Cheers to a new month and another chance for us to Git the festivities started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Heroku Free Tier Deprecation | Migrating to other DB vendors Description: In this post, we will look at how to take a backup of your Heroku Postgres DB and Hasura Metadata / Migrations so that you can migrate to a different DB vendor. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Heroku recently announced that they are deprecating free resource plans, starting November 28, for their Heroku Dynos, Heroku Postgres among others.What does this mean for you as a Hasura Cloud user?You will need to either upgrade your Heroku account or migrate your Heroku Postgres database to a different database provider before November 28.In this post, we will look at how to take a backup of your Heroku Postgres DB and Hasura Metadata / Migrations so that you can migrate to a different DB vendor.Export Heroku Postgres DB using pg_dumpIf you are on Heroku free tier Postgres, you can make use of standard Postgres tools like pg_dump and psql to export/import the database.Find the Heroku Postgres connection string of the DB attached to your Hasura Cloud instance. Head to Hasura Cloud Dashboard and navigate to the project details -> ENV vars section. Under Custom ENV vars, look out for PG_DATABASE_URL env. It should have the Postgres Connection String of the Heroku DB.If you have multiple Heroku Postgres instances connected to your Hasura Cloud project, then the ENVs would follow the pattern of PG_DATABASE_URL* where * would represent the number of such connections. You will get an ENV value that looks something like the one below:Heroku Postgres Connection String Hasura Cloud ENVThe connection string is of the format:postgres://<username>:<password>@<host>:5432/<db-name>Copy the individual sections of the connection string if required. But since we are using pg_dump to take a backup, we can simply pass the connection string directly.pg_dump <connection-string> > dump.sqlThis dump.sql will now have all the schemas along with the data in the database.Import Database Dump You can now import this SQL into the database vendor of choice. Some of the popular vendors that give a free tier Postgres Database (with limits).Fly.ioRenderElephantSQLRailwayThere are more of course and you can choose the vendor and import the SQL into their instance.Hasura CLI Migrations and MetadataIf you already have a Hasura CLI project that contains migrations and metadata, you can skip this step. In case you don't, follow the below steps to create migration files and export metadata from your Heroku Postgres instance.# create a project\nhasura init\ncd hasura\n\n# create an aggregate migration file from the SQL dump\nhasura migrate create init --sql-from-file <path-to-dump.sql>\n\n# head to migrations directory to mark the generated one as applied\ncd migrations/default\n\n# copy the version number of the first migration directory/file\nhasura migrate apply --endpoint <hasura-project-url> --admin-secret <admin-secret> --version 1627564572630 --skip-execution\n\n# export metadata\nhasura metadata export --endpoint <hasura-project-url> --admin-secret <admin-secret>\nIn case you have more than one such free Heroku Postgres DB attached to your instance, repeat the above steps for the pg_dump and the Hasura CLI migrations on the same project with the db name being different.Connect Hasura Project to New DBThis will involve the following steps to remove the old DB, connect the new DB and apply hasura metadata.Remove the old Heroku Postgres DB connection.Connect the new database as a data source in HasuraApply migrations (skip execution) and apply metadata.Remove the old Heroku Postgres DB connection.In your project's Hasura Console, head to Data tab. Click on Remove button for the Heroku Postgres DB source that is already added. Type the source name to confirm and delete.Connect new database as a data source in HasuraOnce the old connection is removed, add the new DB connection string to your Cloud Project's ENV vars so that it can be referenced in the Hasura Console Data tab connect database step.Head to Cloud Dashboard -> Project Details -> ENV Vars and update the connection string of PG_DATABASE_URL to point to the new value.Apply migrations and metadataOnce you have imported your DB to a new vendor, you can apply the migrations and metadata to the same project.# apply migrations but skip execution\nhasura migrate apply --endpoint <hasura-project-url> --admin-secret <admin-secret> --version 1627564572630 --skip-execution\n\n# apply metadata\nhasura metadata apply --endpoint <hasura-project-url> --admin-secret <admin-secret>\n\n# reload metadata\nhasura metadata reload --endpoint <hasura-project-url> --admin-secret <admin-secret>\nThat's it! You can now verify if the new connection and the API is working by heading to Hasura Console API Explorer and trying out a GraphQL query.Next stepsWe have guides for connecting to various cloud database vendors in your Hasura Cloud project apart from the free tier Postgres vendors mentioned in this post above.In case you are stuck with the migration, ping us or the community on Discord or create a GitHub issue for help.We have an exciting announcement to make soon around a native DB integration that will have a similar or better experience than connecting to Heroku Postgres. Watch out for the announcement :)"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing General Availability of BigQuery with Hasura Description: We recently announced the general availability of BigQuery in Hasura! Now you can connect a BigQuery database to your Hasura application to consume data. Author: Catalin Pit"}, {"role": "assistant", "content": "Introduction\nWe recently announced the general availability of BigQuery in Hasura! Now you can connect a BigQuery database to your Hasura application to consume data.\nThis article teaches you how to:\n\nconfigure BigQuery with Hasura\nconnect BigQuery database to Hasura\nset up relationships between BigQuery tables\nuse Hasura permissions to perform data validation\n\n\nSupported versions: Hasura GraphQL engine v2.0.0-alpha.1 onwards.\n\nConfigure BigQuery with Hasura\nTo connect a BigQuery database to Hasura, you need a \"service account key\" file. That file contains the credentials required by Hasura to connect to the database.\nNavigate to the project's settings, create the BIGQUERY_SERVICE_ACCOUNT environment variable and set it to the content of the \"service account key\" file.\n\nIf you need help obtaining your service account key, check this section in the documentation.\nConnect BigQuery Database to Hasura\nNavigate to the Connect Existing Database page in the project console to set up the database:\n\nChoose a name for the database\nChoose Big Query for the \"Data Source Drive\"\nSelect the Environment Variable option\nEnter the newly created environment variable BIGQUERY_SERVICE_ACCOUNT\nEnter your GCP project id\nEnter the dataset (or datasets)\n\n\nAfter connecting the database, you should be able to track the tables from the specified dataset. Click on \"Track All\".\n\nThe database is now ready! You can perform GraphQL queries on your data. For a more comprehensive guide on getting started with BigQuery on Hasura Cloud, check the documentation.\nLet's test the integration with the following query:\nquery authors {\n  publication_authors {\n    id\n    name\n  }\n}\n\nRunning the query returns a list of all authors from the database, as shown in the figure below.\n\nYou can query authors and articles individually, but there is no relationship between them. For example, you cannot retrieve all authors and their articles.\nSet up relationships between BigQuery tables\nNested object queries refer to fetching data for a type and data from a nested or related type.\nTo make nested object queries, you need to set up relationships between the two tables - authors and articles.\nCreate array relationship\nNavigate to the Relationships page in the authors table and click on the \"Configure\" button.\n\nThat opens a new section where you can configure the relationship. Configure it as follows:\n\nChoose Array Relationship (one-to-many) for the \"Relationship Type\" field\nName your relationship - e.g. articles\nLeave the \"Reference Schema\" field as it is\nChoose articles for the \"Reference Table\" field\nSelect id for the \"From\" field and author_id for the \"To\" field\nSave the relationship\n\n\nLet's test the relationship by running the following query:\nquery author_articles {\n  publication_authors {\n    id\n    name\n    articles {\n      id\n      title\n      published_on\n      body\n    }\n  }\n}\n\nRunning the query returns all the authors and their articles, as illustrated in the image below.\n\nCreate object relationship\nThe next step involves creating an object relationship between the articles and authors tables. Configure the relationship as follows:\n\nChoose Object Relationship (one-to-one) for the \"Relationship Type\" field\nName your relationship - e.g. authors\nLeave the \"Reference Schema\" field as it is\nChoose authors for the \"Reference Table\" field\nSelect author_id for the \"From\" field and id for the \"To\" field\nSave the relationship\n\n\nLet's test the relationship by fetching the articles and their authors.\nquery articles_author {\n  publication_articles {\n    id\n    title\n    published_on\n    body\n    author {\n      id\n      name\n    }\n  }\n}\n\nThe image shows the list of articles and the id and name of each article's author.\n\nBy this point, you set both object and array relationships. These relationships enabled you to perform nested queries. Check the documentation on BigQuery: nested object queries for more information.\nData validation with BigQuery\nEven though BigQuery does not support constraints natively, you can use Hasura permissions to perform data validation.\nWith this example application, let's consider the following scenarios:\n\nauthors should only be able to access their details\nauthors should only be able to access their articles\n\nNavigate to the \"Permissions\" tab in the \"authors\" table and add the author role.\n\nClick on the \"X\" icon to open the configuration section and add the following custom check:\n{\n  \"id\": {\n    \"_eq\": \"X-Hasura-User-Id\"\n  }\n}\n\nThen toggle all the columns and save the permissions.\n\nBefore performing any queries, you need to set the X-Hasura-Role and X-Hasura-User-Id headers to the author role and the author's id, respectively.\nOnce the headers are in place, you can fetch the author's details.\nquery getAuthor {\n  publication_authors {\n    id\n    name\n  }\n}\n\nThe x-hasura-user-id header is set to \"1\", meaning the query returns the author's details with the user_id of \"1\".\n\nSimilarly, configure the author role for the \"articles\" table as follows:\n{\n  \"author_id\": {\n    \"_eq\": \"X-Hasura-User-Id\"\n  }\n}\n\nThen toggle all the columns and save the permissions.\n\nLet's test the permissions by running the following query in the GraphiQL editor:\nquery getArticles {\n  publication_articles {\n    id\n    title\n    published_on\n    body\n    author {\n      id\n      name\n    }\n  }\n}\n\nIt should return the author's articles specified in the x-hasura-user-id header. The image illustrates that it works as expected.\n\nIf you remove the x-hasura-user-id header, Hasura returns an empty array.\nThis is how you perform data validations with the Hasura permission system. If you want to read more, check the documentation on BigQuery: Data Validation.\nNext steps\nThere is also a video that covers the topic. You can watch it here:\nWe would love to hear about your use cases with BigQuery. Let us know in the comments!\n\nNote: Check the documentation for information on the features supported.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How automating PR review infra helped Hasura with shipping docs Description: In this post, we look at how our PR infrastructure was automated to help us deliver quicker reviews and help the developer community with good turnaround times. Author: Rob Dominguez"}, {"role": "assistant", "content": "The need for automationIn all fields, repetitive tasks abound. However, in the software industry, we're fortunate (or cursed) to have the knowledge and tools to automate away different aspects of our work. Hasurians were recently recommended the book Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations (Forsgren et al., 2018). At the heart of this book is the message that what can be automated should be automated.Accelerate focuses on several key points as to why we should automate tasks:Humans get bored: computers succeed at doing simple tasks quickly. On the other hand, humans are problem solvers. If we can offload repetitive tasks to a piece of software, we are free to focus on more engaging and exciting work.Humans are prone to error: especially when we're bored! With a concrete set of conditional logic, given the right APIs, most simple tasks can be handled by a piece of software.Humans can only sustain a pace for so long: critical to high-performing organizations is the ability to regularly deploy with as little friction and fatigue as possible. Computers should take as much of this load as possible to help humans maintain cadence.It's no surprise that these points overlap and feed into each other. An engineer, bored with work, is bound to make mistakes. Those mistakes trip up a release cadence and cause the organization to perform sub-optimally. This pattern repeats itself, and - before long - burnout overcomes this engineer.As Hasura is OSS at heart, the Documentation (Docs) team receives a lot of PRs from both the community and members of Hasura. With the other tasks we're working on during a sprint, it can be challenging to keep up with the incoming review requests. Add to this the firehose-level deluge of services competing for our attention and pinging us about new PRs (GitHub notifications, internal messages on Slack, Discord, Gmail), and we're bound to miss something.How to automate your PR infrastructureWe utilize Jira to track tasks during a sprint and map out our upcoming work. When designing this automation, we knew we wanted to achieve several goals:Create a single source of truth: instead of parsing through notifications from different services, we wanted to be able to look at our board and know what PRs were up for review and dependent on us.Offload the responsibility: instead of tasking team members with periodically checking for new PRs, we wanted to free ourselves from the repetitive grind of creating an issue, linking to the PR, and adding it to the board.Deliver quicker reviews: instead of letting a PR sit in some limbo state, we wanted to provide feedback as quickly as possible to ensure updates merged swiftly.To meet these goals, we designed a GitHub Action. This Action looks for PRs containing changes to files in the /docs directory of the Hasura GraphQL Engine and automatically creates a ticket in Jira for us.What we'll build below is a model of that same Action. We'll utilize Jira and GitHub Actions to build an automated PR-ticketing system for easy capture and tracking. While it's helpful to be familiar with YAML, it's an easy language to pick up and is intended to be human-readable. Follow the steps below to start automating your PR review infra \ud83e\udd19Step 1: Generate a token and store secretsTo begin, you'll need a Jira API token. You'll use this token to authenticate your Action and link your Jira board with a GitHub repository. Generating a token is a straightforward process that Atlassian explains in their documentation; follow these steps.Once you've followed the steps above, we need to store this and other pertinent information inside the repository's secrets. GitHub secrets are used similarly to environment variables. Instead of hardcoding these values in our Action, we'll reference them using a familiar syntax and ensure our API key is safe. The three pieces of information we'll store as secrets are:JIRA_API_TOKEN: The Jira API token you generated earlier.JIRA_BASE_URL: The Jira board's base URL.JIRA_USER_EMAIL: The email address associated with the user account generating the ticket (most likely yours or a service account belonging to your organization).You can access your repository's secrets by going to the repository's settings. From here, in the Security section, choose Secrets and then Actions.Step 2: Write the Action boilerplateGitHub Actions all follow a common implementation pattern. Within any repository, you'll begin by creating a .github directory and - within this folder - a subdirectory called workflows. This workflows directory is where your Actions will live.Design of Actions should be limited in scope to unique tasks or jobs you're seeking to automate. You may have numerous Actions listed as separate YAML files within this workflows directory.Begin by naming the Action descriptively (as these names will appear whenever your CI runs). For our example, we'll create a new file called create-docs-ticket.yml and fill it with the following boilerplate to begin:name: create docs ticket\non:\n  pull_request:\n    types: [opened]\n    paths:\n      - 'docs/**'\n\njobs:\n  create-docs-ticket:\n    runs-on: ubuntu-latest\nThe name key is a copy of the kebab-cased filename we used earlier. Actions allow us to utilize different events as triggers. For our purposes, we're concerned with new PRs opened on our repository; thus, we're using the [opened] value on the types key. Finally, the jobs listed are the set of tasks, in order, that need to run for our automation to occur.To ensure we only generate tickets on updates to files in the /docs directory, we utilize the paths key and tell the Action we only want the following jobs to run if a PR makes changes to files - of any type - inside this directory.Step 3: Bring in Jira templatesWe don't have to write all of our Actions' logic from scratch. A marketplace exists where you can find numerous templates and Actions that take care of specific, common tasks. GitHub designed Actions to be modular, so we can quickly piece together prebuilt Actions that solve common problems and create unique solutions. We'll utilize two Atlassian-generated Actions to authenticate who we are and then make the Jira ticket:jobs:\n  create-docs-ticket:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Login\n        uses: atlassian/gajira-login@master\n        env:\n          JIRA_BASE_URL: ${{ secrets.JIRA_BASE_URL }}\n          JIRA_USER_EMAIL: ${{ secrets.JIRA_USER_EMAIL }}\n          JIRA_API_TOKEN: ${{ secrets.JIRA_API_TOKEN }}\n\n      - name: Create ticket\n        id: create\n        uses: atlassian/gajira-create@master\nUsing the above values will ensure we can authenticate to Jira and have the necessary permissions to generate a ticket on our board. From here, we must tell the Action to which board it should add the ticket and which values to populate inside it. Our board is named DOCS, so we use that as the value on project. You should update this to the name of your board / project in Jira. Actions can also handle dynamic values, so we can reference information from the PR using strings for the summary and description keys:jobs:\n  create-docs-ticket:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Login\n        uses: atlassian/gajira-login@master\n        env:\n          JIRA_BASE_URL: ${{ secrets.JIRA_BASE_URL }}\n          JIRA_USER_EMAIL: ${{ secrets.JIRA_USER_EMAIL }}\n          JIRA_API_TOKEN: ${{ secrets.JIRA_API_TOKEN }}\n\n      - name: Create ticket\n        id: create\n        uses: atlassian/gajira-create@master\n        with:\n          project: DOCS\n          issuetype: Review\n          summary: Review ${{ github.event.pull_request.title }}\n          description: ${{ github.event.pull_request.html_url }}\nIn all, the Action file should look like this:name: create-docs-ticket\non:\n  pull_request:\n    types: [opened]\n    paths:\n      - 'docs/**'\n\njobs:\n  create-docs-ticket:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Login\n        uses: atlassian/gajira-login@master\n        env:\n          JIRA_BASE_URL: ${{ secrets.JIRA_BASE_URL }}\n          JIRA_USER_EMAIL: ${{ secrets.JIRA_USER_EMAIL }}\n          JIRA_API_TOKEN: ${{ secrets.JIRA_API_TOKEN }}\n\n      - name: Create ticket\n        id: create\n        uses: atlassian/gajira-create@master\n        with:\n          project: DOCS\n          issuetype: Review\n          summary: Review ${{ github.event.pull_request.title }}\n          description: ${{ github.event.pull_request.html_url }}\n\n      - name: Log created issue\n        run: echo \"Issue ${{ steps.create.outputs.issue }} was created for docs team to review your PR \ud83e\udd19\"\nStep 4: Test it outAt this point, you're ready to go! Give it a run by creating a PR that modifies a file based on whatever condition you set. Whenever someone opens a PR, you should see the Action run as a part of your repository's CI. Head over to your Jira board and see if there's a new ticket waiting for you!Community-facing resultsAs a team, we're happy with the results. Not only have we decreased our lower-order workload, but the first-order implications for those we work with and all of you have been positive. While we have a steady stream of backlogged issues and PRs to work through, we know this measure will improve our responsiveness to all new PRs opened since its introduction.Internal resultsOne recent trend within the company has been how we collectively examine and provide feedback on pull requests. As a core value, ownership plays a vital role in ensuring we hold ourselves accountable and continue to ship improvements to the product. While everyone within our team is listed as a 'code owner' to the files within /docs, we don't operate with one collective brain. Simply assigning Docs to review changes to documentation leaves some room to be desired. To put it bluntly, \"The fastest way to starve a horse is to assign two people to feed it.\"With that in mind, we'll be taking our GitHub Action one step further. The Docs team has two engineers; we've split the load to be \"on call\" for new PRs. Using Jira's REST API, our Action will eventually determine who is on call whenever a new PR is opened and assign that engineer to be the reviewer.Put it to the test!We want you to create some PRs and put this Action to work! With Hacktoberfest starting soon, the different teams at Hasura have been hard at work creating good-first-issues. We in the Docs team could certainly use some help from all of you, especially first-time contributors! If you've never contributed to open-source, consider checking out our list of issues. We're here to help and appreciate your support!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura GraphQL on Your Hydra Data Warehouse Description: In this post, we will discuss how to Instant GraphQL APIs on Hydra using Hasura. Hydra is an open-source data warehouse built from the ground up on Postgres. Author: Catalin Pit"}, {"role": "assistant", "content": "Developers around the globe have trusted Hasura\u2019s GraphQL Engine. Hasura provides an extremely lightweight, high performance product that gives you instant realtime GraphQL APIs on a Postgres database.\nHydra is the open source data warehouse, built from the ground up on Postgres. With Hydra\u2019s columnar tables, Postgres can now execute analytic reads 30X faster and with 4X better data compression than standard row-based tables. Together with Hasura, Hydra now supports: Instant GraphQL & REST APIs, Declarative Role Based Authorization, Advanced Security and Performance with rate limiting, allow lists, and caching. Get your open source data warehouse at hydras.io. Email founders@hydras.io with the subject line \u201cHasura\u201d to bypass the Hydra sign-up waitlist.\nWe'll cover:\n\nInstant GraphQL APIs on Hydra using Hasura\nAuthentication and Authorization\nRun GraphQL on a Columnar Table\nAdd Caching to GraphQL API over Hydra\n\nHYDRA: The Open Source Data Warehouse\nA Source of Truth Built on Postgres\n\nA data warehouse aggregates data from different sources into a single, central, consistent data store to support data analysis, data mining, artificial intelligence (AI), and machine learning. Data warehouses are referred to as an organization\u2019s source of truth because comprehensive business problems can be solved when querying and analyzing the full data set across multiple dimensions. While data warehouses have traditionally been deployed in enterprise, early-stage teams can also see business intelligence (BI) advantages to aggregating multiple data sources, such as tracking a prospects journey across marketing campaigns, sales opportunities, product engagement, and retention. Hydra brings the source of truth advantages to every organization by offering an open source data warehouse built on Postgres.\nAs Hydra is built on Postgres, developers can use the full Postgres ecosystem with their data warehouse. All existing Postgres tools, database extensions, language drivers, data pipelines, and ORMs just work.\nInstant GraphQL APIs on Hydra using Hasura\nHasura is an open source GraphQL engine that gives you instant GraphQL and REST APIs over multiple data sources. Postgres compatible DBs are natively supported and Hydra being a Postgres compatible data warehouse, can be connected with Hasura as a data source to get GraphQL APIs instantly. Hydra supports both row and columnar tables and the instant GraphQL APIs work out of the box for both.\nApart from the instant CRUD APIs, Hasura comes with built-in support for Authorization, Triggers on Database events and the ability to extend your GraphQL API with custom schema via Remote Schemas and Actions.\nFinally, you can also join the data across different data sources. Imagine joining data between two data warehouses on Hydra! This is possible via GraphQL Joins that lets you combine data from different database tables and servers in the same GraphQL query.\nLet\u2019s look at a quick demo to connect Hydra with Hasura GraphQL Engine.\nRUN HYDRA AND HASURA USING DOCKER COMPOSE\nCopy the following docker-compose.yaml to your local machine for setting up the environment.\nversion: '3.6'\nservices:\n  postgres:\n    image: postgres:14\n    restart: always\n    volumes:\n    - db_data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_PASSWORD: postgrespassword\n    ports:\n    - \"5432:5432\"\n  hydra:\n    platform: linux/amd64\n    image: ghcr.io/hydrasdb/hydra\n    restart: always\n    environment:\n      PGPASSWORD_SUPERUSER: hydra\n    ports:\n    - \"6432:5432\"\n  graphql-engine:\n    image: hasura/graphql-engine:v2.11.1\n    ports:\n    - \"8080:8080\"\n    depends_on:\n    - \"hydra\"\n    restart: always\n    environment:\n      ## postgres database to store Hasura metadata\n      HASURA_GRAPHQL_METADATA_DATABASE_URL: postgres://postgres:postgrespassword@postgres:5432/postgres\n      ## this env var can be used to add the above postgres database to Hasura as a data source. this can be removed/updated based on your needs\n      PG_DATABASE_URL: postgres://postgres:hydra@hydra:5432/postgres\n      ## enable the console served by server\n      HASURA_GRAPHQL_ENABLE_CONSOLE: \"true\" # set to \"false\" to disable console\n      ## enable debugging mode. It is recommended to disable this in production\n      HASURA_GRAPHQL_DEV_MODE: \"true\"\n      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log\nvolumes:\n  db_data:\n\nNow run docker compose up -d to start Hasura and Hydra along with an isolated Postgres instance for handling Hasura metadata.\nCONNECT HYDRA WITH HASURA\nHead to Hasura Console to establish the Hydra data source as a connection.\n\nClick on \u201cConnect Database\" in the end. Once the data source is added successfully, you can start creating tables and inserting rows to play around with the API.\nEXPLORE GRAPHQL CRUD ON HYDRA\nWe can start by creating a table on Hasura Console. The console has a UI to create tables or if you prefer raw SQL, head to the SQL page on the Data tab and execute the following to create the table:\nCREATE TABLE public.message (\n  id bigserial NOT NULL,\n  username text NOT NULL,\n  text text NOT NULL,\n  \"timestamp\" timestamp with time zone DEFAULT now() NOT NULL\n);\n\nThis will create a table called message with id, username, text and timestamp columns, a basic realtime chat app schema used for ingesting and consuming large amounts of data.\nHead to the API Explorer tab to see the generated CRUD APIs for the above table:\n\nNow, let\u2019s insert some data using GraphQL mutations:\nmutation insertMessage {\n  insert_message_one(object: {\n    text: \"Hello Hydra!\",\n    timestamp: \"now()\",\n    username: \"praveenweb\"\n  }) {\n    id\n  }\n}\n\nNote that, we have an auto-generated GraphQL mutation that now inserts data into Hydra.\nLet\u2019s fetch the inserted data via a GraphQL query.\nquery fetchMessages {\n  message {\n    id\n    username\n    text\n    timestamp\n  }\n}\n\nThis will give back a response that looks like:\n{\n  \"data\": {\n    \"message\": [\n      {\n        \"id\": 1,\n        \"username\": \"praveenweb\",\n        \"text\": \"Hello Hydra!\",\n        \"timestamp\": \"2022-09-06T17:29:16.724629+00:00\"\n      }\n    ]\n  }\n}\n\nWe have now tried both GraphQL queries and mutations! What about realtime stream?\nJust substitute the query with subscription and add _stream to the GraphQL field and you can now start streaming realtime data! \ud83e\udd2f\nHere\u2019s a sample subscription stream that you can try out:\nsubscription fetchMessages {\n  message_stream(cursor: {initial_value: {timestamp: \"2022-09-01\"}, ordering: ASC}, batch_size: 1) {\n    id\n    username\n    text\n    timestamp\n  }\n}\n\nThis would stream all the rows from the above cursor timestamp value, to the client. Learn more about how backpressure and scaling is handled when a large influx of data comes in realtime.\nAuthentication and Authorization\nHasura supports Authentication via JWT and custom webhooks. In order to expose the GraphQL API to a public facing client, you need to start defining Authorization rules to be able to restrict access to data. With a data warehouse use case, different administrators can access different data sets in an app. It is important to be able to define role based access control (i.e RBAC) to allow / restrict data access for these large amounts of data.\nHasura supports a declarative Authorization layer over Hydra DB tables and you can define row level and column level permissions for the same.\nFor example, in the above realtime streaming app use case, if we want to restrict access for users to the messages stream to access only their own data, we could set up a role based permission for select access on table message.\nHead to the \u201cPermissions\u201d tab of the messages table and apply a row level check like where the X-Hasura-User-Id would come in from a JWT custom claim. Add the following custom check\n{\"username\": {\"_eq\": \"X-Hasura-User-Id\"}}\n\n\nMoving on to column level permissions gives you the ability to hide data from selected columns to be available on the API.\n\nThe next time you make a request with a JWT token containing the X-Hasura-User-Id values in the claims, the permission rules will be applied automatically and only the authorized data will be available for access on the client. Read more about Authorization with Hasura.\nRun GraphQL on a Columnar Table\nINSERT SAMPLE DATA WITH PGBENCH\nAfter setting up tables and AuthZ rules for the API, let\u2019s setup some performance benchmarks to test Hydra\u2019s performance with a Hasura GraphQL API.\nWe will make use of pgbench to generate 5 million records quickly, to perform a small benchmark. Replace the last argument -s to insert more records.\npgbench -h localhost -p 6432 -U postgres postgres -i -s 50\n\nThis will generate tables named pgbench_accounts among others. Head to the Console -> Data -> public schema and track the table pgbench_accounts.\nCREATE COLUMNAR TABLE AND INSERT RECORDS\nOnce we have the records for testing, let\u2019s make use of columnar storage in Hydra to reduce the storage size and optimize for performance. Head to Console -> Data -> SQL tab to execute the following statements:\nCREATE TABLE pgbench_accounts_columnar\n(LIKE pgbench_accounts)\nUSING COLUMNAR;\n\nand follow it up with the inserts.\nINSERT INTO pgbench_accounts_columnar\nSELECT * FROM pgbench_accounts;\n\nHasura can now be used to configure Authorization rules on these columnar tables as well.\nOnce the columnar inserts are done, you can verify the compression ratio by executing the following command:\nSELECT \n    pg_total_relation_size('pgbench_accounts')::numeric / \n    pg_total_relation_size('pgbench_accounts_columnar')\n  AS compression_ratio;\n\nThese records are simple data and the compression ratio turns out to be ~50x.\nQUERY USING GRAPHQL API\nThe tables are already tracked by Hasura, including the one with columnar storage, we can try out the GraphQL APIs to query the data.\nYou can execute the following GraphQL query to fetch all the records.\nquery {\n  pgbench_accounts_columnar {\n    aid\n    bid\n    abalance\n  }\n}\n\nThe pgbench_accounts_columnar table will have columnar storage enabled and would result in smaller storage size and faster performance for large amounts of data, i.e. millions of records.\nThis is just the beginning of what you can do with the data warehouse now. Using a columnar storage already results in faster performance for large data sets. In case you are fetching smaller data sets, you could make use of GraphQL Query Response Caching with Hasura to speed up queries.\nAdd Caching to GraphQL API over Hydra\nHasura Cloud supports query response caching that gives faster response times by automatically caching authenticated data. Going back to the message query that we used earlier to fetch the latest message stream of a user; We can quickly add an @cached directive to the query to start caching data, thus reducing latency and a roundtrip to Hydra.\nFor example:\nquery fetchMessages @cached {\n  message {\n    id\n    username\n    text\n    timestamp\n  }\n}\n\nThis will cache the response for the first time, and any subsequent hits will retrieve data from the cache that Hasura Cloud maintains.\nThere\u2019s a whole bunch of security configurations that can be added to your API and you can monitor them on Hasura Cloud.\nCombining Hasura GraphQL API with Hydra enables a clean data access layer over a large data warehouse, aggregating data from multiple sources with a secure Authorization model and a performant API at both the database layer and at the API layer with caching.\nBreak the Lock!\nAt Hydra, we believe that an organization\u2019s source of truth must be built on an open source foundation. Hydra users have the control to create a pull request, the visibility to flag issues or review Hydra code, and is accessible to every developer to use freely and without corporate permission on their local environment.\nOpen source is the standard that drives our product development and decisions. People can build amazing things when you give them equal footing with the best tools available. We don\u2019t believe your source of truth should sit behind proprietary software, do you?\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Hasura User Groups (HUGs) Description: Hasura User Groups are a collective of Hasura user meetups in various locations around the world run by local Hasura champions. Author: Harsha"}, {"role": "assistant", "content": "Meetups are a great way to connect with other folks in your local community who share the same passions and interests as you. Tech-focused meetups are no different \u2013 they are a quicker, and typically more fun way, to learn, share and grow your knowledge in a particular tool or topic. We have seen the magic and energy of in-person Hasura user meetups before the great pandemic forced us all to go virtual.Now with the world moving back to in-person, we want to go back to the old days and create more opportunities for local Hasura users to come together. It\u2019s also something we are seeing renewed interest from community members. And so with that context, we have an exciting announcement! We are thrilled to announce Hasura User Groups \u2013 or HUGs for short \u2013 a collective of Hasura user meetups in various locations around the world.Our goals with HUGs to build and nurture local Hasura user communities \u2013 either directly or by working with and enabling local Hasura champions. You can see a list of HUGs that are already active in different cities across the world here.So, what\u2019s in a HUG?If you are a Hasura user, or are broadly interested in learning more about how Hasura can fit into your tech stack or solve your problem, a HUG is a great local resource for you.Meet other Hasura users: find out about the unique ways they\u2019re using Hasura, learn from and help them solve the challenges they\u2019re facing, and maybe even find someone to join your team :)Meet the Hasura team We\u2019ve got engineers and employees all over the world \u2013 HUGs are a great way to connect with Hasura team members in your city.Teach Love to spread knowledge? Meetups are a great way to demo (and even run workshops) on what folks can do with HasuraMeet GraphQL community: Hasura users are typically also GraphQL users: invite them over and talk about all things GraphQLHUG meetup structure and formatsWe want the local HUG organizers to have flexibility to structure the HUG to suit the preference and needs of the local community \u2013 as long as it broadly fits into the Hasura / GraphQL theme. So a HUG meeting might take various flavors:Talks - A standard meetup format, where invited speaker to talk about what they are doing with HasuraGroup learning sessions - A small group of Hasura users get together to dive into and learn a particular concept, feature or product area together.Workshops - Typically a hands-on affair where HUG members can explore a relevant topic in-depth and do some live-coding.Watch parties - Hasura hosts a number of live streamed events (webinars, conferences, etc). Get your local HUG together, to watch it in a group and chat about it after!Want to organize a HUG in your city?While the Hasura team will be organizing and running HUGs in many locations, we would love help from the massive global Hasura community to run HUGs in your area (with ample help from us, of course).How to become a HUG organizerWe got you covered with this in-depth HUG Organizer\u2019s Guide, which also details all the monetary and logistical support you will receive from the Hasura team.Ready to bring the local Hasura community together? Apply here to organize a HUG in your area. Got questions? Reach out to harsha@hasura.io and he will help you out.We look forward to seeing you at a local HUG, or working with you in putting one together!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Event triggers on MS SQL Server database Description: The event triggers in Hasura\n[https://hasura.io/docs/latest/event-triggers/index/] enable you to perform\ncustom business logic on your data. They capture events (insert, update, delete)\non a specified table and then invoke an HTTP webhook to run the custom business\nlogic.\n\nThis article shows how to use event triggers with an MS SQL Server database.\n\nSQL Server database\nBefore creating a Hasura application, you should have a SQL Server database\navailable. Additionally, ensure that the database is Author: Catalin Pit"}, {"role": "assistant", "content": "The event triggers in Hasura enable you to perform custom business logic on your data. They capture events (insert, update, delete) on a specified table and then invoke an HTTP webhook to run the custom business logic.\nThis article shows how to use event triggers with an MS SQL Server database.\nSQL Server database\nBefore creating a Hasura application, you should have a SQL Server database available. Additionally, ensure that the database is reachable by Hasura.\nUpdate the database settings to allow connections from the IP address of your Hasura project. You can find the IP address in the project's settings.\n\nAfter that, you can connect the database to Hasura.\nConnect SQL Server database to Hasura\nCreate a new Hasura project and launch the project console.\n\nThen navigate to the \"Data Manager\" and connect Hasura to your existing SQL Server database. Choose a database name, select MS SQL Server and enter the database URL.\nThe database URL has the following format:\nDriver={ODBC Driver 17 for SQL Server};Server=tcp:<b>hasura-test.database.windows.net</b>,<b>1433</b>;Database=<b>db-name</b>;Uid=<b>username</b>;Pwd=<b>password</b>;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\n\n\nAfter connecting the database, create the user table with the following fields:\n\nid of type \"int\" (Primary Key)\nname of type \"varchar(250)\"\nemail of type \"varchar(250)\"\n\n\nChoose the id field as the primary key and save the table.\nIt's time to test the database connection. Navigate to the GraphiQL page and insert a new user into the database. Here's an example mutation you can use:\nmutation MyMutation {\n  insert_user(objects: {id: 1, name: \"Hasura User\", email: \"hasura_user@email.com\"}) {\n    affected_rows\n    returning {\n      id\n      name\n      email\n    }\n  }\n}\n\nIf the mutation is successful, you should see a response similar to the one in the image below.\n\nSet up Hasura event trigger\nWith the event triggers, you need to have a webhook available that Hasura can invoke. In this case, let's assume you have a server that sends a welcome email to new users.\nEach time a new user is added to the database, the server automatically sends a welcome email to that user.\nNavigate to the \"Events\" page and create a new event trigger with the following details:\n\nTrigger name - send_email\nDatabase - default\nSchema/Table - public/users\nTrigger Operations - insert\n\nIn the \"Webhook (HTTP/S) Handler\" field, enter the endpoint Hasura needs to invoke when a new user is inserted into the database.\n\nNow scroll down until you see the section \"Configure REST Connectors\" and click on the \"Add Request Options Transform\" button.\nThat opens a new section where you can modify the request method and URL template. You need to change the URL template to pass the name and email of the newly created user.\nIn the \"Query Params\" section, add the following parameters:\n\nname - {{$body.event.data.new.name}}\nemail - {{$body.event.data.new.email}}\n\n\nSave the new event trigger and you are done!\nTest the event trigger\nTo test the event trigger, you need to insert a new user into the database. Run the following mutation in the GraphiQL editor:\nmutation {\n  insert_user(objects: {id: 2, name: \"Charlie Kirby\", email: \"chby@email.com\"}) {\n    affected_rows\n    returning {\n      id\n      name\n      email\n    }\n  }\n}\n\nThe figure illustrates the mutation in action. Since the mutation is successful, the event trigger should invoke the webhook, and the user should receive a welcome email.\n\nChecking the email inbox confirms that the user received the email.\n\nYou can also check the \"Invocation logs\" to see whether the event trigger was successful or not.\n\nThis is just one example of using event triggers with a SQL Server database. The Hasura documentation covers event triggers in-depth if you want to learn more. There is also an extensive section covering MS SQL Server.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Naming conventions for GraphQL schema Description: When you build a GraphQL API with Hasura, the GraphQL Engine automatically\ngenerates names for the GraphQL schema objects.\n\nStarting with Hasura v2.8.0\n[https://github.com/hasura/graphql-engine/releases/tag/v2.8.0], you can\ncustomize the name of fields, types, and arguments.\n\nWhy would you customize the names? In certain use cases, one Hasura application\nmight have multiple databases. With the \"naming convention\" feature, you can:\n\n * group together all fields belonging to a database\n * solve co Author: Catalin Pit"}, {"role": "assistant", "content": "When you build a GraphQL API with Hasura, the GraphQL Engine automatically generates names for the GraphQL schema objects.\nStarting with Hasura v2.8.0, you can customize the name of fields, types, and arguments.\nWhy would you customize the names? In certain use cases, one Hasura application might have multiple databases. With the \"naming convention\" feature, you can:\n\ngroup together all fields belonging to a database\nsolve conflicts with existing GraphQL objects\neasily identify fields & types of a particular database\n\nThere are 2 naming conventions available - hasura-default and graphql-default. By default, Hasura applications use the hasura-default naming convention.\n\nThis article will help you get started with the \"naming convention\" feature.\nHow to enable the Naming Convention feature\nThe \"naming convention\" feature is experimental, meaning that you need to enable it in the \"Env vars\" section from your Hasura project settings.\n\nAfter adding the env variable, navigate to the \"Data Manager\" page and click on the \"Edit\" button from your chosen database (in case you have multiple databases).\n\nThen scroll until you see the \"GraphQL Field Customization\" field. Click on it to expand the section.\n\nIn the new section, you can choose a different naming convention and customize the root & type names.\nTry it out via Hasura\nThis article uses a Hasura application with 2 databases. The first database holds information about users and their notes. The second database stores private notes.\n\nIf you want to follow along, the quickest way to use Hasura is via the Hasura Cloud. Create a new Hasura project and launch the console.\n\nNote: If you prefer local development using Docker, check out the documentation on getting started with Docker.\nRoot fields namespace\nLooking at the \"Explorer\" tab, you cannot determine which tables belong to which database. With the naming convention feature, you can group all tables belonging to a database.\nFor example, let's use a custom namespace for the database with \"public\" notes.\n\nIf you navigate to the Hasura console, you can see all the tables from the public database under the public namespace. Now it's easier to see which tables belong to which database.\n\nYou could go a step further and use a namespace for the other database as well. You could group all the tables from that database under the private namespace. Thus, you would have the public and private namespaces.\nDifferent naming convention\nBy default, all Hasura applications use the hasura-default naming convention. With hasura-default:\n\nall names use snake casing (snake_case)\nenum values are not changed\n\nIf you want to change to another naming convention, you can use graphql-default. In this naming convention:\n\nPascalCase is used for type names\ncamelCase is used for field names and arguments\nUPPERCASE is used for enum values\n\nLet's change the naming convention for the second database (private notes) to graphql-default.\n\nThe figure below illustrates how the GraphQL query and the response look with the graphql-default naming convention.\n\nThis example shows how to change the naming convention for one source. However, you can set a default naming convention for all the sources in a Hasura application.\nYou can do it by setting the environment variable HASURA_GRAPHQL_DEFAULT_NAMING_CONVENTION to the preferred naming convention.\nPrefixes and suffixes\nLastly, you can customize the root fields and type names by adding prefixes/suffixes. Let's assume you want to prefix and suffix public notes with public_ and _db, respectively.\nNavigate to the \"GraphQL Field Customization\" section and add the prefix and suffix for the root fields.\n\nIf you navigate to the Hasura console, you should see them applied to the root fields.\n\nSimilarly, you can add a prefix and suffix for type names. In the \"GraphQL Field Customization\" section, move the values public_ and _db from \"Root Fields\" to \"Type Names\".\n\nLooking at the image, you can see that the type names now contain the custom prefix and suffix.\nSummary\nThe \"Naming Convention\" feature makes it easier to identify fields & types of a particular database. You can use namespaces, prefixes, and suffixes to organize your application. The documentation for naming conventions is available here.\nThere is also a video talking about this feature.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: A tutorial for using Firebase to add authentication and authorization to a realtime Hasura app Description: A step by step tutorial for using Firebase to add authentication and authorization to a realtime Hasura app with a React Frontend.  Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Junyu Zhan and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.IntroductionHasura helps you build production-level 3factor apps really fast. Read more about building 3factor apps with event-driven programming. In addition to generating CRUD GraphQL APIs from your PostgreSQL database, it also provides ways to authenticate users using webhooks or JWT and helps you define granular access controls rules for your GraphQL schema (authorization). \u00a0However, integrating an auth system with Hasura backend and some \u00a0frontend still requires some effort, and can be tricky. \u00a0This tutorial aims to demonstrate how to do that by making a real-time voting app like this sample app, but with auth built in. We are going to use Hasura for the backend, Firebase Authentication for authentication and React for the frontend.It has mainly 3 steps:Set up Hasura and create the data model using the Hasura ConsoleSet up AuthenticationBuild the React web appPrerequisitesReactGraphQLSQLUsing Firebase Authentication versus building your ownBuilding a robust authentication system is no small effort. It's so important that it can make or break your app. Because the main focus of \u00a0this tutorial is to integrate an auth system with Hasura and React, we are going to use the off-the-shelf Firebase Authentication. It's secure, offers many useful features such as third-party and passwordless sign-in and has a generous free tier.Step One: HasuraThe quickest and easiest way to use Hasura is via the Hasura Cloud. Follow the \"Quickstart with Hasura Cloud\" guide to get a Hasura instance up and running.After creating the Hasura instance, we need to set some environment variables in the project settings.Since we will use \u00a0JWT from Firebase, set HASURA_GRAPHQL_JWT_SECRET to:{\n    \"type\":\"RS256\",\n    \"jwk_url\": \"https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com\",\n    \"audience\": \"<firebase-project-id>\",\n    \"issuer\": \"https://securetoken.google.com/<firebase-project-id>\"\n}Note: Firebase shares JWKs between multiple tenants. They use the aud claim of JWT to specify the intended audience for the JWT. Setting the audience field in the Hasura JWT configuration will make sure that the audclaim from the JWT is also checked during verification. Not doing this check will allow JWTs issued for other tenants to be valid as well. In these cases, you MUST set the audience field to the appropriate value. Failing to do so is a major security vulnerability.Finally, set HASURA_GRAPHQL_UNAUTHORIZED_ROLE to anonymous because we do allow unauthenticated users to write and read some data.Now it's time for data modeling. First, we need a \"programming_language\" table with \"name\" and \"vote_count\" fields.Also, we need a \"loved_language\" table to record whether a language is loved by some users. Since a user can only love a language once, we need to set the primary key as name and user_id combined. There is no way to do that in the \"Add Table\" UI, but Hasura conveniently provides a \u00a0way to execute raw SQL:After you create these two tables, Hasura will notice the one-to-many relationship between them and help you create the corresponding GraphQL relationship.Hooray! Now that we have a data model, you can play with the API in GraphiQL. Insert some of your favorite languages. Give them some votes. Love them as a random \"user_id\". Since we are signed in as admin, we can do anything we want. But we need to set proper permissions for the \u00a0\"anonymous\" and \"user\" roles. We allow both of them to select and update \"programming_language\":For \"loved_language\", we only allow the \"user\" role to insert, select and delete. Notice that for the \"insert\" permissions, the \"user_id\" must come from \"X-Hasura-User-Id\".With permissions set, all we need is a secure way to get \"X-Hasura-User-Id\".Step Two: Firebase AuthGo to the Firebase website to create a new project. By default it's on the free plan, so don't worry about charges.In the Authentication section of the Firebase console, turn on the Google sign-in provider. In this tutorial, we only use Google sign-in, but adding other providers is trivial. Notice at the bottom of the page, in \"Authorized domains\", localhost and a Firebase domain are automatically added. If you later decide to deploy the React app to another domain, you need to add it here for Google sign-in to work.Now we can sign in users and get their id token for Hasura in the React app, using the Firebase JS SDK. But for Hasura to know the identity of these users, whose data are stored in Firebase, we need to \u00a0add some specific \"custom claims\" required by Hasura to the token. We will use Cloud Functions for Firebase to do that, following the example in the Hasura repo.A cloud function is, well, some function that automatically runs \"in \u00a0response to events triggered by Firebase features and HTTPS requests\". In our case the event is firebase user creation. When that happens, we'd \u00a0like to add some extra data to the user's id token. The code is straightforward:The code with comments needs some more explanation. When a user signs in our React app, Firebase immediately gives us that user's data, including the id token we need to send to Hasura. But if it's the first time, in which the user is just created, the token may not include those custom claims. That's the reason we use Firebase real-time database to listen to a token refresh.Deploying a cloud function is simple. Follow the official Get started guide, replace index.js with the code above and run firebase deploy --only functions. That's it.Before you leave the Firebase console, you need to change one more setting. Go to the Realtime Database Rules section and update the rules. We need to define conditions for when data can be read by users.{\n  \"rules\": {\n    \"metadata\": {\n      \"$uid\": {\n        \".read\": \"auth != null && auth.uid == $uid\"\n      }\n    }\n  }\n}You can read more about Firebase Database Rules here to understand different read and write conditions that can be specified.Step Three: ReactFinally, it's time for us to build the exciting UI. We will use Apollo Client to query our GraphQL API. Follow the client setup guide to add all the needed npm packages.Since the app is simple, we are only going to build two components: \u00a0\"Auth\" and \"App\". \"Auth\" uses Firebase SDK to sign in users and pass \u00a0that state to \"App\". \"App\" includes all the business logic: subscribing to the real-time data, voting and loving languages.Auth:The code is straight forward if you are familiar with the new Hooks API. \u00a0Notice how we use the Firebase realtime database to listen to the refreshTime which we set up in the Firebase cloud function. Also, we check whether a user's id token already contains custom claims to avoid useless refreshing.App:Notice how Hasura and GraphQL enable us to flexibly query data we need based on different auth state.Wrapping upIn this tutorial, we have built a real-time voting app with Hasura. We integrated a robust auth system both on the backend and frontend. You can see how Hasura makes tedious and hard jobs easy by providing a pluggable GraphQL and auth interface. Based on this model, there really is no limit on what kind of amazing apps you can build.All the code in this tutorial are in the Github repo.About the author Junyu Zhan is a full-stack web developer living in China. Right now he is building a cross-platform App with Flutter, React and Hasura. \u00a0He loves taking long walks and swimming in his spare time. You can contact him at his email address (thezjy@gmail.com) or Twitter Account (https://twitter.com/thezjy1)."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Tutorial: Fullstack React Native with GraphQL Description: \nA tutorial to build a React Native to-do app with Apollo\u2019s new Query and Mutation components\n Author: Rishichandra Wawhal"}, {"role": "assistant", "content": "A tutorial to build a React Native to-do app with Apollo\u2019s new Query and Mutation componentsOverviewIn this tutorial, we will be building a React Native to-do app that helps us add tasks, view them, mark/unmark them as complete and delete them.To build the app, we will be using:React NativeApollo Client (for client-side GraphQL)Hasura GraphQL Engine (for GraphQL backend)Auth0 (for auth)Note: We will be using the new Query and Mutation components that Apollo introduced in their 2.1.3 release of react-apollo.Part 1: Deploying a GraphQL backendWe need a GraphQL backend where we can store the state of our app. We will be using Hasura Cloud to generate an instant GraphQL backend.DeploymentDeploy Hasura GraphQL Engine by simply clicking on the button below.Note the URL of the deployed app. It should be of the form: <auto-generated-string>.hasura.app. This is your GraphQL Engine URL.Creating the tablesFor storing the user information , we will create a users table.users\n+--------------+------------------------------+\n|  column      |       type                   |\n+--------------+------------------------------+\n| id           | serial NOT NULL primary key  |\n| name         | text NOT NULL primary key    |\n+--------------+------------------------------+Here is the significance of the columns:id: This is a unique integer that will identify each entry in the users table. It is also the primary key of the table.name: This is the name of the userThe data for this table will be coming from Auth0.Note: Setting up Auth0 and integrating with Hasura has already been done and it is beyond the scope of this tutorial. Click here to learn how to do it.For storing our todos, we will need a todos table with the following fields.\ntodos\n+--------------+---------------------------------------------------+\n|  column      |       type                                        |\n+--------------+---------------------------------------------------+\n| id           | serial NOT NULL primary key                       |\n| task         | text NOT NULL                                     |\n| is_completed | boolean NOT NULL                                  |\n| user_id      | integer NOT NULL FOREIGN KEY REFERENCES users(id) |\n+--------------+---------------------------------------------------+Here is the significance of the columns:id: This is a unique integer that will identify each todo. It is also the primary key of the table.text: This is the to-do task.is_completed: This is a boolean flag that marks the task as completed and pending.user_id: This is a foreign key referencing id of the users table. It relates the todo to its author.Let us create the above tables in our backend:Go to your GraphQL Engine URL in your browser (https://<auto-generated-string>.hasura.app). It opens up an admin UI where you can manage your backend.Go to the Data section on top and click on \u201cCreate Table\u201d and add the aforementioned column names and types.Creating todos tableCreating todos tableTable relationshipsAs you see above, there is meant to be a foreign-key based relationship between todos and users . Lets add the foreign key constraint and the relationship. Go to the Data tab on top and click on todos table. Now, in the modify section, edit the user_id column and make it a foreign key. After this go back to the Data tab and click on Track all relations .Adding foreign keysAdding relationshipsOnce you track the relationship, you can make complicated nested GraphQL queries to https://<auto-generated-string>.hasura.app/v1/graphql . To try out, go to the GraphiQL tab in the console and try making a query.GraphiQL - An API\u00a0ExplorerTable permissionsIn our todos table, we want users to CRUD only their own todos. Hasura provides an access control layer for setting up rules to restrict data to specific roles. In this app, we will have only user role. Let us set permissions for it.Go to /data/schema/public/tables/user/permissions in your Hasura console and enter the role user and allow CRUD in the user table only when x-hasura-user-id is equal to id . This means that Hasura will ensure that a user can CRUD only when the X-Hasura-User-Id from the JWT in the header is equal to the id of the user that they are CRUDing over.Setting insert permission for users\u00a0tableThe above screenshot shows the permission condition for insert query, add similar permissions for select , update and delete queries.Similarly, add permissions for todos table with a condition: { 'user_id': 'X-Hasura-User-Id' } . This means that a user can CRUD only their own todos.With this, we have set up our backend. Let us work on React Native now.Part 2: Setup React Native ProjectWe will be using Expo for this tutorial. Get started with a boilerplate project by running:npm install -g expo-cli\nexpo init Todo\ncd Todo\nnpm startThis will create an empty React Native project where App.js is the entry point. This App.js must maintain a state called isLoggedIn which if false, it should renders the Auth screen, else render the the app (currently just Hello world). It should also pass login and logout functions as props to the AuthScreen and the app respectively. The App.js should currently look something like:Part 3: Setup AuthSince we are using JWT, install the package jwt-decode from npm.npm install --save jwt-decodeCreate a directory called src at the top level and create another subdirectory in it called auth. Inside auth , create a file called Auth.js and perform authentication with auth0 using Expo\u2019s AuthSession. The Auth0.js should look something like this.The above component does the following:Render a button called login pressing which, Auth0 login is performed using Expo\u2019s AuthSession.After the authentication is complete, the session variables are stored in AsyncStorage and isLoggedIn of the parent component is set to true so that the app is navigated to the app.Once auth is complete, next, we have to instantiate Apollo client for client side GraphQL.Configuring Apollo ClientFirstly, let us install the dependencies related to Apollo client. Run the following command from the todo-app directory.$ npm install apollo-boost react-apollo graphql-tag graphql --saveCreate a file called apollo.js and export a funtion that accepts a token and returns an instance of Apollo Client. You have to configure the Apollo client with the GraphQL endpoint and the token. (Replace with your own GraphQL endpoint)import { ApolloClient } from 'apollo-client';\nimport { HttpLink } from 'apollo-link-http';\nimport { InMemoryCache } from 'apollo-cache-inmemory';\nconst GRAPHQL_ENDPOINT = `https://<auto-generated-string>.hasura.app/v1/graphql`;\nconst createApolloClient = (token) => {\n  const link = new HttpLink({\n    uri: GRAPHQL_ENDPOINT,\n    headers: {\n      'Authorization': `Bearer ${token}`\n    }\n  });\n  return new ApolloClient({\n    link,\n    cache: new InMemoryCache()\n  })\n}\nexport default createApolloClient;Now create a directory in the src folder called app and create a file called Main.js . This will be the entrypoint of your todo app where you instantiate the Apollo client using the above function and provide it to the children components using ApolloProvider . The child component currently is just TodoList. We will write this component in the next section.Before that, we have to insert the user that logged in into the users table with an insert_mutation using the client. We can do it in the componentDidMount of this component itself before setting client in state.client.mutate({\n  mutation: gql`\n    mutation ($username: String, $userid: String){\n      insert_users (\n        objects: [{ name: $username, id: $userid}]\n      ) {\n        affected_rows\n      }\n    }\n  `,\n  variables: {\n     username: this.props.username,\n     userid: this.props.userid\n  }\n});Note: gql from graphql-tag is like a query parser that parses a graphql-string into an AST document that Apollo client understands.Your Main.js should look something like this:Also modify the render of App.js to pass the appropriate flag.render() {\n    const { isLoggedIn, userId, username, loading, jwt } = this.state;\n    if (loading) {\n      return <View><Text>Loading...</Text></View>\n    }\n    if (isLoggedIn) {\n      return (\n        <Main\n          userId={userId}\n          username={username}\n          token={jwt}\n          logout={this.logout}\n        />\n      )\n    } else {\n      return (<Auth login={this.login}/>)\n    }\n}Creating our first Query componentLets write our TodoList component. We will use Apollo\u2019s Query components to fetch all the todos from the server. Lets see how to use the Query componentThe flow goes like:import {Query} from 'react-apollo';import gql from 'graphql-tag'; graphql-tag is just like a parser that parses a GraphQL query intoPass the GraphQL query string as prop to the Query component.<Query query={gql`\n  query {\n    todos {\n      id\n      text\n      is_completed\n    }\n  }\n`}\n>Wrap your custom component inside the Query component.<Query query={gql`GRAPHQL_QUERY`}>\n  {(data, error, loading) => {\n   return (<MyComp data={data} error={error}, loading={loading} />)\n  }}\n</Query>MyComp in the above component receives the state and response of the GraphQL query.We will write our TodoList component similarly. Create a file called TodoList.js in the src directory. Write a TodoList using the Query component, similar to what is shown above. It will look something like:The above component simply fetches all the todos and renders their text in a FlatList.Writing our first Mutation componentMutation components work just like the Query components except, they also provide a mutate function that can be called whenever you want. In case of mutations, we also need to update the UI after the mutation succeeds.Insert todosCreate a file called Textbox.js and add the following content to it:In the above component, we use the <Mutation> component that provides a render prop with a function to insert todo. The Mutation component also takes an update prop which takes a function to update the Apollo cache after the mutation success.Update the render method of the Main.js component to render the above Textbox as well.render () {\n    if (!this.state.client) {\n      return <View><Text>Loading...</Text></View>;\n    }\n    return (\n      <ApolloProvider client={this.state.client}>\n        <Textbox\n          userId={this.props.userId}\n          username={this.props.username}\n          logout={this.props.logout}\n        />\n        <TodoList\n          userId={this.props.userId}\n          username={this.props.username}\n          logout={this.props.logout}\n        />\n      </ApolloProvider>\n    );\n}Update and delete todosAs of now, we are just rendering the todo text in the FlatList. We also want the ability to mark the todo as complete and to delete the todos. To do this, we will render each todo item as a separate component instead of just the text. In this component, we can have the marking complete functionality and the delete functionality.Create a file called TodoItem.js . It would look something like this:The above component again uses the Mutation components and we follow the same flow that we did while inserting todos. If you observe well, you will notice that we have not updated the cache in case of update mutation. This is because, Apollo cache automatically updates the items if it is able to match the id of a mutation response with the id of an item in the cache.Finally, update the render method of TodoList.js to render the above TodoItem in the Flatlist.render() {\n    return (\n      <Query\n        query={FETCH_TODOS}\n      >\n        {\n          ({data, error, loading}) => {\n            if (error || loading) {\n              return <View> <Text> Loading ... </Text> </View>\n            }\n            return (\n              <ScrollView style={styles.container} contentContainerStyle={styles.container}>\n                <FlatList\n                  data={data.todos}\n                  renderItem={({item}) => <TodoItem todo={item}}\n                  keyExtractor={(item) => item.id.toString()}\n                />\n              </ScrollView>\n            )\n          }\n        }\n      </Query>\n    )\n  }Wrapping upWe covered the following in this blogpostDeployed a GraphQL server in the form of Hasura GraphQL EngineSet up tables and permissionsSet up a React Native project and performed auth using Auth0.Set up Apollo client with a GraphQL endpoint and JWTUse Apollo\u2019s Query components to fetch todosUse Apollo\u2019s Mutation componentsWe did not:Use Hasura\u2019s GraphQL SubscriptionsImplement a logout buttonGo into styles of React Native. All code snippets are more like pseudo code code snippets.Hasura gives you instant GraphQL APIs over any Postgres database without having to write any backend code."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Instant streaming APIs with built-in authorization for new or existing Postgres Description: This post discusses a recent capability added to Hasura GraphQL Engine: Streaming over GraphQL subscriptions for Postgres. Author: Praveen Durairaju"}, {"role": "assistant", "content": "This post discusses a recent capability added to Hasura GraphQL Engine: Streaming over GraphQL subscriptions for Postgres.\nTL;DR\nIf you have a large amount of data, or \"fast moving\" data in Postgres, Hasura now allows you to instantly create an API for clients to fetch that data as a continuous stream. This API can be safely exposed to internal or external HTTP clients.\n\nUses GraphQL subscriptions, and works with any GraphQL client that supports subscriptions over websockets\nEach client can maintain an independent stream cursor (aka offset) and prevent any dropped or missing events - aka not fire-and-forget\nEach client can only read relevant events in a stream: Create fine-grained authorization rules at a \"row\" (or event) and \"column\" (or field) level, that integrates with any authentication provider\nUse relationships to enrich event payload data with data in other models at query time\nScale to a massive number of concurrent clients. This post discusses a benchmark of over 1M clients concurrently streaming data with independent stream offsets and independent authz rules\nWorks with new or existing data in any Postgres database and with read-replicas\nNo special configuration around sticky sessions, back-pressure, dropped connections, roll-outs, scaling out etc required\n\n\nMotivation\nToday we have a variety of solutions to ingest and store a large amount of data or a stream of data. However, once this data has been captured, securely exposing this data as a continuous stream to a large number of HTTP clients concurrently is a challenge.\nThese are the challenges that Hasura aims to address:\n\nAllow for authorization rules so that clients can only consume the subset of the stream they have access to\nPrevent missing events and allow each client to move through its stream independently\nScale and manage websocket connections to support hundreds of thousands to millions of HTTP clients concurrently\n\nHasura's new streaming API on Postgres addresses these challenges so that teams focus on how their streams are modelled and secured instead of building and scaling the API.\n\n\nTable of contents\nIn this post, we will look at the architecture of streaming subscriptions, how our batching and multiplexing works with declarative authorization. We will take a look at initial performance benchmarks that test for high-concurrency and volumes of data. And finally, we\u2019ll highlight a few common use-cases and modeling patterns.\n\nTry it out\nCommon use-cases\nArchitecture\n\nBatching and Multiplexing\n\n\nHandling Authorization with large data\nBackpressure and Scaling\nPerformance Benchmarks\nNext steps\n\nTry it out in 60 seconds\n\nStep 1: Deploy Hasura as a docker image or on Hasura Cloud\nStep 2: Connect a Postgres database to Hasura as a source\nStep 3: Track an existing table, or create a new table with an monotonically increasing, unique id (eg: bigserial).\n\nCREATE TABLE public.message (\n  id integer NOT NULL,\n  username text NOT NULL,\n  text text NOT NULL,\n  \"timestamp\" timestamp with time zone DEFAULT now() NOT NULL\n);\n\n\nStep 4: Run a GraphQL subscription to try out the streaming API on that table\n\nmessage_stream {\n  id\n  username\n  text\n}\n\nUse-cases\nStream log-like data\nLet's say you have log-like data that is being continuously generated and needs to be streamed to web clients.\nWhat you need to do:\n\nProcess and ingest data into a Postgres cluster\nRun Hasura and add your Postgres (primary & read-replicas) as sources\nSet up authorization rules in Hasura to decide who can stream what data.Read log if log.service.viewers contains session.user_id\n\n\n\nArchitecture examples:\n\nIngest data into a Kafka-like queue, which is processed and written to a global \"edge\" AWS Aurora Postgres cluster\n\n\n\nIngest data into TimescaleDB, setup a continuous materialized view and stream realtime aggregations\n\n\nCreate messaging channels\nMany use-cases that involve web/mobile clients require an ability to setup a persistent and secure channel that allows clients to publish and subscribe to messages.\nWhat you need to do:\n\nCreate a messages model that has references to channel or groups that determine authorization rules\nWhen messages are sent (or published), messages can be ingested directly based on authz rules via Hasura GraphQL mutations, or can be handled by custom logic that processes them and inserts them into the messages channel\n\n\nArchitecture example:\n\n\nNote: Messaging channels are a great candidate for sharding by channel_id or other channel metadata. Hasura supports YugaByte, Citus with support for Cockroach coming soon.\nCapture and stream data changes on an existing table\nWhat you need to do:\n\nCreate an audit trigger that captures data from the table, and inserts the change event into a changes table\nSetup authorization rules on the changes table so that only changes to the authorized rows and the right subset of columns are streamed\n\n\nArchitecture:\n\n\nNotes on preventing load on the primary:\n\nUse logical replication to replicate a subset of the data to a secondary Postgres\nSet up triggers on the second Postgres as suggested above\n\nNote on capturing data from the WAL:\n\nLR slots are expensive. Maintaining unique cursors and authorization rules for each HTTP client can get prohibitively expensive. Instead, capture data from the WAL and insert that into a flattened table in a second Postgres. This table will now contain the entire WAL, and Hasura can help clients stream data from that table.\n\nCreate a fire-and-forget channel for ephemeral data\nTyping indicators, live locations sharing multiplayer mouse-pointer like information are ideal candidates for fire-and-forget type channels. These types of applications usually require lower e2e latency as well.\nArchitecture:\n\nCreate (or alter) an UNLOGGED table in Postgres\nSet up a authorization rule (optimize the authorization predicate so that the validation can be done with data within the same row)\n\n\nNote: The advantage of using Postgres is that we can leverage its query engine for authorization (via Hasura's predicate push down) and independent cursors per stream. Furthermore, the data retention policy can also be fine-grained and controlled quite easily. Furthermore, given that Hasura allows adding multiple Postgres instances to the same GraphQL Engine configuration, we can tune an entirely separate Postgres server to trade increased read & write performance for data-loss.\nNotes on architecture\nHasura already supports the ability to subscribe to the latest value of a query (aka a live query) over GraphQL subscriptions, which works well for use-cases that don't need a continuous stream of data or events. Eg: the current set of online users, the latest value of an aggregate etc. Read more about how these live queries work in Hasura here.\nStreaming however, is a different GraphQL subscription field that is now provided by Hasura, ideal for streaming a large result set continuously or for consuming events continuously in a robust way.\nAPI design goals\nThese were the goals and constraints we put on the design of the API:\n\nShould work with GraphQL subscriptions out of the box\nShould compose well with Hasura's existing relatonships and authorization system\nShould allow the client to quote its own offset\n\nMaking GraphQL calls efficient\nBefore we dive in to the specifics, let us look at how Hasura handles an incoming subscriber, ie. a GraphQL query.\nPart of Hasura is a transpiler that uses the metadata of mapping information for the data models to the GraphQL schema to \u201ccompile\u201d GraphQL queries to the SQL queries to fetch data from the database.\nGraphQL query \u2192 GraphQL AST \u2192 SQL AST \u2192 SQL\n\nThis gets rid of the N+1 query problem and allows the database to optimise data-fetching now that it can see the entire query.\nLet\u2019s apply this transpiler to streaming subscriptions.\n\nThe SQL is run with current cursor value, the response is sent to the subscriber and once the response is processed, the cursor value is updated for the next data set. This works well for a single subscriber for a given query.\nBatching multiple streaming consumers into one SQL query\nWhen there are a multiple connected clients that perform streaming subscriptions, the graphql-engine \"multiplexes\" subscriptions that are similar.\nFor example:\nLet's say n clients run the following streaming subscription with different values of id.\nsubscription StreamLogs ($id: Int!) {\n  logs_stream(initial_value: {id: $id}, batch_size: 10) {\n    log_type\n    log_level\n    log_data\n  }\n}\n\nThe SQL query generated for the above GraphQL query will look something like\nSELECT log_type, log_level, log_data, MAX(id) as cursor\nFROM\n( SELECT * FROM logs WHERE id > $id LIMIT 10 )\n\nWe can see that even if there are n different subscriptions being run, they all run the same generated parameterized SQL query. The graphql-engine now leverages this by running one single parameterized query with n parameters.\nThis is done by \"joining\" the SQL query to an array of id values and the response then returns n rows and each row corresponding to an output to a distinct value of id.\nBy \"multiplexing\" subscribers, the number of DB connections will be lesser than the number of subscriptions.\nHandling authorization with a predicate push-down\nWe have seen how Hasura makes performant queries. But what about Authorization?\nThe naive approach would be to fetch the data from the database (involves IO), apply authorization checks for each element in the response (involes IO and compute) and then send the result back to the client. The problem with this approach is that the initial data fetch is not definitive. The data is being filtered based on rules post the fetching from the database and as data becomes bigger, compute and latency becomes high and so does the load on the database.\nIt is impossible to load large streams of data in memory and apply authorization rules for filtering data before sending it back to the client. The Hasura approach to this problem is two fold. Make the data fetching performant and make authorization declarative by applying them at the query layer.\nWe just saw above how the transpiler in Hasura with batching helps in making performant queries. But this in itself isn't enough as resolvers also enforce authorization rules by only fetching the data that is allowed. We will therefore need to embed these authorization rules into the generated SQL.\nMake Authorization declarative\nAuthorization when it comes to accessing data is essentially a constraint that depends on the values of data (or rows) being fetched combined with application-user specific \u201csession variables\u201d that are provided dynamically. For example, in the most trivial case, a row might container a user_id that denotes the data ownership. Or documents that are viewable by a user might be represented in a related table, document_viewers. In other scenarios the session variable itself might contain the data ownership information pertinent to a row, for eg, an account manager has access to any account [1,2,3\u2026] where that information is not present in the current database but present in the session variable (probably provided by some other data system).\nAuthorization is declarative and available at a table, view or even a function (if the function returns SETOF) level. It is possible to create a single SQL query that has these authorization rules added to it as additional predicateds in the query (additional clauses to the WHERE clause).\nThe resultant query processing pipeline is now: GraphQL query \u2192 GraphQL AST \u2192 Internal AST with authorization rules \u2192 SQL AST \u2192 SQL\n\nBackpressure\nBackpressure is the mechanism that \u201cpushes back\u201d on the producer to not be overwhelmed by data.\nWhile streaming, it can happen that the server keeps on sending new events to a client while the client already has a huge backlog of events to process. In such a case, when the client is overwhelmed with a large amount of events and is not able to cope up with the events sent by the server, then the client should manually disconnect the subscription noting down the last cursor value. Once the client is ready to accept new events it can start a new streaming subscription from the last cursor value it processed.\nScaling\nHasura\u2019s subscriptions can easily be scaled horizontally by adding more Hasura instances and can also be scaled at the DB layer by adding read-replicas to the primary database.\nPerformance Benchmarks\nRead more about the benchmark setup here - https://github.com/hasura/streaming-subscriptions-benchmark\nWe use a similar schema and authorization rule setup as in the \"messaging channels\" use-case above.\nOur aim in this benchmark is to verify that Hasura can handle a large number of concurrent subscribers without undue load on the underlying database.\n\nEach subscriber has an independent stream. Each maintains its own offset, and connects with a different authorization rule.\nWe scale from 20k concurrent to 1M concurrent and observe:\na. Load on Hasura instances\nb. Database CPU load\nc. Number of database connections required\nConfiguration:\na. Database: Single Postgres instance (RDS - 16x CPU, 32GB RAM)\nb. Hasura: 100x instances on App runner (4xCPU, 8GB RAM)\n\nHere is how the load scales:\n\n\nDB CPU percentage at peak usage was 15%\nPeak Hasura CPU usage was 20%\n\nNext steps:\nIf you have a streaming workload, try Hasura out and feel free to reach out to us for questions, feedback and for any other architectural discussions.\n\nTry it out: Hasura Cloud or with Docker\nReach out to us Discord or Github Discussions\nContact Us\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Disable GraphQL Query and Subscription Root Fields selectively with RBAC Description: Learn how to disable GraphQL query or subscription root fields in Hasura applications with RBAC Author: Catalin Pit"}, {"role": "assistant", "content": "For each database table in a Hasura app, the GraphQL engine creates three root fields by default. Assuming there is a note table, the three root fields would be:notenote_by_pknote_aggregateHowever, there are use cases where you might want to expose only certain root fields or none at all. For example, you might want to hide note_aggregate, expose only note_by_pk, or hide all of them. Now it's possible to do that.Hasura launched this feature in the v2.8.0 version. You can check the v2.8.0 changelog here.In the next step, you will build and use a Hasura application to test the different use cases.Try it out via HasuraThe Hasura application you will build is a basic note-taking app. Users can add multiple notes, whereas each note belongs to only one user.The quickest way to use Hasura is via the Hasura Cloud. Create a new Hasura project and launch the console.Note: If you prefer local development using Docker, check out the documentation on getting started with Docker.After that, connect the Hasura application to a database and create two tables - user and note.The user table has the following columns:id of type UUID (PK)name of type TextCreate the user table in HasuraThe note table has the following columns:id of type UUID (PK)title of type Textcontent of type Textauthor_id of type TextCreate the note table in HasuraThe next step involves creating the relationships between the two tables. Navigate to the \"Permissions\" tab in the user table and create an array relationship between the user and note tables.The id column from the user table maps to the author_id column from the note table.Create an array relationship between user and note tablesSimilarly, create an object relationship between the note and user table. The author_id column from the note table maps to the id column from the user table.Create an object relationship between note and user tablesYou finished configuring the application! Add some data (users and notes) to the application so you can test the different use cases.How to use the featureIt's important to mention that you can disable query/subscription root fields only through the metadata file. You need to export and import metadata whenever you want to customize the root fields.Export and import metadata in Hasura CloudYou will export the metadata file, make the corresponding changes and then upload the metadata file to Hasura.Note: An alternative way of managing the metadata is to use the Hasura command-line interface (CLI). The documentation covers the hasura metadata in detail.The console support will come soon too!Use case: Disable GraphQL operationsThe feature allows you to disable a specific GraphQL operation (queries or subscriptions) or all of them. You can grant a role to make queries, but not subscriptions, and vice-versa. Or you can disallow both.Using the example Hasura application, you can disable all the root fields of the user table for the guest role as follows:{\n   \"role\": \"guest\",\n   \"permission\": {\n     \"columns\": [\n       \"id\",\n       \"title\",\n       \"content\",\n       \"author_id\"\n     ],\n     \"filter\": {},\n     \"allow_aggregations\": true,\n     \"query_root_fields\": [],\n     \"subscription_root_fields\": []\n   }\n}\nThe empty arrays from the last two fields indicate that no root fields should be displayed for the user table.The image illustrates what you should see after applying the permissions.Disabled GraphQL operations for the \"user\" table\u26a0\ufe0f It's important to note that the guest role can still indirectly access the user table through the notes relationship.query {\n  note {\n    id\n    title\n    content\n    author_id\n    author {\n      id\n      name\n    }\n  }\n}\nIf you run the above query, you will see the author details for each note. Keep this in mind when disabling direct access to a table.You can see the complete metadata code for this use case in this Gist.Use case: Primary key access onlyAnother use case would be to access the table only through its primary key. The client can only fetch data from the table if the client knows the primary key of a particular row.Add the following permissions to the note table in the metadata file:{\n  \"role\": \"guest\",\n  \"permission\": {\n    \"columns\": [\n      \"author_id\",\n      \"content\",\n      \"title\",\n      \"id\"\n    ],\n    \"filter\": {},\n    \"query_root_fields\": [\"select_by_pk\"],\n    \"subscription_root_fields\": [\"select_by_pk\"]\n  }\n}\nAfter you import the updated metadata file, the guest role should only have access to the note_by_pk root field, as shown in the image below.Access data in a table through the primary keyYou can see the complete metadata code for this use case in this Gist.Use case: Access table through a relationshipThe last scenario from this post involves providing indirect access to a table. That means accessing a table through a relationship.In this application example, you could make the user table accessible only through the notes relationship.Go to the metadata file and apply the following permissions for the user table:{\n  \"role\": \"guest\",\n  \"permission\": {\n    \"columns\": [\n      \"id\", \n      \"name\"],\n    \"filter\": {},\n    \"query_root_fields\": [],\n    \"subscription_root_fields\": []\n  }\n}\nIf you go to the Hasura console and perform the query shown in the image, you should be able to access the rows from the user table.Access a table indirectly through a relationshipYou can see the complete metadata code for this use case in this Gist.SummaryThe feature allows you to disable root fields and customize your GraphQL API. You can disable queries, subscriptions, or both of them. It also allows you to make tables accessible only through a relationship.There is also a video that talks about hiding root fields.Hiding root-level fields"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Types of Realtime GraphQL Subscriptions Description: In this post, we will look into the different approaches to real-time GraphQL subscriptions and walk you through the ones that Hasura supports and recommends. Author: Catalin Pit"}, {"role": "assistant", "content": "GraphQL has a particular type of operation, called subscription, that enables you to add real-time or reactive features to your applications.How does a GraphQL Subscription work? The client subscribes to a particular event on the server, and whenever that event occurs, the server pushes the update to the client.An event can represent data creation, modification, or deletion. For instance, let\u2019s say you have a voting application. You can set up a subscription to notify you when someone votes. Here is a subscription example:subscription votes {\n  vote {\n    created_at\n    option {\n      poll {\n        question\n      }\n      text\n    }\n  }\n}\nThe subscription returns the updated results whenever a new vote is inserted into the database.How are GraphQL Subscriptions implemented? They use the WebSocket protocol to keep a persistent connection between the client and the server. The connection stays open until either party terminates it.Types of SubscriptionsHasura comes with two types of subscriptions - live queries and streaming subscriptions.Live QueriesLive queries represent one way of retrieving real-time data and are a pattern of GraphQL subscriptions.You can see an example of a live query below:query @live {\n  vote {\n    created_at\n    option {\n      poll {\n        question\n      }\n      text\n    }\n  }\n}\nThe structure looks similar to a regular query, but it uses the @live directive and works differently under the hood.Rather than observing specific events, a live query observes data. Live queries react to changes in the latest query result. The server sends the updated result to the client whenever the result changes.Streaming SubscriptionsHasura also supports streaming over GraphQL subscriptions, known as Streaming Subscriptions.With a streaming subscription, the client opens a new connection and specifies an offset (or cursor value). The subscription then sends ordered data from that offset. The offset acts as a starting point.It\u2019s recommended for the offset (or cursor value) to represent unique or sortable values. The server keeps track of each subscription offset, and after streaming the data, it updates the cursor. This way, the client receives each piece of data exactly once. In other words, it avoids duplicate data. It also acts as some sort of state that ensures data does not get lost from the stream.In other words, streaming subscriptions uses WebSockets and GraphQL subscriptions to stream data from a starting point specified by the client.Streaming Subscriptions vs. Live QueriesSubscriptions react to specific events, whereas live queries observe data. A subscription reacts to an event like a record insertion, whereas the live query reacts to data changes.Their syntax differs as well. The live queries use the @live directive, which is not present in the subscription syntax. It\u2019s important to note that live queries in Hasura do not use the @live or any other directive. In Hasura, live queries are just a pattern in which you fetch the entire result of a given query every single time, mimicking the liveness of it.Lastly, the streaming subscriptions have some sort of state, whereas the live queries do not. The offset in streaming subscriptions allows the client to store the state of where they are reading from. With live queries, the client subscribes and gets the latest values.@defer and @streamThe @defer and @stream directives represent another way for GraphQL servers to stream data to the client. They enable GraphQL servers to split the requested data into multiple priority levels. This way, the GraphQL server can stream the critical data first and resolve the rest in separate response payloads.Consider the following example:query AuthorQuery($id: String) {\n    author(id: \"8j32hugjijgreij\") {\n        name\n        bio\n        avatar\n        ...SocialMediaLinks @defer(label: \"socialMediaLinksFragment\")\n    }\n}\n\nfragment SocialMediaLinks on Author {\n    socialMediaLinks {\n        twitter\n        instagram\n        youtube\n    }\n}\nThe use of the @defer directive on the SocialMediaLinks fragment indicates that it has a lower priority than the other fields. As a result, the fragment is excluded from the initial response and sent in the subsequent response payloads.With the @defer directive, you can specify the query parts that are not essential and can be delivered later. It\u2019s important to mention that the @defer directive can only be used on fragments.The @stream directive works similarly, but it can be applied only to fields of type List.Consider the following example:query AuthorQuery($id: String) {\n    author(id: \"8j32hugjijgreij\") {\n        name\n        bio\n        avatar\n        articles @stream(initialCount: 1, label: \"articlesStream\") {\n            title\n            url\n            brief\n        }\n    }\n}\nThe use of the @stream directive on the articles field means that it only returns one article in the initial response payload. The initialCount argument specifies how many records to return in the first payload.The rest of the articles are resolved asynchronously and returned in the following payloads.The @stream and defer directives solve the problem where the GraphQL response is sent only after the whole request is completed. With these two directives, you can send part of the data to the client in the initial response and send the rest in subsequent payloads.GraphQL ApplicationTo put the new learnings into practice, you will build an application that uses GraphQL subscriptions. The application is split into two parts:the GraphQL server - you build a server from scratchthe client - you build a client that consumes data from the serverGraphQL Server ImplementationYou will build the GraphQL server with Node.js, Express, and Apollo. The server does not use a database to avoid introducing extra complexity to the tutorial. It uses a JSON file to act as a database.Start by creating the project structure as follows:mkdir graphql-app\n\ncd graphql-app\n\nmkdir src\n\ntouch src/index.js src/schema.js src/resolvers.js data.js\nAfter running the above commands, you will have 4 files inside the src folder.The next step is initializing the Node.js project and installing the required dependencies. Run the following commands in the graphql-app folder:npm init -y\n\nnpm i express graphql graphql-subscriptions apollo-server-express apollo-server-core graphql-ws ws @graphql-tools/schema\n\nnpm i nodemon --save-dev\nThese dependencies are all required to build and run the GraphQL server. Before going further, open package.json and replace the \"scripts\" object with the following code:\"scripts\": {\n    \"start\": \"nodemon src/index.js\"\n }\nThis way, you do not have to stop and restart the server each time you make a change. The server automatically refreshes and shows the new changes.Server codeNow that the project is set up, it\u2019s time to build the server. Open the file index.js and write the following code:// GraphQL Server - index.js\n\nconst { ApolloServer } = require(\"apollo-server-express\");\nconst { ApolloServerPluginDrainHttpServer } = require(\"apollo-server-core\");\nconst { makeExecutableSchema } = require(\"@graphql-tools/schema\")\nconst { WebSocketServer } = require(\"ws\");\nconst { useServer } = require(\"graphql-ws/lib/use/ws\");\nconst express = require(\"express\");\nconst http = require(\"http\");\nconst typeDefs = require(\"./schema\");\nconst resolvers = require(\"./resolvers\");\n\n// from docs\n// https://www.apollographql.com/docs/apollo-server/integrations/middleware/#swapping-out-apollo-server\n// https://www.apollographql.com/docs/apollo-server/data/subscriptions\nasync function startServer() {\n    const app = express();\n    const httpServer = http.createServer(app);\n    const wsServer = new WebSocketServer({\n        server: httpServer,\n        path: '/graphql'\n    });\n\n    const schema = makeExecutableSchema({ typeDefs, resolvers });\n\n    const serverCleanup = useServer({ schema }, wsServer);\n\n    const server = new ApolloServer({ \n        schema,\n        plugins: [\n            ApolloServerPluginDrainHttpServer({ httpServer }),\n            {\n                async serverWillStart() {\n                    return {\n                        async drainServer() {\n                            await serverCleanup.dispose();\n                        }\n                    }\n                }\n            }\n        ]\n    });\n\n    await server.start();\n\n    server.applyMiddleware({ app });\n\n    httpServer.listen({ port: 4000 }, () => {\n        console.log(`\ud83d\ude80 Server ready at http://localhost:4000${server.graphqlPath}`);\n    });\n}\n\nstartServer();\nWhat\u2019s happening in the above code?Firstly, you import the required packages, schema, and resolvers at the top of the file.After that, you create an Express app and an httpServer. The httpServer is required to set up the HTTP and WebSocket servers, and it takes the Express app as an argument.Going further, you create a new WebSocket server using the httpServer created previously.You also need to build a schema from the type definitions and resolvers. That's what the method makeExecutableSchema allows you to do. The WebSocket server and ApolloServer then use the schema.After that, you use the userServer method to start the WebSocket server with the schema created previously.Lastly, you create the Apollo server.GraphQL SchemaIt\u2019s time to define the GraphQL schema for this application. Open the schema.js file and add the following code:// GraphQL Schema - schema.js\n\nconst { gql } = require(\"apollo-server-express\");\n\nconst typeDefs = gql`\n    type Note {\n        id: ID!\n        title: String!\n        content: String!\n        username: String!\n    }\n\n    type Query {\n        notes: [Note]\n        note(id: Int): Note \n    }\n\n    type Mutation {\n        addNote(id: ID!, title: String!, content: String!, username: String!): Note\n    }\n\n    type Subscription {\n        noteCreated: Note\n    }\n`;\n\nmodule.exports = typeDefs;\nThe above code describes the shape of the data. You can see that it describes what a \"Note\" looks like. It also specifies the available queries and the mutation a client can perform.In this case, the client can:retrieve all notesretrieve a specific noteadd a new noteget updates when a new note is addedGraphQL ResolversPreviously, you defined the available operations but did not implement the logic. For example, when someone uses the mutation addNote, how should it behave?That's what you will do now - you will implement the GraphQL resolvers. In the simplest terms, the GraphQL resolvers implement the logic required to resolve the queries, mutations, and subscriptions.// GraphQL Resolvers - resolvers.js\n\nconst { PubSub } = require(\"graphql-subscriptions\");\nconst notes = require(\"./data\");\n\nconst pubsub = new PubSub();\n\nconst resolvers = {\n    Query: {\n        notes() {\n            return notes;\n        },\n        note: (_, args) => {\n            const note = notes.find((note) => note.id === args.id);\n            return note;\n        }\n    },\n    Mutation: {\n        addNote: (_, args) => {\n            const newNote = {\n                id: args.id,\n                title: args.title,\n                content: args.content,\n                username: args.username\n            }\n\n            notes.push(newNote);\n            pubsub.publish(\"NOTE_CREATED\", { noteCreated: args })\n\n            return newNote;\n        }\n    },\n    Subscription: {\n        noteCreated: {\n            subscribe: () => pubsub.asyncIterator([\"NOTE_CREATED\"])\n        },\n    }\n}\n\nmodule.exports = resolvers;\nIn the above code, you use the PubSub class from the graphql-subscriptions package. The PubSub stands for \"publish/subscribe\", which is a messaging system. The system allows the publishers to broadcast messages asynchronously to the subscribers interested in a specific event.In this example, the clients can subscribe to the NOTE_CREATED event and receive updates whenever a new note is added.It\u2019s important to mention that the PubSub class is intended only for development purposes. It\u2019s not recommended for production. You can read more here.Dummy dataThe last step is to add some dummy data to the data.js file. Open the file and add the following code:// Dummy data - data.js\n\nlet notes = [\n    {\n        \"id\": 1,\n        \"title\": \"Learn React Hooks\",\n        \"content\": \"This is a reminder to continue learning about React hooks\",\n        \"username\": \"John Doe\"\n    },\n    {\n        \"id\": 2,\n        \"title\": \"Learn Hasura\",\n        \"content\": \"Learn how to build a backend with Hasura\",\n        \"username\": \"Kris Noel\"\n    }\n];\n\nmodule.exports = notes;\nNow you have some initial data for the application to work with.Test the applicationStart the application by running npm start in the \"graphql-app\" directory. After it starts, you can access the server at http://localhost:4000/graphql.You can use the Apollo studio to query and mutate the data. You can also subscribe to new updates.Apollo Studio showing the applicationThe above image shows an example of querying all the notes. Play around and try out the other queries, mutations, and subscriptions.GraphQL ClientThe last step involves building the front-end client with React. Run the following commands inside the graphql-app folder:npx create-react-app client\n\nnpm install @apollo/client graphql graphql-ws\nThe first command creates a React project and the second command installs the necessary packages. After the installation finishes, modify the project to look as follows:\ud83d\udcc2 graphql-app\n \u2514 \ud83d\udcc2 client\n   \u2514 \ud83d\udcc1 node_modules\n     \ud83d\udcc1 package-lock.json\n     \ud83d\udcc1 package.json\n     \ud83d\udcc1 public\n     \ud83d\udcc1 README.md\n     \u2514 .env\n     \ud83d\udcc2 src\n      \u2514 App.css\n      \u2514 App.js\n      \u2514 apolloClient.js\n      \u2514 index.css\n      \u2514 index.js\n      \u2514 \ud83d\udcc2 components\n        \u2514 InitialNotes.jsx\n        \u2514 LatestNotes.jsx\n        \u2514 Note.jsx\nNow that you have the project structure in place, you can start by configuring the Apollo client. Open the apolloClient.js file and add the following code:// Apollo client configuration - apolloClient.js\n\nimport { ApolloClient, InMemoryCache, split, HttpLink } from '@apollo/client';\nimport { getMainDefinition } from '@apollo/client/utilities';\nimport { GraphQLWsLink } from '@apollo/client/link/subscriptions';\nimport { createClient } from 'graphql-ws';\n\nconst httpLink = new HttpLink({\n    uri: 'http://localhost:4000/graphql'\n  });\n\nconst wsLink = new GraphQLWsLink(createClient({\n    url: 'ws://localhost:4000/graphql'\n}));\n\nconst splitLink = split(\n    ({ query }) => {\n      const definition = getMainDefinition(query);\n      return (\n        definition.kind === 'OperationDefinition' &&\n        definition.operation === 'subscription'\n      );\n    },\n    wsLink,\n    httpLink,\n);\n\nconst client = new ApolloClient({\n    link: splitLink,\n    cache: new InMemoryCache()\n});\n\nexport default client;\nSince the application supports subscriptions, you must set up a WebSocket connection. Queries and mutations use the HTTP protocol, but the subscriptions use the WebSocket protocol.You also have a split function that checks the operation type to determine what transport protocol to use. If it\u2019s a query or mutation, it uses the HTTP link or the WebSocket link if it\u2019s a subscription.Lastly, you create the Apollo client and export it so you can use it in other places.Note componentYou need to create the component that renders one note. Open the src/Note.jsx file and add the following code:// Note component - note.jsx\n\nimport \"../App.css\";\n\nfunction Note({ note }) {\n    return (\n        <div className=\"card\">\n            <h2>{note.title}</h2>\n            <p>{note.content}</p>\n            <p className=\"author\">by {note.username}</p>\n        </div>\n    )\n}\n\nexport default Note;\nYou will use this component in the InitialNotes.jsx and LatestNotes.jsx files.Initial notesWhen you built the GraphQL server, you added some dummy data - 2 example notes. The InitialNotes.jsx component is the one that renders those notes.// Initial notes - InitialNotes.jsx\n\nimport { gql, useQuery } from \"@apollo/client\";\nimport Note from \"./Note\";\n\nconst ALL_NOTES = gql`\n    query {\n        notes {\n            id\n            title\n            content\n            username\n        }\n    }\n`;\n\nfunction InitialNotes() {\n    const { data, loading, error } = useQuery(ALL_NOTES);\n\n    if (loading) return <div>Loading...</div>\n\n    if (error) return <div>Error occured...</div>\n\n    const notes = data.notes;\n\n    return (\n        <div>\n            { notes.map((note) => (\n                <Note key={note.id} note={note} />\n            )) }\n        </div>\n    )\n}\n\nexport default InitialNotes;\nThe component uses the useQuery hook from the Apollo client to execute the ALL_NOTES query. It then renders each note on the page.Latest notesYou also need a component to display the latest notes added to the database. That's the purpose of the LatestNotes.jsx component.The component uses the useSubscription hook to execute the LATEST_NOTE subscription. Whenever the hook receives data, it updates the allNotes state with the new data. Then it renders the latest notes on the page.// Latest notes - LatestNotes.jsx\n\nimport { gql, useSubscription } from \"@apollo/client\";\nimport { useState } from \"react\";\nimport Note from \"./Note\";\nimport '../App.css';\n\nconst LATEST_NOTE = gql`\n  subscription {\n    noteCreated {\n      id\n      title\n      content\n      username\n    }\n  }\n`;\n\nfunction LatestNotes() {\n  const [ allNotes, setAllNotesState ] = useState([]);\n\n  const { data, error, loading } = useSubscription(LATEST_NOTE, {\n    onSubscriptionData: ({ subscriptionData: { data } }) => {\n      setAllNotesState((state) => [...state, data.noteCreated])\n    }\n  });\n\n  if (error) return <div>Error occured...</div>\n\n  return (\n    <div>\n      { allNotes.map((note) => (\n        <Note key={note.id} note={note} />\n      )) }\n    </div>\n  )\n}\n\nexport default LatestNotes;\nEach time a new note is added, the LatestNotes.jsx component renders it on the page.Update index.js and .envBefore running the application, replace the content of index.js with the following code:import { ApolloProvider } from \"@apollo/client\";\nimport client from \"./apolloClient\";\nimport LatestNotes from \"./components/LatestNote\";\nimport InitialNotes from \"./components/InitialNotes\";\nimport './App.css';\n\nfunction App() {\n  return (\n    <ApolloProvider client={ client }>\n      <div className=\"App\">\n        <h1 className=\"title\">Notes</h1>\n        <div>\n          <InitialNotes/>\n          <LatestNotes/>\n        </div>\n      </div>\n    </ApolloProvider>\n  );\n}\n\nexport default App;\nNow open the .env file and add the following line:PORT=1234\nSave the files, and the client code is done!Test the applicationStart the client by running npm start in the client directory. Also, make sure the GraphQL server is running.Once the app starts, you can navigate to http://localhost:1234/ to test the application.The application pageAs you can see, it renders the initial notes on the page. You can test the subscription by going to the GraphQL server and adding a new note, as shown in the image below.Adding a new note with a mutation in Apollo StudioYou can copy the mutation and variables from this gist.You should see the new note displayed if you visit the client after performing the mutation. The image below shows the webpage with the new note.The application showing the new noteTo see the subscriptions in action, open the server and client in separate browser tabs and put them side by side. Then add new notes, and you will see them displayed on the page in real-time.You are done, congratulations! You learned about the different types of subscriptions and built an application that shows them in action.Custom GraphQL Server vs. Hasura\nIn this article, you built a GraphQL server manually. Even if the server is relatively basic, you had to use eight packages and write quite a bit of code. Imagine when the server gets more complex.\nSo what if I told you there is another way to build a GraphQL API? You can use Hasura, a GraphQL Engine that turns your database into an instant, real-time GraphQL API.\nIn simpler terms, Hasura connects to the provided database, and it automatically generates an API based on the tables and views. It gives you GraphQL schema and resolvers out of the box without writing any backend code.\nTo see the difference between a custom-built GraphQL API and Hasura, you will implement the same API with Hasura.\nBuild the GraphQL API with Hasura\nThe fastest and easiest way to use Hasura is via the Hasura Cloud. Click the button to sign-up:\n\nAfter that, create a new Hasura project and launch the console. The figure below illustrates what you should see.\n\nThe next step involves connecting the application to a database. You can connect your app to an existing database or a new one.\nIn this example, the application uses a new database.\n\nClick the button saying \"Create Database\" to add a new database.\nAfter adding the database, navigate to \"Data -> Create table\" to create a new table. Create the table note with the following columns:\n\nid of type UUID (Primary Key)\ntitle of type Text\ncontent of type Text\nusername of type Text\n\n\nSave the table and you are done! You have a GraphQL API, which you can use to query and mutate data. It also comes with GraphQL subscriptions by default.\n\nIf you need help with getting started on the cloud, adding a database, or creating a table, check the documentation.\n\nTest the API\nLet's test the API by adding, reading, and subscribing to notes. Go to the \"API\" tab and perform the following mutation:\nmutation add_note {\n  insert_note_one(object: {\n    title: \"Learn Hasura\", \n    content: \"Learn how to build a GraphQL API with Hasura\", \n    username: \"John Doe\"\n  }) {\n    id\n  }\n}\n\nThe image below illustrates the mutation in action. Looking at the \"Explorer\" tab, you can see all the available mutations. You can add, update and delete notes.\n\nNow let's query the database to see the note added previously. Run the following query:\nquery get_notes {\n  note {\n    id\n    title\n    content\n    username\n  }\n}\n\nThe image illustrates the query in action. You can see it returns the note you added previously.\n\nLastly, you can test the subscriptions. Run this subscription in GraphiQL:\nsubscription notes_subscription {\n  note {\n    id\n    title\n    content\n    username\n  }\n}\n\nNow open the project console in another tab and perform a mutation. If you check the subscription, you should see the new note.\n\nAt this point, you tried all the GraphQL operations: queries, mutations, and subscriptions. You have all those operations out of the box without writing a single line of code.\nComparison\nThe first noticeable difference between a custom-built GraphQL API and Hasura is that you do not need to write any code with Hasura. You have an API up and running in just a few clicks. You do not have to create things like the GraphQL schema and resolvers yourself.\nHasura also comes with real-time capabilities, such as GraphQL subscriptions and live queries by default. You do not need extra configuration.\nIn case you need custom business logic or you need to plug in your existing GraphQL/REST APIs, you can do it through Remote Schemas and Actions.\nMoreover, Hasura has other features such as authorization, caching, rate limiting, monitoring, and analytics. You can see the complete list of features here.\nComparing the two, we can conclude that building a GraphQL API with Hasura is faster, and it also comes with many features out of the box. It speeds up the development process considerably and lets you focus on more important things.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Exploring faster reads with Google AlloyDB PostgreSQL and Hasura GraphQL API Description: In this post, we will explore connecting to AlloyDB step by step and finally make queries to see performance impact in comparison to standard Postgres. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Google announced the launch of AlloyDB, a PostgreSQL compatible SQL database service in their I/O 2022 event. This is comparable to AWS Aurora PostgreSQL but Google claims better performance; 2x faster than Aurora, 4x faster than standard Postgres and even 100x faster for analytical workloads.Update: On December 15, 2022, Google announced the general availability of AlloyDB and also announced the AlloyDB Partner Ecosystem. We are excited to announce that Hasura is a launch partner in this ecosystem.Hasura should be able to connect to a fully compatible PostgreSQL database. In this post, we will explore connecting to AlloyDB step by step and finally make queries to see performance impact in comparison to standard Postgres.Table of Contents\nDeploy GraphQL Engine on Hasura Cloud\nCreate a AlloyDB Cluster\nConnect securely using AlloyDB auth proxy\n\nCreate GCE Instance\nRun auth proxy\nAdd Firewall Rule\n\n\nAdd AlloyDB to Hasura Cloud Project\nQuery performance\n\nDeploy HasuraThe fastest and easiest way to try Hasura out is via Hasura Cloud. Click on the following button to deploy GraphQL engine on Hasura Cloud: \nCreate a AlloyDB ClusterHead to https://cloud.google.com/alloydb to create a new cluster.Head to your Cloud Console AlloyDB page.Create a clusterCreate Cluster AlloyDBEnable the required APIs in the next stepSelect the cluster typeAlloyDB Cluster TypeConfigure your cluster with cluster ID, postgres password, region of choice, network path and click on ContinueConfigure your primary instance with an instance ID and machine type.Click on Create Cluster. The cluster creation takes sometime. Meanwhile, let us setup the Auth proxy so that we can securely connect to AlloyDB from an authorized instance.Connect securely using AlloyDB auth proxyThe AlloyDB Auth proxy gives IAM based connection authorization with a secure and encrypted 256-bit AES cipher between your client and an AlloyDB instance.The idea is to run the Auth proxy somewhere authorized to connect to AlloyDB instance (say a GCE instance with IAM / service account configured) and expose the authorized instance to the outside world (like Hasura Cloud) to be able to establish a connection.Create GCE Instance Create a Compute Engine VM that can connect to AlloyDB instances using private services access.Head to VM Instances pageSelect the project containing the AlloyDB instance you want to connect to.Click on Create instance, enter the name for the instance and set the following:Access scopes (located under Identity and API access): set to Allow full access to all Cloud APIs.fnNetwork interfaces: set to the VPC network configured for private services access to AlloyDB:a. Expand Networking, disks, security, management, sole-tenancy.b. Expand Networking.c. If Network interfaces is not set to the VPC network configured for private services access, expand it and then set Network to the VPC network.Tip: Select the region of GCE instance closer or same as Hasura Cloud projectGet IP Address of the AlloyDB InstanceHead to the cluster listing page for AlloyDB and get the private IP address of the instance. Now this will be used in the next step to run the auth proxy connecting to the AlloyDB postgres instance.Run Auth ProxyDownload the Auth proxy from hereMake the client executablechmod +x alloydb-auth-proxy\n        Since client host is a Compute Engine instance, you can use the Compute Engine default service account to authorize connections.Run the following command to start the Auth proxy./alloydb-auth-proxy \"projects/<project-id>/locations/<region>/clusters/<alloydb-cluster-id>/instances/<alloydb-instance-id>\" --address \"0.0.0.0\"This would start the Auth proxy client and expose it to the public. Do note that this setup is a ephermeral one. You should run the auth proxy in a permanent mode (for example via docker detached mode).Add Firewall RuleOnce the Auth proxy is running, we should add a firewall rule that allows connection from Hasura Cloud project to this AlloyDB instance.Copy the Hasura Cloud IP for your project from the Hasura Cloud dashboard.Create a new firewall rule for the network interface attached to the AlloyDB instance and allow tcp:port 5432 and in the source IPv4 range, specify the Hasura Cloud IP. For example: 154:176:149:52/32.Add AlloyDB to Hasura Cloud projectAll the configuration to connect an AlloyDB postgres to Hasura cloud project is done. Let us form the connection string now.postgresql://postgres:<postgres-password>@<ip-address-of-gce-instance>:5432/postgresReplace the postgres passwordReplace the IP address by copying the external IP of the GCE instance running the Auth proxy to AlloyDB.Head to Hasura Cloud project details page and navigate to Env vars.Add a new ENV var called PG_DATABASE_URL and give the above postgres connection string value.Go back to the Hasura Console of the project and add this ENV var to the new data source.This completes the hard part of the whole setup. Let us look at the query performance impact.Query PerformanceComparing same query with Google Cloud SQL, AlloyDB and Aurora Postgres, all in the same region connected to a Hasura Cloud instance.Notes on configurationAlloyDB - 8vCPU, 64GBCloud SQL - 8vCPU, 64GBPostgres version: 14.2 (AlloyDB says Postgres 14 compatible).Hasura Cloud version - v2.8.4-cloud.1Region - us-centralRunning pgbench to load sample dataWe need to insert some large amounts of data into both the standard cloud SQL and AlloyDB instance. Making use of pgbench, we inserted a million rows.pgbench -h <postgres-host> -p 5432 -U postgres postgres -i -s 10Fine tuning AlloyDBWe need to enabe the AlloyDB Columnar Engine (CE). First, we need to adjust the instance so that the new google_columnar_engine.enabled flag is set to ON. This would restart the instance, so please be patient.Next up, create the columnar engine extensionCREATE EXTENSION google_columnar_engine;Optionally set the google_columnar_engine.memory_size_in_mb to a larger value.Load the table into the column store.SELECT google_columnar_engine_add('pgbench_accounts');GraphQL Query via HasuraLet us track the table pgbench_accounts to start making the GraphQL query. Finally make the following GraphQL query:query {\n  pgbench_accounts {\n    aid\n    bid\n    filler\n  }\n}\nThis will fetch all the 1M rows from the pgbench_accounts.There is a notable difference in query response times and improved execution times on the AlloyDB cluster instance.We will update detailed benchmark numbers here soon, but roughly we saw a 3x improvement in response times.Do you have a workload or a use case that runs analytical queries or heavy transactional queries? If so, AlloyDB might be worth a look for the performance gains."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura achieves ISO 27001 Certification Description: Hasura has achieved certification to the International Organization for Standardization\u2019s information security standard 27001. ISO 27001 standard is widely recognized international standard detailing best practices for the management of information security and privacy. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Hasura has achieved certification to the International Organization for Standardization\u2019s information security standard 27001. ISO 27001 standard is widely recognized international standard detailing best practices for the management of information security and privacy.Hasura achieves ISO 27001 CertificationISO/IEC 27001:2013The ISO/IEC 27001:2013, establishes requirements for the effective management of information security via an Information Security Management System (ISMS), and provides the foundations for an effective security program.Hasura is at the key layer of the application stack providing data access from various sources via performant APIs to the clients. This certification is an important milestone in our efforts to continuously improve security across our product lines and operations and cater to mission critical infrastructure and application use-cases.We achieved multi-site certification, covering our primary office locations in San Francisco, United States and Bangalore, India.Hasura had earlier achieved SOC2 Type 2 and HIPAA compliance certifications to re-inforce the confidence in Hasura Cloud\u2019s ability to securely store and transmit your data, and pass the requirements of your organization\u2019s IT department.Hasura undergoes rigorous security compliance audits and you can learn more about that in our security page.Get Started!If you haven\u2019t tried Hasura Cloud, I encourage you to give it a try and see how quickly you can create a GraphQL API from an existing database. \u00a0Check out our Getting Started guide, as well as our GraphQL learn tutorials!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Vercel Integration with Hasura Cloud Description: Connect your front-end Vercel applications with a Hasura Cloud project and keep them in sync as you keep deploying changes to your projects Author: Praveen Durairaju"}, {"role": "assistant", "content": "We are happy to announce the Vercel integration with Hasura Cloud letting you connect your Hasura Cloud GraphQL and REST APIs with Vercel projects for a seamless fullstack development experience.Hasura Vercel IntegrationWhat does the integration help with?The integration helps you create a new Hasura Cloud project that can be attached to one or more Vercel projects serving as the unified backend for your frontend apps. The Hasura Cloud GraphQL API endpoint and the admin secret is auto populated as environment variables accessible to your frontend project.At any point of time, the Hasura Cloud project can be mapped to one or more of your Vercel projects. This is useful if you are running multiple environments of your app with the same backend for testing. You can configure the access and scope of projects in the integrations page of your Vercel account.Environment variablesThe admin secret is exposed safely only to the server side of your app through the HASURA_GRAPHQL_ADMIN_SECRET environment variable. This is useful inside the serverless functions that you write with Next.js or other frameworks where you directly interact with the GraphQL API as an admin. The functions need this admin privilege for fetching or inserting some data with elevated privileges.The API endpoint is exposed on NEXT_PUBLIC_HASURA_PROJECT_ENDPOINT and you can use this on the frontend to construct both the GraphQL and REST API endpoints as necessary.Try it outHead over to the Vercel <> Hasura integration on Vercel's integrations marketplace. Click on Add Integration and configure the projects.How do I know if the integration is setup?As soon you install the Integration on Hasura Cloud, you should see the right environment variables populated on your Vercel project(s). You can find this by heading to https://vercel.com/<username>/<project-name>/settings/environment-variables. On Hasura Cloud dashboard, you should see the Vercel tag added to your project.You can check the list of installed integrations on Vercel here.Help & ResourcesIf you are facing any issues with the integration, file a support ticket by navigating to the Help & Support tab on the Hasura Cloud dashboard. Join our discord channel for community support.Check out the Vercel Integration documentation on Hasura Cloud.Learn more about Vercel IntegrationsDemo App to try it outLooking to learn more about the integration? We did an episode on The Data & API show featuring Lee Robinson from the Vercel team going over the integration along with Jesse from Hasura.Jesse also gave a demo at this year's HasuraCon. Do check that out as well."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Ultimate Guide To Developing an Angular App With GraphQL, Amazon Rds and Hasura Description: In this tutorial, you will learn how to bind GraphQL with Angular framework using the Apollo Client. AWS RDS will be our Postgres database and we will deploy Hasura on it. Author: Hasura"}, {"role": "assistant", "content": "Writing data and querying an API using GraphQL can make your software development process faster. Using GraphQL you get exactly what you asked for, compared to a REST API which gives you a whole data set that could lead to data transfer waste. Querying APIs with GraphQL technology means we can finally bid farewell to over-fetching.Binding any software application with GraphQL will improve your app\u2019s performance as you only select fields you need. In this tutorial, you will learn how to bind GraphQL with Angular framework using the Apollo Client. AWS RDS will be our Postgres database and we will deploy Hasura on it. Hasura GraphQL engine will enable us to connect to our GraphQl API swiftly. To develop our GraphQL API efficiently we will use Hasura to do the following;Create a table.Set a foreign key.Query response caching.To grasp the concept better, our Angular app will be creating news articles using mutations and fetching articles so that they can be injected into the user interface.TABLE OF CONTENTS1. Deploying Hasura on AWS RDS Postgres.2. Database modeling.3. Binding GraphQL with Angular using Apollo.4. Mutation5. Querying6. Query Response caching using Hasura GraphQL engine.7. ConclusionTL;DR: You will learn how to integrate GraphQL with Angular using the Apollo client. And also get to deploy Hasura on AWS RDS Postgres database and use Hasura\u2019s query response caching feature.Prerequisites and RequirementsThis tutorial will simply explain GraphQL concepts so you don\u2019t need to have a lot of knowledge on GraphQL. You will use the following technologies in this tutorial.\u25cf Node.js\u25cf Angular framework.\u25cf Hasura GraphQL engine.\u25cf Apollo GraphQL Client.\u25cf Amazon RDS as the Postgres database.Deploying Hasura on AWS RDSHasura GraphQL engine improves accessibility by connecting to your Postgres database and offering you a swift real-time GraphQL API making software development much easier. To connect Hasura to Amazon RDS Postgres we will start off by creating a database and then move on to getting the database credentials required when creating a Postgres URL.Creating an AWS accountI will give a guideline on how to create an AWS RDS Postgres database. For more on the Hasura deploying documentation check this link.The Amazon web services provide a free Tier for the amazon RDS which helps new AWS customers get started with a managed database service in the cloud for free. For verification purposes it will carry out a $1 transaction for credit card validation when creating the AWS account and the $1 will be returned.Go ahead and create the AWS account here. Choose the root user account type on the account options.Creating a database and constructing the Amazon RDS Postgres URLTo deploy Hasura on a Postgres database you will need a Postgres URL. We will first create the AWS RDS Postgres database and then go on to creating the Postgres URL using our database credentials.After you have successfully logged in, search for AWS RDS on the search bar. Once you find it in the list click on it to access it. Once you have chosen the Amazon RDS, on the dashboard you will see the create a database button. Click on it to start creating the Amazon RDS Postgres database.In Engine options, select Postgres as your Engine type.When configuring the settings choose a DB instance identifier as the name for your database and postgres is the Master user name by default. You can choose to write your own password or allow amazon to automatically generate one for you.If you have allowed amazon to create the database password for you, the password will be shown in the database credentials after you have created the database.Enable Public access and choose an existing VPC security group or add a new group. The security group will function as a virtual firewall for your instance in order to control inbound traffic.When you are done click on the create button.Allowing connections to your Database from Hasura CloudClick on the active VPC security group you chose.Click on the security group highlighted in blue.Now click on the edit inbound rules button below.The deploying partLogin to Hasura cloud and create a new project. After creating the project, it will automatically redirect you to the project dashboard.In the project dashboard, you can see the Hasura Cloud IP, which you will need in the next step.The next step is to click on the add rule button and enter the Hasura IP address you copied in the previous step and set the port to 5432.After adding the rule click on the save rules button.Creating your database connection URLContents of database connection URLuser-name: The default user name is postgres but if you have specified and have a separate database the user name will be your name.Password: Here, use the password you chose when creating the password.Public ip: Your endpoint is the public IP. You can find it under the connectivity and security tab.postgres-port: When creating the rules we set the port to 5432 as it is the default port.db is postgres by default.This is the structure of our URL below.postgresql://<user-name>:<password>@<public-ip>:<postgres-port>/<db>Now let's finish the task by inserting the URL. Add the database connection string in the \u201cDatabase URL\u201d and click \u201cConnect Database\u201d.And that\u2019s it!Database Design and structureBefore we hop onto GraphQL schema creation let's take a look at the database structure. Our articles table will have these fields article_Id, author, title, story. The articles_Id field will be our primary key. The Users table will have two fields which are category and author.Let us create a table using the Hasura GraphQL engine. To create a table in Hasura GraphQL engine click on the Data tab and select create table.Linking related data fieldsWith Hasura we can create a foreign key and it will automatically suggest a suitable relationship between the fields looking at table details. The author field is a foreign key as it points to the primary key of the Article table. To create a foreign key, scroll down and click on create a foreign key.Setting up AngularTo set up or install the angular framework, you must have node.js installed on your computer. You can download node.js from here.The following command lets you install angular using your favorite command line.npm install -g @angular/cliCreating a projectAfter installing angular you can go ahead and type in the following command to create a new project.ng new news-appBinding GraphQL with Angularhttps://www.apollographql.com/docs/intro/platform/Apollo is a GraphQL client, its task is to handle network requests and inject data into the web user interface. With a large developer ecosystem, it is easy to find tutorials and documentation compared to its rival Relay. Since Apollo supports any GraphQL server and GraphQL schema, we will integrate it with the Angular framework.Installing ApolloIn order to use Apollo Client for Angular, we need to add a few packages to our project. Execute the following command within the Angular project directory:npm install @apollo/client graphqlImport the following angular modulesThe ApolloModule provides entrance to Apollo client features in our web app while the HttpLinkModule is used to fetch data in Angular.import { BrowserModule } from '@angular/platform-browser';import { FormsModule } from '@angular/forms';import { NgModule } from '@angular/core'import { HttpClientModule\u00a0 } from \"@angular/common/http\";import { APOLLO_OPTIONS } from \"apollo-angular\";import { HttpLink } from 'apollo-angular/http';import { InMemoryCache } from '@apollo/client/core';\u00a0We will enable in-memory caching by adding the caching property and passing an instance of InMemoryCache. In order to connect to our GraphQL endpoint, we use the create method which takes in a configuration object. This object contains a uri property, which will be your GraphQL endpoint.imports: [\u00a0\u00a0\u00a0\u00a0BrowserModule,\u00a0\u00a0\u00a0\u00a0AppRoutingModule,\u00a0\u00a0\u00a0\u00a0FormsModule,\u00a0\u00a0HttpClientModule\u00a0\u00a0],\u00a0\u00a0providers: [{\u00a0\u00a0\u00a0\u00a0provide: APOLLO_OPTIONS,\u00a0\u00a0\u00a0\u00a0useFactory: (httpLink: HttpLink) => {\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return {\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cache: new InMemoryCache(),\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0link:\u00a0 httpLink.create({\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0uri: '[URL]',\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0headers: {\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Authorization: `Bearer ${localStorage.getItem('token')}`\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0})\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0};\u00a0\u00a0\u00a0\u00a0},\u00a0\u00a0\u00a0\u00a0deps: [HttpLink]\u00a0\u00a0}]\u00a0Let's start bindingThere are two ways we can create GraphQL schemas. The first way is to express GraphQL schemas as Javascript objects based on the graphql-js library and the SDL will be auto-generated from source code. The second way is to describe our GraphQL schemas in SDL and link up business logic using the Apollo graphql-tools library. For this tutorial, we will use the first approach.Creating a news article\u201cIf queries are GET requests, mutations can be seen as POST/PATCH/PUT/DELETE requests (although really they are synchronized versions of queries)\u201d~Amaury MartinyThe process of writing data in GraphQL is called Mutation. This procedure can involve creating data or deleting data. We will use mutation to create a news article by adding details to these fields title, author, story. The mutation endpoint on the server is shown below.type Mutation {createArticle(title: String!, author: String!, story: String):Article}Now lets bind and access the mutation endpoint using the Apollo client.import { Component, OnInit } from '@angular/core';import { Apollo } from 'apollo-angular';import gql from 'graphql-tag';@Component({\u00a0\u00a0selector: 'app-root',\u00a0\u00a0templateUrl: './app.component.html',\u00a0\u00a0styleUrls: ['./app.component.scss']})export class AppComponent implements OnInit {\u00a0\u00a0data: any;\u00a0\u00a0constructor(private apollo: Apollo) { }\u00a0\u00a0ngOnInit() {\u00a0\u00a0\u00a0\u00a0this.apollo.mutate({\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0mutation: gql`mutation {\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0createArticle(title:\"Why Use GraphQL\", author: \"Hasura\", story: \"Graphql deletes overfetching in many ways..\")\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0article_Id\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}`\u00a0\u00a0\u00a0\u00a0}).subscribe(data => {\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0//successfully created an article.\u00a0\u00a0\u00a0\u00a0});\u00a0\u00a0}}In this call, we only asked for the article_Id field and the server will give us this field only from the data object. When using the Apollo mutate method we do not have to explicitly handle the results, the results will be automatically integrated into the cache.Fetching news postsA query simply describes the data that you want to fetch from a GraphQL server. Mutations and queries both have similar structures the main difference is that they have different operation names. We will fetch our articles using the Apollo watchquery method. To achieve this task we need to start off by parsing our query into a GraphQL document using the gql tag from apollo/client/core library. The GetArticles query will fetch the article title, author, and story.//\u00a0 use the gql tag to parse our query string into a query documentconst GET_ARTICLES = gql`\u00a0\u00a0query GetArticles {\u00a0\u00a0\u00a0\u00a0articles {\u00a0\u00a0\u00a0\u00a0title\u00a0\u00a0\u00a0\u00a0author\u00a0\u00a0\u00a0\u00a0story\u00a0\u00a0\u00a0\u00a0}\u00a0\u00a0}`;Inside the Articlescomponent class Create a result object which contains a boolean loading. After fetching articles the boolean will be set to false. The Apollo Watchquery method can continue setting the boolean to false if you specify and set its parameters as notifyOnNetworkStatusChange to true. When the query has been accomplished it contains a data object with articles.class ArticlesComponent implements OnInit, OnDestroy {\u00a0\u00a0loading: boolean;\u00a0\u00a0articles: any;\u00a0\u00a0private querySubscription: Subscription;\u00a0\u00a0constructor(private apollo: Apollo) {}\u00a0\u00a0ngOnInit() {\u00a0\u00a0\u00a0\u00a0this.querySubscription = this.apollo.watchQuery<any>({\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0query: GET_ARTICLES\u00a0\u00a0\u00a0\u00a0})\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0.valueChanges\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0.subscribe(({ data, loading }) => {\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0this.loading = loading;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0this.articles = data.articles;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0});\u00a0\u00a0}\u00a0\u00a0ngOnDestroy() {\u00a0\u00a0\u00a0\u00a0this.querySubscription.unsubscribe();\u00a0\u00a0}}Improving query response performanceTo improve query response performance for our GetArticles query which will be frequently used we will enable the query response caching in Hasura. To do so we will simply add @cached in our query directive.query GetArticles @cached(ttl: 240) {\u00a0\u00a0articles {\u00a0\u00a0\u00a0\u00a0title\u00a0\u00a0\u00a0\u00a0author\u00a0\u00a0\u00a0\u00a0story\u00a0\u00a0}}The ttl argument specifies the lifetime of the query response. The maximum you can set is 5 minutes. Once our response is cached fruitfully the HTTP response will include a X-Hasura-TTLheader .In conclusionThanks for reading! Those were the basic concepts to start off with building an angular app with GraphQL. I hope you have learned a lot from this tutorial and now ready to advance onto major topics such as caching and testing GraphQL API.\u201cThere is no real ending. It\u2019s just the place where you stop the story.\u201d~Frank Herbert"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Join data from a Database with GraphQL API using Database to Remote Schema Joins Description: The Database-to-Remote Schema Joins feature enables you to connect the data from your existing database with an external GraphQL service. Author: Catalin Pit"}, {"role": "assistant", "content": "GraphQL Joins allow you to create a unified GraphQL API by joining the data from different GraphQL sources.With GraphQL Joins, you can federate your queries and mutations across multiple GraphQL sources as if they were a single GraphQL schema. You do not have to write extra code or change the underlying APIs.Hasura supports the following types of joins:Remote Schema to Remote Schema (RS-to-RS) JoinsDatabase to Database (DB-to-DB) JoinsDatabase to Remote Schema (DB-to RS) JoinsRemote Schema to Database (RS-to-DB) JoinsThis article will help you learn about the Database to Remote Schema Joins.How do DB-to-RS Joins work?The Database-to-Remote Schema Joins feature enables you to connect the data from your existing database with an external GraphQL service.That's possible by creating a relationship from a field in the database to a field in the external GraphQL service.E-Commerce Example with HasuraFor this example, you have a fulfillment service and a Hasura application.The fulfillment service is an example of a shipping company schema, with each fulfillment having an order ID and a status.enum Status {\n  PACKING\n  SHIPPED\n  DELIVERED\n}\n\ntype Fulfillment {\n  id: Int!\n  orderId: Int!\n  status: Status!\n}\n\ntype Query {\n  fulfillment(orderId: Int!): Fulfillment!\n  fulfillments: [Fulfillment]!\n}There is also a Hasura application that stores the users and keeps track of their orders. When you query the database, you can retrieve the users and their order status.The figure below shows the structure of the \"users\" table.Hasura \"users\" TableThe next step involves adding the fulfillment service as a remote schema to Hasura.\"fulfillment\" Remote SchemaNow that the service is present as a remote schema, you can create a relationship between the database and the \"fulfillment\" remote schema.Go to the \"users\" table, then to the \"Relationship\" tab, and click the button \"Add a remote schema relationship\". That opens a new tab where you can configure the relationship.Adding a new remote schema relationshipFor the name, you can choose something like order_status. For the \"Remote Schema\" field, choose fulfillment. Lastly, for the \"Configuration\" select fulfillment > orderId > From Column > order_id.The orderId field from the fulfillment remote schema maps to the order_id field from the Hasura application.Save the relationship and you are done! You can test the relationship with the following query:query {\n  users {\n    name\n    address\n    order_status {\n      orderId\n      status\n    }\n  }\n}The query retrieves all users, their order id, and the status of that specific order. Running the query would return something similar:{\n  \"data\": {\n    \"users\": [\n      {\n        \"name\": \"Ali Mar\",\n        \"address\": \"15 Santana Avenue, Devtown, 471621284\",\n        \"order_status\": {\n          \"orderId\": 1,\n          \"status\": \"PACKING\"\n        }\n      }\n    ]\n  }\n}The Hasura application and remote schema are now connected! The users and their order status can be queried simultaneously. You managed to do it without altering the external GraphQL API or writing any code.If you want to read more about GraphQL Joins, you can do it here."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Product Announcements at HasuraCon\u201922: making GraphQL the ultimate Data API Description: Hasura releases GraphQL Data Connectors alpha SDK, Streaming Subscriptions API, Hasura availability on AWS Marketplace, support for Open Telemetry as well as General Availability of a GitOps workflow with Hasura Cloud and Hasura Cloud on Google Cloud Platform.  Author: Rajoshi"}, {"role": "assistant", "content": "The third edition of HasuraCon has just kicked off, and our biggest announcement this year just dropped - an SDK to make it easy to extend Hasura to any data source, making GraphQL available on any datasource! Here is the list of all of the product announcements at this year\u2019s HasuraCon. Join us to learn more about each of these at the dedicated sessions during the next 2 days at HasuraCon:\nExtend Hasura to any data-source:\n\nAnnouncing Hasura GraphQL Data Connector SDK (alpha release)\n\n\nAPI evolution to support emerging data workloads:\n\nStreaming Subscriptions: capture large data and fast-moving data\nMS SQL Event Triggers: To build event driven workflows on SQL Server with native CDC\n\n\nGraphQL DevOps improvements:\n\nGitOps for GraphQL: Automated CI/CD with Github Integration in GA on Hasura Cloud\nEnhanced observability: support for OpenTelemetry Traces in Hasura Cloud & EE\n\n\nIncreased cloud-vendor availability:\n\nHasura Enterprise on AWS Marketplace\nHasura on Google Cloud Platform\n\n\n\nIntroductionGraphQL is an API loved by developers - it\u2019s a joy to consume, has a very rich ecosystem of tooling, but remains hard and time-consuming to build, maintain and integrate with existing workflows. When we launched Hasura in 2018 - as an instant GraphQL API on Postgres, we immediately saw tremendous adoption across the ecosystem. Developers were able to get GraphQL, but didn\u2019t have to spend time building the GraphQL server. It just worked.Hasura has seen adoption across the spectrum, from developers building side-projects, to teams in tech giants reducing the amount of time they spend on undifferentiated heavy lifting, to mission-critical performance and security critical workloads in some of the most regulated environments. Here are the top 3 use-cases for Hasura users:Application Backend: Hasura is used by application development teams to build the backend of their applications. Hasura takes care of all CRUD APIs, real-time APIs as well as access control for the APIs, leaving only business logic APIs to be written by the developers.Centralised Data Access Layer: Hasura is used by data & platform teams to have realtime APIs and centralised authorization and governance for the API for analytical and operational data in their data warehouses & data lakes. Data from warehouses is ELT'd into a Hasura supported relational databases like Postgres to be able to use Hasura's realtime engine. (Watch Dan Keeley's talk at HasuraCon'22 for to see this in action) .Federated Data API: Hasura is also used by platform and app teams to create a unified data API for their organization, for other teams or external vendors. Hasura is connected to the different data systems directly to create a unified GraphQL API. With Hasura\u2019s powerful querying abilities, including data federation APIs (cross domain joins between databases and GraphQL APIs) and the authorization engine, the GraphQL Data API can be directly used by applications and other APIs. The key motivation in all three of these use-cases for Hasura users is to be able to build products faster - by eliminating the need to write data-access APIs and build authorization rules for each model in their API. \u00a0Today\u2019s announcements are a huge step for Hasura across 3 vectors:Data Sources (input to Hasura): increasing our coverage for being where your data is, in different data sources, or cloud vendors.GraphQL API (output from Hasura): The instant data API that Hasura provides is constantly evolving to support more sophisticated data workloadsDevOps with GraphQL: As GraphQL becomes part of your stack, we are constantly working towards improving how GraphQL can fit into your existing workflows & production checklists. Hasura GraphQL Data Connector SDK (alpha release)Hasura is only as powerful as the data sources it can connect to. Since we launched our database waitlists, one of the most frequent requests from the community, customers as well as database vendors, has been to add support for the data sources they use most often. This year, we made a critical decision to create an architecture where not only are we able to add first party support for databases faster, but we can also enable the community to add support for their favorite data sources - whether it\u2019s an operating system, any SaaS vendor (say Airtable) or a proprietary database unique to the organization. As a first step of this journey, we\u2019re delighted to announce an alpha-release of an SDK that makes it easy to build a GraphQL Data Connector to any data source.We have a talk starting at 9:30AM PDT today (28th June) that will go into more details about how this works, and you can read more about it in this blogpost.The GraphQL Data Connector Architecture\u00a0Here are the other announcements from the conference - and we have dedicated sessions going into more details about each of these. So make sure you join us for the rest of the conference: Hasura Cloud Enterprise Availability in AWS Marketplace: Hasura's Cloud Enterprise offering is now available on the AWS Marketplace. AWS customers will benefit from simplified procurement and billing and the ability to take advantage of AWS discount programs. Get started here. Hasura Cloud Enterprise on AWS MarketplaceHasura on Google Cloud Platform (GCP) General Availability lets Hasura Cloud and Cloud Enterprise users with databases and other services on GCP create Hasura projects in GCP, reducing egress charges and improving performance due to co-location. Hasura customers using Virtual Private Cloud (VPC) on GCP are now able to peer with their own VPCs - increasing performance and security. With Google announcing some incredible databases, we\u2019re excited to see our users use Hasura with GCP Postgres, BigQuery and new databases like AlloyDB!Streaming Subscriptions API allows users to build high-performance real-time apps using Hasura, streaming data to their clients in batches and updating data in real-time. Streaming subscriptions in Hasura GraphQL Engine allows users to stream data using a GraphQL subscription API and the websocket transport protocol. The API can be used to build real-time apps that can receive \u201csubscribed\u201d data as a stream (continuous events). This makes fetching large volumes of data possible and getting new data instantly possible.An API for streaming dataEvent Triggers Support for MS SQL Server enables users to connect business logic APIs with Hasura to integrate asynchronous backend logic into their applications by calling webhooks on table events such as insert/update/delete for SQL Server data sources. Event triggers have been one of the most loved features that Hasura first introduced on Postgres, and we\u2019re now excited to have support for event triggers for MS SQL Server databases enabling users to connect business logic APIs with Hasura to integrate asynchronous backend logic into their applications. GitHub integration General Availability increases ease of CI/CD pipeline management, reliability and development speed by helping developers move quickly and easily between local development and Hasura Cloud. A preview apps feature (still in beta) enables users to quickly test changes to metadata migrations and link to GitHub PRs to automatically deploy with every push.Support for OpenTelemetry Traces in Hasura Cloud and Hasura Enterprise Editions gives customers enhanced observability through traces in the OpenTelemetry format via OpenTelemetry compatible agents and direct connections with APM tools such as New Relic and Dynatrace. This enables them to provide better reliability and end user experiences for their mission-critical applications through rapid problem diagnosis. Join us for the remainder of HasuraCon to learn more about each of these announcements, hear from our users how they\u2019re using Hasura, and learn about the larger trends in the ecosystem with GraphQL & Serverless and the infrastructure layer that is the Data API.If you're looking to take the features announced today for a spin, head to Hasura Cloud or our docs to get started."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura GraphQL Data Connectors Release Description: Today we\u2019ve released the alpha of the Hasura GraphQL Data Connectors as open source. This is a combination of an API Agent, an SDK to develop against that agent, and a reference application to show a working example of how it all works to provide new connections to varied data sources. Author: Adron Hall"}, {"role": "assistant", "content": "Today we\u2019ve released the alpha of the Hasura GraphQL Data Connectors as open source, to make it easy for anyone to build a data connector to any data source and get instant GraphQL APIs on it. For those new to Hasura, Hasura provides an instant GraphQL API on a data source with authorization built in. Today, Hasura natively supports Postgres (and Postgres family of databases), SQL Server and BigQuery and will be adding support for many more databases over time. Hasura GraphQL Data Connectors will allow users, vendors and companies to now get Hasura\u2019s GraphQL experience on any data source that they use.The release today is a combination of an API Agent, an SDK to develop against that agent, and a reference application to show a working example of how it all works to provide new connections to varied data sources. This post will help you get started writing your own Hasura GraphQL Data Connectors and provides all the links to the pertinent resources to get started, or to add your request for the connector you'd like to see implemented next!Agent & API (docs)The Hasura GraphQL Data Connector, or more specifically the Hasura GraphQL Data Connector Agent, is a service that acts as an intermediary abstraction between a data source and the Hasura GraphQL Engine.Hasura GraphQL Data Connector High Level ArchitectureFor more information specifically about the agent check out the data connector agent Docs.SDK (download)To help in building, testing, and deploying Data Connectors for your databases we've built the Hasura GraphQL Data Connector SDK. The focus of the SDK helps to build with confidence, ensuring complete, correct, and idiomatic data connection implementations. The SDK, to help with continuous integration and delivery, is versioned with the Hasura GraphQL Engine.When authoring a new Data Connector the process the SDK provides follows this path:Start with a docker compose up.Verify the tests passed.Now review & make changes to the reference agent for your specific database and intended implementation.Rebuild as required as dependent on your stack.Rerun the tests with docker compose run testsInteract with the agent via Hasura GraphQL Engine @ http://localhost:8080 and view the OpenAPI Schema @ http://localhost:8300.For more details on the SDK check out the repo README.md.Reference Connector (code)The reference connector is located under the path within the SDK and a great way to get started with a working connector example. This reference connector is written in TypeScript and has several key code snippet examples within the reference itself. For detail on the schema, relationships, and related connector details check out the section on implementing data connectors.Tying it TogetherThe overall steps for building a connector can be summarized in three steps.Step 1 - Get the SDK and run the reference implementation.Step 2 - Change the reference implementation to connect and meet the needs of your preferred database.Step 3 - Deploy!Those are paraphrased, so let\u2019s dig into what each of these steps involves.For the first step, getting the SDK can be done in a few ways. There is a download (latest tar file coming end of day), one can clone the graphql-engine repository and the SDK, agent, and reference application are available in the /dc-agent path. For details on running the reference connector as a container or locally with Node.js, check out the README.md file in the /reference path.The second step, as you look through the reference connector you\u2019ll find the schema, relations, types, and other key features you\u2019d expect in a GraphQL API. Each of these are implemented based on the schema and API spec as detailed here. Follow these reference connectors implementations as a guide and connect to whichever database needed.Finally deploy! This step can be expansive. For production, there are a number of thing to take into consideration. Some good guidelines to follow is to deploy the agent container along with the Hasura GraphQL Engine & other containers but keep the agent behind or in the DMZ, unexposed to the public internet.For development needs it\u2019s a great idea to put the agent build, in whatever language you decide to implement in, into one\u2019s CI/CD build. Whatever the architecture, with the SDK a container can be built and deployed for maximum flexibility.SummaryThe Hasura GraphQL Data Connectors (GDC) provide a way to develop connections to any data source using any language stack that will work with Hasura GraphQL Engine. With today\u2019s release we\u2019ve released the SDK, Agent API, and reference connector implementation. In the coming days and weeks we\u2019ll be working to move this alpha release to beta and toward a v1 with cloud first, Hasura developed connectors.Sign up for MongoDB, MySQL, Oracle, Elastic, or Snowflake so we can notify you the second support is available or join the discussion and request another database here!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Combine Multiple GraphQL APIs using Hasura's GraphQL Joins Description: With GraphQL Joins, you can federate your queries and mutations across multiple GraphQL services as if they were a single GraphQL schema. You do not have to write extra code or change the underlying APIs. Author: Arjun Yelamanchili"}, {"role": "assistant", "content": "The Hasura GraphQL Joins feature, also know as Remote Schema to Remote Schema (RS-RS) joins, enables you to create a unified GraphQL API by joining the data from different GraphQL sources.\nWith GraphQL Joins, you can federate your queries and mutations across multiple GraphQL services as if they were a single GraphQL schema. You do not have to write extra code or change the underlying APIs.\nThere are four types of Joins with Hasura:\n\nRemote Schema to Remote Schema (RS-to-RS) Joins\nDatabase to Database (DB-to-DB) Joins\nDatabase to Remote Schema (DB-to RS) Joins\nRemote Schema to Database (RS-to-DB) Joins\n\nThis article will teach you about Remote Schema to Remote Schema GraphQL Joins.\nHow do RS-to-RS Joins work?\nRemote Schema to Remote Schema Joins allow you to combine multiple GraphQL APIs via user-defined relationships.\nThis feature lets you easily create integrations as more companies adopt GraphQL.\nE-Commerce Example with Hasura\nIn our example, we create a store service for handling orders and a fulfillment service for handling shipping. We use GraphQL Code Generator and GraphQL Yoga.\nStore Service\nThe store service is an example of an e-commerce schema with items and orders containing items\ntype Item {\n  id: Int!\n  name: String!\n  price: Float!\n}\n\ntype LineItem {\n  quantity: Int!\n  itemId: Int!\n  item: Item\n}\n\ntype Order {\n  id: Int!\n  lineItems: [LineItem!]!\n}\n\ntype Query {\n  item(id: Int!): Item\n  items: [Item!]!\n  order(id: Int!): Order\n  orders: [Order]!\n}\n\nFulfillment Service\nThe fulfillment service is an example of a shipping company schema, with each fulfillment having an order ID and a status\nenum Status {\n  PACKING\n  SHIPPED\n  DELIVERED\n}\n\ntype Fulfillment {\n  id: Int!\n  orderId: Int!\n  status: Status!\n}\n\ntype Query {\n  fulfillment(orderId: Int!): Fulfillment!\n  fulfillments: [Fulfillment]!\n}\n\nJoining the Remote Schemas\nOnce we add the services as remote schemas, we can create a relationship using GraphQL Joins.\nBy mapping the store schemas orderId to the fulfillment schemas order type, we can join the two services with no code! Also, we didn't have to modify the original APIs.\n\nThe query\n{\n  fulfillment(orderId: 1) {\n    status\n    order {\n      lineItems {\n        item {\n          name\n        }\n      }\n    }\n  }\n}\n\ngives us\n{\n  \"data\": {\n    \"fulfillment\": {\n      \"status\": \"PACKING\",\n      \"order\": {\n        \"lineItems\": [\n          {\n            \"item\": {\n              \"name\": \"Sunglasses\"\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n\nEnjoy!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Join data from a GraphQL API with Database Table using Remote Schema to Database Joins Description: Remote Schema to Database allows you to join the data coming from the GraphQL API with the existing data from the database.  Author: Catalin Pit"}, {"role": "assistant", "content": "The GraphQL Joins feature enables you to create a unified GraphQL API by joining the data from different GraphQL services.With GraphQL Joins, you can federate your queries and mutations across multiple GraphQL services as if they were a single GraphQL schema. You do not have to write extra code or change the underlying APIs.There are four types of joins with Hasura:GraphQL to GraphQL JoinsDatabase to Database (DB-to-DB) JoinsDatabase to Remote Schema (DB-to RS) JoinsRemote Schema to Database (RS-to-DB) JoinsIn this article, you will learn about the Remote Schema to Database Joins.How do RS-to-DB Joins work?Remote Schema to Database allows you to join the data coming from the GraphQL API with the existing data from the database. You achieve that by creating a relationship between the GraphQL API (remote schema) and your Hasura app. That means you are driving a relationship from the Remote Schema to the database.GraphQL JoinsThe above figure illustrates Remote Schema to Database Joins. Let\u2019s see how it works with a real-world example.E-Commerce with HasuraIn this example, you have a store service, and a Hasura application.The store service is an example of an e-commerce schema containing items and orders. Each order contains items.The Hasura application keeps track of users and their orders. When you query the orders, you can retrieve the information of the user associated with the orders.The image illustrates the structure of the \"users\" table.Hasura \"users\" TableThe next step is to add the \"store\" service as a remote schema in Hasura.\"store\" Remote SchemaIf you want to learn more about adding remote schemas in Hasura, the documentation covers the topic in detail here.At this point, you have the service connected to Hasura as a remote schema, but there is no relationship between them. That means there is no connection between the orders and the users from the application.The next is to connect the \"store\" service and the database with a Remote Schema to Database Join. The reason for joining the two is to be able to query user information alongside the orders. For instance, you might want to see all the orders and the person who made the order.Create an array relationship from the remote schema to the database. The id field from the \"store\" remote schema maps to the order_id field from the database.Remote Schema to Database JoinSave the relationship and you are done! You should be able to retrieve the orders and the user associated with each order.Let\u2019s take as an example the following query:This query retrieves all the orders including the:order IDitems from the orders - name, price, and quantityinformation about the user who placed the order - id, name, and addressRunning the query returns the following response:Now the orders and the users are linked, meaning they can be queried together.In this article, you joined two services together and also connected an external API to the database. You did that without writing any code or modifying the external GraphQL APIs. How cool is that?If you want to read more about GraphQL Joins, you can do it here."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Trigger Serverless DigitalOcean Functions with Hasura Events Description: DigitalOcean recently released DigitalOcean Functions, which is their serverless solution. It enables you to invoke and run functions when a web event happens.\nIn this post, you will see how to integrate DigitalOcean Functions with Hasura. Author: Catalin Pit"}, {"role": "assistant", "content": "DigitalOcean recently released DigitalOcean Functions, which is their serverless solution. It enables you to invoke and run functions when a web event happens.In this post, you will see how to integrate DigitalOcean Functions with Hasura. What Can You Use Digitalocean Functions For?In the context of Hasura, you can use serverless functions for the following use cases:Custom business logic via Hasura ActionsEvent logic via Hasura Event TriggersScheduled / Cron triggers via Hasura Scheduled TriggersWe are going to look at the example of triggering DigitalOcean Functions with Hasura Events.Create New DigitalOcean FunctionBefore creating a new function, you need to create a namespace. A namespace represents a collection of one or more functions.To create a namespace, go to the \"Functions\" page and click on the \"Launch\" button.Launch DigitalOcean NamespaceOnce the creation process is done, it redirects you to the newly created namespace, where you can add new functions.Click on the \"Actions\" button and choose the first option - \"Create Function\".Create New DigitalOcean FunctionTo create a new function, you need to:select a runtimechoose a function nameSetting a package name is optional, so you can leave it empty.Configure New FunctionAfter configuring the new function, click on \"Create\". DigitalOcean creates a greeting function by default.When you invoke the function, it returns a greeting:\"Hello <name> from Hasura!\" where <name> is replaced with the name from the ?name= query parameter\"Hello stranger from Hasura!\" when the ?name= query parameter is not presentThe Function CodeIf you invoke the function, you should see something similar:Test the Newly Created FunctionFor this article, the function is a good entrypoint to showcase the integration between DigitalOcean Functions and Hasura. The function is triggered each time a new user is added to the database connected to Hasura. In other words, it greets each new user.Configure Hasura EventHasura allows you to create event triggers on database tables. The event triggers capture events (insert, update, delete) on specified tables and then invoke an HTTP webhook to perform custom business logic.In this case, the event trigger captures an insert event on the users table and then invokes the serverless DigitalOcean function.The image below shows the Hasura application used for this article.Hasura ApplicationCreate a new event trigger with the following details:Trigger Name => greet_usersDatabase => defaultSchema/Table => public/usersTrigger Operations => insertIn the \"Webhook Handler\" field, add the URL of your DigitalOcean function.Then scroll down to modify the request options. It allows you to change the request method and the URL template. You need to change the URL template to pass the name of the newly created user.For the \"Query Params\" section, choose name as the key and {{$body.event.data.new.name}} as the value.After that, scroll until you see the \"Create Event Trigger\" button and click it.Test DigitalOcean + Hasura IntegrationIt's time to test the event trigger. Add a new user to your Hasura application, as shown in the image below.Add New User in HasuraAfter saving the user, the event trigger should invoke the serverless DigitalOcean function. If you check the invocation logs of the event trigger, you should see the response from the DigitalOcean function.Event Trigger Invocation LogsYou successfully invoked a serverless DigitalOcean function with the Hasura Event Triggers!ConclusionThe example from the article is rather basic because the focus is on illustrating how to integrate the new DigitalOcean functionality with Hasura.More examples of how you could use the serverless functions from DigitalOcean with Hasura Event Triggers are as follows:sending push notifications and emails based on database eventstransforming and loading data into external data-storesdeploying a static site with content backed by Postgres - whenever the Postgres data is updated, the site is rebuilt and redeployed automaticallyHowever, the possibilities are endless. If you already use the two or you have some exciting ideas, you can reach out to us on our Discord server.Check out the documentation to read more about Hasura Event Triggers. Alternatively, you can read more about DigitalOcean functions here."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: 10 Reasons We\u2019re Super Excited About HasuraCon\u201922 Description: We\u2019re bringing the global Hasura community together to share exciting product releases, inspiring keynotes, real-world use cases, hands-on workshops, and our very first HASURAwards! Author: Olivia LaMar"}, {"role": "assistant", "content": "Our annual user conference, HasuraCon, is back for the third time June 28th-30th. At Hasura, we\u2019re changing how software is being architected by making a GraphQL-powered Data API a key leverage point to unlock data and developer productivity. Hasura has been growing fast and building faster so we have a lot to share. We\u2019re bringing the global Hasura community together to share exciting product releases, inspiring keynotes, real-world use cases, hands-on workshops, and our very first HASURAwards! Here are the top 10 things you won\u2019t want to miss at HasuraCon. 1. Product Announcements: What would HasuraCon be without product announcements? We are in constant communication with the community and power users seeking ways to improve our tech. Getting together every year at HasuraCon is our chance to showcase the hard work of our engineers, along with the community to further grow what Hasura can do. Some announcements at a glance include:Building new Data Backends for HasuraHasura GraphQL Engine - Key UpdatesHasura Cloud and Enterprise - What's new in 2022Observability for your Hasura-based apps2. Killer Keynotes: Our keynote speakers will cover a range of topics like:The ultimate data API with GraphQLLearnings from Vint Cerf one of the internet\u2019s Founding FathersScaling DevOps with Emily Freeman from AWS. 3. Real-world Hasura & GraphQL at scale: A goal of ours is to provide real-world, tangible uses of Hasura and GraphQL from users and customers. Here\u2019s a quick glance of the use cases you can expect to see at HasuraCon:Hasura Cloud being built using HasuraInsurtech Modernization with Datalakes and GraphQL - from hubb TechnologyHasura in Journalism: Finding Stories and Speeding up Fact-Checking - from The Marshall ProjectIncrementally adopting Hasura in a multi-tenant healthcare SaaS app - from RxVantage \u00a04. Specialized track for frontend & fullstack devs: The frontend and serverless ecosystems are evolving at a rapid pace and we have some very exciting sessions to showcase these innovations and how GraphQL and Hasura fit in to give more firepower to frontend developers! 5. How we build Hasura: HasuraCon deep dives will bring to life technical sessions on monitoring, building, migrating, and improving the way you\u2019re building applications and accessing your data. Whether you\u2019re at a startup, or enterprise, come ready to dive deep and ask questions. Our product, engineering, and design teams are opening up their playbooks on how we\u2019re building things at Hasura.6. Workshops: We know the value of hands-on workshops. So HasuraCon offers an array of workshops that will fit your experience with GraphQL and Hasura. Beginner topics include Getting started with Hasura and Securing your GraphQL API with Hasura. Advanced topics include Creating and federating data across a unified GraphQL API and Reusing REST APIs with Hasura. Workshops are 90 minutes in duration and are stacked back to back (with breaks) so you can attend multiple to grow your skill set. Requirements can be found under each workshop description here.7. HASURAwards: HasuraCon and community events would not be possible without the people. This year, for the first time, we want to shine a spotlight on the users who have made cool stuff, come up with funny Discord names, worked on a project for good, and more. It\u2019s a small way to say thank you to people whose work has made a big impact.8. Sponsors: And, of course, our Sponsors who are bringing to the table talks, workshops, swag, job postings, and more. Don\u2019t miss the chance to join the conversation and hear what\u2019s new with The Guild, Netlify, and StackHawk.9. Custom Avatars: Build your very own Hasura avatar for HasuraCon! And get entered for a chance to win some custom swag when you share your avatar on twitter! Just tag @hasuraHQ10. Our community: One of the things we absolutely love about HasuraCon is seeing familiar faces, the chatroom discussions, and meeting new community members. We\u2019re looking forward to making some new friends, albeit virtually!Basically, HasuraCon\u201922 is one you don\u2019t want to miss. Register now for this free community event."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top 7 Authentication Providers for Building Apps With JWT (2022) Description: This article presents the benefits and drawbacks of using an Authentication Provider and compares the existing ones that support JWT integration. Author: Catalin Pit"}, {"role": "assistant", "content": "An \u201cAuthentication Service Provider\u201d refers to an entity that provides authentication services. The Authentication Provider abstracts the complexity of implementing the authentication system manually from scratch so you can focus on more important tasks.\nThis article presents the benefits and drawbacks of using an Authentication Provider and compares the existing ones that support JWT integration.\nPros and Cons of Auth Providers\nBefore choosing an Auth Service Provider, it\u2019s essential to assess the benefits and drawbacks of using one.\nPro: Abstracts Complexity\nUsing an Authentication Service Provider abstracts the complexity of implementing the auth system yourself. Building such a system is a complex, lengthy process requiring many resources.\nYou can use an existing solution that frees up those resources for other, more critical tasks.\nPro: No User Information Storage\nStoring confidential information about people is not something to take lightly. Opting to use a provider means you do not have to worry about storing the users\u2019 details on your servers.\nPro: Leverage Social Logins\nYou can enable social logins with minimal effort when using an Auth Service Provider. Social login refers to logging in using an existing social network account such as Twitter, GitHub, Google, and so on.\nImplementing each social provider individually requires effort and resources. When using an Auth Provider, adding social logins becomes easier and quicker.\nPro: Better Security\nOpting to use an Auth Provider means better security. The whole focus of an Auth Provider is on authentication and authorization. That means they are better trained and equipped to keep the users\u2019 information secure.\nCons: Difficult to Migrate\nOnce you choose an Authentication Service Provider, it can be difficult to migrate away. Changing to another provider or rolling a proprietary system can be difficult.\nIt\u2019s essential to decide whether it makes sense to use an Auth Provider or not. If you choose to do it, make sure you choose the provider that fits your needs and requirements.\nCons: Provider Lock-in\nAnother drawback is that you might become too reliant on the provider you are using. The reasons might be that your implementation is too specific to that vendor, or it\u2019s nearly impossible to export your current user data.\nMigrating to another provider might become too costly and time-consuming, so you are locked with the current one.\nCons: Costs\nIn the beginning, the costs might not be an issue. However, after the application grows bigger and has more users, the costs can become a problem.\nMany of the Auth Providers charge based on the number of monthly active users. The more users you have, the bigger the bill becomes.\nConclusion\nShould you use an Authentication Service Provider or implement a proprietary system? There is no definitive answer.\nThe decision to use a Provider or not depends on many factors. For some, using an Auth Provider is the appropriate solution, whereas, for others, it is not.\nIn case you decide to use one, keep reading to learn about the various Authentication Service Providers available.\nChoosing an Auth Provider\nThe article compares the following Authentication Service Providers:\n\nAuth0\nFirebase Auth\nClerk\nKeyCloak\nCognito\nSuperTokens\nNhost\n\nThese auth providers are evaluated based on six evaluation criteria.\n1. Open Source\nAn open source project comes with a handful of benefits. One benefit would be the cost savings. Since they are open source, they do not require licensing or usage fees.\nAnother benefit is flexibility. If the business requirements change or you need additional features, you can modify and extend the code yourself. You do not have to ask the vendor for changes or to change it altogether.\nIt\u2019s important to mention transparency as well. The code is available for everyone to see. Since so many professionals have access to the code, they can make the software better and more secure.\nThere are other benefits of using an open source auth provider as well, but these benefits drive the point home.\n2. Custom JWT Claims\nWhen integrating any Authentication Service Provider with Hasura, it\u2019s important to ensure they support custom JWT claims.\nHasura checks the custom JWT claims to learn about the role of the user making the request. As a result, Hasura can apply the appropriate authorization rules.\n{\n    'x-hasura-default-role': 'user',\n    'x-hasura-allowed-roles': ['user'],\n    'x-hasura-user-id': user.user_id\n};\n\nThe above snippet shows an example of custom JWT claims. It contains essential information about the request.\n3. Single Sign-On (SSO) Support\nIn the simplest terms, SSO refers to the type of authentication where the user logins to multiple platforms with one set of credentials.\nOne example is the Social SSO, where the user uses a social network account such as the Google account to log into an application.\n4. Machine to Machine Authentication\nIt\u2019s not always the users who make the requests. There are scenarios where two machines communicate between them. There is no user to input the credentials, so the client (another machine) needs to be authenticated in another way.\nMachine to machine (M2M) authentication refers to authenticating machines without user input. That\u2019s possible by using the OAuth 2.0 Client Credentials Flow. The credentials include a client_id and client_secret, which are used to authenticate the client.\n5. Free Tiers\nAll the auth providers from this article have a free tier. The difference lies in what they offer in that tier.\nThey measure the monthly active users (MAUs) and charge based on that. As a result, some providers offer a free tier with thousands of MAUs, whereas some offer only a couple hundred MAUs.\n6. Ease of Integration\nThe integration of an auth service provider depends on many factors. It\u2019s challenging to present how easy or difficult it is to integrate a provider.\nHowever, there are some points that might indicate the difficulty level of the implementation, such as:\n\nthe quality of documentation\navailable SDKs\nthe amount of configuration required\nthe available material\n\nProvider Comparison\nThe image illustrates a comparison between the Auth Providers presented in this article.\n\nAuth0\nAuth0 is a well-established and popular solution for auth services. It comes with a generous free tier and has support for most of the things you need.\n\n\n\nOpen Source\nNo\n\n\n\n\nSaaS\nYes\n\n\nSSO Support\nYes\n\n\nJWT Custom Claims\nYes\n\n\nMachine to Machine Authentication\nYes\n\n\nFree Plan\nYes - up to 7000 MAU\n\n\n\nWhen it comes to the ease of integration, Auth0 is neither easy nor difficult to integrate. Since it\u2019s a well-established and widespread solution, a great deal of material is available to help you with the integration. The amount of configuration you need to do depends on the type and complexity of your application.\nLearn how to integrate Auth0 with Hasura\nFirebase Authentication\nFirebase Authentication is another popular auth service that allows you to add authentication to your applications.\nFirebase allows you to add sign-in methods such as:\n\nidentity providers such as Google, Facebook, and others\nemail and password\nphone\n\n\n\n\nOpen Source\nNo\n\n\n\n\nSaaS\nNo\n\n\nSSO Support\nYes\n\n\nJWT Custom Claims\nYes\n\n\nMachine to Machine Authentication\nYes\n\n\nFree Plan\nFree except for Phone Authentication\n\n\n\nThe amount of available resources makes the integration process more straightforward. The Firebase community creates lots of open source resources you can use. It\u2019s also a well-established provider, which means that you are likely to find a solution to almost any problem.\nLastly, it has in-depth documentation covering how to implement Firebase Auth in both web and mobile applications.\nLearn how to integrate Firebase with Hasura\nClerk\nCompared to the other providers, Clerk is the newest one. Clerk enables you to implement sign-up, sign-in, and user profiles into your applications. According to their website, they are a \u201ccomplete Customer Identity Platform\".\nClerk also comes with Hasura support. They have a special documentation section that showcases how to integrate Clerk with Hasura.\n\n\n\nOpen Source\nNo\n\n\n\n\nSaaS\nYes\n\n\nSSO Support\nSocial SSO\n\n\nJWT Custom Claims\nYes\n\n\nMachine to Machine Authentication\nNo\n\n\nFree Plan\nYes - up to 500 MAU\n\n\n\nSince it\u2019s relatively new, you might run into issues that are not documented/solved yet. Other than that, Clerk has extensive, in-depth documentation.\nThe configuration needed to integrate Clerk with your application is minimal. You need a Clerk account, and then you can use the SDKs and components provided by them.\nLearn how to integrate Clerk with Hasura\nKeycloak\nKeycloak is a popular open source Auth Service Provider. Since it\u2019s open source, you can self-host it, which means it\u2019s free regardless of the number of monthly active users.\nEven though it\u2019s open source and open to everyone to contribute, the project is maintained by Red Hat. That means you can trust the project.\nKeycloak supports most authentication use cases, and it\u2019s also free, so it can be a great choice.\n\n\n\nOpen Source\nYes\n\n\n\n\nSaaS\nNo\n\n\nSSO Support\nYes\n\n\nJWT Custom Claims\nYes\n\n\nMachine to Machine Authentication\nYes\n\n\nFree Plan\nFree for unlimited users\n\n\n\nKeycloak has a moderate level of implementation difficulty. Besides integrating Keycloak into your applications, you also have to take care of deployment and maintenance.\nEven though it might look complicated at first sight, you can get accustomed to Keycloak quickly. It also is an established product, which means you are likely to find solutions to most of your issues. Lastly, it has good documentation.\nLearn how to integrate Keycloak with Hasura\nCognito\nCognito is Amazon\u2019s product for handling authentication. It allows you to implement authentication into your web and mobile applications.\nCognito is one of the most generous auth providers, giving you a free plan with up to 50,000 monthly active users. If costs are a concern and you want a managed solution, Cognito is an excellent choice.\n\n\n\nOpen Source\nNo\n\n\n\n\nSaaS\nNo\n\n\nSSO Support\nYes\n\n\nJWT Custom Claims\nYes\n\n\nMachine to Machine Authentication\nYes\n\n\nFree Plan\nFree up to 50000 monthly active users (who sign in directly to Cognito User Pools)\n\n\n\nRegarding the ease of integration, Amazon Cognito can get quite challenging to implement compared to other providers. Cognito requires a complex setup, even for basic applications.\nHowever, there are many official and community materials to help you integrate Cognito with your application. A handful of SDKs and sample apps are also available to help and inspire you.\nLearn how to integrate Cognito with Hasura\nSuperTokens\nSuperTokens is a relatively new service. It comes with support for almost everything you need, and it\u2019s also open source. That means you can self-host it and avoid extra charges. If you choose to self-host, it\u2019s free forever for unlimited users.\nSuperTokens comes with Hasura support, having a special section on how to integrate SuperTokens with Hasura.\n\n\n\nOpen Source\nYes\n\n\n\n\nSaaS\nYes\n\n\nSSO Support\nYes\n\n\nJWT Custom Claims\nYes\n\n\nMachine to Machine Authentication\nYes\n\n\nFree Plan\nFree up for unlimited users (Self Hosted) and up to 5000 MAUs (SaaS)\n\n\n\nWhen it comes to the ease of integration, you might encounter some difficulties when trying to integrate it with your application. The reason is that it\u2019s a new service and there is not a lot of material available on the internet at the moment.\nRegarding configuration, it\u2019s reasonably easy to get started with SuperTokens since the documentation provides all the settings and information you need.\nSuperTokens also has guides on integrating SuperTokens with Hasura and sample SuperTokens + Hasura demo applications.\nLearn how to integrate SuperTokens with Hasura\nNhost Authentication\nNhost Authentication is an open source authentication service for Hasura. With Nhost Authentication, people can sign-in with:\n\nEmail and password\nOAuth Providers such as GitHub, Google, Apple, Facebook, Twitter, LinkedIn and others\nMagic links\nSMS\n\nSince it's an open source project, you can self-host it and avoid the extra charges you would pay when using a managed service.\n\n\n\nOpen Source\nYes\n\n\n\n\nSaaS\nNo\n\n\nSSO Support\nYes\n\n\nJWT Custom Claims\nYes\n\n\nMachine to Machine Authentication\nNo\n\n\nFree Plan\nFree for unlimited users\n\n\n\nWhen it comes to the ease of integration, Nhost Authentication is fairly straightforward to integrate with Hasura applications. You can either create an application through Nhost, which gives you an out-of-the box backend with Hasura, Hasura Auth and Postgres or you can use the Nhost Authentication Docker image to self host it.\nYou can check Nhost Authentication here.\nA Note on Scalability\nScalability refers to the authentication service\u2019s ability to handle your application growth and changes in business requirements. The auth service should keep up with your user growth and requirements change without affecting your application.\nIf you decide to use a self-hosted solution, you have full responsibility when it comes to scaling. You need to manage the database, servers, and the whole infrastructure. It\u2019s your responsibility to make sure everything works smoothly.\nWhen you use a managed solution, the auth provider takes care of scaling up and down. The auth provider has pre-defined rate limits and quotas, which they use for scaling.\nConclusion\nAll providers presented in this article are excellent choices, but there is no one universal answer. Choosing an Authentication Service Provider depends on one\u2019s needs and requirements.\nSuperTokens and Clerk are two notable providers that support Hasura officially. \u201cSupport\" means that they have a special section in their documentation illustrating how to integrate them with Hasura. That\u2019s a plus, and it helps a lot when integrating any of them with Hasura.\nIt\u2019s important to note that this is not an exhaustive list. There are other providers available which are not present in the article. However, the article will be updated occasionally, and new providers will be added.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Open Beta of GCP Region Support on Hasura Cloud Description: We're excited to announce the support of more regions on Hasura Cloud. You can now deploy Hasura Cloud projects to geographies where Google Cloud Platform offers their solution, starting with select regions today in our public beta. Author: Praveen Durairaju"}, {"role": "assistant", "content": "We're excited to announce the support of more regions on Hasura Cloud. You can now deploy Hasura Cloud projects to geographies where Google Cloud Platform offers their solution, starting with select regions today in our public beta. We have been in private beta since December 2021 and have worked on improving the experience and reliability with the help of all our private beta users. Today we are happy to announce wider availability with the open beta.Who is this for?A lot of Hasura users have their production databases and custom business logic hosted on Google Cloud Platform (via products like Cloud SQL, Cloud Functions, GKE etc). Having the Hasura Cloud endpoint hosted in the same region will reduce latency and improve connectivity and performance.Besides the database connectivity, co-locating custom business logic and Hasura project is key to improve response times and for those hosted on GCP products can start leveraging this.What regions are supported today?We are starting with support for us-west-2 for the free tier. On the paid tier, we have support for the following regions:us-west-2us-central-1europe-west-3As we near the GA release, we will add support for more regions.Try it outAs we are nearing GA, we would love for those who have been waiting for GCP Region support to try this out and give us feedback.Head to https://cloud.hasura.io and try creating a new project with any of the new regions we have added today!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How we use the Dataloader pattern for optimising GraphQL Joins at Hasura Description: In this post, we will look at how the dataloader pattern is used for GraphQL Joins which has performance implications. Author: Antoine Leblanc"}, {"role": "assistant", "content": "Unlike joins across two tables in the same Postgres database, all \u201cremote\u201d joins must be performed by the GraphQL Engine, regardless of whether the data originates from a database or a remote schema: each source is independent from all the others, and only the Engine has the full knowledge of all of them. To perform such a join, we use a \u201cdataloader\u201d approach: we analyse the query ahead of its execution to identify each individual part, and we modify each individual sub-query to also extract the required join keys; the engine then issues them one by one and stitches the results together. For instance, consider the following GraphQL query:query {\n  album {\n    name\n    artist {\n      name\n    }\n  }\n}\nWhere album is on a local database, artist is on a remote schema, and the relationship between them is defined such that album.artist_id is the argument to artist. The engine would identify that there are two steps to this process: call the local database, to get the data from album, extract the relevant join keys, and perform a subsequent call to the remote schema. In practice, we would first issue a SQL query that would look like this:SELECT\nname\nartist_id AS artist_id_join_col_1\n\u2018placeholder\u2019 AS artist\nFROM\nalbum;Internally, we keep track of which columns were added for the purpose of retrieving join keys that must be removed from the result: we call those phantom columns. Additionally, we add a placeholder for where the result of the join will be, to preserve the order of fields as originally requested in the GraphQL query without having to do additional internal bookkeeping.We can then issue a subsequent query to the remote schema for each artist; this time, without the need for placeholder nor join columns, as this is the last step of the process:query ($hasura_var_1: Int!) {\n  artist(id: $hasura_var_1) {\n    name\n  }\n}\nThis is how remote joins have always worked; but extending them to work across databases (and more recently across remote schemas) has made the process more complicated: there might now be nested joins, where the result of a previous join must now contain the join keys for the following one. The overall logic remains the same, however we do now internally construct a full join tree, an internal recursive representation of the steps to be performed. This structure is the union of all paths within the first query that will require a join: each such node, in turn, may contain a subsequent join tree, for the joins that must be performed on the result. An example of such a tree, for a more complicated example, would be something like this:From the artist object, we perform two remote joins: one to get the album, and one to get a picture; however, after performing that second remote join, we perform another one, from the picture to its metadata: in our join tree, the picture node contained another recursive join tree.In the engine, instead of the joins being one step, they are now performed iteratively: after performing the very first step of the query, we traverse the join tree. At each leaf, each join that needs to be performed, we repeat the process recursively with that node\u2019s sub-tree; we perform, in essence, a depth-first traversal, constructing the resulting JSON object on the way back up.However, when extending the Engine to allow joins from remote schemas, we encountered an additional difficulty, which forced us to refine how our join trees are defined: the path at which a join must be performed can be ambiguous! Consider, for example, the following example:query ($nodeId: Int!) {\n  node(id: $nodeId) {\n    ... on Artist {\n      articles {\n        name\n      }\n    }\n    ... on Writer {\n      articles {\n        name\n      }\n    }\n  }\n}Depending on the actual GraphQL type of the object that is being returned, we must perform one of two possible joins: either one from artist to articles, or one from writer to articles. But, in both cases, the part of the answers\u2019 object that needs to be modified is at the path node.articles! To be able to differentiate between the two, we need to be able to distinguish them in the join tree, and furthermore we must be able to know, in the engine, what the actual GraphQL type of the returned object is.To do so, we have changed our join tree definition to allow each branch to be annotated with a typename: in this case, our join tree would have two joins: one at the path node.(Artist.articles), and one at the path node.(Writer.articles). We only do this for paths that depend on the runtime GraphQL type. And to retrieve the runtime type, we add a phantom field in the request that extracts the __typename of the object, that we then use when traversing the join tree.To go further:Our join trees are very similar to tries, also known as prefix trees, with the difference that we only store data in the leaves: https://en.wikipedia.org/wiki/TrieThe definition of this join tree in our code, with examples:https://github.com/hasura/graphql-engine/blob/8b0b4e5c35d1628c1faa3986fe50598deba33e75/server/src-lib/Hasura/GraphQL/Execute/RemoteJoin/Types.hs#L44"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura and Well-Typed collaborate on Haskell tooling Description: Well-Typed and Hasura have been working together since 2020 to improve Haskell tooling for commercial Haskell users, taking advantage of Well-Typed's expertise maintaining the Glasgow Haskell Compiler and Hasura's experience using Haskell in production at scale. Author: Hasura"}, {"role": "assistant", "content": "Well-Typed and Hasura have been working together since 2020 to improve Haskell tooling for commercial Haskell users, taking advantage of Well-Typed's expertise maintaining the Glasgow Haskell Compiler and Hasura's\nexperience using Haskell in production at scale.  Over the last two years we\nhave continued our productive relationship working on a wide variety of\nprojects, in particular related to the profiling and debugging capabilities of the compiler, many of which have had a reinvention or spruce-up. In this post we'll look at back at the progress we have made together.\nMemory profiling and heap analysis\nghc-debug\nOne of the first big projects we worked on was ghc-debug, a new heap analysis tool that can gather detailed information about the heap of a running process or analyse a snapshot. This tool gives precise results so it can be used to reliably investigate memory usage issues, and we have used it numerous times to fix bugs in the GHC code base.  Within Hasura we have used it to investigate fragmentation issues more closely and also to diagnose a critical memory leak regression before a release.\nSince GHC 9.2, ghc-debug is supported natively in GHC. All the libraries and executables are on Hackage so it can be installed and used like any normal Haskell library.\n\nMemory Fragmentation: A Deeper Look With ghc-debug\nZuriHac21: Understanding Memory Usage with eventlog2html and ghc-debug\n\nInfo table profiling\nAlso in GHC 9.2 we introduced \"info table profiling\" (or -hi profiling), a new heap profiling mode that analyses memory usage over time and relates it to source code locations. Crucially, it does not require introducing cost centres and recompiling with profiling enabled (which may distort performance).  It works by storing a map from info tables to meta-information such as where it originated, what type it is and so on.\nThe resulting profile can be viewed using eventlog2html to give a detailed table about the memory behaviour of each closure type over the course of a program.\nWe have used info table profiling extensively on GHC itself to find and resolve memory issues, meaning that GHC 9.2 and 9.4 bring significant reductions in compile-time memory usage.\n\nA First Look at Info Table Profiling\n\nUnderstanding memory fragmentation\nOur early work with Hasura investigated why there was\na large discrepency between the memory usage reported by the operating system and the Haskell runtime. The initial hypothesis was that, due to the extensive use of pinned bytestrings in Hasura's code base, we were losing memory due to heap fragmentation.\nWe developed an understanding of how exactly fragmentation could occur on a Haskell heap, tooling to analyse the extent of fragmentation and ultimately some fixes to GHC's memory allocation strategy to reduce fragmentation caused by short-lived bytestring allocations.\nThis investigation also led to a much deeper understanding of the memory\nretention behaviour of the GHC runtime and led to some additional improvements in how much memory the runtime will optimistically retain. For long-lived server applications the amount of memory used should return to a steady baseline after being idle for a long period.\nThis work also highlighted how other compilers trigger idle garbage collections.\nIn particular, we may want to investigate triggering idle collections by allocation rate rather than simple idleness, as applications may continue to still do a small amount of work in their idle periods.\n\nUnderstanding Memory Fragmentation\nMemory Fragmentation: A Deeper Look With ghc-debug\nImprovements to memory usage in GHC 9.2\n\nRuntime performance profiling and monitoring\nLate cost centre profiling\nCode centre profiling, the normal tool recommended for GHC users profiling their Haskell programs, allows recording both time/allocation and heap profiles.\nIt requires compiling the project in profiling mode, which inserts cost centres to the compiled code.  Traditionally, the issue with cost centre profiling has been that adding cost centres severly affects how your program is optimised. This means that the existing strategies for automatically inserting cost centres (such as -fprof-auto) can lead to major skew if they are inserted in an inopportune place.\nWe have implemented a new cost centre insertion mode in GHC 9.4, -fprof-late, which inserts cost centres after the optimiser has finished running. Therefore the cost centres will not affect how your code is optimised and the profile gives a more accurate view of how your unprofiled program would perform. The trade-off is that the names of the cost centres contain internal names, but they are nearly always easily understandable.\nThe utility of this mode can not be understated, you now get a very fine-grained profile that accurately reflects the actual runtime behaviour of your program. It's made me start using the cost-centre profiler again!\nWe also developed a plugin which can be used to approximate this mode if you are using\nGHC 9.0 or 9.2.\nTicky-ticky profiling\nHasura have a suite of benchmarks that track different runtime metrics, such as bytes allocated.[1] Investigating\nregressions in these benchmarks requires a profiling tool geared\ntowards profiling allocations. GHC has long had support for\nticky profiling, which gives a low level view about which functions are allocating.\nHowever, in the past ticky profiling has been used almost exclusively by GHC\ndevelopers, not users, and profiles were only consumable in a rudimentary\ntext-based format.\nWe added support to emit ticky samples via the eventlog in GHC 9.4, and support for rendering the information in the profile to an interactive HTML table using eventlog2html.  In addition, we integrated the new info table mapping (as used by ghc-debug and -hi profiling) to give precise locations\nfor each ticky counter, making it easier to interpret the profile.\nLive profiling and monitoring via the eventlog\nFor a long time we have been interested in unifying GHC's various profiling\nmechanisms via the eventlog, and making them easier to monitor.  We developed a prototype live monitoring setup for Hasura,\neventlog-live, that could\nattach to the eventlog and read events whilst the program was running. This\nprototype was subsequently extended thanks to funding from IOG.\nNative Stack Pointer register\nGHC-compiled programs use separate registers for the C stack and Haskell stack.\nOne consequence of this is that native Linux debugging and statistical profiling tools (such as perf) see only the C stack pointer, and hence provide a very limited window into the behaviour of Haskell programs.\nHasura commissioned some experimental investigatory work to see whether it would be possible to use the native stack pointer register for the Haskell stack, and hence get more useful output from off-the-shelf debugging tools.  Unfortunately we ran into issues getting perf to understand the debugging information generated by GHC, and there are challenges related to maintaining LLVM compatibility, but we remain interested in exploring this further.\n\nConsider using native stack pointer register for STG stack (#8272)\nTowards system profiler support for GHC\n\nHaskell Language Server\nLately we have started to support maintenance of the Haskell Language Server\n(HLS). The language server is now a key part of many developers' workflows, so it is a priority to make sure it is kept up-to-date and works reliably, and sponsorship from companies like Hasura is crucial to enabling this.\nRecently our work on HLS has included:\n\nSupporting the GHC 9.2 release series, as Hasura were keen to upgrade and\nhave access to all the improved functionality we discussed in this post.\nDiagnosing and resolving difficult-to-reproduce segfaults experienced by HLS users. It turned out that the version compatability checks were not strict enough, and HLS could load incompatible object files when running Template Haskell. In particular, you must build haskell-language-server with exactly the same version of GHC with which you compiled your package dependencies, so that object files for dependencies have the correct ABI.\nStarting to take advantage of the recently completed support for Multiple Home Units in GHC to make HLS work more robustly for projects consisting of multiple components.\n\nConclusion\nWell-Typed are grateful to Hasura for funding this work, as it will benefit the whole Haskell community. With their help we have made significant progress in the last two years improving debugging and profiling capabilities of the compiler, and improving the developer experience using HLS. We look forward to continuing our productive collaboration in the future.\nAs well as experimenting with all these tools on Hasura's code base, we have\nalso been using them to great effect on GHC's code base, in order to reduce\nmemory usage and increase performance of the compiler itself (e.g. by profiling GHC compiling Hasura's graphql-engine). The new profiling tools have been useful in finding places to optimise: ghc-debug and -hi profiling made eliminating memory leaks straightforward, the late cost centre patch gives a great overview of where GHC spends time, and ticky profiling gives a low level overview of the allocations.  They have also been very helpful for our work on\nimproving HLS\nperformance.\nWell-Typed are actively looking for funding to continue maintaining and\nenhancing GHC and HLS. If your company relies on robust Haskell tooling, and you could support this work, or would like help improving the developer experience for your Haskell engineers, please get in touch with us via\ninfo@well-typed.com!\n\n\n\nThe number of bytes allocated acts as a proxy for the amount of computation performed, since Haskell programs tend to allocate frequently, and allocations are more consistent than CPU or wall clock time. \u21a9\ufe0e\n\n\n\nAbout AuthorThis post was written by Matthew from Well-Typed.Matthew learned Haskell as an undergraduate at the University of Oxford before completing a PhD in Computer Science from the University of Bristol in 2020. His thesis focused on improvements to Typed Template Haskell and other challenges related to multi-stage programming. He has also contributed to the development of many open source Haskell libraries including ghcide, worked on profiling and debugging tools and implemented many patches for GHC."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing GraphQL Joins for federating data across GraphQL services Description: We are excited to announce the release of GraphQL Joins with which developers can federate their queries and mutations across multiple GraphQL services as if they were a single GraphQL schema, available in Hasura v2.6.0. Author: Praveen Durairaju"}, {"role": "assistant", "content": "We are excited to announce the release of GraphQL Joins, available in Hasura v2.6.0.With GraphQL Joins, developers can federate their queries and mutations across multiple GraphQL services as if they were a single GraphQL schema without modifying any of the underlying APIs or writing any extra code or requiring any changes to the upstream GraphQL service.At its core, GraphQL joins is the simple idea of creating a relationship from one type in the GraphQL schema to another resolver in the same or a different GraphQL schema.GraphQL Joins with HasuraGraphQL Joins enables the following types of use-cases:For Hasura users: Run custom mutations and queries on your GraphQL service that return references to entities in your database powered by Hasura (learn more)For teams with multiple GraphQL services: Unify and join across multiple in-house or 3rd party GraphQL services (learn more)For teams with great REST APIs already in place: If you have an OpenAPI (swagger) spec-ed API that maps well to a GraphQL schema automatically, use GraphQL Joins to create relationships between the different entities and prevent N+1 queries on the client.This is useful for users building GraphQL backends with Hasura or users looking at publishing a unified GraphQL data API for other teams and developers.In this blog post, we cover the following topics:\nEvolution of data federation in Hasura\nHow to set up GraphQL joins in Hasura\n\nCreating relationships\nManaging type conflicts\n\n\nManaging schema evolution and allowing upstream GraphQL services to change automatically\nAuthentication to upstream GraphQL services\nAuthorization\n\nScope access to specific parts of the unified GraphQL API\n\n\nCaching\nHasura internals: How query execution works\n\nEvolution of data federation at Hasura - joining data across databases, REST and GraphQL APIsHasura supports one database source at a time (Our first OSS release)Hasura supports GraphQL services as sourcesHasura supports REST APIs as a source (from v1.2)Hasura supports multiple databases at the same time (from v2.0)Hasura allows joins across multiple databasesHasura allows joins across GraphQL services (now)Hasura allows joins across REST APIs (coming soon)Setting up GraphQL JoinsTesting out GraphQL joins should take you all of 60 seconds with Hasura! Watch this video to see what it looks like or try it out yourself!\n\nLet us look at how GraphQL Joins can be tried out in a few simple steps with Hasura.Add any two GraphQL compliant schemas to Hasura as a Remote Schema. Just plug the GraphQL endpoint, and add any relevant headers to be forwarded (including the tokens, secrets etc).Add a new remote schema via Hasura Console2. Head to the source Remote Schema from where you want the relationship to be set up and navigate to the Relationships tab.3. Configure the relationship by selecting the source type, target remote schema and the join argument to be used that will be a common parameter in both the APIs. For example, orderId as in the example below.4. Create the relationship and try out the GraphQL query that uses the relationship. For example:{\n  fulfillment(orderId: 1) {\n    status\n    order {   <---- order is coming remote schema 2         \n      lineItems {\n        item {\n          name\n        }\n      }\n    }\n  }\n}Bonus Tip: You can also set up a remote relationship between the same schema! Just select the same schema for both the source and target and set up the relationship. Q: What if I want to add a GraphQL service maintained by a 3rd party? A: It just works. There are no changes required in any of the upstream GraphQL servers to add them to Hasura for the federation setup. All your favourite public GraphQL APIs like GitHub, Contentful, GraphCMS etc can be added and used to set up remote relationships without any extra code or configuration. Check out Data Hub for more public GraphQL APIs.Handling type conflictsA common issue that you might encounter, especially with a vendor or 3rd party GraphQL API are conflicts in the merged GraphQL API. For example, common GraphQL types like \u201cusers\u201d might overlap. You will need the ability to handle these conflicts while you add them as a remote schema to Hasura so that when Hasura unifies them into a single API all consistency checks go through.Hasura lets you transform your Remote Schema using a prefix or suffix for the root query or for the individual types in the schema. If you want to go deeper, you can also transform individual types to have their own unique names to avoid schema collision or for use cases like a better naming convention.Q: I\u2019m encountering a type conflict error while adding a remote schema. How do I resolve this?A: The error should tell you which type that is already existing in the GraphQL schema is conflicting with the new remote schema that is being added. The conflict could be with the fields and queries generated from the databases connected to Hasura, or with another remote schema that was added to Hasura earlier. Once you identify the conflicting type from the error message, you can head to the relevant page on the console (either the database tables or remote schema modify) and set up the transformations and proceed with the addition of the new remote schema without any hassles. GraphQL CustomizationsUse-case examplesYour custom resolvers that return references to entities in a databaseThe data coming in from a GraphQL API can be joined with data already existing in a database. This is possible by creating a relationship from a field in the custom GraphQL server to a field in the database table.GraphQL Joins\u00a0For example, consider the following query:{\n  order(id: 1) {      <---- order is served by a GraphQL API\n    status\n    customer {        <---- customer data comes from a Postgres database\n      name\n      email\n    }\n  }\n}Joining across multiple GraphQL APIsIn this use case, you have two independent GraphQL APIs that have some form of related data which can be configured to return the joined data.GraphQL Joins with HasuraFor example: consider the following query:{\n  fulfillment(orderId: 1) {  \n    status    <--- fulfillment data is coming from remote schema 1\n    order {   <---- order data is coming remote schema 2         \n      lineItems {\n        item {\n          name\n        }\n      }\n    }\n  }\n}From an OpenAPI REST service to a GraphQL API with relationships between typesYou can create relationships within the same remote schema types. This is useful in cases where you are incrementally migrating from a REST API to GraphQL using tools like OpenAPI to GraphQL where the relationship setup is lost during the translation of the API.Approaching Schema EvolutionThe initial data federation setup will continue to work till the upstream API(s) change. As the application evolves, the API will undergo schema changes, either additive, removal or via deprecation of types. Here we have two scenarios that need to be handled.Simple schema updates (non-breaking changes)Breaking changesBoth need to be handled differently. Hasura lets you apply schema changes in your CI/CD system via the metadata API that is exposed. If there are simple non-breaking schema changes, you can just reload the metadata to reflect the updates. This will work automatically and no more additional configuration or code changes are required.If there are breaking changes, post the metadata reload, they are handled gracefully at the Hasura layer. For example, If an existing GraphQL type is used in a remote relationship and if the schema undergoes a breaking change, Hasura throws metadata inconsistency warning(s) wherever relevant. This can be used to debug what will stop working before going to production.Hasura throwing inconsistent metadata warning!Some of the common breaking changes include fields, inputs, types, and arguments having been modified for the existing schema. And of course, any removal of any of these would be considered a breaking change as well.You can run regression tests in your CI/CD workflow to identify such issues and make the appropriate changes to the client code. Hasura Cloud makes this setup easy to configure and run these tests on your staging/production environments.Integrating with other toolsThere are OSS tools and services that let you manage the schema changes at the CI/CD layer. graphql-inspector is an OSS tool that can be used as a GitHub action to monitor breaking changes to the schema connected to the GitHub repo. There are also managed services like Hive by The Guild and Apollo Studio which lets you manage schema in their registry and view the schema diff with breaking changes as the API evolves. The managed services work well for monitoring GraphQL APIs that you write and have control over.Keeping remote schemas up-to-dateIf the upstream GraphQL API is maintained by someone else where you have no control over the schema or when there might be a breaking change, you can set up a monitoring step to do a schema diff on a continuous basis and check if there are any changes (especially breaking)Tip: If you want a DIY monitoring solution, you can make use of Hasura Scheduled Triggers to run a function that uses graphql-inspector to continuously do a schema diff on a regular basis and set up a notification system to know if and when a vendor API changes.Once a schema change is identified, you can trigger the CI/CD workflow to reload the Hasura metadata for the updates to reflect.Handling Authentication for upstream GraphQL serversThe upstream GraphQL servers will most likely require some form of authentication to introspect and expose the schema. In order to handle this authentication, you can set up the tokens during the creation of the remote schema by forwarding the right headers. You can also forward any additional headers from the client to the upstream server. This way, if the frontend client is using a specific token that authenticates and authorises certain parts of the upstream API, that can be taken care of by Hasura with the forward headers set up.Enable forward all headers from client on remote schemaAuthorization for role-based schemasHasura supports role-based authorization where access control is done by creating rules for each role and operation.By default, the entire schema of your external GraphQL API is part of the unified API that Hasura exposes. But you might want to restrict access to certain parts of the schema for security reasons. To enable permissions, an environment variable needs to be configured. HASURA_GRAPHQL_ENABLE_REMOTE_SCHEMA_PERMISSIONS: \"true\"Now, let us look at different authorization use cases:Expose a subset of the schema for a given roleThe remote schema added to Hasura would potentially have a larger schema, where every query need not be exposed to the API layer. Consider the GitHub's public GraphQL API available at https://api.github.com/graphql. We want to be able to only fetch repository details from the API for a role called user. All other schemas are not required to be exposed in the API for this given role. We can configure this at the Hasura permissions layer.Role based Authorization for GitHub's public GraphQL APICaching for Remote Schemas and Federated DataHasura Cloud supports query response caching that helps improve performance for queries that are executed frequently. If in case, your federated GraphQL query is executed frequently, you can make use of caching in Hasura Cloud.For example, the query can be modified to have the `@cached` directive and the response from the joined data will be cached.{\n  fulfillment(orderId: 1) @cached(ttl: 120) {\n    status\n    order {\n      lineItems {\n        item {\n          name\n        }\n      }\n    }\n  }\n}That\u2019s it. The federated data can be cached automatically with the addition of a directive. Note that, the caching will not work if you have enabled forward headers from the client while setting up the remote schema. This is currently a limitation.Hasura Internals: How query execution worksUnlike joins across two tables in the same Postgres database, all \u201cremote\u201d joins must be performed by the GraphQL Engine, regardless of whether the data originates from a database or a remote schema: each source is independent from all the others, and only the Engine has the full knowledge of all of them. To perform such a join, we use a \u201cdataloader\u201d approach: we analyse the query ahead of its execution to identify each individual part, and we modify each individual sub-query to also extract the required join keys; the engine then issues them one by one and stitches the results together.Consider a complex nested joins like the one below:From the artist object, we perform two remote joins: one to get the album, and one to get a picture; however, after performing that second remote join, we perform another one, from the picture to its metadata: in our join tree, the picture node contained another recursive join tree.In the engine, instead of the joins being one step, they are now performed iteratively: after performing the very first step of the query, we traverse the join tree. At each leaf, each join that needs to be performed, we repeat the process recursively with that node\u2019s sub-tree; we perform, in essence, a depth-first traversal, constructing the resulting JSON object on the way back up.However, when extending the Engine to allow joins from remote schemas, we encountered an additional difficulty, which forced us to refine how our join trees are defined: the path at which a join must be performed can be ambiguous! Consider, for example, the following example:query($nodeId: Int!) {\n  node(id: $nodeId) {\n    ... on Artist {\n      articles {\n        name\n      }\n    }\n    ... on Writer {\n      articles {\n        name\n      }\n    }\n  }\n}\nDepending on the actual GraphQL type of the object that is being returned, we must perform one of two possible joins: either one from artist to articles, or one from writer to articles. But, in both cases, the part of the answers\u2019 object that needs to be modified is at the path node.articles! To be able to differentiate between the two, we need to be able to distinguish them in the join tree, and furthermore we must be able to know, in the engine, what the actual GraphQL type of the returned object is.To do so, we have changed our join tree definition to allow each branch to be annotated with a typename: in this case, our join tree would have two joins: one at the path node.(Artist.articles), and one at the path node.(Writer.articles). We only do this for paths that depend on the runtime GraphQL type. And to retrieve the runtime type, we add a phantom field in the request that extracts the __typename of the object, that we then use when traversing the join tree.Try out GraphQL Joins with a demoWe have created an example of GraphQL Join where two custom GraphQL servers written using GraphQL Yoga are added as a remote schema and data is being joined based on an argument.Head over to https://github.com/hasura/graphql-joins-example and follow the instructions on the README to get started.Check out our docs on GraphQL Joins here.Get started with GraphQL Joins today on Hasura Cloud. Try it out for yourself and let us know what you think!We really want your feedback. We\u2019ve created a GitHub issue and would love to hear from you. Ping us on discord. If you\u2019d like a demo of different use-cases and want to chat about how this will work with what you\u2019re doing, reach out to us.Join our webinarJoin Co-founder and CEO, Tanmai Gopal, on May 12th for A Deep Dive into Data Federation with GraphQL Joins. Hear how Hasura GraphQL Joins enhances data federation for developers who have: - More than one GraphQL API \u2013 whether their own or a third party\u2019s- Existing investments in GraphQL servers to which they would like to add new data sources- Databases for which they haven't yet created APIs"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Distributed Tracing for Datadog and Azure Monitor Description: Hasura now has distributed tracing for Datadog and Azure Monitor. Set up tracing with a couple of clicks! Author: Jesse Martin"}, {"role": "assistant", "content": "Distributed tracing for GraphQL apps on HasuraAs serverless technologies and microservice architectures continue to expand, tracing information is vital to understand the entire request-response lifecycle to ensure a stable and performant experience for your users. Today we are excited to announce that we have added distributed tracing to both our Datadog and Azure Monitor integrations!The quick takeDistributed tracing is available on both Hasura Cloud and Hasura EE. Hasura Cloud provides the low-ops benefit of exposing this as a configuration primitive on the project Dashboard.The tracing format is output will vary based on integration, with Datadog and Azure both specifying their own telemetry format. Open Telemetry is coming in a future release.Telemetry built-inOne of the core benefits of using Hasura Cloud and Hasura EE is the low/no-ops support needed to get access to telemetry data out-of-the-box. In the following two videos, Jeremy Jacobson shows how to quickly add telemetry data from Hasura into both of these integrations.\n\n  Your browser does not support the video tag.\n\n\n  Your browser does not support the video tag.\nGive this new integration a try and let us know what you think! With these newest integrations, it\u2019s never been faster to get full observability on a production-ready GraphQL endpoint.New to Hasura Cloud? Signup for a free account and try these new features in minutes! Hasura Cloud is the fastest way to a GraphQL API. Now with tracing."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build Responsive Cross-Platform Vue Apps with Quasar Framework and GraphQL Description: Build responsive SPAs, SSR Apps, PWAs, Hybrid Mobile / Electron Apps using a single codebase with Quasar Framework powered by Vue.js and GraphQL APIs over postgres using Hasura Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DR: Build responsive SPAs, SSR Apps, PWAs, Hybrid Mobile / Electron Apps using a single codebase with Quasar Framework and the Hasura GraphQL Engine. Instant setup. Tutorial/boilerplate -> quasar-framework-vue-graphqlQuasar FrameworkQuasar Framework is a high-performance Vue.js framework that enables developers to build different types of applications with a single codebase.With Quasar, you can have a single codebase for:Single Page Apps (SPA)Server-Side Rendered Apps (SSR)iOS/Android Mobile AppsDesktop AppsPWAsHybrid AppsIn simple terms, Quasar allows you to build cross-platform applications.Although it is powered by Vue.js, it comes with its own set of built-in web components that can be imported to construct user interfaces.Hasura with QuasarHasura is an open-source GraphQL engine that gives you real-time GraphQL APIs on new or existing Postgres databases. It comes with built-in support for stitching custom GraphQL APIs, and it allows you to trigger webhooks on database changes.Hasura GraphQL fits in this workflow of building cross-platform apps neatly. All the clients (mobile app/electron app) use the same set of APIs controlled by the same set of permissions and auth. That means it can leverage the power of Postgres. Quasar gives endless possibilities with one codebase, and a standard GraphQL API from Hasura adds to the easier development workflow.In the next step, you will learn how to configure the Apollo Client with Quasar and perform GraphQL Queries.Quasar App ExtensionsQuasar enables developers to extend the application\u2019s functionalities with extensions. An example of an extension would be @quasar/apollo, which allows you to use GraphQL and the Apollo Client in your Quasar app.In Quasar, you can install extensions as follows:quasar ext add @quasar/apollo@next\nThe above command installs the extension and creates two additional files:src/apollo/index.js - you need to add your Hasura app endpoint in this filesrc/boot/apollo.js - you can leave this untouchedThe next step is to register the extension in the quasar.config.js file. Open the file and add \"apollo.js\" in the boot array. See the part of the config file where you need to make the changes:module.exports = configure(function (ctx) {\n  return {\n    boot: [\"apollo.js\"],\n   };\n});\nYou are done with the extension installation & configuration. The next step involves using the Apollo Client to perform GraphQL Queries.Fetch AuthorsThe main page of the application will list all the authors. You can click on the name of the authors to see their articles.Quasar comes with all sorts of UI components that you can use to create an application with a native feeling.In the code snippet below, you make use of those UI components to build the user interface. Add this code in the layouts/MainLayout.vue file:<template>\n  <q-layout view=\"lHh Lpr lFf\">\n    <q-header elevated>\n      <q-toolbar>\n        <q-btn\n          flat\n          dense\n          round\n          icon=\"menu\"\n          aria-label=\"Menu\"\n          @click=\"toggleLeftDrawer\"\n        />\n\n        <q-toolbar-title\n          class=\"my-box cursor-pointer q-hoverable\"\n          @click=\"this.$router.push(`/`)\"\n        >\n          Quasar App with Hasura GraphQL Engine\n        </q-toolbar-title>\n\n        <div>Running on Quasar v{{ $q.version }}</div>\n      </q-toolbar>\n    </q-header>\n\n    <q-drawer v-model=\"leftDrawerOpen\" show-if-above bordered>\n      <q-list>\n        <q-item-label header> Author List </q-item-label>\n\n        <q-item v-for=\"author in authors\" :key=\"author.id\">\n          <q-item-section class=\"my-box cursor-pointer q-hoverable\">\n            <q-item-label @click=\"fetchArticle(author)\">\n              {{ author.name }}\n            </q-item-label>\n            <q-item-label caption>ID: {{ author.id }}</q-item-label>\n          </q-item-section>\n        </q-item>\n      </q-list>\n    </q-drawer>\n\n    <q-page-container>\n      <router-view />\n    </q-page-container>\n  </q-layout>\n</template>\nThe above code loops over an array of authors, and it also uses a method named fetchArticles. That means you need to retrieve the authors from the database and define the method for fetching articles.Write the following code in the same file:<script>\nimport { defineComponent, ref } from \"vue\";\nimport { useRouter } from \"vue-router\";\nimport { useQuery, useResult } from \"@vue/apollo-composable\";\nimport gql from \"graphql-tag\";\n\nexport default defineComponent({\n  name: \"MainLayout\",\n\n  setup() {\n    const leftDrawerOpen = ref(false);\n    const router = useRouter();\n\n    const { result, loading, error } = useQuery(gql`\n      query {\n        author {\n          id\n          name\n        }\n      }\n    `);\n\n    const authors = useResult(result, null, (data) => data.author);\n\n    const fetchArticle = (author) => {\n      return router.push(`/author/${author.id}`);\n    };\n\n    return {\n      authors,\n      leftDrawerOpen,\n      fetchArticle,\n      toggleLeftDrawer() {\n        leftDrawerOpen.value = !leftDrawerOpen.value;\n      },\n    };\n  },\n});\n</script>\nYou use the Apollo Client to query the database and retrieve the authors. You also define the fetchArticle method, which routes users to the right page when clicking on the author\u2019s name.The code is still the same kind of code you would have written in a Vue.js app to make a GraphQL query with vue-apollo. The difference is the <template> part where Quasar uses native-like components to build hybrid apps.Fetch ArticlesEach author can have one or more articles, so you need to build the \"Articles\" page to list those articles. You can use the Quasar CLI to create a new page:quasar new page Articles\nThe command creates a .vue file in the src/pages folder. Now you need to reference it in the routes file so that the users can access it.Open the src/router/routes.js and add the route:{\n    path: \"/\",\n    component: () => import(\"layouts/MainLayout.vue\"),\n    children: [\n      {\n        path: \"/author/:authorId\",\n        component: () => import(\"pages/Articles.vue\"),\n      },\n      { path: \"\", component: () => import(\"pages/IndexPage.vue\") },\n    ],\n  },\nObserve the /author/:authorId path. You can see an example of such a URL in the image below.When you visit an author, you can see all the articles written by that author.Go to the src/pages/Articles.vue file and replace the content with this template:<template>\n  <q-page padding>\n    <q-item v-for=\"article in articles\" :key=\"article.id\">\n      <q-item-section class=\"my-box cursor-pointer q-hoverable\">\n        <q-item-label>\n          {{ article.title }}\n        </q-item-label>\n        <q-item-label caption>ID: {{ article.id }}</q-item-label>\n        <q-item-label>\n          {{ article.content }}\n        </q-item-label>\n      </q-item-section>\n    </q-item>\n  </q-page>\n</template>\nYou are using the Quasar UI components to display the article:titleidcontentNow it's time to fetch the articles from the Hasura backend. Add the following code after the template section:<script>\nimport { watch } from \"vue\";\nimport { useQuery, useResult } from \"@vue/apollo-composable\";\nimport { useRoute } from \"vue-router\";\nimport gql from \"graphql-tag\";\n\nexport default {\n  name: \"PageName\",\n\n  setup() {\n    const route = useRoute();\n\n    const { result, loading, error, refetch } = useQuery(\n      gql`\n        query articleQuery($authorId: Int!) {\n          article(where: { author_id: { _eq: $authorId } }) {\n            id\n            title\n            content\n          }\n        }\n      `,\n      {\n        authorId: route.params.authorId,\n      }\n    );\n\n    const articles = useResult(result, null, (data) => data.article);\n\n    watch(\n      () => route.params.authorId,\n      async (newId) => {\n        refetch({ authorId: newId });\n      }\n    );\n\n    return {\n      articles,\n    };\n  },\n};\n</script>\nIn the above code, you:use the Apollo Client to retrieve the articles from the databasepass the authorId parameter, so it fetches the articles for that specific authorre-fetch the articles when the author id (from the URL) changesThus, when you visit a URL like https://your-app.com/author/1, the page displays the articles of the author whose ID is \"1\".Build TargetsQuasar requires you to specify the mode of the app spa|ssr|pwa|cordova|electron and the target cordova|electron - in case it's a hybrid app. You can also specify themes like material and ios, which will appropriately apply each component\u2019s styles.You can read more about build targets here.There is a boilerplate and a short tutorial to help you get started quickly! Check it out on Github.Take it for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on Twitter, Github, or our Discord server.This post was originally published on March 20, 2019 and updated on April 14, 2022."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build and Deploy Vue.js Static Sites using Gridsome and GraphQL Description: Build JAMStack style modern static sites in Vue.js with Gridsome and Hasura GraphQL, sourcing data from postgres database. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DR: Source data from your existing Postgres database to your Gridsome static site. Instant setup. Tutorial/boilerplate -> gridsome-postgres-graphqlGridsome <- GraphQL <- Hasura <- PostgresGridsome is a Vue.js framework that allows you to build modern JAMStack websites and PWAs. It is fast and accepts custom data sources such as GraphQL. If you are familiar with React, the equivalent framework to build static sites would be Gatsby.In this article, you will learn how to build and deploy a Vue.js Static Site with the following technologies:GridsomeGraphQLHasuraPostgresDeploy Hasura InstanceHasura is an open-source GraphQL engine that gives you real-time GraphQL APIs on new or existing Postgres databases. It comes with built-in support for stitching custom GraphQL APIs, and it also allows you to trigger webhooks on database changes.For this tutorial, you will need to deploy a Hasura instance. You can do it by clicking on this button:\nAfter creating the instance, follow the instructions from this README to create the required tables for the application.Clone and Run ApplicationThe demo app walks you through building a simple blog application. The application lists all authors on the homepage and all articles on the articles page.You can clone and run the demo app as follows:$ git clone git@github.com:hasura/graphql-engine.git\n$ cd graphql-engine/community/sample-apps/gridsome-postgres-graphql\n$ yarn installNote: This app was created using gridsome-cli which generates a boilerplate hello world app.After cloning the app, you will have to configure the app to use the GraphQL endpoint that you got in the previous step.Open gridsome.config.js and configure the url to point to your Hasura app GraphQL endpoint. The fieldName is configured to be hasura at the root level to avoid conflicts with other data sources.module.exports = {\n  siteName: 'Gridsome',\n  plugins: [{\n      use: '@gridsome/source-graphql',\n      options: {\n        url: 'http://localhost:8080/v1/graphql',\n        fieldName: 'hasura',\n        headers: {\n          // Authorization: `Bearer ${process.env.AUTH_TOKEN}`,\n        },\n      },\n    }]\n}Save the file and run the app:yarn startVisit localhost:8080 to see the homepage:Gridsome sample app with GraphQLIf you added authors and articles as mentioned in the README, you should see the data templated on the UI if you navigate to /articles.GraphQL Data FetchingFor this app, you fetch data from the Postgres database and template it on a Vue component. The GraphQL query for fetching all authors is as follows: query {\n  hasura {\n    author {\n      id\n      name\n    }\n  }\n}The query for fetching all articles and the author related to the article:query {\n  hasura {\n    article {\n      id\n      title\n      author {\n      \tname\n      }\n    }\n  }\n}The above query makes use of relationships to fetch the articles and the author for each article.The routes are auto-generated based on the files in the pages directory.Production BuildThe static site can be generated using the gridsome build, which generates the HTML and JavaScript bundles.Run the following command to build the assets:yarn buildThis will generate HTML, JS, and CSS bundles for all static pages of the app in the dist directory.Now to serve the production app, use the command:gridsome serveThis will serve the production build at port 8080.DeploymentOne way to deploy the application is to do it with Netlify for free. Click on New site from git and choose the following settings for this app.Build Command: gridsome buildPublish directory: distFinal ThoughtsIf you are into Vue and looking to build static sites, Gridsome is a great choice. With custom data source support and schema stitching with different GraphQL sources, it gives a powerful and flexible way to build modern static sites.There is a boilerplate and a short tutorial to help you get started quickly! Check it out on Github.Take the demo for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on Twitter, Github, or on our Discord server.Read more about JAMStack to learn about the benefits of a static site in front of a CDN."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Create Nuxt.js Universal Apps using GraphQL on Postgres Description: Server side render (SSR) websites using Nuxt.js and GraphQL APIs over Postgres using Hasura GraphQL Engine. Instant setup. Tutorial/boilerplate -> nuxtjs-postgres-graphql Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DR: Server side render websites using Nuxt.js and GraphQL APIs over postgres using Hasura GraphQL Engine. Instant setup. Tutorial/boilerplate -> nuxtjs-postgres-graphqlBuilding universal applications can be difficult, but Nuxt.js aims to make it easier. Nuxt.js is a performant and modular framework that enables you to develop Universal Vue.js Applications.What is a Universal Application? A Universal Application, also known as an Isomorphic Application, is an application whose code runs both on the client and server sides.This article showcases a Universal Application that uses Nuxt.js and Hasura GraphQL Engine. The application is a blog-like application that displays a list of authors and their articles.Note: The article illustrates the most important parts of the application. You can view the complete code in this GitHub repository.SSR with Nuxt.jsNuxt.js is a Vue.js framework that allows you to build server-side rendered (SSR) Vue applications. In a universal app built with Nuxt, the pages are rendered on the server-side and then sent to the client.Due to its capabilities of supporting both server-side rendering (SSR) and single-page application (SPA), Nuxt.js enables you to build universal applications.How does it work:Pages: Nuxt is opinionated on the project directory structure. It transforms the .vue files inside the pages directory into application routes. For instance, the file mypage.vue corresponds to mywebsite.com/mypage. In the boilerplate example, you can see how the pages are structured. For more information on the directory structure that Nuxt enforces, read more here.Server-side data fetching: The Nuxt community has built the apollo-module that lets you use the Apollo Client with a GraphQL endpoint.Project ImplementationIn this section, you will see the server-side data fetching in action. You will use the apollo module to fetch the list of blog authors and their articles.Before going further, create the Nuxt project as follows:npx create-nuxt-app blog-app\nAfter running the command, Nuxt asks a couple of questions to help you configure the project. Choose these answers:Programming Language - JavaScriptPackage Manager - npm or yarn (whatever you want to use)Rendering Mode - UniversalDeployment Target - any of the twoFor the other questions, you can omit them or leave the default answers.Project DependenciesFor this project, you need a couple of packages such as \"apollo\", \"graphql-tag\", and others. See the complete list of dependencies:  \"dependencies\": {\n    \"@nuxtjs/apollo\": \"^4.0.1-rc.5\",\n    \"apollo-cache-inmemory\": \"^1.6.6\",\n    \"core-js\": \"^3.21.1\",\n    \"cross-env\": \"^7.0.3\",\n    \"graphql-tag\": \"^2.12.6\",\n    \"nuxt\": \"^2.15.8\"\n  },\n  \"devDependencies\": {\n    \"nodemon\": \"^2.0.15\"\n  }\n}\nAdd them to your .package.json file and run npm install.Configure Apollo ClientThe next step involves configuring the Apollo Client so you can perform GraphQL Operations.With Nuxt.js, you can extend the core functionalities with modules. For this application, you will use the \"apollo\" module for Nuxt.Go to the nuxt.config.js file and add the following code:  modules: [\n      \"@nuxtjs/apollo\"\n  ],\n\n  apollo: {\n    cookieAttributes: {\n      expires: 7,\n    },\n    includeNodeModules: true,\n    authenticationType: \"Bearer\",\n    errorHandler: \"~/plugins/apollo-error-handler.js\",\n    clientConfigs: {\n      default: \"~/apollo/clientConfig.js\",\n    },\n  },\nIn the beginning, you are registering the \"apollo\" module in your project. After that, you configure the \"apollo\" module:the expires field in the cookieAttributes specifies when the cookie will expire and get removedthe authenticationType field configures the auth type of authorized requeststhe errorHandler sets a global error handlerapollo-module in Nuxt accepts a clientConfig property which is used to configure the endpoint, authentication, and other parameters.Since \"/apollo/clientConfig.js\" does not exist, the next step is to create it.Create a new folder named \"apollo\" and then add a new file inside it called \"clientConfig.js\". Open the newly-created file and add the following code:import { InMemoryCache } from \"apollo-cache-inmemory\";\n\nexport default function (context) {\n  return {\n    httpLinkOptions: {\n      uri: \"http://localhost:8080/v1/graphql\",\n      credentials: \"same-origin\",\n    },\n    cache: new InMemoryCache(),\n    wsEndpoint: \"ws://localhost:8080/v1/graphql\",\n  };\n}\nIn the above code, you configure the endpoints needed to perform the GraphQL queries.Before moving on, you need to define the error handler as well. Create the file \"plugins/apollo-error-handler.js\" and add the following code:export default (error, nuxtContext) => {\n  console.log(\"Global error handler\");\n  console.error(error);\n};\nThe above plugin is a global error handler, and it's taken from the \"@nuxtjs/apollo\" documentation.GraphQL QueriesFor this application, you have two queries:one to fetch the authorsone to fetch their articleInside the same folder - \"apollo\", create a new \"queries\" folder. After that, create a new file called fetchAuthor.gql in the \"queries\" folder.\ud83d\udcc2 blog-app\n \u2514 \ud83d\udcc2 apollo\n    \u2514 \ud83d\udcc2 queries\n        \u2514 fetchAuthor.gql\nAdd this query in the \"fetchAuthor.gql\" file:query {\n  author {\n    id\n    name\n  }\n}\nSimilarly, create a new file named fetchArticle.gql to store the query for fetching articles:query article($id: Int) {\n  article(where: { author_id: { _eq: $id } }) {\n    id\n    title\n    content\n  }\n}\nYou will import the above queries whenever you need to use them in a page. With that being said, it's time to build the pages that display the authors and articles.Fetch AuthorsOpen the pages/index.vue file and add the following code:<template>\n  <div>\n    <h3>Authors</h3>\n    <ul>\n      <li v-for=\"item in author\" :key=\"item.id\">\n        <nuxt-link :to=\"`/article/${item.id}`\">\n          {{ item.name }}\n        </nuxt-link>\n      </li>\n    </ul>\n  </div>\n</template>\n\n<script>\nimport author from '~/apollo/queries/fetchAuthor'\nexport default {\n  apollo: {\n    author: {\n      prefetch: true,\n      query: author\n    }\n  },\n  head: {\n    title: 'Authors of Blog'\n  }\n}\n</script>\nIn the above code, author is a GraphQL query that fetches the list of authors from the database and updates the author property in Vue.After that, it loops over the array of authors and displays each author on the page. Clicking on an author takes you to their article.How does it work?This GraphQL query is executed on the server and the data is available in <template>, which allows the page to be rendered server-side. The same query is then executed on the client-side, making it universal/isomorphic.Fetch ArticleCreate the file pages/article/_id and add the following code:<template>\n  <div v-if=\"article\">\n    <div v-for=\"item in article\" :key=\"item.id\">\n      <h3>{{ item.title }}</h3>\n      <p>{{ item.content }}</p>\n      <p>\n        <nuxt-link to=\"/\">\n          Home page\n        </nuxt-link>\n      </p>\n    </div>\n  </div>\n</template>\n\n<script>\nimport article from '~/apollo/queries/fetchArticle'\nexport default {\n  apollo: {\n    article: {\n      query: article,\n      prefetch: ({ route }) => ({ id: route.params.id }),\n      variables() {\n        return { id: this.$route.params.id }\n      }\n    }\n  },\n  head() {\n    return {\n      title: 'Articles by Author'\n    }\n  }\n}\n</script>\nLooking at the file name, you can observe the underscore (_). That means the page is dynamic - the \"name\" of the page comes from the Hasura backend.If you go back to the index.vue file, you should see:<nuxt-link :to=\"`/article/${item.id}`\">\nThe \"item.id\" comes from the database, which means that each article has a different URL. Nuxt (and Vue) allows you to create dynamic pages for such cases.Also, here you are using reactive parameters. Observe the following piece of code:variables() {\n    return { id: this.$route.params.id }\n}\nWhenever the parameter changes - the article id, it will re-fetch the query and thus update the page and show the requested article.Other than that, it works like the previous query that fetches the authors.Nuxt.js + Hasura + apollo-module = Universal Apps with GraphQL!There is a boilerplate and a short tutorial to help you get started quickly! Check it out on Github.Take it for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on Twitter, Github, or on our Discord server.This post was originally published on Jan 30, 2019 and updated on April 11, 2022."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Adding Multiple JWTs, Environment Flags, Preview Apps, and more! Description: Introducing multiple JWTs, environment flags, preview apps, and even more features in this latest release for Hasura Cloud. Author: Jesse Martin"}, {"role": "assistant", "content": "We\u2019re back with another round of features and services to help you build better, faster, and for a wider series of use-cases. This release includes a bunch of project organization improvements, CI/CD improvements, and even more federation support to the console. These features can be broadly categorized into workloads, performance, and deployments.Starting with our workload focus, we\u2019ve introduced two new features that allow for a wider set of uses, multiple JWTs, and support for remote database relationships.Multiple JWTsYou can now define multiple JWT configurations for your different clients and services. This feature is particularly useful for creating service-level tokens, creating differentiated tokens between desktop and mobile clients, or in cases where you might be switching over to a different auth provider while keeping the older one active for backward compatibility.Catch the recording from the Hasura Community call from Lyndon Maydwell about Multiple JWTs in action.\n\n  Your browser does not support the video tag.\nSupport for remote database relationshipsHasura now supports creating relationships between databases in the console. Using the familiar workflow of creating relationships between remote schemas, you can now define relationality between your federated data sources. As different databases bring different workload functionality to the Hasura ecosystem, this feature allows you to make that connection between the transactional and time-series, relational, and wide-table data.\n\n  Your browser does not support the video tag.\nAlong with these features, we\u2019re introducing improved performance for querying from the database itself with experimental SQL optimization.Experimental SQL OptimizationRow-level permissions are applied by transforming into SQL \u2018WHERE\u2019 clauses. However, if some tables have similar row-level permissions, the generated SQL can produce repetitive code which may not perform well. We\u2019ve added an experimental feature flag to improve on this. Note, it\u2019s explicitly experimental due to some underlying security implications of accessing row-level data. Users should carefully analyze the generated queries for any unexpected disclosure.On top of these improvements to clients and database improvements, we\u2019re introducing a bunch of new features to assist those managing multiple projects or relying heavily on a CI/CD pipeline for their deploys. These features are designed to help with the deployment lifecycle of a Hasura project. From searching for product deployments to ephemeral instances on feature branches for those working with CI/CD, these features are a welcome addition to the Hasura Cloud dashboard!Project Search on Cloud dashboardA special feature for multi-project users, you can now search for projects in the project dashboard! For agencies or in-house teams running multiple projects, this is a welcome feature to quickly find the exact project you are looking for.\n\n  Your browser does not support the video tag.\nPreview Apps YAML GeneratorPreview apps in Hasura Cloud allow you to generate temporary HGE instances with ephemeral database support for previewing versions of metadata and migration data without applying that data to a production environment. This is particularly useful for feature branches and temporary experiments without the need to set up another dedicated project in a cloud environment. You can read more about preview apps in the new docs, or try out the YAML generator here.\n\n  Your browser does not support the video tag.\nEnvironment labels on dashboard and consoleAnother much-requested feature landing in the product today is environment labels (including color banners) for both the dashboard and console! If you want to provide visual signaling to the entire team about which environment they are viewing. This will be particularly helpful for teams working with metadata and migrations as this will help deter accidental editing of the production environment, causing inconsistent metadata in your CI/CD workflows.\n\n  Your browser does not support the video tag.\nGitHub integration notificationsGithub CI/CD integration is a powerful feature for automating the metadata and migrations to Hasura Cloud. Now, you can customize the notifications you get for both successful or errored deploys to find the perfect balance of signal versus noise in your system operations.\n\n  Your browser does not support the video tag.\nAnd with that, we wrap up this release and get back to working on bringing even more features to the Hasura Cloud platform. We are excited to have you on this journey with us and we appreciate all your feedback and contributions as we build the tools for tomorrow\u2019s builders."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building file upload and downloads with Hasura GraphQL Description: Learn how to build a simple storage service to manage file uploads/downloads on S3 with the Hasura GraphQL Engine.  Author: Praveen Durairaju"}, {"role": "assistant", "content": "This tutorial was written by Johan Eliasson and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.Hasura is great at handling data with its realtime GraphQL over Postgres but Hasura does not come with a built in solution on how to handle user generated files. Such as when a user wants to upload a file or download a file.In this blog post you will learn how to manage user generated files in your Hasura app. \u00a0Files will be handled with a microservice that uses a S3 compatible object storage. The storage microservice will also handle read and write permissions using JWT claims.So your users want to upload a file. This file could be an image if you are building an image hosting service. Or it could be an Excel document if you are building a CRM. It does not matter what type of file the user wants to upload, you need to be able to save and serve these files.PrerequisitesThis blog post assumes you have previous knowledge in setting up a project with React and Apollo GraphQL. Before we can start building our storage microservice for Hasura we must first get a few variables from both Hasura and our S3 service. We set up our domain correctly.HasuraWe need the followingHasura EndpointAdmin secretThe Hasura Endpoint can be found in your Hasura Console under the tab \u201cGraphiQL\u201dHasura Console EndpointThe admin secret is located in your docker-compose.yaml file.Admin SecretS3 BucketWhat we need:S3_ACCESS_KEY_IDS3_SECRET_ACCESS_KEYS3_ENDPOINTS3_BUCKETNext we will create a S3 bucket. The S3 bucket is where all files will be stored. There are several S3-compatible object storage services. AWS S3 being the most popular. However, we will be using DigitalOcean Spaces for this example. There are several guides online on how to get started with S3 with DigitalOcean.Storage MicroserviceNow we will set up our storage microservice. Our microservice will consist of two files. Index.js and storage.js.index.jsUpload (write)Next create the file storage.js and add the following code. storage.jsTo this file we need config.jsconfig.jsThe code will handle upload request to endpoint /storage/upload and stream all files from the client, via this microservice, then to the S3 bucket. The endpoint will return a list of keys (paths to the files) for the files in the S3 bucket.Next we will create a React component to handle file uploads:upload.jsThe window.config.backend is the url to your backend. It can be localhost:3010.Make sure your storage microservice has all appropriate packages installed and is running. You can run the index.js with the node index.js command. Now file uploads are working correctly. Next we want to both list all files and be able to view and download all files.Download (read)First lets list all files. We will create a subscription to our files table and get updates in real time.Before we can view the actual file we need to add a new route to our storage.js that will stream stream back the content from S3 to the client.We now have a working solution for upload files, list files and download files in Hasura.SecurityRight now, anyone can upload and download files through our storage microservice. This is not good. We probably only want logged in users to upload and download content. Also, we might have use cases where we want to isolate files from users from one organisation.This can be handled in different ways and we will look at one way of handling security in an overview. Security is tightly connected to your authentication service and different authentication service solutions can differ quite a lot.When using JWT Tokens shared between Hasura and your storage microservice you have to make sure they are sharing the same JWT SECRET. The JWT SECRET is a environment variable in hasura and is located in your docker-compose.yaml file.JWT ClaimsHsaura uses JWT Claims to handle CRUD permissions in the database so it make sense to use JWT Claims to secure files as well. So, in order for our storage microservice to handle read and write access using JWT Claims we need to somehow send our JWT token to our storage microservice for it to be used.Cookies are one way of sending JWT claims with the requests to our storage microservice. Another way is to send the JWT claims as a header.Once you can retrieve the JWT claims in your microservice you can start making rules based on the file, the key and the JWT claims to figure out if the request should be handled or not.Here is an example:In the example above a user is allowed to upload and download files to its /company/<company_id> folder. Every user is also allowed to read files from the /public folder. All other requests can be dismissed.DeploymentOur code can be deployed anywhere were we can run Node.js, ex: DigitalOcean, AWS Lambda, OpenFaaS, Zeit Now etc. We will start our own express server to handle all requests and let us see how to set that up.Hasura CloudHead to the Getting Started with Hasura Cloud section to deploy graphql-engine server on Hasura Cloud.The express server will be deployed on a DigitalOcean Droplet.DigitalOcean Marketplace has a Node.js One-Click application that can be used to quickly create droplet optimised for Node.js. It comes with Node.js version 8.10.0 and npm 3.5.2.Choose a standard $5 plan, we can always scale up / resize later.Choose a suitable datacenter region, add your SSH key and click on Create.Once the droplet is created, note the IP address of the droplet.We will need to ssh into the droplet to continue the setup. In the terminal, run the following command:$ ssh root@<do-ip-address>\n\nReplace <do-ip-address> with your own droplet\u2019s address.We will use PM2, a process manager for Node.js, to make sure our Node.js and express server remains up and running as a daemon in the background.Let\u2019s get started by installing PM2 globally$ npm install -g pm2\n\nLet\u2019s create a pm2.json config file to configure PM2 process manager.pm2.jsonNext, we will install all npm dependencies.npm install --save pm2 express cors morgan joi boom uuid multer multer-s3 aws-sdk\n\nConfigure S3 CredentialsCreate config.js with the following:config.jsCreate index.jsCreate the full index.js file, configuring express and routes as follows:index.jsCreate storage.jsCreate storage.js which handles file uploads as follows:storage.jsConfigure environment variablesHead to https://github.com/elitan/hasura-backend-plus#env-variables and configure these environment variables by exporting them.For example, to configure USER_REGISTRATION_AUTO_ACTIVE: 'false', run the following in the terminal$ export USER_REGISTRATION_AUTO_ACTIVE=\u2019false\u2019\n\nSimilarly configure the environment variables required by this service.Finally, start the server using the following command:$ pm2 start pm2.json\n\nMake sure that the server is running using PM2s monitor, pm2 monit.We now have a working approach to handle user generated files for Hasura using a S3-compatible object storage services.What is next?Once you got this server working and after reading the code, you should have a better understanding on how to handle user generated files together with Hasura. Next thing you would want to take a close look is your permission rules, together with your JWT token provider, to enforce correct read and write rules for all your files.Also you would want to make sure the server connection is encrypted using HTTPS. Caddy Server (https://caddyserver.com) is a great tool to serve site with automatic HTTPS.About the AuthorJohan Eliasson is an entrepreneur from Sweden with a strong background in computer science. He is currently in the process of rebuilding a big internal business system to React / Apollo / GraphQL / Hasura. In his free time he likes to spend time with his family, read books and work out. You can reach out to him at johan@eliasson.me or on Twitter."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Regenerate Next.js Pages On-demand (ISR) with Hasura Table Events Description: New in Next.js 12.1 is Incremental Static Regeneration which allows us to create and update pages on demand. We can pair this with Hasura table events to keep our web pages always up to date and only rebuild when data changes. Author: Arjun Yelamanchili"}, {"role": "assistant", "content": "New in Next.js 12.1 is Incremental Static Regeneration which allows us to create and update pages on demand. We can pair this with Hasura table events to keep our web pages always up to date and only rebuild when data changes.\nLets setup an example blog app to check this out.\nSetup Hasura\n\n\nDownload the Hasura Docker Compose file.\n\n\nIn the Docker Compose graphql-engine environment variable section add SECRET_TOKEN: <a random string you come up with>\n\n\nStart Hasura and launch the console.\n\n\nIn the data tab we create a new table post with the following columns:\n\nid UUID from the frequently used columns\ntitle type text\ncontent type text\n\n\n\nIn the API tab we construct our GraphQL query\n\n\n{\n  post {\n    content\n    id\n    title\n  }\n}\n\nSetup Next.js\n\n\nWe create an example typescript Next.js app npx create-next-app@latest --ts\n\n\nWe will query Hasura using graphql-request. Install with npm install graphql-request graphql\n\n\nIn index.tsx we setup our getStaticProps data fetching\nimport { request, gql } from \"graphql-request\";\n\ninterface Props {\n  posts: {\n    id: string;\n    title: string;\n    content: string;\n  }[];\n}\n\nconst query = gql`\n  {\n    post {\n      content\n      id\n      title\n    }\n  }\n`;\n\nexport async function getStaticProps() {\n  const { post: posts } = await request(\n    \"http://localhost:8080/v1/graphql\",\n    query\n  );\n  return {\n    props: {\n      posts,\n    },\n  };\n}\n\n\n\nFinally we display our blog posts\nconst Home: NextPage<Props> = ({ posts }) => {\n  return (\n    <main>\n      {posts.map((post) => (\n        <article key={post.id}>\n          <h2>{post.title}</h2>\n          <p>{post.content}</p>\n        </article>\n      ))}\n    </main>\n  );\n};\n\nexport default Home;\n\n\n\nSetup Incremental Static Regeneration\nWhen running Next.js in production mode if we add a new blog post in Hasura Next.js has no way to know about it. In the past, before Incremental Static Regeneration, we would have to set a revalidate time, where it would rebuild a page every x amount of time even if there wasn't anything new.\nNow we can setup a webhook where we can tell Next.js to rebuild specific pages when our data changes! Following the Next.js on-demand revalidation guide:\n\n\nWhen we setup our Docker Compose we came up with a SECRET_TOKEN, we use that as a password to communicate with Next.js. Create .env.local file\nSECRET_TOKEN=<Same as what you set in Docker Compose>\n\n\n\nCreate the revalidation API route, the only difference from the official example is we look for the secret token in the header instead of a query variable.\n// pages/api/revalidate.ts\n// From Next.js docs https://nextjs.org/docs/basic-features/data-fetching/incremental-static-regeneration#using-on-demand-revalidation\nimport type { NextApiRequest, NextApiResponse } from \"next\";\n\nexport default async function handler(\n  req: NextApiRequest,\n  res: NextApiResponse\n) {\n  // Check for secret to confirm this is a valid request\n  if (req.headers.secret !== process.env.SECRET_TOKEN) {\n    return res.status(401).json({ message: \"Invalid token\" });\n  }\n\n  try {\n    await res.unstable_revalidate(\"/\");\n    return res.json({ revalidated: true });\n  } catch (err) {\n    // If there was an error, Next.js will continue\n    // to show the last successfully generated page\n    return res.status(500).send(\"Error revalidating\");\n  }\n}\n\n\n\nBack in the Hasura console events tab, create an event trigger.\n\nName the trigger anything, select the post table, and select all trigger operations.\nWith docker, the webhook handler should be http://host.docker.internal/api/revalidate\nUnder Advanced Settings we add the SECRET header from the SECRET_TOKEN environment variable\n\n\n\nSave the event trigger and run Next.js in production mode\nnpm run build\nnpm run start\n\n\n\nNow when you add a post in Hasura, when you refresh you Next.js app you'll see your updated data!\nSource Code\nAvailable on Github: https://github.com/hasura/graphql-engine/tree/master/community/sample-apps/nextjs-incremental-static-regeneration\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Realtime Games for the Web with WebRTC and Hasura GraphQL Subscriptions Description: We recently created a small game to drive engagement at one of our event booths. It uses WebRTC with Hasura GraphQL Subscriptions to create a real-time game. Author: Jesse Martin"}, {"role": "assistant", "content": "cWe recently created a small game to drive engagement at one of our event booths. We needed to solve a couple of obstacles. The game needed to be broadcast onto a large screen to drive attention. There needed to be a way for an admin to interact with the requested games so that not everyone could just broadcast their gameplay. And everyone needed to bring their own controller. Yes, really.Project requirementsTaking these requirements to the drawing board we came up with the following interface mechanics. A large screen would display the actual gameplay. Players would scan a code presented in a number of locations and be taken to a webpage where they could initiate a game request.An admin would see the requested games, and could effectively \u201cstage\u201d a request on the big screen, which would let the game begin. Once the game was staged, the player\u2019s controller would transform into an interface with some buttons to commence gameplay.Architecting multiple clients togetherFrom a technical point of view, the challenges of this workflow were far from solved. There\u2019s a minimum of three actors \u2013 player, admin, and stage. They need to respond to interactions prompted by one of the other actors in the chain, and, in the case of the stage, do so autonomously unless we want to have someone literally running back and forth to accept staged game requests from the admin.Fortunately, subscriptions were able to help us here. Creating a request to play created an insert into a game table. The admin interface was subscribed to these requests and could \u201caccept\u201d a requested game. The stage also had subscriptions and would use a small hack that would ultimately immediately respond with approval as soon as it saw a new \u201cactive game\u201d request. Meanwhile, after creating the game request, the player would have subscribed to the status of their game request and see when the admin staged their game, and when the stage auto-confirmed the game, which would then let the player begin playing.Latency in real-time web gamesManaging the player/admin/stage mechanics with subscriptions works with very effort. But subscription latency simply won\u2019t be fast enough to broadcast actual game-play mechanics between the player and the stage. For that, we need an ultra-low-latency web connection, and as of writing, WebRTC is the only game in town for that. In the future, the web Bluetooth API should make this work better for trusted sources, but generally speaking, don\u2019t let untrusted devices connect to your machine with Bluetooth.Understanding WebRTC high-level viewWebRTC is a basic protocol that establishes a connection through an offer/answer interchange in a format known as Session Description Protocol, or SDP. The requesting party sends a description of their network configuration as a proposal for how the two devices might find each other under current network conditions (usually looping through the local network hub). The responding device sends an answer with the same information, and the response also acts as a confirmation that the other party is wanting to connect.Both of these offers and answers are created by using a lookup at an intermediary STUN server (or TURN if on mobile/secured networks) which ultimately provide a type of GPS for your device\u2019s network path. It explains the path it took to get to and from your computer, and so, when compared between two devices, they can quickly find the lowest common node.Many services provide free STUN servers, including Google. TURN servers are a bit more complicated as they handle more edge cases for routers that have different ports turned off (such as is the case for mobile data networks.) Twillio offers an affordable option to consider as a backup.The shape of this response is in the ICE protocol (Interactive Connectivity Establishment). So, a network configuration conforming to ICE is sent and stored in a format known as SDP. Party one sends the first half (offer) and party two accepts by replying with the other half (answer). Of course, there\u2019s lot\u2019s more to be read on the topic of WebRTC elsewhere. For the visual learners, I personally found Hussein Nasser\u2019s tutorial quite helpful.Sending offers and answersInterestingly, there\u2019s no requirements in how the offer and answer need to be transported. As soon as an offer is created (and remains in memory) you can send this payload however you want! It could be through a text, email, bluetooth file transfer, etc. It\u2019s up to you! And that\u2019s where Hasura subscriptions become a helpful intermediary. The subscription latency is perfectly accessible to know someone is calling!So our player device creates an offer, writes that payload as a JSON object to Hasura. Our stage device (when approved by the admin) is subscribing to the current \u201cactive\u201d game\u2019s offer, and wil auto reply with an answer. Meanwhile, the device is subscribed for answers and so the two are able to create a connection.Storing gameplay data in HasuraAs we already have the tooling in place to use Hasura for creating and initializing our games, Hasura provides a very convenient method for us to get access to gameplay data as well. We are able to write data over the course of the game to our database such as which characters were chosen, and the score achieved which lets us access a running high-score and player list.Building the gameAs with building anything with Hasura, the actual Hasura plumbing is the fastest part to work with. Writing a game from scratch with very little prior experience, however, is not fast. Fortunately, we came across a fun library called Kaboom.js which lets you write canvas-based games in a functional way, which turned a quirky concept into an actual game, complete with Hasuraas, badies, and powerups.The video below will give you an example of how the game came together.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing support for Prometheus and Hasura 2.2.1 Description: Learn about the latest releases at Hasura including the newest support for Prometheus, REST Connectors on Cloud, and more. Author: Jesse Martin"}, {"role": "assistant", "content": "Hasura now supports PrometheusPrometheus is one of the most popular observability tools that are particularly well-tuned for large-scale, time-series data like metrics. Hasura Cloud and EE now support Prometheus. The integration works by providing a specialized endpoint for Hasura logs that are returned in the Prometheus format. Using a Prometheus agent, you can export and load your metrics into your Prometheus instance for further analysis. With Hasura doing so much heavy lifting in your stack, monitoring for availability and performance is a vital component in building resilient tech stacks.View the following video for a simple walk-through on how this works.\n\n  Your browser does not support the video tag.\nHasura 2.2.1 now available across all Hasura projectsHasura 2.2.1 is now on all versions of Hasura. On Hasura Cloud, you will find 2,2.1 on all l free tier projects. Over the next few days, all our standard and enterprise tier users will also be able to start using \u00a02.2.1 Here is a quick review of all the features:Function field name customization: If you track a custom function in Postgres, you can now customize the names under pg_track_function or pg_set_function_customization metadata endpoints.Customizing root names per source (Namespaces): It\u2019s possible to now set a prefix or namespace on new sources being added to Hasura, this helps avoid name collisions in type definitions.REST Transforms: REST transforms let you modify an outgoing action or event request and payload to match the required inputs of various REST services. This minimizes or eliminates the need to write custom handlers.Nested Types in Actions: Extending the benefits of GraphQL to any external REST endpoint. Query your REST endpoints with GraphQL query syntax.Support for MSSQL upserts: Adding another feature in our journey towards bringing the full power of Hasura to MSSQL databases.Support for querying MSSQL transaction status: Transaction status can be checked by querying the XACT_STATE() scalar function.OpenAPI spec for REST endpoints: Providing even easier integration of Hasura into your existing infrastructure by supporting the Swagger format for documenting and consuming REST endpoints.Along with these features, we\u2019re rolling out dozens of patches and improvements like support for remote database relationships, Kriti improvements, support for \u201cdistinct_on\u201d,\u201d_like/_nlike\u201d, and spatial operators in Big Query, and many more. As always, you can track all the details of the current and future releases with Hasura at our changelog. And join us on discord for our weekly office hours if you would like to chat about any of this!Community CallTo see the latest in the happenings at Hasura as well as to catch the growing narrative around what the Hasura product is and is becoming, catch us every last Thursday of the month at 9 am pacific for demos, updates, and community stories."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Learn GraphQL and Build Apps with Hasura in #HasuraHackathon Description: Learn GraphQL, build apps with Hasura and win cool prizes in this March Hackathon organized in collaboration with Hashnode. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Learn GraphQL, build apps with Hasura and win cool prizes in this March Hackathon organized in collaboration with Hashnode.You can read more about the prizes and important dates here, but here's the summary.There are 5 grand prizes worth $1000 each with goodies and 10 runner up prizes worth $500 each with goodies. Hackathon starts on March 1, 2022 and ends on March 31, 2022. You can join at any point as long as you are able to submit your app before the deadline.Begin your journey with GraphQL BasicsIf you are completely new to GraphQL, we recommend taking our Introduction to GraphQL course. This will get you from Zero to GraphQL in less than an hour.The key to learning GraphQL is to understand that it is just a query language and popular implementations on the client and server leverage existing JSON standards for requests and responses over HTTP.Once you are familiar with this concept, it will become easier to grok the benefits of GraphQL. The Intro to GraphQL tutorial covers some fundamental concepts likeWhat is GraphQL?GraphQL vs RESTCore ConceptsIntrospectionSetup the backend with HasuraOnce you have understood what GraphQL is and the core concepts around it, you can progress to the easiest step in the journey. i.e Setting up a GraphQL API with Hasura.We have put together a concise tutorial for Hasura Basics to create a production ready GraphQL API from scratch without writing much code. You will be able to use the power of database, set up Authorization rules for security and write custom business logic code specific to a realtime use case.Once you go over this Hands-on tutorial, which should take about ~30 mins to an hour, you will be able to start integrating the GraphQL API with any frontend app.Integrate GraphQL backend with frontendOnce you are familiar with GraphQL and setting up a backend with Hasura, you can start trying out the real power of GraphQL in the frontend app. In this frontend series, you will learn how modern GraphQL client side tooling can be integrated with a GraphQL backend like Hasura.There are tutorials available in most popular frontend frameworks and mobile first languages.Ideas for HackathonApart from what Hashnode has listed on the evaluation criteria, we recommend a few ideas for the Hasura GraphQL use case that you can experiment with. Apps you build can potentially have any of the followingBackend Clones of popular apps (eg. Twitter, Instagram, Clubhouse)A nice use case for Authorization (app that uses permission rules for data access)Realtimes apps using the power of Hasura's GraphQL subscriptions. (we have a few examples)REST Connectors (and potentially a data hub integration/contribution)Join Hasura Discord for SupportIf you have any queries about using Hasura during the Hackathon, you can join our Discord channel dedicated for the hackathon. It could also give you ideas for building the app for the hackathon."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build a real-time location tracking app with GraphQL Subscriptions Description: In this article, you will learn how to build an application that tracks the location of a vehicle in real-time. Author: Catalin Pit"}, {"role": "assistant", "content": "In this article, you will learn how to build an application that tracks the location of a vehicle in real-time.\nThe application is built with:\n\nHasura GraphQL Engine for backend\nReact and Apollo for frontend\n\nThe Hasura GraphQL Engine enables you to create the real-time GraphQL API for the application without writing any backend code.\nThe application also makes use of Hasura\u2019s GraphQL Engine real-time capabilities using subscriptions.\n\nThe above picture illustrates the final real-time location tracking application.\nData Modeling\nIt\u2019s important to mention that the focus of the tutorial is on the data models we store in Postgres and the GraphQL operations on the frontend. Rather than the complete application functionality.\nVehicle\nSince the application tracks the location of a vehicle, we need to store vehicle information in the database.\nvehicle (\n  id TEXT PRIMARY KEY\n  name TEXT NOT NULL\n)\n\nWe only store the id and the name of the vehicles.\nVehicle Location\nThe vehicle will have multiple locations during its journey, so we need to store those locations.\nvehicle_location (\n  id INT NOT NULL PRIMARY KEY nextval(vehicle_loc)\n  vehicle_id TEXT FOREIGN KEY REFERENCES vehicle(id) NOT NULL\n  location TEXT NOT NULL\n  \"timestamp\" timestamp with time zone DEFAULT now() NOT NULL\n)\n\nFor each location, we need the following:\n\nlocation id\nvehicle id\nlocation - the location is a set of coordinates\ntimestamp\n\nVehicle Location Sequence\nIn the previous table - vehicle_location - we stored the id of the location. Instead of auto-incrementing the id, we create and use a sequence.\nA sequence represents an ordered list of integers and it can be used as a primary key.\nCREATE SEQUENCE public.vehicle_loc\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\nThe above sequence starts from \"1\" and then it gets incremented by \"1\" each time.\nNext Step\nWe are done with the data modeling and we can move on to the next step - writing the GraphQL Operations.\nFrontend - GraphQL Operations\nThere are four GraphQL operations for this application:\n\na mutation to insert a vehicle\na mutation to insert the location of a vehicle\na subscription to get the vehicle\na subscription to get the location of a vehicle\n\nInsert Vehicle\nWhen you start the application and click on the TRACK LOCATION button, a new vehicle gets inserted into the database.\nmutation insert_vehicle ($objects: [vehicle_insert_input!]! )  {\n  insert_vehicle (objects: $objects){\n    returning {\n      id\n    }\n  }\n }\n\nThe application tracks the newly-inserted vehicle during its journey.\nInsert Vehicle Location\nOnce the vehicle is inserted into the database, we also need to insert the vehicle location.\nmutation insert_vehicle_location ($objects: [vehicle_location_insert_input!]! )  {\n  insert_vehicle_location (objects: $objects){\n    returning {\n      id\n    }\n  }\n}\n\nConsidering that the vehicle has multiple locations during its journey, the mutation insert_vehicle_location runs every three seconds.\nWith React Hooks, you can run the mutation every three seconds with the help of useEffect and useInterval hooks. The useEffect hook comes with React by default, but the useInterval is a custom hook.\nSubscribing to the Vehicle Location\nWe want to be notified each time the vehicle changes its location. GraphQL Subscriptions allow us to do that!\nThe following query only returns the latest vehicle location:\nsubscription getLocation($vehicleId: String!) {\n  vehicle(where: {id: {_eq: $vehicleId}}) {\n    locations(order_by: {timestamp:desc}, limit: 1) {\n      location\n      timestamp\n    }\n  }\n}\n\nGraphQL notifies the client about the changes whenever the vehicle location changes and the user interface gets updated.\nAs a result, you see the vehicle moving on the map.\nVehicle Subscription\nSimilarly, we can subscribe to the vehicle.\nsubscription getVehicle($vehicleId: String!) {\n  vehicle (where: { id: { _eq: $vehicleId }}) {\n    id\n    name\n  }\n}\n\nThe subscription returns the id and the name of the vehicle.\nApplication Code and Demo\nHead over to realtime-location-tracking.demo.hasura.app to see the application live.\nThe complete application code is available on GitHub.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing our $100M Series C funding Description: We are excited to announce our funding of $100M led by Greenoaks, Lightspeed growth, and participation from existing investors Nexus, Vertex, and Strive. Author: Hasura"}, {"role": "assistant", "content": "Tanmai Gopal, February 22 2022I\u2019m excited to announce our financing of $100M led by Greenoaks along with the participation of Lightspeed\u2019s growth arm, Vertex, Nexus, and Strive!We\u2019ve been careful while building Hasura to make sure that we\u2019re building a sustainable OSS project and a business around it. From project market fit, to product market fit, to scaling GTM and company ops. Last year was a year of keeping our heads down and building out the foundation of our GTM, which was critical to establishing the long-term foundation of Hasura Inc. While we still have a long way to go, we drove ~4x growth, with ~10x growth on our managed cloud service. It was an incredible year, and with the support of our users and customers, our team delivered massively on building something extremely challenging - delivering on a managed cloud service and on an on premise enterprise product. We\u2019re excited to double down on all the work we\u2019ve been doing, and continue working alongside partners who share our vision to build a lasting infrastructure company for developers in the next decade and beyond.We\u2019ve worked with our users and customers to add business value via the applications and APIs they build, and to extract value from even their existing data rapidly, using Postgres and recently SQL Server & Big Query. We\u2019ve also always been guided by the community in our roadmap and these conversations, suggestions, contributions (and frustrations!) have been our compass in navigating the jungle and/or turbulent ocean that is the landscape of a startup creating a New Kind Of Thing.Ergo, going forward, our growth strategy is really quite simple:Catalyze Hasura being everywhere, and keep solving for what our users want and need.We\u2019re building the product and the company that enables our community to:Take Hasura to all the places they wish they had a powerful and lovable data API.Further enhance and solve for emerging use-cases, when they\u2019re already using Hasura.Areas of workCommunity:\n\nWe will enable the community to extend and expand Hasura to the data sources and the use-cases they see.\n\nIn particular, there is incredible prior work here done in the Postgres (and the broader database) ecosystem with things like Postgres extensions (FDWs!), and UDFs in Postgres that we\u2019ve been taking inspiration from. Watch out for some major announcements at Hasura Con this year, to see what we\u2019ve been up to!\n\n\n\nWe will accelerate our ability to respond to what our community wants and needs.\n\n\nWe will invest (contributor time and financial support) in open-source projects that we depend on and that are adjacent to Hasura.\n\n\nWe will continue to invest in educational resources to help the community learn and get productive with GraphQL and Hasura.\n\n\nProduct & Engineering:We will increase velocity - continuing to serve critical enterprise-grade workloads while innovating at the speed our users and customers need us toWe\u2019re now publishing a new minor release every two weeks to get the latest improvements to the community quickly.At the same time we\u2019re prioritizing the time needed to respond to issues, feature ideas, and pull requests on each development team.In particular, we\u2019re investing in engineering effort that will allow us to provide a stable continuous release channel to our users on Hasura Cloud so that they can opt-in to benefit from the latest and greatest in Hasura, immediately!Customer Success:We will accelerate the growth of our customer success organization.Hasura is a powerful and critical portion of your stack and it's not a responsibility we take lightly.As Hasura usage expands within our customers and as new enterprises rapidly onboard themselves on to Hasura, our customer success & support organization has been critical and will continue to play a critical role.Sales and Marketing\tOver the last year, we have onboarded some of the world's largest enterprises and fastest growing startups as customers, thanks to the efforts of our sales and marketing teams. We will continue making significant investments in our go to market teams so we can accelerate this effort - a crucial part of building a sustainable organization.What exciting stuff are we up to?But some of this sounds like regular old boring company building stuff. What\u2019s the exciting product stuff we\u2019re working on, you ask? \ud83e\udd13\n\nFirstly, there\u2019s a tonne of exciting stuff you\u2019ve already asked for that\u2019s been in our backlog, so expect these to be getting released incrementally and quickly this year!\n\n\nMOAR databases, and a way for you as a user and contributor to easily contribute to and extend the Hasura GraphQL Engine project (yes, without Haskell!) and get all the features of Hasura you like, on your favorite source of data:\n\nA lot of OSS projects and businesses built on OSS, often go the route of \u201cOSS is for marketing\u201d and \u201ctime to change the license now\u201d and that doesn\u2019t bode well for anyone.\nFor an OSS project and a company built on it to survive into the future it is critical that the users and contributors are able to contribute to, extend and own parts of the OSS project they know and love, so that they can guide the project into the ever-changing future.\n\n\n\nHasura Cloud Edge: Enhancing edge delivery and features around caching & security. Our aim on Hasura Cloud is to make sure that you never need to think about caching or security for a GraphQL based data API ever. You shouldn\u2019t need to figure out how to set it up, and you shouldn\u2019t need to figure out how it works.\n\n\nAnd yes, we\u2019re hiring!A few of us got together and started Hasura because we knew something drastic had to be done to help more people solve problems they knew existed and wanted to solve. There\u2019s a lot to do and there isn\u2019t enough time. But the good thing is that there\u2019s enough people in the world who\u2019re ready to build the things that need to exist. Today, Hasura is enabling mission-critical applications in data-driven journalism, tele-heath, electric car companies, modern manufacturing and logistics, space technology and more. Enabling developers, teams and businesses to build towards the future they\u2019re imagining, is for us, some of the best work we can devote our time to.If this speaks to you, and you want to join a small team of people helping do big things, come join us and check out our open roles at: https://hasura.io/careers"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: E-commerce Order Management Dashboard with Hasura GraphQL and Appsmith Description: We will look at building an e-commerce order management dashboard.\nTo build this app, we will connect Appsmith with a GraphQL API and build an interface that will fetch information from a database. Author: Praveen Durairaju"}, {"role": "assistant", "content": "This was originally posted on AppSmith, written by Vihar Kurama.GraphQL is a query language for APIs and a server-side runtime for executing queries using the system you define for your data. It provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools.Today, developers spend a lot of time building internal applications, dashboards, and admin panels with GraphQL APIs as backend. But guess what? It\u2019s now possible for you to write customizable applications with the ability to quickly prototype and deploy in a few minutes. With Appsmith, a low code app builder, you can build entirely customizable internal tools without spending too much time or effort.In this blog, I am going to be building an e-commerce order management dashboard.To build this app, we will connect Appsmith with a GraphQL API and build an interface that will fetch information from a database. We will use Hasura to create, manage, and customize GraphQL APIs.What exactly are we building?By the end of the tutorial, we will have a fully customizable e-commerce order management dashboard! Typically, such dashboards enable the app users to:Manage the entire product catalogueView sales, track all details of total and recent ordersSend email to customersNote: We will be using data from Hasura Super App, which can be found here. You can directly migrate the data here to your Hasura instance in just a few steps listed here: Moving from local development to staging and going production with Hasura.Alternatively, you can connect to the instance we created for you to test and play with GraphQL APIs on Appsmith. These details of the instance are disclosed in the next section :)All right, let's dive in!Connect to Hasura on Appsmith with GraphQLWe will be connecting to the Hasura instance via the REST API datasource on Appsmith. For this, we will need the GraphQL API end-point and Admin Secret key from Hasura Dashboard. If you're using your instance, this is how your dashboard looks like, where you can copy the mentioned details.If you want to know more about the data schema and customize the relations between tables, you can always use Hasura cloud explorer.Now, let's quickly take a look at all the tables that are available on Hasura Super App Dashboard in brief:user: This table contains a list of all the customers who signed up for the Hasura super app.address: The address tables contain all the user delivery addresses that will be further linked to the order.admin: Details of all the super-admin who have access to all the dashboardsorder: The order table contains all the order information a user placesorder_product: This table contains a list of products that are included in the orderorder_status: Contains details of order delivery statusproduct: Details of all the products on the e-commerce storeproduct_review: Reviews of products that the customers gaveAwesome, now that we had a glance at our tables, let's build the app! First up, we'll need an account on Appsmith Cloud. You can create it here for free if you don't have one. After we log in, we will be redirected to our dashboard to make a new Appsmith; this can be under any organization of your choice.Now, to connect Hasura APIs, follow the below steps:Click on the + icon next to datasources and choose Create new API under the Create New tab.Rename the query to getProducts by double-clicking on the existing one.Set the query type to POST, and paste the following URL:https://subtle-platypus-36.hasura.app/v1/graphqlUnder the headers pane, add the following key-values:content-type: application/json\nx-hasura-admin-secret: <YOUR_SECRET_KEY>Lastly, navigate to the Body property under the API pane, set the body type to JSON, and paste the following:{{\nJSON.stringify({\n    variables: null,\n    query: \n    query {\n        product(limit: 300) {\n            id\n            brand\n            category_display_name\n            created_at\n            description\n            name\n            image_urls\n        }\n}\n})\n}}Now, we can test this GraphQL query by clicking on the RUN button to the top-right of the application; with this, we should see all the products list from the backend in the response body.Following is the screenshot:Here, inside the body pane, we write the GraphQL query to fetch all the products; this is similar to how we do it on a regular GraphiQL playground.Now let's bind this data onto the table widget.For this, click on the + icon next to widgets and drag and drop a table widget onto the canvas.Select the table on the canvas; this will open the property pane on the right sidebar; now, under table data, we bind the query using the moustache syntax:{{ getproducts.data.data.product }}And just like that, we should see all the data from the query binded onto the table.Note that all the columns are not required for us. Hence, we can hide them by clicking on the eye-icon on the column name under the columns property.Adding a New Product to DatabaseDoing a simple READ operation using Hasura's GraphQL API is easy now; let's look at how we can do a WRITE operation by building a form on Appsmith. Follow the below steps:First, let's drag and drop a new button widget on the canvas; the idea is to open a modal and show a form to create a new product on the e-com store.Next, add a text widget and a few form widgets on the modal by dragging and dropping them onto the modal from the widgets section.We can also set the form's label by updating the label property from the form's property pane.Following is a screenshot of how the form should look like:Now, update the form names to the following to access them in the query:You can update the widget names from the property pane by double-clicking on the existing one.Name: getNameDescription: getDescriptionCategory: getCategoryBrand: getBrandPrice: getPriceProductID: getIDNow, add a new button, update its label to Add Product, and set its onClick property to \"Run a query\" and choose Create a new API. This will create a new API datasource under the datasources section. Now, follow the below steps to create a new query that lets us post new data onto PostgresDB on Hasura.Rename the query to addProductUpdate the URL to:https://subtle-platypus-36.hasura.app/v1/graphqlSet the method type to POST, and copy the headers x-hasura-admin-secret, content-type from getProducts query to here.Update the query body to the following:{\n    \"query\": {{JSON.stringify(\n    mutation ($object: insert_product_insert_input!) {\n        insert_product_one(object: $object) {\n    id\n            brand\n            category_display_name\n            description\n            price\n            name\n        }\n    }\n    )}},\n    \"variables\": {\n        \"object\": {{\n          JSON.stringify({\n    id: getID.text,\n            brand: getBrand.text,\n            category_display_name: getCategory.text,\n            description: getDescription.text,\n            price: getPrice.text,\n            name: getName.text,\n        })}}\n    }\n}Here, we have an object consisting of a query key that holds the GraphQL query and a variables key with all the variables that need to be posted to the database.In the query, we have a mutation that lets us modify the server-side data. We'll send the object that references the data querying inside the mutation. The insert_product_one allows us to post one object to the database. Inside this query, we can add the attributes we need to post. Next, we've associated the variables to the object from the created UI input widgets. The .text on the input widget name helps us get the text written inside the input.That's all, and our query is now ready! We can test this out by opening the form and creating a new product on our e-commerce store.Add Stats on DashboardIn this section, we will be showing some statistics on our dashboard; for this, let's drag and drop three stat box widgets onto the canvas. Following is a screenshot of how it looks like:As we can see, these stat box widgets compute the total products, total categories, and unique brands. The data here is computed directly from the Hasura instance, using JavaScript. Following is how it works:On the first stat box widget, select the text-widget and paste the following in the Text property:{{getproducts.data.data.product.length}}This small JS code will return the array's length from the getproducts query, displaying the total products.Similarly, update the other two stat box widgets with the following JS code:To compute Total Categories, we use an IIFE function and filter the getproducts query using its category_display_name attribute:{{\nfunction(){\n    const filterProducts = getproducts.data.data.product.map(item => item.category_display_name)\n  .filter((value, index, self) => self.indexOf(value) === index)\n    return filterProducts.length\n}()\n}}To compute Unique Brands, we use an IIFE function and filter the getproducts query using its brand attribute :{{\nfunction(){\n    const filterProducts = getproducts.data.data.product.map(item => item.brand)\n  .filter((value, index, self) => self.indexOf(value) === index)\n    return filterProducts.length\n}()\n}}\nThere we go; we now have our dashboard up and running! Not just that, Appsmith allows us to create charts on top of Hasura data. Let's learn more about it in the next section.Charting on Appsmith on Hasura DataTo get started, let's rename the page as Recent Orders; the idea is to create a chart that shows daily sales. Drag and drop a new chart widget onto the page from the widgets section.This will show you a column chart with some pre-defined data; let's update this by creating our new query that fetches all the sales data:Click on the + icon next to datasources and choose Create a new API.Set the query type to POST, and update the URL to https://subtle-platypus-36.hasura.app/v1/graphqlUnder the headers pane, add the following key-values:content-type: application/json\nx-hasura-admin-secret: YOUR_SECRET_KEYLastly, navigate to the Body property under the API pane, set the body type to JSON, and paste the following:{{\nJSON.stringify({\n    variables: null,\n    query: \n    query {\n\norder {\n    order_products(limit: 200) {\n      product {\n        order_products {\n          quantity\n        }\n      }\n      created_at\n    }\n  }\n    }\n})\n}}This query will return a bit complex JSON; we will see something like:{ \"data\": { \"order\": [ { \"order_products\": [ { \"product\": { \"order_products\": [ { \"quantity\": 2 } ] }, \"created_at\": \"2021-02-22T18:16:17.776779+00:00\" } ] }, \u2026 }Here, order_products are nested in an array; hence, to process this and put it on a chart widget, we create a new JS object, allowing us to write full code on Appsmith and utilize its widgets. Confused? Follow the below steps:Click on the + icon next to JS Objects, or use the JSObject1Update myFunc1 to the following:myFun1: () => {\n\n  const data = _.compact(salesGraph.data.data.order.map((row) => \n  {\n        const quantity = _.get(row, 'order_products[0].product.order_products[0].quantity');\n        const created_at = moment(_.get(row, 'order_products[0].created_at')).format(\"LL\");\n\n        // if any of those in undefined\n        if (!quantity || !created_at) return undefined;\n\n        return {\n      x: created_at,\n             y: quantity,\n    }\n  }))\n    return data.slice(1, 20)\n    },Here, we do a simple ordering using the _.compact method from lodash to aggregate the total number of sales done on a particular date. We return these in a special {x: value, y: value} format, which is accepted by the chart widget.Now, let's head back to the chart widget and use this JS Object:Update the series data property with the following:{{JSObject1.myFun1()}}Boom! There you go, we have all our sales aggregated based on the date on a beautiful chart widget. Following is how it looks like:We also extended this app to show all the sales and orders on stat boxes and list widgets. Here's the link to the complete application!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Team Q&A on Reactiflux: Questions & Notes Description: The Hasura Team (Tanmai Gopal, Praveen Durairaju and Jesse Martin) took part in the latest Q&A on Reactiflux and we had a lot of interesting questions from the community. We have handpicked the ones related to Hasura and featured them in this post. Author: Praveen Durairaju"}, {"role": "assistant", "content": "The Hasura Team (Tanmai Gopal, Praveen Durairaju and Jesse Martin) took part in the latest Q&A on Reactiflux and we had a lot of interesting questions from the community. We have handpicked the ones related to Hasura and featured them in this post. Not surprisingly, we have gotten questions around the naming history of Hasura, which brings a smile :)You can read the full transcript from the official reactiflux channel.Question: How can I build a custom GraphQL query using raw SQL in hasura? \u2013 AkiIf you have a raw SQL query and want that to be exposed as a GraphQL query, you can either create a Postgres View or create a Postgres Function and expose it as a GraphQL API to the client.Question: What do you see as the perfect positioning of Hasura in the overall infrastructure? e.g. use it directly as backend, or place a backend between it for handling logic, authentication, etc. In the same manner, what would you consider the perfect \"Hasura-stack\"? \u2013 meesGood question! I'd say it's really up to you. \ud83d\ude42 We're building Hasura so that it can be the frontend / recipient to API consumers directly. However if you prefer having something in the middle, that's totally fair as well \ud83d\ude42Choosing a good Hasura stack depends on one of 2 use-cases:Is your end deliverable an API?Is your end deliverable an app?I like a serverless stack for the logic/authz backend piece.So something like AWS Lambda (or choose your fave serverless function vendor) + Hasura + Postgres DBaaS + (choose your fave authn SaaS vendor) would be my recommended Hasura stack!Question: What other services are competition to Hasura? And how do you differ with them? \u2013 f8_overcomerWhile there's no direct equivalent to Hasura, there are perhaps 3 main types of services/technologies that Hasura overlaps with:BaaS type products: Here Hasura's edge (and also downside in a way depending on what you want!) is that it doesn't take opinions on other things like storage, authn and is designed to provide the data API on new or existing databases. Today with Postgres-family / SQL Server / BigQuery and gradually with other popular databases as well. This is very impactful esp for teams that already have databasesGraphQL databases: We're gradually seeing databases that natively speak GraphQL becoming popular! This is awesome. Here Hasura's edge over them, is that a) Postgres is often a better option that betting on a new database entirely b) You can always use a GraphQL database along with Hasura + Postgres by bringing that in as a remote schema to Hasura! So you can use 2 databases simultaneously depending on your use-case.DIY API: Here you're thinking about the choice of building your own API vs using Hasura. Hasura aims to give you an experience where you only think about your domain's data models and your domain's business logic (that could be deployed with, say, a serverless vendor) and not really the build/setup/deploy/operate/optimize lifecycle of an API server. I linked to this post above, but this blogpost might help in understanding how Hasura works: https://hasura.io/blog/how-hasura-works/Question: Do you have any tips on handling authorization in a mid to large app in hasura? I have found that it can get quite confusing setting individual permisions for every field and worry about the security implication of \"missing something\" \u2013 roboticsoundThis has indeed been raised in the past when the app gets complex with lots of permissions. The Hasura Console currently has a Permissions Summary view to give an overall picture of what permissions have been configured. Another way to think about security is to use the Allow Lists feature to specifically allow only a list of GraphQL operations to avoid exposing everything on the schema. But we are working on improving the UX around managing permissions in bulk so that you can apply them in bulk.Question: I'm a noob when it comes to scaling/provisioning, especially when I need to interpret \"20 GB data pass-through\". Could you tell me what kind of project are supported by the standard cloud offering of Hasura? By kind, I mean the size of the project, the number of users using it, etc. Thank you for your input \ud83d\ude4f \u2013 binajmenThe standard cloud plan is a pay as you go plan that auto-scales as your app scales. This means apps of any size are good to be hosted with High availability and auto-scaling. The data pass through is dependent on your API usage and response sizes. If you are caching your queries, then that would help in reducing the data pass through usage.Question: what are your thoughts on postgraphile, and do you have any plans to allow [self-hosted] extensibility ala postgraphile's makeExtendSchemaPlugin to allow foreign data sources? \u2013 (d,f,g)=> {}This might not be a direct answer to your question since I'm not very familiar with makeExtendSchemaPlugin, but on the theme of allowing foreign data sources and making that easy: We're working on something very interesting (currently named GDWs - GraphQL Data Wrappers, inspired by Postgres's FDW Foreign Data Wrappers ofcourse) that will allow people at Hasura, data-source vendors, and the broader OSS community to build Postgres FDW like extensions to Hasura in their favourite programming language and get the power of the Hasura GraphQL API!We haven't talked about this anywhere yet \ud83e\udd10, so you heard it here first and more news coming soon! \ud83d\ude42Question: If a friend would ask you about which Postgres DBaaS to choose, what would be your answer? \ud83d\ude07 \u2013 binajmenIf they are testing it out and hacking around, I'd say go with the built in Heroku integration. If they are wanting to kick off a serious project, I'd suggest Aiven or Fly.io - if they have investors who might get mad, I'd suggest having a look at an AWS solution. \ud83d\ude04 But Aiven and fly.io are my hipster picks for the moment. I've also been happy with raleway."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: A Simple, Realtime, Event Driven Architecture with QR Codes Description: Hasura event triggers, REST endpoints, and subscriptions APIs allow for simple, event-driven patterns with many use-cases. Author: Jesse Martin"}, {"role": "assistant", "content": "If QR codes were beginning to enter the mainstream before the pandemic, the pandemic itself has ensured the QR code is likely to be an indelible (if not marginally insecure) mainstay in contemporary culture for some time.The flow can feel somewhat magical. We scan this obscure puzzle of black and white squares, and somehow this converts into an access pass, evidence of having the correct paperwork, a shortcut to an app, or access to marginalia.Of course, these are ultimately just bits of data, and the most common form these bits take is a web request. Given the right client (essentially all modern phones), scanning this code and requesting the underlying URL can trigger any number of custom behaviors.A simplified event-driven architectureUsing the right tools, any number of these custom flows simply reproduce this magical experience. Let\u2019s consider the total number of actions needed.Actions:The user creates content that should be accessible behind a QR code.Service creates a QR code representation with instructions on accessing the content.Some service exposes the QR code.A user scans the QR code.Our external service receives a trigger from this flow.Using Hasura, we can simplify the API for these services. Hasura can store the messages, the associated QR code, and act as the destination for scans.And for a bonus, we add real-time subscriptions into the mix.Our simple appOur app displays a user and an admin view side-by-side. The app displays the created QR codes and the number of times scanned on the right and displays the scannable QR code on the left.This pattern could work for promotional purposes (total times scanned), check-in purposes (has someone scanned the code?), or even authentication (does this QR code show the pertinent information?).Building the appLet\u2019s assemble our list of services:A handler to create the QR codeA handler to read and process the QR codeA service to persist the contentA service to show the QR codeWe\u2019ll use a simple service handler, Pipedream, to create our actions. These functions could live anywhere, but Pipedream gives us a straightforward context for reasoning about the logic. Using Hasura, we can focus on our domain code and worry less about the individual handlers.I\u2019ve opted for creating the QR handler as an external service for flexibility purposes, but Hasura REST endpoints could just as easily handle the QR code scans. We have some flexibility around redirects and final behavior with an external handler.Creating the QR CodeThe QR creation code process is:An event arrives from Hasura (an event trigger when we create new content requests), NodeJs generates a QR code, and we write the data back into Hasura.Using Hasura subscriptions, we can listen for the write-back and show this as a real-time response to the person generating the code. This pattern results in a high degree of scalability.Here is the QR code script:Here is the GraphQL mutation to write the data back into Hasura:Processing the QR CodeThe Pipedream service handler to process the QR code follows a similar pattern. Here is the Pipedream workflow:The trigger is the QR code HTTP request, the post_request. The ending node code is simply ending the workflow. The bulk of work occurs in the post_request, a simple REST request to a Hasura REST endpoint, which fronts a stored GraphQL mutation.Orchestrating the eventsTo connect the event triggers, we need to set the proper coordination in Hasura. We use both event triggers and REST handlers.We define the event trigger in the events tab.We persist the REST endpoint to handle the update in the REST tab of the API explorer.Defining these, we have provided both an endpoint to handle pre-defined behavior at Hasura and a way to trigger behavior from Hasura.Using the APIWe can now invoke the API using our standard GraphQL API with these services in place and our event orchestration defined.Creating a QR code that can be scanned and incrementing its read value is called using the following mutation.Note: We provide for the optional client/IP capture here.If we subscribe to this ID, the process sends us a notification when our service returns the QR code.Our API is feature is now complete and performs as expected. Using two services and Hasura, we can replicate any number of custom behaviors driven by QR code interaction.\n\nYour browser does not support the video tag.\nSummaryWhen tools such as Hasura give us the primitives we need to trigger events and respond to actions, we can simply scaffold what would have been an otherwise complex pattern to emulate. This simplified process enables us to focus on our domain and not on writing queue management, resolvers, or other code that doesn\u2019t add value to our product.We encourage you to try event-driven workflows, and let us know what you come up with!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a realtime chat app with GraphQL Subscriptions Description: Here is how I used subscriptions in Hasura GraphQL Engine to build a full-fledged real-time group chat app on Postgres with React and\u2026\n Author: Rishichandra Wawhal"}, {"role": "assistant", "content": "Learn how subscriptions in Hasura GraphQL Engine can be used to build a full-fledged real-time group chat app on Postgres with React and\u00a0Apollo. Pro Tip: There's no backend code involved. Everything is declarative.TLDR\n\n\nHasura allows us to build a realtime GraphQL API without writing any backend code.\n\n\nTo save bandwith we use a subscription to listen for message events, then run a query to fetch only the new messages.\n\n\nEvery two seconds our frontend runs a user_online mutation to populate an online users list.\n\n\nA user_typing mutation is run whenever we type few characters and a subscription is used to fetch the last typing user.\n\n\nIntroduction\nHasura allows us to instantly create a realtime GraphQL API from our data. In this tutorial we walk through creating a group chat application, without needing to write any backend code, using React and Apollo. The focus is on data models that we store in Postgres rather than full chat functionality.\nData Modelling\nUsers\nWhen a user signs up we insert their chosen username and generate their ID. We also track when they last typed and were seen.\nuser (\n  id SERIAL PRIMARY KEY\n  username TEXT UNIQUE\n  last_seen timestamp with time zone\n  last_typed timestamp with time zone\n)\n\nMessages\nFor our tutorial we will just be inserting messages, not editing or deleting, but if we wanted to in the future Hasura will autogenerate the mutations for us. We could also extend this by adding features such as multiple different chatrooms.\nmessage (\n  id SERIAL NOT NULL PRIMARY KEY\n  \"text\" TEXT NOT NULL\n  username INT FOREIGN KEY REFERENCES user(username) NOT NULL\n  \"timestamp\" timestamp with time zone DEFAULT now() NOT NULL\n)\n\nOnline users\nTo query the users online we create a Postgres view that fetches all users with last_seen less than 10 seconds ago.\nCREATE OR REPLACE VIEW public.\"user_online\" AS\nSELECT \"user\".id,\n  \"user\".username,\n  \"user\".last_typed,\n  \"user\".last_seen\nFROM \"user\"\nWHERE (\"user\".last_seen > (now() - '00:00:10'::interval));\n\nTyping Indicator\nTo query the last person typing we create a similar view with last_typed within the past 2 seconds.\nCREATE OR REPLACE VIEW public.\"user_typing\" AS\nSELECT \"user\".id,\n  \"user\".username,\n  \"user\".last_typed,\n  \"user\".last_seen\nFROM \"user\"\nWHERE (\"user\".last_typed > (now() - '00:00:02'::interval));\n\nFrontend\nCreating a user\nAt user signup we insert a single row into the users table\nmutation ($username: String!) {\n  insert_user_one(object: { username: $username }) {\n    id\n    username\n  }\n}\n\nWe take the returned id and username and store it in our app's state management.\nUser online events\nEvery two seconds we run a mutation to update our user's last_seen value. For example, with React using UseEffect or componentDidMount we could call the mutation inside a setInterval. Be mindful when using setInterval inside a React hook.\nmutation ($userId: Int!) {\n  update_user_by_pk(pk_columns: { id: $userId }, _set: { last_seen: \"now()\" }) {\n    id\n  }\n}\n\nSubscribing to new messages\nWe can use Graphql subscriptions in two ways:\n\n\nSubscribe to a query and render all the data that comes back.\n\n\nUse the subscription as an event notification then run our own custom fetching logic.\n\n\nOption two is best for our chatroom use case because we can do some work to just fetch new messages. Otherwise every new message we would also come with all previous ones.\n\n\nOn load we fetch all existing messages by setting $last_received_id to -1 and $last_received_ts to a timestamp well into the past such as '2018-08-21T19:58:46.987552+00:00'\nquery ($last_received_id: Int, $last_received_ts: timestamptz) {\n  message(\n    order_by: { timestamp: asc }\n    where: {\n      _and: {\n        id: { _neq: $last_received_id }\n        timestamp: { _gte: $last_received_ts }\n      }\n    }\n  ) {\n    id\n    text\n    username\n    timestamp\n  }\n}\n\n\n\nNow we subscribe to new message events\nsubscription {\n  message(order_by: { id: desc }, limit: 1) {\n    id\n    username\n    text\n    timestamp\n  }\n}\n\n\n\nOnce we detect a new message we call our message query with our last known id and timestamp from our client state.\n\n\nWith a bit of magic from realtime subscriptions we saved our users a ton of data by only fetching new messages.\nSending messages\nA user sends a message by inserting a row into the messages table. In the future if we setup JWT based authentication in the future we can automatically set the username from the JWT custom claims.\nmutation insert_message($message: message_insert_input!) {\n  insert_message_one(object: $message) {\n    id\n    timestamp\n    text\n    username\n  }\n}\n\nUI Typing Indicator\n\n\nSimilar to the user online events, when a user types a few characters we run a mutation on their last_typing timestamp.\nmutation ($userId: Int!) {\n  update_user_by_pk(\n    pk_columns: { id: $userId }\n    _set: { last_typed: \"now()\" }\n  ) {\n    id\n  }\n}\n\n\n\nThen we subscribe to our Postgres view of the last user typing, making sure to exclude ourselves.\nsubscription ($selfId: Int) {\n  user_typing(\n    where: { id: { _neq: $selfId } }\n    limit: 1\n    order_by: { last_typed: desc }\n  ) {\n    last_typed\n    username\n  }\n}\n\n\n\nOnline Users list\nWe can easily get a list of all users online by subscribing to the user_online view we created.\nsubscription {\n  user_online(order_by: { username: asc }) {\n    id\n    username\n  }\n}\n\nTry it out on Hasura Cloud to create your own backend.Live DemoHead to https://realtime-chat.demo.hasura.io/ and try out the Live demo of the chat app."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura GraphQL Engine v2.2.0 Description: Today we are releasing database updates, better interop with the REST ecosystem,  and one of the biggest updates to actions since the first release of the feature. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Let\u2019s first take a moment to appreciate the numerology of today\u2019s release. No matter how you slice it, releasing version 2.2.0 on 02.02.22 ticks all the boxes. But there\u2019s a lot more in today\u2019s release than serendipitous semvertology. Today we are releasing database updates, better interop with the REST ecosystem, \u00a0and one of the biggest updates to actions since the first release of the feature \u2013 let\u2019s go!MSSQL upsert and \u201cundo\u201d (aka transactions ;))We\u2019ve released two new functionalities to MSSQL auto-generated GraphQL API: upserts and transactions.UpsertsUpserts allow users to create or update depending on if an item exists or not present in the API, saving extra round-trips to the server. Under the hood, we watch for if_matched filter inputs and then wrap up each insert by setting the identity_insert for the table. Ultimately creating a temporary table state of the affected rows and a temporary table of the values, merging them both into a final upsert statement.Consider the two following queries:mutation simple_insert {\n  insert_user(objects: {age: 20, id: 1, name: \"george\"}) {\n    affected_rows\n    returning {\n      id\n      age\n      name\n    }\n  }\n}Here, we simply run the insert as usual. But if we pass an `if_matched` input, we now have a proper upsert behavior.mutation upsert {\n  insert_user(objects: {age: 21, id: 1, name: \"lucas\"}, if_matched: {match_columns: id, update_columns: age}) {\n    affected_rows\n    returning {\n      id\n      age\n      name\n    }\n  }\n}You can read more about the design and implementation in the original RFC or check out the documentation here. Here's a video demo:\n\n\nSorry, your browser doesn't support embedded videos.\nTransactional rollbacks based on state, AKA \u201cundo\u201dError handling in MSSQL can be tricky, and we were experiencing an issue where a cascading error would attempt to roll back the identical transaction, but the net effect was that the error of rolling back an already rolled-back transaction surfaced, not the actual cause of the original exception.To solve this, we introduced the ability to query transaction state with the XACT_STATE() scalar function. This allows for monitoring transactional state \u00a0without creating problematic duplication which increases error occurrences. More observability is always a good thing.OpenAPI body specificationsA while back we introduced the ability to persist queries behind REST endpoints. This has allowed companies to improve reverse interoperability with existing infrastructure for cases where REST is required. To improve this interoperability, we\u2019ve added OpenAPI documentation for body responses at the /api/swagger/json endpoint. Now, consuming restified GraphQL is even simpler than before.\n\n\nSorry, your browser doesn't support embedded videos.\nNested action typesStarting today, we\u2019ve introduced nested types in action response handlers. Previously, an action could return deeply nested JSON structures, but the type definitions would need to be buried in either JSON, or JSONB. With this update, you can provide typings for actions and get instant GraphQL for your external rest endpoints as well. You can see just how impactful this is in the following video.\n\n\nSorry, your browser doesn't support embedded videos.\nYou can take a look at the detailed changelog on GitHub - https://github.com/hasura/graphql-engine/releases/tag/v2.2.0We think these latest releases will continue to help you build great software.Be sure to catch us at all our regular intervals:\ud83e\udd13 \u00a0Data and API Show: Every Tuesday 10am pacific on YouTube\ud83d\udee0\ufe0f \u00a0Office Hours: Every Wednesday 9am pacific on DiscordCommunity Call: Last Thursday of every month 9am pacific!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a GraphQL API with Golang, Postgres, and Hasura Description: This article will explore building a GraphQL API for a cinema movie catalog using Golang connected to a Postgres database on Hasura. Our API can retrieve a list of movies and add a new movie to the catalog. Author: Hasura"}, {"role": "assistant", "content": "Known for its simplicity, advanced performance, concurrency, and distributed systems benefits, Golang (or Go) is growing in popularity among the development community, and so is GraphQL \u2014 a simplified and efficient query language for retrieving and mutating data from APIs.By building GraphQL APIs in Golang, developers get the efficiency, fast performance, and concurrency benefits of Golang combined with GraphQL\u2019s data simplification and performance features, making it easy for developers to build scalable, reliable, and efficient software.This article will explore building a GraphQL API for a cinema movie catalog using Golang connected to a Postgres database on Hasura. Our API can retrieve a list of movies and add a new movie to the catalog.Check out the Go backend course. In this course, you will learn how to integrate Go in a GraphQL backend server stack with Hasura.To Get Started, Set Up a Few PrerequisitesYou will need the following prerequisites to begin this tutorial.Golang installed on your local development machine (see the official docs)Your preferred code editor (we recommend Visual Studio Code)A basic understanding of GraphQL, which you can find in our GraphQL tutorialStep 1: Select a Golang GraphQL LibraryThere are several library options for building GraphQL APIs in Golang: graphql-go, Thunder, gophers, and gqlgen;however, we will be using gqlgen in this article. gqlgen is a popular library for building GraphQL APIs in Golang. It is a popular choice because of type safety and code generation features, which help developers build GraphQL APIs quickly and efficiently.Another advantage of the gqlgen GraphQL Golang library is its schema-first approach. The schema-first approach allows you to define the data requirements (schema) of the GraphQL service using the GraphQL Schema Definition Language (SDL) and then focus on the actual implementation (business logic) later. With gqlgen, you get automatic code generation for query and mutation resolvers based on your GraphQL schema.Step 2: Set Up Your Golang GraphQL Project and gqlgenIn this step, we will be setting up our Golang project and installing the gqlgen Golang GraphQL library.Create a directory for your project. In your working directory, open your terminal and initialize it as a Go Module.go mod init github.com/<user-name>/golang-graphqlExecute the following command to install the gqlgen library.go install github.com/99designs/gqlgen@latestCreate a file called tools.go at the root of your project, where you will list all your dependencies and paste the following code snippet.// tools.go\n\npackage tools \n\nimport _ \"github.com/99designs/gqlgen\"Open your terminal and execute the tidy command to add all missing dependencies.go mod tidyCreate the Golang GraphQL project structure using the init command.go run github.com/99designs/gqlgen initThis command creates the project layout and files to run the GraphQL API. The server.go file is for running the GraphQL API, and the schema.graphqls contains some boilerplate schema definitions for the GraphQL API.Open http://localhost:8080/ to test the GraphQL playground.Let's define the GraphQL schema for our Movie API now that we've successfully set up our project.Step 3: Define Your GraphQL SchemaA GraphQL schema defines the data requirements that clients can request from the GraphQL API. In this next step, we will describe the GraphQL schema for our Movie API by modifying the schema. graphqls file.Navigate to the schema.graphqls file and replace the contents with this code snippet, defining the Query and Mutation schema for our Movie API.// schema.graphql\n\ntype Movie {\n id: ID!\n title: String!\n url: String!\n releaseDate: String!\n}\n\ntype Query {\n movies: [Movie!]!\n}\n\ninput NewMovie {\n title: String!\n url: String!\n}\n\ntype Mutation {\n createMovie(input: NewMovie!): Movie!\n}The code above defines a Query and a Mutation type. The Query type retrieves all movies, and the Mutation type contains a createMovie method that takes in a new movie object as its input parameter.Step 4: Generate Your GraphQL ResolversGraphQL resolvers are functions that generate responses for a GraphQL query. The gqlgen library automatically generates resolvers based on the schema's defined schema. graphqls file, so you don't need to write the boilerplate code manually for generating resolvers.Navigate to the schema.resolvers.go file and delete the file's contents.Execute the following command to use the gqlgen code generation feature to generate resolvers based on your defined schema.go run github.com/99designs/gqlgen generateThe command above generates the skeleton of two GraphQL resolvers: one Query resolver function and one Mutation resolver function. The createMovie method will implement the movie creation logic, and the function Movies will contain the movie retrieval logic. Later in this tutorial, we will come back to implement the logic for these resolvers.Step 5: Provision Your Postgres Database on Hasura CloudWe will be creating a managed Postgres database on Hasura cloud to store data for our movie application. (You can also use any database of your choice and connect it to your Golang project.)Sign in to Hasura Cloud.Create a new project and click on the Launch Console button.Navigate to the Data tab and connect an existing database or create a new managed Heroku Postgres database.After creating the database, click on the Create Table button in the Data tab and create the table movies with the following columns defined in your GraphQL schema.Head over to the Project \u2192 Settings \u2192 Env vars \u2192 Custom Env vars to copy your Postgres database URL, which you will connect to in your Golang project.Step 6: Connect Your Postgres Database to GolangWe'll use the Postgres URL we copied in step 5 to link our Postgres database to the Golang project in this step.Create an a .env file in the root of your project folder and paste the Postgres database URL as shown below.DB_URL=postgres://znydojgffymetm:bb7d8540de0280a0lru2oau96q3After that, you'll need a database driver to connect to the Postgres database using Golang's native SQL package. go-pg is a Golang library that converts ORM (object-relational mapping) queries into Postgres SQL Queries. godotenv is a Golang package that allows you to load environment credentials from an a.env file into your app.Execute this command to install these drivers.go get github.com/go-pg/pg/v10 github.com/joho/godotenvOpen your terminal and execute the tidy command to add any other missing dependencies.go mod tidyIn the graph project folder, create a db.go file and paste the following code block.// db.go\n\npackage graph\nimport (\n   \"os\"\n   \"github.com/go-pg/pg\"\n)\nfunc Connect() *pg.DB {\n   connStr := os.Getenv(\"DB_URL\")\n   opt, err := pg.ParseURL(connStr)\n   if err != nil {\n       panic(err)\n   }\n   db := pg.Connect(opt)\n   if _, DBStatus := db.Exec(\"SELECT 1\"); DBStatus != nil {\n       panic(\"PostgreSQL is down\")\n   }\n   return db\n}The above code block defines a function that connects to the Postgres database using the database URL specified in the environment variable.We will set up this connection in our server.go file to establish a link to the Postgres database as the server starts.Modify the contents of the server.go file using the code snippets highlighted in yellow.// server.go\n\npackage main\n\nimport (\n\n\"github.com/joho/godotenv\"\n)\n\nconst defaultPort = \"8080\"\n\nfunc main() {\n  \n   // ...\n   err := godotenv.Load(); if err != nil {\n    log.Fatal(\"Error loading .env file\")\n   }\n\n // ...\n\n   Database := graph.Connect()\n   srv := handler.NewDefaultServer(\n       generated.NewExecutableSchema(generated.Config{Resolvers: &graph.Resolver{DB: Database}}))\n // ...\n}The above code block calls the Connect function in the db.go file and stores that connection within the Resolver struct.In the graph folder, navigate to the resolver.go file to add the DB connection field in the Resolver struct as a dependency.// resolver.go\npackage graph\nimport \"github.com/go-pg/pg\"\n\ntype Resolver struct {\n   DB *pg.DB\n}We have successfully configured our Postgres database in our Golang project. In the next step, we will be implementing our resolver functions.Step 7: Implement Your Generated ResolversThis step will implement the logic for the resolver methods in the schema.resolvers.go file.Paste the following code snippet in the createMovie function; it takes in the movie object as input and inserts it into the Postgres database.// schema.resolvers.go\n\nfunc (r *mutationResolver) CreateMovie(ctx context.Context, input model.NewMovie) (*model.Movie, error) {\n   movie := model.Movie{\n       Title: input.Title,\n       URL:   input.URL,\n   }\n\n   _, err := r.DB.Model(&movie).Insert()\n   if err != nil {\n       return nil, fmt.Errorf(\"error inserting new movie: %v\", err)\n   }\n\n   return &movie, nil\n}Paste the following code snippet in the Movies function. The function retrieves a collection of movies from the Postgres database based on the defined movie model.// schema.resolvers.go\n\nfunc (r *queryResolver) Movies(ctx context.Context) ([]*model.Movie, error) {\n   var movies []*model.Movie\n\n   err := r.DB.Model(&movies).Select()\n   if err != nil {\n       return nil, err\n   }\n\n   return movies, nil\n}Now that we have successfully connected our Postgres database and set up our resolvers, we can test the project.Step 8: Test Your Golang GraphQL APIIn our final step, we will be testing our Golang GraphQL API\u2019s query and mutation.Start your Golang server go run ./server.go and head over to your browser, then navigate to http://localhost:8080 to access the in-built GraphQL playground.To test for Mutations, paste the GraphQL Query below into the playground editor to insert a new movie object.mutation createMovie {\n createMovie(\n   input: {\n     title: \"Rise of GraphQL Warrior Pt1\"\n     url: \"https://riseofgraphqlwarriorpt1.com/\"\n   }\n ){\n   id\n }\n}To test Queries, paste the GraphQL Query below into the playground editor to fetch all created movie records.query getMovies {\n movies {\n     title\n     url\n     releaseDate\n }\n}Get Instant GraphQL APIs on Your Databases with HasuraAs you can see, building a GraphQL API with Golang is a simple process, but it can be made even easier with Hasura. Setting up a GraphQL API could be as easy as clicking a button. Hasura gives you instant GraphQL APIs on new and existing data sources. Connect Hasura to your data source and get APIs in under a minute. With built-in support for caching, authentication, and role-based permission down to the column level, Hasura speeds up the time required to create and configure GraphQL APIs, allowing you to focus on the things that matter."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Best submissions of Hasura Tips and Tricks 2021 Description: Hasura\u2019s first \"tips and tricks\" contest has concluded, and we loved seeing all the amazing submissions! Without particular order, here's the final collection of the top community Tips and Tricks from 2021. Author: Jesse Martin"}, {"role": "assistant", "content": "Software development is now a trade, and as any tradesperson knows, the journey to mastery goes beyond the formal training to the discovery and sharing of knowledge. This collection of knowledge are the tips and tricks that ship products faster, reduce the number of bugs in production, and ultimately, make for a richer experience. Sharing this knowledge makes for a richer community of tradespeople.Hasura\u2019s first \"tips and tricks\" contest has concluded, and we loved seeing all the amazing submissions! Without particular order, here's the final collection of the top community Tips and Tricks from 2021.How to connect Hasura GraphQL real-time Subscription to a reactive Svelte frontend using RxJS and the new graphql-ws Web Socket protocol+libraryReal-time is the only time that matters in today's fast-paced digital ecosystem. While Hasura is here to send you the data as and when it becomes available, consuming this data on the front-end takes finesse. Raul Nohea writes up a compelling stack for solving exactly this problem with Svelte, RxJS, and the all-new graphql-ws library.Hasura Permissions ViewerUser Kazam released a great utility for viewing permissions in Hasura to get a nice overview of what data is accessible and to whom. This is a fantastic utility for diffing migrations in a CI/CD workflow. If you want to catch the video version of this tool, you can see the full presentation at the December 2021 Community Call.Hasura Change Summary - GitHub Action DemoSpeaking of CI/CD and community calls, Nick Amoscato (namoscato) shared their open-source Github action for diffing all Hasura metadata changes. It\u2019s integrations like this that make being an open-source company so rewarding. Hasura example on RailwaySpeaking of open-source, while we are partial to Hasura Cloud and all the benefits it adds to the Hasura stack, we\u2019re also big fans of the many amazing providers in the community that let you run Hasura on your own stack, whatever the reason may be. Add some open analytics into the stack and there\u2019s nothing not to like in this stack from commandodev.Load Balanced Hasura on EC2Continuing along the vein of self-hosted Hasura, when you put a load-balancer in front of an ec2 instance, you can ping the /healthz endpoint for a server health check. If self-hosting Hasura on AWS is important to you, check out our support for VPC Hasura installs. Shout out to user sachsom for this submission.Allow list scriptWe\u2019ll never bash our users, but we love it when users bash (script) Hasura. If you\u2019re looking for a fast-path to allow lists, this script has you covered. Automated API attack-surface restriction for the win. Thanks leica!SqrlyBesides objectively being the best-named project, Sqrly provides the tooling and opinionated methodology for working with custom SQL in a Hasura project. Custom SQL functions enable next-level power user functionality in Hasura, and this workflow is a must-see. Kudos jasonpjason, let us know when you set up the names-as-a-service company.Multiple Instances of HasuraIf you can\u2019t get enough of Hasura, which we completely understand, and want to run all the instances, then this guide is for you. Thanks for the submission Yasnil Dyap!Computed Fields in custom checksJulien Goux takes us deep into power-user mode with this one. Computed fields in custom checks is a very useful tip to add in your toolbelt for cases where the query is just too hairy or even impossible without it.created_by and updated_by from session variablesSometimes you just need to know who. David Wippel has this great, and easy to implement tip for adding customer audit paths to DB interactions. A real-world tip and trick that could go on any customer table.Hasura without DockerSometimes you need to avoid the whale-of-a-resource-drain that certain infrastructure tooling in the community can be. If you\u2019re looking for a stand-alone Hasura, for whatever reason, you can have your binary cake, without eating your memory, too. Thanks to the minimalist coder, aziez for closing out our list!Whether you are writing custom functions for access-controls, auto applying SQL statements from source control, automating security infrastructure or diffing metadata deployments \u2013 the wealth of knowledge from these tips and tricks provide massive benefits to the whole community.The winners have now been notified, and everyone should have received a message asking for the address to send some swag. If you haven\u2019t received that message, do let us know!Until next time, keep the tools sharp."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Dusty Domains Recap Description: Dusty Domains is a wrap! As a new initiative piloted by the fantastic team at Netlify, Dusty Domains was a call to action to turn an unused domain into a published website on Netlify. Author: Jesse Martin"}, {"role": "assistant", "content": "Dusty Domains is a wrap! As a new initiative piloted by the fantastic team at Netlify, Dusty Domains was a call to action to turn an unused domain into a published website on Netlify.In turn, Netlify and several other partners would provide matching contributions to a few select charities, and Hasura was grateful to be asked to participate!Whether the sites were the authoritative guide to using dash punctuation, putting together resources for accessible design, or creating a pet match-making service (yes, really), all put together the sites raised over $100k, spread across Code2040, Resilient Coders, STEM Tank, and Black Girls Code.We even got to use one of our new features in contributing to the Dusty Domains site! Using the new REST Connectors, we created a GraphQL API wrapper for the underlying data in AirTable, providing a real-time data feed for the funding bar graph, how neat is that?Real-time APIs with tooling like Hasura and Netlify Functions are just some of the ways the modern Jamstack is changing how fast websites are built, and deployed.You can read more about Hasura REST connectors for AirTable over at the Data Hub.Thanks again to the whole Netlify team for coming up with such a great idea and asking us to be a part of it. And a big shout out to the other sponsors who contributed to making this a great end-of-year for some well deserving organizations.Planet Scale\n\nHasura\n\nCockroachDB\n\nRemix\n\nSanity\n\nNew Relic\n\nDomainr logomark\n\nSingleStore\n\nMUX\n\nNetlify\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How a Serverless SaaS application eliminated a race condition with Hasura Triggers Description: In this article, we will look into how a serverless SaaS app used Hasura and how Event Triggers helped them to eliminate a race condition. Author: Hasura"}, {"role": "assistant", "content": "This is a guest post written by one of our users Siddharth Borderwala, who was part of the team that built and ran a serverless SaaS app called Turtlewig.It does not come by as a surprise that, while creating web applications including complex business logic, one may run into a race condition. One such condition occurred in our 100% server-less SaaS application Turtlewig. In this article, I will discuss how we used Hasura and how Event Triggers helped us to eliminate the race condition.What is Turtlewig?Turtlewig is a digital assets delivery platform for digital creators including indie creators and agencies. The creators can create and upload the assets to our cloud for hand-overs and receive payments (without commission) through our app. Our app flaunted a lot of features such as role-based access control and permissions, reviews on individual assets, version control, content watermarking, AI-powered label extraction, and colour palette extraction.What was the race condition?Let\u2019s now understand what the race condition in our app was. As I mentioned before, we had developed features like label extraction and colour palette extraction, these required each of the assets to be processed and their data be updated in the database once the processing was complete.The user would start with uploading an asset to our cloud, let\u2019s say it is an image. The client would generate a uuid4 for the asset and create a storage path using the project id and the asset id. The image would be uploaded to firebase storage while the user waits for the upload to complete. Once the upload is complete, the client would call a graphQL mutation and insert a new row in the assets table. This entry would also contain important metadata like the file-URL, colour palette, and labels.On completion of the upload, a firebase listener function would be triggered on completion of the upload, which would perform a very important part of the business logic. The listener was responsible for checking the size of the asset and updating its metadata inside the database entry for the particular asset. As the database entry would be created by the client, there is a possibility that due to network latency on the client\u2019s part, the entry isn\u2019t created and the firebase listener tries to update the non-existent entry. This would mean that we would fail to update the consumption quota of the workspace that the asset belongs to and also fail to update the size of the asset. Let\u2019s call it the sizing race condition.Following this, other functions for post-processing of the asset to generate related metadata were called from the client. Due to this, there was a possibility of the functions not being called at all due to some client error or network issues, making asset processing quite unreliable. We knew we had to think of a better way to perform everything.Eliminating the race conditionOne possible solution to the sizing race condition was to perform an upsert operation to update the metadata. An upsert operation means if there is a database row with the specified keys, update the data, else create an asset entry altogether. But there were obvious limitations of this method as we also needed the id of the user and the workspace for the uploaded asset. One way to overcome this was, to store the user id and workspace id on the storage object\u2019s metadata, but that would lead to redundancy as the same data would be present in the database and upon an update, the data would have to be changed in two different places.Now, let\u2019s talk about the solution that worked. It consists of 5 different parts, firebase storage, some client action, Hasura event triggers, a webhook, and an asset processing pipeline.Firebase Storage - It starts with the user uploading the file to our cloud, i.e. firebase storage.Client Action - Once the storage is complete, the user may be expected to choose some options related to version control. If and once the user completes their part, the client performs a graphQL mutation to create the database entries for each of the assets.Hasura Event Triggers - Once the database entries are created, Hasura would send a post request carrying the data in the whole row in JSON format to a webhook endpoint hosted in our firebase functions. It looked something like thisconst asset = {\n  id: \"uuid-of-the-asset\",\n  created_at: \"ISO string of time when asset was created\"\n  updated_at: \"ISO string of time when asset was last updated\",\n  size: null,\n  mimeType: \"mime-type\",\n  metadata: {\n    fileURL: \"firebase-storage-url-of-the-file\"\n  },\n  uploaded_by: \"id of the user who uploaded the asset\";\n\t// other fields\n};\nWebhook - This webhook would perform some checks to determine if the newly created asset is using up previously free space in the storage or not. If it is, then the size of the asset and consumption quota of the workspace is updated in the database.Asset processing pipeline - This pipeline is inside the body of the webhook itself. Having access to the mimeType of the asset, the pipeline would decide what processing tasks need to be carried out on the asset. Some examples would be AI-based label generation, colour palette extraction. Although, all assets had a common task, updating the elastic search index. All these tasks are then queued in google-cloud-tasks, with immediate but independent and parallel execution. These tasks would call firebase functions with payloads specified when they were queued by the webhook, and as the functions executed, they would update the generated data in the database (Hasura) using GraphQL mutations.This way, as and when the data is generated and made available, the client may fetch it from the database when the user refreshes the view, and display it to the user.Take a look at the picture below to understand the solution better -Handling edge casesThere was an edge case to this solution, what if the upload to firebase storage finishes and the client gets the data of the created files, but due to some network error, the client fails to perform successful graphQL mutations to make the database entries for the files. Due to our solution, there was no possibility of failure of the business logic and the asset processing pipeline, but there was an issue of loss of data for the users. To handle this, we used indexedDB to store the data of the uploaded files, and only clear it when the database entries are successfully done. The app checks if there are such files, and lets the user know. Upon the user\u2019s consent, it retries the mutations to create the database entries.The was not foolproof yet, there was another issue. What if the user does not consent to the entry of the assets in the databases, that would lead to dangling files in firebase storage. To take care of this, upon file upload completion, a task can be queued with an interval of 24 hours from the upload to check if the files are there in the database. When the queued task run and the database entries are not found, the files can be deleted from the storage bucket, hence saving storage usage on our side.Looking back on the approachThe approach we took to solve our specific problem was quite sophisticated and had different parts that depended on each other. The two most important components were the Hasura Event Triggers and Google Cloud Tasks, they helped make the solution event-driven and mitigated the issues of race conditions.I hope the article gave you a good idea of how some engineering challenges require you to think differently and re-architect your software using different tools, to be solved. Thanks to the amazing Event Triggers, we were able to make our solutions a reality and the app even better!About the authorSiddharth Borderwala is a Javascript Fullstack developer, specialising in the JAMStack ecosystem. He loves building apps, and experimenting with tools, languages and frameworks from time to time."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Exploring GraphQL Clients: Apollo Client vs Relay vs URQL Description: We\u2019ll be exploring Apollo Client, Relay, and URQL, as well as each of their pros and cons, to help you make the right choice when selecting a GraphQL client for your future projects. Author: Hasura"}, {"role": "assistant", "content": "So, you\u2019ve finally decided to build that GraphQL project. You\u2019ve chosen your programming language, database, and Hasura is powering your GraphQL APIs, but what GraphQL client do you use on the front end? One of the most common challenges front-end developers face when starting a new GraphQL project is choosing which GraphQL client to use. With more than 12 JavaScript client libraries listed on GraphQL.org, in addition to Fluent GraphQL clients that we have been exploring in Hasura, choosing the right GraphQL client for your unique needs might seem challenging.Exploring GraphQL Clients: Apollo Client vs Relay vs URQLApollo Client, Relay, and URQL are some of the most popular GraphQL client libraries based on GitHub stars and npm downloads. With these GraphQL clients, you get a fast, robust, scalable, and efficient way of querying and managing GraphQL APIs without having to worry about lower-level networking details or maintaining a local cache. Whether you are looking for the right GraphQL client for your next project or your current GraphQL client is not meeting your demands, one of these top three GraphQL clients can help you move in a better direction. Here, we\u2019ll be exploring Apollo Client, Relay, and URQL, as well as each of their pros and cons, to help you make the right choice when selecting a GraphQL client for your future projects.What Is Apollo Client?Apollo Client is a fully-featured, comprehensive state management GraphQL client that enables you to manage local and remote data with GraphQL. It was released in 2016 by the Meteor Development Group shortly after the GraphQL project was open-sourced with the ambition to build a GraphQL client library for every front-end web framework.Querying data with Apollo ClientLet\u2019s walk through a simple example of querying GraphQL data using Apollo Client in a React application.First, install the Apollo Client library from within your React application.npm install --save @apollo/client graphqlOnce you\u2019ve installed Apollo Client, you will need to import the following modules to make a basic query, as shown below.//index.js\n\nimport {\n ApolloClient,\n InMemoryCache,\n gql\n} from \"@apollo/client\";Next, we initialize an instance of Apollo Client and pass in the URI and cache parameters in the constructor.//index.js\n\nconst client = new ApolloClient({\n uri: 'https://api.spacex.land/graphql/',\n cache: new InMemoryCache()\n});The URI specifies the GraphQL endpoint you want to retrieve data from. (We are using SpaceX's public GraphQL API for this example.)The cache is an instance of InMemoryCache, which allows you to cache query results and configure Apollo Client's caching strategies.We can then specify our query data by using the client.query() method with the query string, as shown below.//index.js\n\nclient.query({\n query: gql`\n   {\n     missions {\n       name\n       website\n     }\n   }\n `,\n}).then(result => console.log(result));Run your code and inspect the network tab to check the result object. You should see a data property along with the mission's data.This example shows a basic way of querying data with Apollo Client. However, Apollo Client offers many other features especially geared toward React applications, such as direct integration with React Hooks.To explore Apollo Client further, including queries, mutations, and subscriptions, check out this Apollo Client tutorial on our learning platform. We build a to-do app using React, Apollo Client, and Hasura.Pros of Apollo ClientApollo Client doubles as a caching and state management library: Apollo Client ships with a built-in caching and state management feature to help you manage local state, irrespective of whether you have a GraphQL API to connect to. Additionally, you can cache the state you receive the Remote API in and combine it with the local state, saving network bandwidth and time when dealing with data.Apollo Client ships with many built-in features: You get features such as error handling, pagination, data prefetching, and integration with React Hooks built-in. Having these features can accelerate development time and boost productivity.Apollo Client supports many frameworks: Apollo Client supports React, Angular, Vue, Ember, iOS, and Android\u2014all of which can be advantageous if you use GraphQL in multiple projects as it saves you time and effort in switching between GraphQL projects.Apollo Client 3.0 features reactive variables: Reactive variables can store data of any type and can be accessed anywhere in the application. Reactive variables can be an advantageous mechanism for representing local states outside the Apollo Client cache.When does Apollo Client not fit in?Large bundle size: One of the biggest downsides to adopting Apollo is the bundle size. The smallest version of Apollo which makes it usable, comes in at 30.7KB, twice the size of other GraphQL clients like URQL. This large bundle size could lead to poor app performance in bandwidth-constrained applications.Lack of extensive support for other front-end frameworks aside from React: Apollo Client was built with React in mind and ships with many custom features for React applications. Often, developers have to rely on unofficial third-party tooling for other front-end frameworks, which can be buggy and bloated.What Is Relay?Relay is a highly opinionated GraphQL client for managing and fetching data from GraphQL APIs in React applications. It was created by Facebook and first released in 2015. Relay Modern was released in 2017 as a more straightforward, optimized, and extensible version of the original Relay.Querying data with RelayLet\u2019s walk through a simple example of querying GraphQL data using Relay in a React application. We first need to install the necessary packages. Relay is comprised of three main pieces:A compiler (which is used at build time)A core runtime (that is React-agnostic)A React integration layerInstall the three packages from npm using the following commands:npm install --save relay-runtime react-relay\nnpm install --save-dev relay-compiler graphql@^15.0.0 babel-plugin-relayreact-relay: This contains the main functionality of the Relay runtime and is responsible for caching and networking tasks.relay-compiler: The Relay Compiler validates and optimizes the GraphQL code you develop during build time.babel-plugin-relay: Relay uses a Babel plugin to convert the GraphQL code you write in a project into the format required by the Relay Compiler.Configure the Relay CompilerWe need to configure the Relay Compiler to link to the GraphQL server schema. To do this, we download the GraphQL schema from the GraphQL server and store it in a .graphql file. Luckily, we created a graphqurl tool that can help with this.From your terminal, run the following commands:npm install -g graphqurlgq <uri_endpoint> --introspect > schema.graphqlThe graphqurl tool downloads the GraphQL schema from our URI endpoint (https://api.spacex.land/graphql/) and stores it in a schema.graphql file.Now that we have the GraphQL schema, we can modify the package.json to run the compiler whenever we build or start our app:// <your-app-name>/package.json\n{\n ...\n \"scripts\": {\n   ...\n   \"start\": \"npm run relay && react-scripts start\",\n   \"build\": \"npm run relay && react-scripts build\",\n   \"relay\": \"npm run relay-compiler --schema schema.graphql --src ./src/ --watchman false $@\"\n   ...\n },\n ...\n}Configure the Relay runtime and environmentNow that our compiler has been configured, we can set up the runtime to tell Relay how to connect to our GraphQL server.Inside the src folder, create a RelayEnvironment.js file and paste the following code.// <your-app-name>/src/RelayEnvironment.js\n\nimport { Environment, Network, RecordSource, Store } from \"relay-runtime\";\n\nconst store = new Store(new RecordSource());\n\nconst network = Network.create((operation, variables) => {\n return fetch(\"https://api.spacex.land/graphql/\", {\n   method: \"POST\",\n   headers: {\n     Accept: \"application/json\",\n     \"Content-Type\": \"application/json\"\n   },\n   body: JSON.stringify({\n     query: operation.text,\n     variables\n   })\n }).then(response => {\n   return response.json();\n });\n});\n\nexport default new Environment({\n network,\n store\n});In the above code snippet, we import the required modules Environment, Network, RecordSource, and Store from Relay runtime. We then create an instance of Store, which will hold the cached data. The next thing we do is set up a network interface using the fetch client to call our GraphQL API. Finally, we export our Relay environment to use it in other modules.Now that our environment has been configured, let's render a query to get data. Open your App.js file and paste in the following code snippet.//App.js\n\nimport React from \"react\";\nimport { QueryRenderer, graphql } from \"react-relay\";\nimport environment from './RelayEnvironment';\n\nconst query = graphql`\n query AppRepositoryMissionQuery{\n       missions {\n               name\n               website\n           }\n       }\n`;\n\nexport default function Root() {\n return (\n   <QueryRenderer\n     environment={environment}\n     query={query}\n     render={({ error, props }) => {\n       if (error) {\n         return <p>{error.message}</p>;\n       } else if (props) {\n         return <p>Mission name is {props.name} with website {props.website}</p>;\n       }\n       return <p>Loading your Relay Mordern data...</p>;\n     }}\n   />\n );\n}Here we imported the RelayEnvironment we configured earlier, as well as the QueryRenderer and graphql from react-relay. We then defined the data we wanted to receive from our GraphQL server. Finally, we passed the environment andthe query to the QueryRenderer. The render function receives the result from the GraphQL server and passes it to the HTML elements to display. Start your application and inspect the networking tab to check the data that was sent back by the GraphQL server.As you might have already realized, setting up and configuring Relay was not as straightforward as Apollo Client, but on the bright side, you only have to do the initial setup once. To learn more about Relay features such as Fragments, you can refer to the official documentation.What works for RelayRelay is designed for high performance and automatically scales as your app grows: Relay is heavily focused on application performance. Its incremental compiler keeps iteration speed fast as the application grows and optimizes queries ahead of time to make the runtime as fast and efficient as possible.Relay applies and relies on GraphQL best practices: Relay ensures GraphQL APIs are built according to their predefined schema format. For example, Relay relies on globally unique IDs across the entire schema type to provide reliable caching and data prefetching. This enforcement of strong patterns ensures API consistency.Relay supports declarative data fetching: With Relay, you can declare your data requirements and let Relay figure out how and when to fetch your data. This feature reduces the time and complexity required to consume GraphQL APIs in React applications.Relay reduces the probability of failure in front-end applications: Relay supports type safety and automatic schema consistency and forces developers to follow a strict convention. These features ensure a clean architecture and minimize breaking changes that could cause application failure.What doesn't work for RelayRelay can be complex to set up and has a large bundle size: Because Relay is highly opinionated and conforms to a strict standard, the GraphQL API Relay connects to has to be built with a particular convention, which means less flexibility in GraphQL API design. Additionally, setting up Relay can be bulky and complex. It has a large bundle size, and you need to add a custom configuration when setting it up. This complexity, bulkiness, and lack of flexibility can be disadvantageous, especially when your team needs a quick, straightforward GraphQL client option.Relay can only be used with React and React Native applications: Relay only supports React and React Native applications. If your team uses other front-end languages aside from React, you will need a different GraphQL client to support it, leading to more time and effort learning about a new GraphQL client.What Is URQL?URQL, which stands for Universal React Query Library, is a lightweight, highly customizable, extensible GraphQL client that exposes a set of helpers for several frameworks, including React, Svelte, and Vue. It was released in 2018 by Formidable Labs as a lightweight, minimalist alternative to GraphQL clients such as Apollo and Relay.Querying data with URQLLet\u2019s walk through a simple example of querying GraphQL data using URQL in a React application.First, install the URQL library from within your React application.npm install --save urql graphqlOnce you\u2019ve installed URQL, you will need to import the following method to create a basic GraphQL client, as shown below.//index.js\n\nimport { createClient } from 'urql';\n\nconst client = createClient({\n url: 'https://api.spacex.land/graphql/',\n});Here, we pass in the GraphQL URL in the createClient constructor (we are still making use of the SpaceX public GraphQL API).We then specify our query data by passing the query string to the client.query() method, as shown below.//index.js\n\nconst query = `\n   {\n     missions {\n       name\n       website\n     }\n   }`;\n  \nconst results = await client.query(query).toPromise();\nconsole.log(result);Run your code and inspect the network tab to check the result object. You should see a data property along with the mission\u2019s data.As you can see, URQL is relatively easy to set up and integrate; as with the Apollo and Relay, there are still so many features URQL has to offer. You can refer to the URQL documentation to learn more.Pros of URQLURQL is lightweight and easy to set up: URQL ships with a bundle size of just 12KB, which is incredibly lightweight, especially compared to other GraphQL clients like Apollo. URQL is also the easiest to set up among the three GraphQL clients, as shown in the example above. These two features are advantageous when building lightweight, performant applications.URQL is flexible and extensible: URQL provides an extensibility API that allows app developers to customize the URQL GraphQL library to suit their needs. This extensibility and flexibility allow developers to build custom implementations on top of URQL and tweak them to suit their custom scenarios.Caching in URQL is fully customizable through its exchanges. URQL supports document caching for content-heavy pages and normalized caching for dynamic and data-heavy applications. This flexibility allows developers to easily set up caching on your client applications based on your specific requirements.URQL provides first-class support for functionality, such as offline mode and file uploads. URQL has built-in support for additional functionality, such as offline mode. This first-class support means you spend less time implementing features such as offline mode and file uploads in your client application.What can be better for URQL?A relatively new and small community when compared to other GraphQL clients: URQL was released in 2018 and currently has 157 contributors on GitHub. However, compared to Relay, which was released in 2015 and has 492 contributors, and Apollo, which was released in 2016 and has 644 contributors, it is noticeably smaller. URQL is relatively new, and its community is not as vibrant and mature as the Relay and Apollo communities.Lack of some built-in features: URQL lacks first-class support for features such as query batching, local state management, and pagination, which ship out of the box with Apollo and Relay. This means developers spend more time manually implementing these features in their applications.Apollo vs. Relay vs. URQL: A Quick OverviewThere are a couple of features all three clients support, such as caching, queries, mutations, and subscriptions. However, their implementations of these features differ. For example, both Apollo and Relay support normalized caching, while URQL supports document and normalized caching.The most fundamental difference between the three clients is in their core philosophy. Apollo is flexible and easygoing, Relay is opinionated and structured, and URQL is lightweight and extensible.This table provides a quick overview of the three GraphQL clients, along with some of their noteworthy features.ApolloRelayURQLSupported FrameworksAlmost all front-end frameworksReact and React NativeReact, Svelte, VueFlexibilityFlexibleFixed structureVery flexibleCachingNormalized cachingNormalized cachingNormalized and document cachingComplexityEasy to set upSlightly more complex conceptsEasy to get startedBundle Size32.9KB27.7KB (34.1KB with bindings)5.9KB (7.1KB with bindings)For a complete overview of all their core features, including framework bindings, caching, and state, refer to this article on the URQL page.Apollo vs. Relay vs. URQL: Which of These GraphQL Clients Is Right for Your Needs?Choosing the right GraphQL client will ultimately depend on your tech stack, the size and maturity of your engineering team, and your application's data requirements. Going through the features, advantages, and disadvantages of each GraphQL client and analyzing it for your specific use case is a good way to make a choice.Apollo is good for teams with multiple development environments looking for a fully-featured \u201call-rounder\u201d GraphQL client solution.Relay is suitable for large-scale applications with complex data requirements and numerous dependencies between different application parts, where manually managing these dependencies would be highly time-consuming and error-prone.URQL is a good option for bandwidth-constrained applications and teams that require a lightweight, flexible, and easy-to-use GraphQL client approach.Whichever solution you choose, Hasura can power your GraphQL APIs. Hasura's GraphQL engine gives you real-time, instant GraphQL APIs over your database. Hasura makes it easy to build your back-end GraphQL app or add a GraphQL layer to your existing application. Sign up for free today!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Operation Time Limit in Hasura Cloud Description: Hasura Cloud added a security feature where you can timeout a GraphQL operation if it is taking more than configured time. We will look into more detail with example in this post. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Hasura Cloud supports a slew of advanced security and performance features. One of the recent additions is the ability to time limit/timeout an operation if it takes more than n seconds.GraphQL Operation Time LimitWhy do you need an operation time limit or timeout?Operation timeouts are not new. They have been used in REST APIs to prevent large POST requests from being sent and abused. GraphQL is pretty flexible in that, it allows clients to query arbitrarily for large amount of data. Though this power is useful, it can be used with malicious intent. By making a large query, which hits the database, it can cause the underlying database to slow down, despite your frontend application not needing data from such a complex query and thus slowing down the experience of all users.An operation time limit basically ensures that each operation only gets a certain configured time to process the request successfully. Imagine a single GraphQL query hitting the database with many operations and statements internally being executed in a transaction. Multiply that with number of GraphQL requests being sent. The database will become the bottleneck due to this intent.Global Time LimitHasura Cloud supports adding a time limit for operation globally. This will be a base to which the server falls back in case the specific role doesn't have a time limit configured.Any upstream database queries are also cancelled for supported sources. Note that it currently supports only Postgres.API Limits Global TimeoutTime Limit per roleYou can also configure a timeout per role. Lets say your project has a role called user, you can configure timeout for that particular role.The metadata spec for enabling time limit look like this:api_limits:\n  time_limit:\n    per_role:\n      user: 10\n    global: 5Long running Database QueriesLong running queries may interfere on the overall database performance. You can find the ones which are long running by executing the following SQL statement on the Data->SQL tab of the Console.SELECT\n  pid,\n  now() - pg_stat_activity.query_start AS duration,\n  query,\n  state\nFROM pg_stat_activity\nWHERE (now() - pg_stat_activity.query_start) > interval '1 minute';The above statement shows the list of queries running for more than a minute. If the time limit was configured on Cloud, you can see that these queries will not go through Hasura Cloud and will error out with a time limit error.What happens on an error?The server responds with the following error object:{\n  \"errors\": [\n    {\n      \"extensions\": {\n        \"path\": \"$\",\n        \"code\": \"not-supported\"\n      },\n      \"message\": \"The operation exceeded the time limit allowed for this project\"\n    }\n  ]\n}The underlying query execution to postgres will also be cancelled. Things to noteThe operation timeout applies to websocket protocol as well, if you are making queries and mutations through that.The timeout also works with remote schemas and action requests.The operation time limit feature is quite useful in cases where the underlying database (postgres) is affected with a long running query, bringing down the experience for all users of the application."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Ultimate Guide to handling JWTs on frontend clients (GraphQL) Description: JWTs are becoming a popular way of handling auth. This post aims to demystify what a JWT is, discuss its pros/cons and cover best practices in implementing JWT on the client-side, keeping security in mind.  Author: Vladimir"}, {"role": "assistant", "content": "\nThe Ultimate Guide to handling JWTs on frontend clients (GraphQL)\nJWTs (JSON Web Token, pronounced 'jot') are becoming a popular way of handling auth.\nThis post aims to demystify what a JWT is, discuss its pros/cons and cover best practices in implementing JWT on the client-side, keeping security in mind.\nAlthough, we\u2019ve worked on the examples with a GraphQL clients, but the concepts apply to any frontend client.\n\nNote: This guide was originally published on September 9th, 2019. Last updated on January 4th, 2022.\n\nTable of Contents\n\nTable of Contents\nIntroduction: What is a JWT?\n\nSecurity Considerations\nJWT Structure\n\n\nBasics: Login\nBasics: Client setup\nBasics: Logout\nSilent refresh\nPersisting sessions\nForce logout, aka Logout of all sessions/devices\nServer side rendering (SSR)\n\nHow does the SSR server know if the user is logged in?\n\n\nCode from this blogpost (finished application)\nTry it out!\nReferences\nSummary\nChangelog\n\nIntroduction: What is a JWT?\nFor a detailed, technical description of JWTs refer to this article.\nFor the purposes of auth, a JWT is a token that is issued by the server. The token has a JSON payload that contains information specific to the user. This token can be used by clients when talking to APIs (by sending it along as an HTTP header) so that the APIs can identify the user represented by the token, and take user specific action.\nSecurity Considerations\nBut can\u2019t a client just create a random JSON payload an impersonate a user?\nGood question! That\u2019s why a JWT also contains a signature. This signature is created by the server that issued the token (let\u2019s say your login endpoint) and any other server that receives this token can independently verify the signature to ensure that the JSON payload was not tampered with, and has information that was issued by a legitimate source.\nBut if I have a valid and signed JWT and someone steals it from the client, can\u2019t they use my JWT forever?\nYes! If a JWT is stolen, then the thief can can keep using the JWT. An API that accepts JWTs does an independent verification without depending on the JWT source so the API server has no way of knowing if this was a stolen token! This is why JWTs have an expiry value. And these values are kept short. Common practice is to keep it around 15 minutes, so that any leaked JWTs will cease to be valid fairly quickly. But also, make sure that JWTs don\u2019t get leaked.\n\nThese 2 facts result in almost all the peculiarities about handling JWTs! The fact that JWTs shouldn\u2019t get stolen and that they need to have short expiry times in case they do get stolen.\n\nThat\u2019s why it\u2019s also really important not to store the JWT on the client persistently. Doing so you make your app vulnerable to CSRF & XSS attacks, by malicious forms or scripts to use or steal your token lying around in cookies or localStorage.\nJWT Structure\nSo does a JWT have a specific kind of structure? What does it look like?\nA JWT looks something like this, when it's serialized:\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.XbPfbIHMI6arZ3Y922BhjWgQzWXcXNrz0ogtVhfEd2o\nIf you decode that base64, you'll get JSON in 3 important parts:  header, payload and signature.\n\nThe 3 parts of a JWT (based on image taken from jwt.io)\nThe serialized form is in the following format:\n[ base64UrlEncode(header) ] . [ base64UrlEncode(payload) ] . [ signature ]\n\nA JWT is not encrypted. It is based64 encoded and signed. So anyone can decode the token and use its data. A JWT's signature is used to verify that it is in fact from a legitimate source.\n\nHere is the diagram of how a JWT is issued (/login) and then used to make an API call to another service (/api) in a nutshell:\n\nA workflow of how a JWT is issued and then used\nUgh! This seems complicated. Why shouldn\u2019t I stick to good old session tokens?\nThis is a painful discussion on the Internet. Our short (and opinionated answer) is that backend developers like using JWTs because a) microservices b) not needing a centralized token database.\nIn a microservices setup, each microservice can independently verify that a token received from a client is valid. The microservice can further decode the token and extract relevant information without needing to have access to a centralized token database.\nThis is why API developers like JWTs, and we (on the client-side) need to figure out how to use it. However, if you can get away with a session token issued by your favourite monolithic framework, you\u2019re totally good to go and probably don\u2019t need JWTs!\n\nBasics: Login\nNow that we have a basic understanding what a JWT is, let's create a simple login flow and extract the JWT. This is what we want to achieve:\n\nSo how do we start?\nThe login process doesn\u2019t really change from what you'd usually do. For example, here\u2019s a login form that submits a username/password to an auth endpoint and grabs the JWT token from the response. This could be login with an external provider, an OAuth or OAuth2 step. It really doesn't matter, as long as the client finally gets a JWT token in the response of the final login success step.\nFirst, we'll build a simple login form to send the username and password to our login server. The server will issue JWT token and we will store it in memory. In this tutorial we won\u2019t focus on auth server backend, but you're welcome to check it out in example repo for this blogpost.\nThis is what the handleSubmit handler for a login button might look like:\nasync function handleSubmit () {\n  //...\n  // Make the login API call\n  const response = await fetch(`/auth/login`, {\n    method: 'POST',\n    body: JSON.stringify({ username, password })\n  })\n  //...\n  // Extract the JWT from the response\n  const { jwt_token } = await response.json()\n  //...\n  // Do something the token in the login method\n  await login({ jwt_token })\n}\n\nThe login API returns a token and then we pass this token to a login function from /utils/auth where we can decide what to do with the token once we have it.\nimport { login } from '../utils/auth'\nawait login({ jwt_token })\n\nSo we\u2019ve got the token, now where do we store this token?\nWe need to save our JWT token somewhere, so that we can forward it to our API as a header. You might be tempted to persist it in localstorage; don\u2019t do it! This is prone to XSS attacks.\nWhat about saving it in a cookie?\nCreating cookies on the client to save the JWT will also be prone to XSS. If it can be read on the client from Javascript outside of your app - it can be stolen. You might think an HttpOnly cookie (created by the server instead of the client) will help, but cookies are vulnerable to CSRF attacks. It is important to note that HttpOnly and sensible CORS policies cannot prevent CSRF form-submit attacks and using cookies require a proper CSRF mitigation strategy.\nNote that a SameSite cookie will make Cookie based approaches safe from CSRF attacks. It might not be a solution if your Auth and API servers are hosted on different domains, but it should work really well otherwise!\nWhere do we save it then?\nThe OWASP JWT Cheatsheet and OWASP ASVS (Application Security Verification Standard) prescribe guidelines for handling and storing tokens.\nThe sections that are relevant to this are the Token Storage on Client Side and Token Sidejacking issues in the JWT Cheatsheet, and chapters 3 (Session Management) and 8 (Data Protection) of ASVS.\nFrom the Cheatsheet, Issue: Token Storage on the Client Side:\n\"This occurs when an application stores the token in a manner exhibiting the following behavior:\"\n\nAutomatically sent by the browser (Cookie storage).\nRetrieved even if the browser is restarted (Use of browser localStorage container).\nRetrieved in case of XSS issue (Cookie accessible to JavaScript code or Token stored in browser local/session storage).\n\n\"How to Prevent:\"\n\nStore the token using the browser sessionStorage container.\nAdd it as a Bearer HTTP Authentication header with JavaScript when calling services.\nAdd fingerprint information to the token.\n\nBy storing the token in browser sessionStorage container it exposes the token to being stolen through a XSS attack. However, fingerprints added to the token prevent reuse of the stolen token by the attacker on their machine. To close a maximum of exploitation surfaces for an attacker, add a browser Content Security Policy to harden the execution context.\nWhere a fingerprint is the implementation of the following guidelines from the Token Sidejacking issue:\n\"Symptom:\"\nThis attack occurs when a token has been intercepted/stolen by an attacker and they use it to gain access to the system using targeted user identity.\nHow to Prevent:\nA way to prevent it is to add a \"user context\" in the token. A user context will be composed of the following information:\n\nA random string that will be generated during the authentication phase. It will be sent to the client as an hardened cookie (flags: HttpOnly + Secure + SameSite + cookie prefixes).\nA SHA256 hash of the random string will be stored in the token (instead of the raw value) in order to prevent any XSS issues allowing the attacker to read the random string value and setting the expected cookie.\n\nIP addresses should not be used because there are some legitimate situations in which the IP address can change during the same session. For example, when an user accesses an application through their mobile device and the mobile operator changes during the exchange, then the IP address may (often) change. Moreover, using the IP address can potentially cause issues with European GDPR compliance.\nDuring token validation, if the received token does not contain the right context (for example, if it has been replayed), then it must be rejected.\n\nAn implementation of this on the client side may look like:\n// Short duration JWT token (5-10 min)\nexport function getJwtToken() {\n    return sessionStorage.getItem(\"jwt\")\n}\n\nexport function setJwtToken(token) {\n    sessionStorage.setItem(\"jwt\", token)\n}\n\n// Longer duration refresh token (30-60 min)\nexport function getRefreshToken() {\n    return sessionStorage.getItem(\"refreshToken\")\n}\n\nexport function setRefreshToken(token) {\n    sessionStorage.setItem(\"refreshToken\", token)\n}\n\nfunction handleLogin({ email, password }) {\n  // Call login method in API\n  // The server handler is responsible for setting user fingerprint cookie during this as well\n  const { jwtToken, refreshToken } = await login({ email, password })\n  setJwtToken(jwtToken)\n  setRefreshToken(refreshToken)\n\n  // If you like, you may redirect the user now\n  Router.push(\"/some-url\")\n}\n\nYes, the token will be nullified when the user switches between tabs, but we will deal with that later.\nOk! Now that we have the token what can we do with it?\n\nUsing in our API client to pass it as a header to every API call\nCheck if a user is logged in by seeing if the JWT variable is set.\nOptionally, we can even decode the JWT on the client to access data in the payload. Let's say we need the user-id or the username on the client, which we can extract from the JWT.\n\nHow do we check if our user is logged in?\nWe check in our if the token variable is set and if it isn't - redirect to login page.\nconst jwtToken = getJwtToken();\nif (!jwtToken) {\n  Router.push('/login')\n}\n\n\nBasics: Client setup\nNow it\u2019s time to set up our GraphQL client. The idea is to get the token from the variable we set, and if it\u2019s there, we pass it to our GraphQL client.\n\nUsing the JWT in a GraphQL client\nAssuming your GraphQL API accepts a JWT auth token as an Authorization header, all you need to do is setup your client to set an HTTP header by using the JWT token from the variable.\nHere's what a setup with the Apollo GraphQL client using an ApolloLink middleware.\nimport { useMemo } from \"react\"\nimport { ApolloClient, InMemoryCache, HttpLink, ApolloLink, Operation } from \"@apollo/client\"\nimport { getMainDefinition } from \"@apollo/client/utilities\"\nimport { WebSocketLink } from \"@apollo/client/link/ws\"\nimport merge from \"deepmerge\"\n\nlet apolloClient\n\nfunction getHeaders() {\n    const headers = {} as HeadersInit\n    const token = getJwtToken()\n    if (token) headers[\"Authorization\"] = `Bearer ${token}`\n    return headers\n}\n\nfunction operationIsSubscription(operation: Operation): boolean {\n    const definition = getMainDefinition(operation.query)\n    const isSubscription = definition.kind === \"OperationDefinition\" && definition.operation === \"subscription\"\n    return isSubscription\n}\n\nlet wsLink\nfunction getOrCreateWebsocketLink() {\n    wsLink ??= new WebSocketLink({\n        uri: process.env[\"NEXT_PUBLIC_HASURA_ENDPOINT\"].replace(\"http\", \"ws\").replace(\"https\", \"wss\"),\n        options: {\n            reconnect: true,\n            timeout: 30000,\n            connectionParams: () => {\n                return { headers: getHeaders() }\n            },\n        },\n    })\n    return wsLink\n}\n\nfunction createLink() {\n    const httpLink = new HttpLink({\n        uri: process.env[\"NEXT_PUBLIC_HASURA_ENDPOINT\"],\n        credentials: \"include\",\n    })\n\n    const authLink = new ApolloLink((operation, forward) => {\n        operation.setContext(({ headers = {} }) => ({\n            headers: {\n                ...headers,\n                ...getHeaders(),\n            },\n        }))\n        return forward(operation)\n    })\n\n    if (typeof window !== \"undefined\") {\n        return ApolloLink.from([\n            authLink,\n            // Use \"getOrCreateWebsocketLink\" to init WS lazily\n            // otherwise WS connection will be created + used even if using \"query\"\n            ApolloLink.split(operationIsSubscription, getOrCreateWebsocketLink, httpLink),\n        ])\n    } else {\n        return ApolloLink.from([authLink, httpLink])\n    }\n}\n\nfunction createApolloClient() {\n    return new ApolloClient({\n        ssrMode: typeof window === \"undefined\",\n        link: createLink(),\n        cache: new InMemoryCache(),\n    })\n}\n\nexport function initializeApollo(initialState = null) {\n    const _apolloClient = apolloClient ?? createApolloClient()\n\n    // If your page has Next.js data fetching methods that use Apollo Client, the initial state\n    // get hydrated here\n    if (initialState) {\n        // Get existing cache, loaded during client side data fetching\n        const existingCache = _apolloClient.extract()\n\n        // Merge the existing cache into data passed from getStaticProps/getServerSideProps\n        const data = merge(initialState, existingCache)\n\n        // Restore the cache with the merged data\n        _apolloClient.cache.restore(data)\n    }\n\n    // For SSG and SSR always create a new Apollo Client\n    if (typeof window === \"undefined\") return _apolloClient\n    // Create the Apollo Client once in the client\n    if (!apolloClient) apolloClient = _apolloClient\n\n    return _apolloClient\n}\n\nexport function useApollo(initialState) {\n    const store = useMemo(() => initializeApollo(initialState), [initialState])\n    return store\n}\n\nAs you can see from the code, whenever there is a token, it\u2019s passed as a header to every request.\nBut what will happen if there is no token?\nIt depends on the flow in your application. Let's say you redirect the user back to the login page:\nelse {\n Router.push('/login')\n}\n\nWhat happens if a token expires as we're using it?\nLet's say our token is only valid for 15 minutes. In this case we'll probably get an  error from our API denying our request (let's say a 401: Unauthorized error). Remember that every service that knows how to use a JWT can independently verify it and check whether it has expired or not.\nLet\u2019s add error handling to our app to handle this case. We'll write code that will run for every API response and check the error. When we receive the token expired/invalid error from our API, we trigger the logout or the redirect to login workflow.\nHere's what the code looks like if we're using the Apollo client:\nimport { onError } from 'apollo-link-error';\n\nconst logoutLink = onError(({ networkError }) => {\n    if (networkError.statusCode === 401) logout();\n})\n\nif (typeof window !== \"undefined\") {\n    return ApolloLink.from([\n        logoutLink,\n        authLink,\n        ApolloLink.split(operationIsSubscription, getOrCreateWebsocketLink, httpLink),\n    ])\n} else {\n    return ApolloLink.from([\n      logoutLink,\n      authLink,\n      httpLink\n    ])\n}\n\nYou may notice that this will result in a fairly sucky user experience. The user will keep getting asked to re-authenticate every time the token expires. This is why apps implement a silent refresh workflow that keeps refreshing the JWT token in the background. More on this in the next sections below!\nBasics: Logout\nWith JWTs, a \"logout\" is simply deleting the token on the client side so that it can't be used for subsequent API calls.\n\nSo...is there no /logout API call at all?\nA logout endpoint is not really required, because any microservice that accepts your JWTs will keep accepting it. If your auth server deletes the JWT, it won't matter because the other services will keep accepting it anyway (since the whole point of JWTs was to not require centralised coordination).\nThe token is still valid and can be used. What if I need to ensure that the token cannot be used ever again?\nThis is why keeping JWT expiry values to a small value is important. And this is why ensuring that your JWTs don't get stolen is even more important. The token is valid (even after you delete it on the client), but only for short period to reduce the probability of it being used maliciously.\nIn addition, you can add a deny-listing workflow to your JWTs. In this case, you can have a /logout API call and your auth server puts the tokens in a \"invalid list\". However, all the API services that consume the JWT now need to add an additional step to their JWT verification to check with the centralized \"deny-list\". This introduces central state again, and brings us back to what we had before using JWTs at all.\n\nDoesn\u2019t deny-listing negate the benefit of JWT not needing any central storage?\nIn a way it does. It\u2019s an optional precaution that you can take if you are worried that your token can get stolen and misused, but it also increases the amount of verification that has to be done. As you can imagine, this had led to much gnashing of teeth on the internet.\nWhat will happen if I am logged in on different tabs?\nOne way of solving this is by introducing a global event listener on localstorage. Whenever we update this logout key in localstorage on one tab, the listener will fire on the other tabs and trigger a \"logout\" too and redirect users to the login screen.\nwindow.addEventListener('storage', this.syncLogout) \n\n//....\n\nsyncLogout (event) {\n  if (event.key === 'logout') {\n    console.log('logged out from storage!')\n    Router.push('/login')\n  }\n}\n\nThese are the 2 things we now need to do on logout:\n\nNullify the token\nSet logout item in local storage\n\nimport { useEffect } from \"react\"\nimport { useRouter } from \"next/router\"\nimport { gql, useMutation, useApolloClient } from \"@apollo/client\"\nimport { setJwtToken, setRefreshToken } from \"../lib/auth\"\n\nconst SignOutMutation = gql`\n    mutation SignOutMutation {\n        signout {\n            ok\n        }\n    }\n`\n\nfunction SignOut() {\n    const client = useApolloClient()\n    const router = useRouter()\n    const [signOut] = useMutation(SignOutMutation)\n\n    useEffect(() => {\n        // Clear the JWT and refresh token so that Apollo doesn't try to use them\n        setJwtToken(\"\")\n        setRefreshToken(\"\")\n        // Tell Apollo to reset the store\n        // Finally, redirect the user to the home page\n        signOut().then(() => {\n            // to support logging out from all windows\n            window.localStorage.setItem('logout', Date.now())\n            client.resetStore().then(() => {\n                router.push(\"/signin\")\n            })\n        })\n    }, [signOut, router, client])\n\n    return <p>Signing out...</p>\n}\n\nIn that case whenever you log out from one tab, event listener will fire in all other tabs and redirect them to login screen.\nThis works across tabs. But how do I \"force logout\" of all sessions on different devices?!\nWe cover this topic in a little more detail in a section later on: Force logout.\nSilent refresh\nThere are 2 major problems that users of our JWT based app will still face:\n\nGiven our short expiry times on the JWTs, the user will be logged out every 15 minutes. This would be a fairly terrible experience. Ideally, we'd probably want our user to be logged in for a long time.\nIf a user closes their app and opens it again, they'll need to login again. Their session is not persisted because we're not saving the JWT token on the client anywhere.\n\nTo solve this problem, most JWT providers, provide a refresh token. A refresh token has 2 properties:\n\nIt can be used to make an API call (say, /refresh_token) to fetch a new JWT token before the previous JWT expires.\nIt can be safely persisted across sessions on the client!\n\nHow does a refresh token work?\nThis token is issued as part of authentication process along with the JWT. The auth server should saves this refresh token and associates it to a particular user in its own database, so that it can handle the renewing JWT logic.\nOn the client, before the previous JWT token expires, we wire up our app to make a /refresh_token endpoint and grab a new JWT.\nHow is a refresh token safely persisted on the client?\nWe follow the guidelines in the OWASP JWT Guide to prevent issues with client-side storage of a token.\nImproper client-side storage occurs when \"an application stores the token in a manner exhibiting the following behavior\":\n\nAutomatically sent by the browser (Cookie storage).\nRetrieved even if the browser is restarted (Use of browser localStorage container).\nRetrieved in case of XSS issue (Cookie accessible to JavaScript code or Token stored in browser local/session storage).\n\nTo prevent this, the following steps are taken:\n\nStore the token using the browser sessionStorage container.\nAdd it as a Bearer HTTP Authentication header with JavaScript when calling services.\nAdd fingerprint information to the token.\n\nBy storing the token in browser sessionStorage container it exposes the token to being stolen through a XSS attack. However, fingerprints added to the token prevent reuse of the stolen token by the attacker on their machine. To close a maximum of exploitation surfaces for an attacker, add a browser Content Security Policy to harden the execution context.\nWhere the implementation of a fingerprint also serves to prevent Token Sidejacking from occuring, and is done according to the guidelines here\nSo what does the new \"login\" process look like?\nNothing much changes, except that a refresh token gets sent along with the JWT. Let's take a look a diagram of login process again, but now with refresh_token functionality:\n\n\nThe user logs in with a login API call.\nServer generates JWT token and refresh_token, and a fingerprint\nThe server returns the JWT token, refresh token, and a SHA256-hashed version of the fingerprint in the token claims\nThe un-hashed version of the generated fingerprint is stored as a hardened, HttpOnly cookie on the client\nWhen the JWT token expires, a silent refresh will happen. This is where the client calls the /refresh token endpoint\n\nAnd now, what does the silent refresh look like?\n\nHere's what happens:\n\nThe refresh endpoint must check for the existence of the fingerprint cookie, and validate that the comparison of the hashed value in the token claims is identical to the unhashed value in the cookie\nIf either of these conditions are not met, the refresh request is rejected\nOtherwise the refresh token is accepted, and a fresh JWT access token is granted, resetting the silent refresh process\n\nAn implementation of this workflow using the apollo-link-token-refresh package, is something like the below.\nUsing this as a non-terminating link will automatically check the validity of our JWT, and attempt a silent refresh if needed when any operation is run.\nimport { TokenRefreshLink } from \"apollo-link-token-refresh\"\nimport { JwtPayload } from \"jwt-decode\"\nimport { getJwtToken, getRefreshToken, setJwtToken } from \"./auth\"\nimport decodeJWT from \"jwt-decode\"\n\nexport function makeTokenRefreshLink() {\n    return new TokenRefreshLink({\n        // Indicates the current state of access token expiration\n        // If token not yet expired or user doesn't have a token (guest) true should be returned\n        isTokenValidOrUndefined: () => {\n            const token = getJwtToken()\n\n            // If there is no token, the user is not logged in\n            // We return true here, because there is no need to refresh the token\n            if (!token) return true\n\n            // Otherwise, we check if the token is expired\n            const claims: JwtPayload = decodeJWT(token)\n            const expirationTimeInSeconds = claims.exp * 1000\n            const now = new Date()\n            const isValid = expirationTimeInSeconds >= now.getTime()\n\n            // Return true if the token is still valid, otherwise false and trigger a token refresh\n            return isValid\n        },\n        // Responsible for fetching refresh token\n        fetchAccessToken: async () => {\n            const jwt = decodeJWT(getJwtToken())\n            const refreshToken = getRefreshToken()\n            const fingerprintHash = jwt?.[\"https://hasura.io/jwt/claims\"]?.[\"X-User-Fingerprint\"]\n\n            const request = await fetch(process.env[\"NEXT_PUBLIC_HASURA_ENDPOINT\"], {\n                method: \"POST\",\n                headers: {\n                    \"Content-Type\": \"application/json\",\n                },\n                body: JSON.stringify({\n                    query: `\n                  query RefreshJwtToken($refreshToken: String!, $fingerprintHash: String!) {\n                    refreshJwtToken(refreshToken: $refreshToken, fingerprintHash: $fingerprintHash) {\n                      jwt\n                    }\n                  }\n                `,\n                    variables: {\n                        refreshToken,\n                        fingerprintHash,\n                    },\n                }),\n            })\n\n            return request.json()\n        },\n        // Callback which receives a fresh token from Response.\n        // From here we can save token to the storage\n        handleFetch: (accessToken) => {\n            setJwtToken(accessToken)\n        },\n        handleResponse: (operation, accessTokenField) => (response) => {\n            // here you can parse response, handle errors, prepare returned token to\n            // further operations\n            // returned object should be like this:\n            // {\n            //    access_token: 'token string here'\n            // }\n            return { access_token: response.refreshToken.jwt }\n        },\n        handleError: (err) => {\n            console.warn(\"Your refresh token is invalid. Try to reauthenticate.\")\n            console.error(err)\n            // Remove invalid tokens\n            localStorage.removeItem(\"jwt\")\n            localStorage.removeItem(\"refreshToken\")\n        },\n    })\n}\n\nReferring back to the section addressing: \"What will happen if I'm logged in on multiple tabs?\", using sessionStorage for this means we won't be authenticated in new tabs (if they weren't created using \"Duplicate tab\") or windows.\nA potential solution to this, while still remaining secure, is to use localStorage as an event-emitter again and sync sessionStorage between tabs of the same base URL on load.\nThis can be accomplished by using a script such as this on your pages:\nif (!sessionStorage.length) {\n    // Ask other tabs for session storage\n    localStorage.setItem(\"getSessionStorage\", String(Date.now()))\n}\n\nwindow.addEventListener(\"storage\", (event) => {\n    if (event.key == \"getSessionStorage\") {\n        // Some tab asked for the sessionStorage -> send it\n        localStorage.setItem(\"sessionStorage\", JSON.stringify(sessionStorage))\n        localStorage.removeItem(\"sessionStorage\")\n    } else if (event.key == \"sessionStorage\" && !sessionStorage.length) {\n        // sessionStorage is empty -> fill it\n        const data = JSON.parse(event.newValue)\n        for (let key in data) {\n            sessionStorage.setItem(key, data[key])\n        }\n    }\n})\n\n\nPersisting sessions\nPersisting sessions is against the OWASP security guidelines for clients and token authentication:\n\"... Retrieved even if the browser is restarted (Use of browser localStorage container).\"\nThere is (at the time of writing) no way deemed acceptable that allows for a persistent user session after a browser has been fully closed and re-opened, unless the browser implementation retains tab session state (sessionStorage).\nYou may choose to store your token in localStorage or a Cookie instead, in order to have persistent sessions across browser restarts, but doing so is at your discretion.\n\nNote: For an ongoing discussion of this topic, see https://github.com/OWASP/ASVS/issues/1141\n\n\nForce logout, aka Logout of all sessions/devices\nNow that are users are logged in forever and stay logged in across sessions, there's a new problem that we need to worry about: Force logout or, logging out of all sessions and devices.\nThe refresh token implementations from the sections above, show us that we can persist sessions and stay logged in.\nIn this case, a simple implementation of \"force logout\" is asking the auth server to invalidate all refresh tokens associated for a particular user.\nThis is primarily an implementation on the auth server backend, and doesn't need any special handling on the client. Apart from a \"Force Logout\" button on your app perhaps :)\n\nServer side rendering (SSR)\nIn server side rendering there are additional complexities involved when dealing with JWT tokens.\nThis is what we want:\n\nThe browser makes a request to a app URL\nThe SSR server renders the page based on the user's identity\nThe user gets the rendered page and then continues using the app as an SPA (single page app)\n\nHow does the SSR server know if the user is logged in?\nThe browser needs to send some information about the current user's identity to the SSR server. The only way to do this is via a cookie.\nSince we've already implemented refresh token workflows via cookies, when we make a request to the SSR server, we need to make sure that the refresh-token is also sent along.\n\nNote: For SSR on authenticated pages, it is vital that that the domain of the auth API (and hence the domain of the refresh_token cookie) is the same as the domain of the SSR server. Otherwise, our cookies won't be sent to the SSR server!\n\n\nThis is what the SSR server does:\n\nUpon receiving a request to render a particular page, the SSR server captures the refresh_token cookie.\nThe SSR server uses the refresh_token cookie to get a new JWT for the user\nThe SSR server uses the new JWT token and makes all the authenticated GraphQL requests to fetch the right data\n\nCan the user continue making authenticated API requests once the SSR page has loaded?\nNope, not without some additional fiddling around unfortunately!\nOnce the SSR server returns the rendered HTML, the only identification left on the browser about the user's identity is the old refresh token cookie that has already been used by the SSR server!\nIf our app code tries to use this refresh token cookie to fetch a new JWT, this request will fail and the user will get logged out.\nTo solve this, the SSR server after rendering the page needs to send the latest refresh token cookie, so that the browser can use it!\nThe entire SSR flow, end to end:\n\n\nCode from this blogpost (finished application)\nSample code for this blogpost with an end to end working app, with SSR capabilities is available here.\nhttps://github.com/hasura/jwt-guide\nThe repository also contains the sample auth backend code.\n\nTry it out!\nSet up a free GraphQL backend with Hasura Cloud to try it out for yourself!\nMake sure you're on version 1.3 or above and you're good to go.\n\nReferences\n\nJWT.io\nOWASP notes on XSS, CSRF and similar things\n\nOWASP JWT Cheatsheet\nOWASP Application Security Verification Standard, v5\n\n\nThe Parts of JWT Security Nobody Talks About | Philippe De Ryck\nLots of other awesome stuff on the web\n\n\nSummary\nOnce you've worked through all the sections above, your app should now have all the capabilities of a modern app, using a JWT and should be secure from the common major security gotchas that JWT implementations have!\nLet us know on twitter or in the comments below if you have any questions, suggestions or feedback!\nChangelog\n\n(12/28/2021) Recommendation to store token in Cookie changed to sessionStorage, per OWASP JWT guidelines to address Issue: Token Storage on Client Side 0\n(12/28/2021) Adopted OWASP Application Security Verification Standard  v5 6 L1-L2 guidelines\n\nOf note: Chapters 3 (Session Management) and 8 (Data Protection)\n\n\n(12/28/2021) Alter section on Persisting Sessions to contain OWASP guidelines on this\n(12/28/2021) Sample application repo code updated 1\n\nUpdate from Next.js 9 -> 12, update @apollo libraries to v3.x\nPassword hashing algorithm changed from bcrypt to native Node.js crypto.scrypt per OWASP guidelines 2 and to reduce number of external dependencies\nAuthentication on frontend and backend modified to make use of a user fingerprint in addition to a token, per OWASP guidelines on preventing Token Sidejacking 3\nExample usage of TokenRefreshLink 4 to manage silent refresh workflow added\nServer endpoints integrated through Hasura Actions 5 rather than directly invoking from client\nAdopt recommended use of crypto.timingSafeEqual() to prevent timing attacks\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Getting Started with Hot Chocolate .NET GraphQL Server Description: We\u2019ll be building a GraphQL API backend with Hot Chocolate GraphQL and integrating it to Hasura via Remote Schema. Author: Hasura"}, {"role": "assistant", "content": "Building a GraphQL API server could be as easy as riding a bike down the street or as complex as launching a rocket ship to Mars, depending on your approach and tools.Hot Chocolate is an open-source GraphQL server built for the Microsoft .NET platform. It removes the complexity of building GraphQL APIs from scratch with built-in features for queries, mutations, and subscriptions. Hasura's Remote Schema feature allows merging remote schemas from multiple remote GraphQL servers to provide a unified GraphQL API layer. With tools like Hot Chocolate and Hasura via Remote Schemas, developers get a fast and straightforward option for building robust, powerful, and extensible GraphQL APIs to cater to custom scenarios such as querying data from multiple data sources.To demonstrate how these tools can build a robust and extensible GraphQL API to cater to your business needs, we\u2019ll be building a GraphQL API backend with Hot Chocolate GraphQL and integrating it to Hasura via Remote Schema.Set Up a Few PrerequisitesTo get started, install the following prerequisites:.NET Core SDKAny code editor of your choice (we recommend Visual Studio Code) Step 1: Create a New .NET Core Web API ProjectCreate a new folder in your preferred directory and change the current directory to that folder. In the working folder, open your terminal and run the following command to create a new web API project.dotnet new webapi --no-httpsThis command creates a new web API project in your current directory with some boilerplate code and disables https for simplicity. Open the project directory with your code editor and review the files.Step 2: Install Hot Chocolate GraphQL in the .NET Core ProjectFrom within your project directory, open your terminal and install the following dependencies from Nuget: HotChocolate.AspNetCore and HotChocolate.Data.dotnet add package HotChocolate.AspNetCore\ndotnet add package HotChocolate.DataStep 3: Create Your GraphQL SchemaEvery GraphQL server uses a GraphQL schema to specify the structure of data clients can query. In this example, we'll create a GraphQL schema for querying a collection of movie titles and actors.Create a folder called Models at the root of your project and create the Actor.cs file inside the folder with the following code snippet//Actor.cs\n\nnamespace Models\n{\n   public class Actor\n   {\n       public string FirstName { get; set; }\n       public string LastName { get; set; }\n   }\n}Create another C# file called Movie.cs inside the Models folder and add the following code snippet//Movie.cs\n\nnamespace Models\n{\n    public class Movie\n    {\n        public int Id { get; set; }\n        public string Title { get; set; }\n        public List<Actor> Actors { get; set; }\n    }\n}These two classes give us a simple model to build our GraphQL schema.Step 4: Create Your DatasetHot Chocolate is not bound to a specific database and can fetch data from any source you connect to (including a database or a REST API). For this article, we'll hardcode some sample data.Create a folder called Data at the root of your project, create the SeedData.cs file inside the folder, and add the following code snippet//SeedData.cs\n\nusing Models;\n\nnamespace Data\n{\n    public class Seed\n    {\n        public static List<Movie> SeedData()\n        {\n            var actors = new List<Actor>\n                {\n                    new Actor\n                    {\n                        FirstName = \"Bob\",\n                        LastName = \"Kante\"\n                    },\n                    new Actor\n                    {\n                        FirstName = \"Mary\",\n                        LastName = \"Poppins\"\n                    }\n                };\n\n            var movies = new List<Movie>\n            {\n                new Movie\n                {\n                 Id = 1,\n                 Title = \"The Rise of the GraphQL Warrior\",\n                 Actors = actors\n                },\n                new Movie\n                {\n                 Id = 2,\n                 Title = \"The Rise of the GraphQL Warrior Part 2\",\n                 Actors = actors\n                }\n            };\n            return movies;\n        }\n    }\n}The code snippet defines a sample movie data set clients can query.Step 5: Define Your Data ResolversResolvers are functions that tell the Hot Chocolate GraphQL server how to fetch the data associated with a particular schema. In simple terms, a resolver acts as a GraphQL query handler. In our example, we\u2019ll create a Query root method that exposes the possible queries a user can drill into.At the root of your project, create a folder called ResolversInside the Resolvers folder, create a C# file called Query.cs and paste the following code snippetusing Data;\nusing Models;\n\nnamespace Resolvers\n{\n    public class Query\n    {\n        public List<Movie> GetMovies() =>\n            Seed.SeedData();\n\n        public Movie GetMovieById(int id) =>\n            Seed.SeedData().FirstOrDefault(x => x.Id == id);\n    }\n}The above snippet defines two resolvers. The GetMovies resolver which takes in no parameters and returns the list of movies in the SeedData function we created in step 4. The GetMoviesById takes in the movie Id parameter and allows us to return a movie object based on it\u2019s Id.We now have everything we need to generate a valid GraphQL API. Let's go ahead and configure our GraphQL server.Step 6: Configure Your GraphQL ServerConfigure your GraphQL server by adding the following code to the ConfigureServices method in the Startup.cs class.//Startup.cs\n\npublic void ConfigureServices(IServiceCollection services)\n{\n   services\n       .AddGraphQLServer()\n       .AddQueryType<Query>();\n}The Hot Chocolate GraphQL \u00a0AddGraphQLServer method adds a GraphQL server configuration to the Dependency Injection(DI), and the AddQueryType<Query>() \u00a0adds the Query root method for our GraphQL schema we created in step 5.Add a GraphQL endpoint to the endpoint configurations by heading over to the Configure method in the Startup.cs and adding MapGraphQL to UseEndpoints, as shown below.//Startup.cs\n\npublic void Configure(IApplicationBuilder app, IWebHostEnvironment env)\n{\n   app\n       .UseRouting()\n       .UseEndpoints(endpoints =>\n       {\n           endpoints.MapGraphQL();\n       });\n}Step 7: Execute Your GraphQL QueryNow that the GraphQL schema and resolver have been set up, we can test our GraphQL server.Start the GraphQL server with the following commanddotnet runOpen your browser and head over to the port your GraphQL server is listening on (ours is http://localhost:5000/graphql) to open the Hot Chocolate built-in GraphQL IDE Banana Cake PopYou should see a screen like this:You can click on the Operations tab and view the schema reference to drill down into the schema structure.Copy the following code snippet into the operations window and click on the Run button to test the GraphQL APIquery {\n  movies {\n   title\n   actors {\n     firstName\n   }\n }\n}This query fetches the list of movies with their titles and the first names of the movie actors.Step 8: Create A Public Endpoint For Your GraphQL APITo integrate your GraphQL server to Hasura, you need a publicly accessible endpoint. For this tutorial, we are using ngrok. ngrok provides public URLs for exposing your local host. Follow the steps below to get a public URL on your local GraphQL API with ngrok.Download the current version of ngrokRegister and get a tokenOpen your terminal and set the ngrok token with this command: ngrok authtoken YOUR_AUTHTOKENCreate a tunnel with this command: ngrok http -host-header=localhost http://localhost:5000The output of the last command will list a public forwarding URL, which will point to your local serverStep 9: Create Your Database on Hasura CloudWith the Hasura Remote Schema feature you can stitch together schema and data from separate data sources. Let's create another data set on Hasura (different from the original data set we hardcoded earlier).Sign in to Hasura CloudCreate a project and click on the Launch Console buttonNavigate to the Data tab and create a new Heroku Postgres databaseAfter the database has been created, click on the Create Table button in the Data tab and create the table Movies with the following columns as shown belowId (Int)MovieUrl (text)ReleaseDate (timestamp now())Remember to select Id as the Primary keyOnce the table has been created, create a new record by heading to the Insert Row tab of the Movies table and inputting details.Step 10: Attach Your Hot Chocolate GraphQL Server With Hasura\u2019s Remote SchemaHasura\u2019s Remote Schema feature is one of the most straightforward methods of consolidating data from separate sources under a single, unified API.We can integrate our hardcoded data schema with the Heroku Postgres instance data by stitching them together using Hasura\u2019s Remote Schema feature.Navigate to the Remote Schemas tab in the Hasura Console to add the Hot Chocolate GraphQL server to Hasura's GraphQL schema, as shown belowNote: The GraphQL server URL will be the /graphql endpoint of the Hot Chocolate GraphQL server obtained from ngrok.The schema in our Hot Chocolate GraphQL server has now been attached with the Postgres database instance on Hasura.Step 11: Merge data with Hasura Remote JoinHasura via Remote Joins allows merging of data between two sources. So we can add the MovieUrl and ReleaseDate data we created in our Hasura Postgres database to the Movie object data we created locally based on its Id.Click on the Data tab, select the table Movies and head to the Relationships tab to configure a new Remote Schema Relationship as shown below:Step 12: Test the GraphQL APIWe can then test our unified GraphQL API to check if everything works.Navigate to the API tab and examine the query explorer. We can see it contains both the schema in the Hot-Chocolate GraphQL server and the automatically generated schema from the Postgres Hasura database instancePaste the following query in the query box. (You can also explore the complete GraphQL schema in the sidebar and toggle based on the fields you want)query MyQuery {\n  Movies {\n    Id\n    MovieUrl\n    ReleaseDate\n    movie_metadata {\n      title\n      actors {\n        firstName\n      }\n    }\n  }\n}Click on the Run button and review the responseWhen we test the GraphQL API we will see that the data from our Hot Chocolate GraphQL server have been merged with the data from the Heroku Postgres instance. The Data for the Movies ReleaseDate and MovieUrl fields come from the database (Postgres), and movie meta data fields like title and actors first name come from our Hot Chocolate GraphQL server.Learn More About Hot Chocolate GraphQL and HasuraTo learn more and try out other Hot Chocolate GraphQL features such as batching, pagination, and filtering, check out their documentation site.Hasura gives you real-time GraphQL APIs on all your databases. Check out our documentation on how to get started.Check out the .NET (dotnet) backend course. In this course, you will learn how to integrate .NET (dotnet) in a GraphQL backend server stack with Hasura."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Community Year in Review - 2021 Description: As 2021 comes to a close, let's look back at a year of releases, engagement, and milestones. Author: Jesse Martin"}, {"role": "assistant", "content": "We are about to turn the corner on another year, and this author is closing in on a year here at Hasura, and to say it\u2019s been a busy year for the community would be an understatement and a half. As one user recently reflected, and I can attest, the amount of content that exists in and around the Hasura community as a whole is impressive! Whether it\u2019s submitting questions or tutorials, speaking at one of our conferences, or one of our community calls, your contributions have supported and shaped the future of what Hasura is \u2013 and for that, we say thank you!Let\u2019s have a look at some of the milestones we\u2019ve hit this last year from the community overall.10k users on Discord!First of all, wow! What an amazing metric to end the year on! Discord has been our evolving platform where we communicate with the community, and where you all can communicate with each other. We love seeing users helping users, and you can expect to see us throwing even more weight at Discord this coming year as we seek to make this \u201cthe place\u201d to hang out with Hasura. Next 10k, here we come!400 Million DownloadsLet\u2019s repeat that number one time. 400 Million downloads. Whether this is a self-hosted container or a migration path to the cloud, that\u2019s a lot of downloads! No product can grow and learn without active users, and, well, 400 Million touch-points looks pretty active. Look for even more refined onboarding paths to Hasura in the coming months.100+ Open source contributorsEvery question, comment, and concern contributes to shaping Hasura and we appreciate, see, and make decisions from all of your feedback. We also want to highlight those contributors digging into the codebase and submitting PRs and RFCs, your dedication to the product, and your desire to solve problems in the true spirit of Open Source speaks volumes. Look to see a much-improved response time and transparency in the coming months, as well. Do know we see them all, and appreciate your feedback very much!Great new StoriesThis year saw the launch of Community Stories at Hasura, where we highlight community members doing new and interesting things with Hasura. Whether it\u2019s a company solving intricate problems with multi-tenant auth patterns, or a scrappy team re-platforming their entire commerce stack in four weeks, these are the wins and learns for you and by you. Have a great story to share? Let us know!Tips & Tricks ContestWe launched our first Tips and Tricks contest and got to see the many creative ways you work with Hasura, the tooling you\u2019ve built around your Hasura workflow, and the interesting abilities you\u2019ve surfaced that we didn\u2019t even know about! We're working out the last details with our swag shipping partner, so for all our participants, look forward to a fresh email in your inbox in January, where winners will also be notified!HasuraCon'21 - Online User ConferenceWe had the 2nd edition of our user conference in June. It was a virtual conference this year as well. There were product announcements, technical sessions, workshops, deep dives, discussion rooms and speakers from Bit, Crunchy Data, Current Desk, GitLab, Magic, Pipe, RedMonk, Taller, The Washington Post, Timescale, Vercel and YugaByte.Community Calls Turned 2 yearsWe entered into year two of community calls, our once-a-month call highlighting the happenings in product, community, and the surrounding ecosystem. If you haven\u2019t joined a community call yet, we\u2019d love to see you in January 2022! You can sign up here.Big Plans for 2022Whether we\u2019ve seen you at one of our weekly office hours calls, around Discord, in the comments of a PR, or simply a reply on Twitter, we\u2019ve enjoyed getting to interact with you over the last year and are looking forward to another great year of learning, growing, and making with all of you. Look forward to more physical interaction with Hasura the brand, more events, more transparency, and of course, more features!Have a great end of the year and we\u2019ll see you on the other side!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Bring REST APIs as a data source to Hasura: Announcing REST Connectors, Data Hub & Google Cloud Support Description: We\u2019re excited to announce some big updates to the Hasura GraphQL engine which further eliminate data silos, enhance security and lower the bar to universal GraphQL API usage. Author: Jesse Martin"}, {"role": "assistant", "content": "We\u2019re excited to announce some big updates to the Hasura GraphQL engine which further simplify data access for developers by eliminating data silos and easing the adoption of GraphQL across teams and organizations. As data proliferation increases, we continue to help bridge the gap for developers to easily access data irrespective of where it is stored. Today\u2019s features work to solve exactly that challenge, and we think you\u2019ll love the new capabilities landing today.And don\u2019t miss the surprise announcement at the end!REST ConnectorsThe first of our new features allows for connecting REST endpoints with low-code simplicity through request transforms. By transforming your GraphQL request using a minimal templating language, you can often bypass the need for writing proxy functions for connecting existing REST endpoints with Hasura. You can modify the request URL, query parameters, method, \u00a0and headers for maximum flexibility. This allows developers to instantly integrate existing REST API as well as leverage SaaS REST APIs:Diagram of data layer with Hasura REST transformsWith Action Transforms, Hasura helps you streamline the code you write even more to focus on domain code, not glue code, transforming the request with the input type parameters.With Event Transforms, you can modify the Hasura payload that gets sent to your event handler using the same mechanisms available in Action Transforms.Action TransformWith REST APIs, the individual endpoints are universal across projects, typically only varying by authentication or query parameters. To help learn from each other and share the different connectors that are being written, we\u2019d like to announce the next launch happening today.Introducing Data HubWe are excited to announce a new home for data connections with Hasura, including the new Action Transforms and Event Transforms \u2013 the Hasura Data Hub. This new hub will act as a single place to discover, learn, and contribute all the ways to connect your existing data to Hasura from Remote Schemas to Action Transforms and more. Looking for an Airtable or Auth0 connector? You can find it on the Data Hub.Want to show the world that they can quickly connect Hasura to your own Data source? Contribute to the Data Hub! Data layers benefit from a community working to build robust connections together, and we\u2019re very excited about offering a streamlined place to do just that.Hasura Data HubOnce you have your data connected together, you\u2019ll need a way to protect and maintain proper access controls on that data \u2013 a feature that people have loved about Hasura\u2019s own auto-generated APIs. That brings us to the next launch happening today.Identity-specific Authorization for ALL GraphQL ServicesHasura now supports role-based access controls for remote schemas and data joins. You can extend your same role-based permissions to any data source connected to Hasura, modifying the shape of the exposed schema, for both individual types and fields. Connect your data, define your permissions, and get immediate benefit, today.Google Cloud support in betaAlong with all the great features we are announcing today, we\u2019d also like to announce that Hasura Cloud is now available in Google Cloud. Hasura Cloud is the fastest way to try out the many great features of Hasura with a no-ops environment, built-in observability, and performance features. Now you can use Hasura Cloud with Google BigQuery and Google Postgres from within Google Cloud, eliminating egress fees and benefiting from even greater read/write performance. Follow the links to request for beta access and let us know which regions you\u2019d like to see added!Hasura Choose Region Select BoxM1 more thingStarting today, the official Hasura docker images are compatible with Arm architecture and run on Apple silicon. Enjoy all the great speed benefits of Apple\u2019s great new architecture along with your favorite GraphQL data layer tool.SummaryThat\u2019s a wrap for the feature announcements today, the team has been hard at work on all the architectural changes needed to make today\u2019s launch possible. All these new features can be found in Hasura or Hasura Cloud running version 2.1.0 \u2013 all new projects will use this version by default.Join us on Discord for even more features to be announced around data connectors in the coming weeks!We also have our end of the year community call this Thursday, at 9am PT and we\u2019ll be showing off some of these features and more! So do join us as we wrap up and celebrate this year with our team and community."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: 10 Popular Open-Source GraphQL Projects and Tools You Have To Try Description: In this article, we\u2019ve compiled a list of the top open-source GraphQL projects and tools based on their stars on GitHub. Our list contains a mix of GraphQL integrated development environments (IDEs), clients, and servers to help you explore the GraphQL metaverse. Author: Hasura"}, {"role": "assistant", "content": "Since GraphQL was open-sourced in 2015, it has been gaining popularity and adoption from the developer community, with developers looking for awesome ways to build stuff with and for GraphQL.If you are trying your hand at GraphQL, building a new project, or looking for cool ways developers build for GraphQL. By leveraging open-source GraphQL projects and implementing GraphQL tools in your projects, you can understand GraphQL best practices, techniques, and proper ways to set up GraphQL in production, which can improve and accelerate your GraphQL development experience. In this article, we\u2019ve compiled a list of the top open-source GraphQL projects and tools based on their stars on GitHub. Our list contains a mix of GraphQL integrated development environments (IDEs), clients, and servers to help you explore the GraphQL metaverse.Top 3 GraphQL IDEsGraphQL integrated development environments (IDEs) are software applications that combine common developer tools into a single graphical user interface (GUI) for viewing, writing, and running GraphQL operations.1. GraphiQL\u2b50 (13k) GitHub stars | \ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 (202) Contributors |\u23f3 (Aug 2015) First release dateWhat: GraphiQL is a popular and lightweight in-browser IDE for exploring GraphQL APIs. It was created by the same Facebook team that created the GraphQL spec and released alongside in 2015.Features:GraphiQL supports the complete GraphQL language spec, including queries, mutations, fragments, and unions. This means that with GraphiQL, you have all you need to write, validate, and test GraphQL APIs.GraphiQL displays snippets of the schema documentation as you type and highlights errors in syntax in real-time, so you don't make mistakes when writing queries and know what you are querying for.GraphiQL keeps a running history of your queries. If you use these queries frequently, you can \"star\" them to keep them at the top of your history. This can save you time, so you can quickly refer to frequently used queries.Why we think you should try it out:GraphiQL is the highest-rated GraphQL IDE on GitHub, with an active developer community of over 200 contributors and 180 thousand weekly downloads on npm. With GraphiQL, you can set up an IDE instance for your server or a more advanced web or desktop GraphQL IDE experience for your framework or plugin, like this one we built graphql- online.2. GraphQL Playground\u2b50 (7.7k) GitHub stars |\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 (103) Contributors |\u23f3 (Aug 2017) First release dateWhat: GraphQL Playground is a popular IDE for exploring GraphQL APIs. It was created by Prisma and built on top of GraphiQL.Features:GraphQL Playground provides more intuitive API documentation, schema exploration, and GraphQL query debugging experience than GraphiQL with a much nicer UI. These features improve development workflows by writing, validating, and running GraphQL queries much smoother.GraphQL Playground features automatic in-session schema reload, which helps avoid schema changes and breakdown during query sessions.With GraphQL Playground, you can share your GraphQL Playground with other developers using the GraphQL share bin option. This can help when collaborating on projects with team members.Why we think you should try it out:Although GraphQL Playground has been retired and donated to the GraphQL Foundation by Prisma, it remains one of the most actively used GraphQL tools, with over 400 thousand weekly downloads on npm. The project is constantly updated with bug fixes and security patches, and the GraphQL Foundation is actively working on GraphQL Playground 2.0. You can embed GraphQL Playground into your application, either as a React component in your front-end app or on the server-side as middleware for frameworks such as Express, Hapi, or Koa.3. GraphQL Editor\u2b50 (5.5k) GitHub stars |\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 (14) Contributors |\u23f3 (Sep 2018) First release dateWhat: GraphQL Editor is an IDE for creating and understanding GraphQL schemas by using a visual blocks system. GraphQL Editor makes understanding GraphQL schemas a lot easier.Features:GraphQL Editor supports API stitching. A developer can import schemas from other projects and use the types declared within the imported schema in the current projects. This feature speeds up the time required to create GraphQL schemas.GraphQL Editor supports query preview. It ships with an integration of GraphiQL that will render a GraphQL API under development with mock data. This feature accelerates development time as front-end developers can easily test and simulate GraphQL APIs without relying on backend engineers to create the GraphQL APIs.With GraphQL Editor, you can create a block diagram of all your GraphQL entities. This quality helps developers visualize and validate their GraphQL schema quickly.Why we think you should try it out:GraphQL Editor has an active GitHub community with a low \"open issue count\" (22) on GitHub. With GraphQL Editor, you can configure graphical elements within the IDE to create visual GraphQL entities that will also be rendered as code in the GraphQL type language right within the IDE. Also, you can work the other way and create the entity in code and have it rendered graphically in the GraphQL editor object palette.Top 3 GraphQL ClientsGraphQL clients are software libraries/frameworks that interface with a GraphQL server/API data. GraphQL clients handle sending queries and mutations to a GraphQL API while abstracting lower-level networking details or maintaining a local cache.1. Apollo Client\u2b50 (17k) GitHub stars |\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 (640) Contributors |\u23f3 (Feb 2016) First release dateWhat: Apollo Client is a comprehensive GraphQL client that enables you to manage local and remote data from GraphQL APIs.Features:Apollo Client requests and caches data from your queries and updates your user interface, which reduces the labor involved in creating client-side applications that interact with GraphQL APIs.Apollo Client has built-in pagination functionality that makes it easy to implement pagination in your client application.Apollo Client 3.0 features reactive variables. These variables can store data of any type and can be accessed anywhere in the application. Reactive variables can be an advantageous mechanism for representing local states outside the Apollo Client cache.Why we think you should try it out:Apollo Client is one of the most popular GraphQL projects with over 640 contributors and 114 thousand users on GitHub. With Apollo Client, you can build robust and scalable web apps for many UI frameworks (React, Angular, Vue, iOS, Android) that easily connect to GraphQL APIs.2. Relay\u2b50 (16.2k) GitHub stars |\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 (484) Contributors |\u23f3 (Aug 2015) First release dateWhat: Relay is a JavaScript framework developed by Facebook for managing and fetching data from GraphQL APIs in React applications.Features:Relay supports declarative data fetching. Declare your data requirements using GraphQL and let Relay figure out how and when to fetch your data. This feature reduces the time and complexity required to consume GraphQL APIs in React applications.Relay allows deferred queries. This feature is particularly useful to render long pages, for example, a news feed where you find a long list of posts where you need to define data priorities.Relay has built-in features for local memory data caching. Relay associates each piece of data with a unique ID compares server data with the local memory cache via that unique identifier and updates the differences. This is useful when dealing with bandwidth-constrained networks.Why we think you should try it out:Relay was created by the original GraphQL creators and continues to release new updates and features. It is heavily optimized for performance and comes with an opinionated structure that helps you manage a high level of complexity, enabling developers to build React applications confidently.3. URQL\u2b50 (6.6k) GitHub stars |\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 (155) Contributors |\u23f3 (May 2019) First release dateWhat: URQL is a lightweight, extensible GraphQL client. It is built to be highly customizable and versatile and exposes a set of helpers for several frameworks (React, Vue, Svelte).Features:Caching in URQL is fully customizable through the exchanges. It supports \"Document Caching\" for content-heavy pages and \"Normalized Caching\" for dynamic and data-heavy applications. This allows you to easily set up caching on your client applications based on your specific requirements.URQL has first-class support for functionality such as offline mode, file uploads, authentication, and Next.js. This support means you spend less time implementing features such as authentication in your client application.URQL provides an extensibility mechanism that allows app developers to scale and customize the URQL GraphQL library to suit their needs.Why we think you should try it out:URQL has an active developer community with over 100 thousand weekly downloads on npm and a low \"open issue count\" (21) on GitHub. URQL gives you complete control over your GraphQL operations and results via \"exchanges.\" URQL exchanges allow you to customize your data layer to suit your needs, helping you build web applications with ease.Top 3 Custom GraphQL ServersGraphQL servers expose your data as a GraphQL API that your client applications can query. GraphQL servers do the heavy lifting of ensuring that the proper quantity of data is retrieved using the fewest possible database lookups and API calls. In this list, we will look at different ways to write a custom GraphQL server from scratch.1. Apollo Server\u2b50 (12.1k) GitHub stars |\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 (464) Contributors |\u23f3 (Apr 2016) First release dateWhat: Apollo Server is an open-source, GraphQL server used to build and run GraphQL APIs on Node.js. With Apollo server, you define the GraphQL schema design, and Apollo server takes care of the mechanics of the GraphQL API.Features:Apollo Server supports automatic persisted queries, which lets clients send a calculated hash instead of a complete query string over the network. This technique reduces query latency and bandwidth requirements, especially for large and complex queries.Apollo Server includes an integrated metrics reporting agent out of the box, making it easy to diagnose GraphQL API performance problems.Apollo Server includes standard error handling capabilities that make it easy to handle re-authentication and build error-aware UI components and interactions.Why we think you should try it out:Apollo Server has an active developer community with over 400 contributors and 460 thousand weekly downloads on npm. Apollo server ships with everything you need to build production-ready self-documenting GraphQL APIs with ease.2. Express GraphQL\u2b50 (6.2k) GitHub stars |\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 (60) Contributors |\u23f3 (Aug 2015) First release dateWhat: The Express GraphQL module from Express provides a simple way to create a GraphQL server in Node.js. With Express GraphQL, you can create a GraphQL HTTP server with any HTTP web framework that supports connect-styled middleware, including Connect itself, Express, and Restify.Features:Express extends its middleware capabilities to GraphQL endpoints. This capability allows you to set up middleware logic such as authentication before hitting the GraphQL API endpoint.Express GraphQL ships with some existing Express.js features such as support for cross-origin resource sharing (CORS), session management, and request body-parser, which means you do not need to reinvent the wheel for features like these when setting up your GraphQL APIs.Express GraphQL includes a TypeScript declaration file which enables auto-complete in compatible editors and type information for TypeScript projects.Why we think you should try it out:With over 600 thousand weekly downloads on npm, Express GraphQL is one of the most used GraphQL servers. With Express GraphQL, you can easily set up production-ready GraphQL APIs in Node.js.3. Hot Chocolate\u2b50 (3k) GitHub stars |\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 (140) Contributors |\u23f3 (Jan 2018) First release dateWhat: Hot Chocolate is an open-source GraphQL server used to create GraphQL APIs for the Microsoft .NET platform. It is fully compliant with the newest GraphQL spec and compatible with all GraphQL compliant clients, such as Relay and Apollo Client.Features:Hot Chocolate has built-in support for data loaders, making batching and caching for query results from databases much faster.Hot Chocolate supports .NET Core authorization policies, so you can use authorization directives on your schema fields to enable base field authorization for better GraphQL API security.Hot Chocolate comes with built-in filtering and pagination, offering rich query capabilities through your GraphQL API.Why we think you should try it out:Hot Chocolate is one of the GraphQL projects expanding the GraphQL community beyond Javascript/Node and is gaining popularity in the .NET ecosystem, with over 2 million downloads on NuGet. Hot Chocolate is quite easy to set up and has an active developer community that continuously improves and implements new features. Hot Chocolate takes away the complexity involved in building GraphQL servers and APIs in .NET.Get Instant GraphQL APIs on All Your Data With HasuraHasura GraphQL Engine is a GraphQL server that gives you real-time, instant GraphQL APIs over your database. Hasura makes it easy to build your backend GraphQL app or add a GraphQL layer to your existing application.Features:Hasura can simultaneously connect multiple databases to a single instance of the Hasura GraphQL Engine. This feature is particularly useful to enable GraphQL APIs on databases with different workloads, i.e., one database for search and another database for high-volume transactions.Hasura GraphQL Engine offers built-in filtering, pagination, and merging remote schemas. This feature accelerates the time required to build and ship production-ready GraphQL APIs.Hasura also caches your GraphQL query plan by storing the query string and variable values in an internal cache along with the prepared SQL statement. This technique leads to fast, highly optimized database calls.Why we think you should try it out:Hasura has an active developer community with consistent feature updates and is used by Fortune 500 companies such as Philips. This year alone, Hasura has included support for BigQuery and MS SQL Server databases in addition to Postgres. Additionally, with Hasura Cloud you have a fully managed, production-ready GraphQL API as a service to help you build modern apps faster. Get started in 30 seconds!Your GraphQL Project or Tool Could Be NextMany of these GraphQL projects and tools began as niche developments and are now being used by top companies like Optum, Shopify, and Netflix. For example, the URQL GraphQL client, launched in 2019 to provide a lightweight, intuitive alternative to GraphQL clients like Relay and Apollo, now has over 100 thousand weekly downloads on npm. The GraphQL developer community is growing, and your GraphQL project could be the next one to take center stage.You can also join the fantastic GraphQL developer community by applying to become a contributor to a GraphQL project or tool. Many GraphQL projects and tools depend on contributors to maintain and develop new features and plugins.If you're working on an open-source GraphQL tool or project or know any awesome ones we should check out, reach out and let us know on Twitter or Discord, and we'll take a look!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Getting Started with Hasura, Netlify, and the Jamstack Description: In this article, we'll take a look at combining Hasura Cloud plus Netlify for Jamstack deployments that scale. Author: Jesse Martin"}, {"role": "assistant", "content": "Front-end and full-stack developers today need to balance any number of microservices, APIs, and continuous deployment requirements. Tools like Hasura and Netlify handle these requirements from back-to-front, helping you rapidly build production-ready applications that will scale and be simple to maintain.For a quick introduction to Hasura, here\u2019s a great video from the community on Fireship.io Hasura + Netlify ResourcesFor more information on getting started with Hasura and Netlify, here\u2019s a curated list of content to get you started. We can\u2019t wait to see what you will build!Getting Started with Hasura documentationOfficial documentation will always be the most up-to-date reference for any product. Find the latest getting started information here.Getting started with Hasura \u2013 Learn course!Our recently updated course on Hasura is an excellent place to get started and start your journey towards being a Hasura power user! Visit the Hasura course.Create a custom auth flow with Hasura and Netlify Here's a helpful guide on creating a custom auth flow from Netlify into Hasura for authenticated API access. Check out the Hasura + Netlify demo here.Watch Tanmai demo Hasura on \u201cLearn with Jason\u201dIf you don\u2019t already know the \u201cLearn with Jason\u201d channel, you are in for a treat. Hear about Hasura from one of the founders, Tanmai Gopal in this fun and informative video.Learn how to do rapid prototyping in HasuraAnother resource for building with Hasura in less than 15 minutes. You can watch the contained video here.It\u2019s dangerous to go alone, take this frameworkHasura has a number of helpful boilerplate frameworks for starting your project quickly. See the whole list of starter projects here!Curious to know more about the Jamstack? We\u2019ve written some more things about the Jamstack here.Let us know what you built in our community discord!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building with the Jamstack in 2022 Description: The Jamstack is changing all the time with new tools and abilities being added daily. Find out the past, present, and future of Jamstack in this article. Author: Jesse Martin"}, {"role": "assistant", "content": "A brief history of the JamstackIn the world of front-end web development, the trends are many and the changes are frequent. Every few years some new technological development to the medium has an exponential influence on the message in shape, depth, and availability.The browser has democratized access to information in a way no other medium could or can. All you need is a browser that (should) adhere to a common approach in interpreting and processing HTML, CSS, and JavaScript, and as long as you are able to access the file, there is no difference between Chromebook or MacBook, Thinkpad or Supercomputer.JavaScript has become more efficient and expressive. CSS has become more consistent. Browsers have a wider set of integrations with the local operating system, and HTML itself has seen a much-needed semantic update. During this time, as Javascript standardized across the browsers and increased in functionality, the rise of frameworks to act as a bridge between the different browser APIs became the standard way to write modern web code.However, browsers historically lack some key features that servers have provided. Servers allow for non-blocking tasks. Servers allow for a protected execution environment. Servers are able to run resource-intensive compute tasks in a centralized way \u2013 not everyone has a 32 gig workstation at hand. Thus APIs were born as a way for servers to communicate with browsers in an agreed-upon way.While the browser as a medium continued to evolve. Server utilization and optimization were also developing in parallel. Many companies began to realize that paying for large servers that were always on accounted for substantial costs in both material and maintenance. The idea of serverless compute was born where lambda functions running on hardware tuned for their execution could run one-off tasks with greater predictability in performance and cost. The idea of \u201cserverless\u201d was born where companies could simply hire a function that executed a specific task to run their workload and return the result, often eliminating the need for a self-managed server entirely \u2013 instead processes and workflows were coordinated efforts run across a network of task functions, all communicating via API.With the browser capable of doing ever-more, the cost-of-entry to server-side execution plummeting, and the ubiquity of APIs dominating the service space - a rise known as Software as a Service (SaaS), the market was ripe for a rethink in how websites were built.Jamstack todayFew terms command as much name recognition share and virality as the idea of the Jamstack. Jamstack as a term originated at Netlify but quickly become a movement. The name has iterated from various forms including JAMStack, and others, but still refers to the same concept. Where websites in the past required servers to compile template code with data and send that as compiled HTML to the browser, Jamstack allowed websites to be built with lean technology, fetching the data from remote services, and calling complex serverless functions from the browser.In the earlier implementations of the Jamstack, there was an outsized focus on Server Side Generated (SSG) \u2013 aka static. A website generator would read template files, add the data, and create a compiled static site that would then be hosted on a cheap web host. These simple websites would then leverage modern Javascript and modern browser features to create rich experiences for the user, though the content was still static in the end.Today, Jamstack can cover a lot of different use cases from statically generated sites to serverless sites serverless-sites that still offer the benefit of a server-side execution environment. Choosing the right pattern will depend a lot on the kind of workload you intent to run with your website.Jamstack patternsModern Jamstack websites tend to fall into one of three buckets. Server Side Generated, Server Side Rendered, or Hybrid.Server Side Generated (SSG)Server Side Generation, as discussed above, offloads the compilation of template plus data to a build time step. Instead of being generated on-demand, requiring a server to do so, you could build all of it ahead of time, as the majority of website pages, if not the entire site, don\u2019t actually have frequently changing content. Think of this performance like hyper-aggressive caching.When to use SSGIf your website content doesn\u2019t frequently change, or changes could be more or less predictable for triggering new builds. This may require your CMS to have support for triggering webhooks on updates.Server Side Rendered (SSR)Server Side Rendered is a relatively newer pattern in the Jamstack and, as the name implies, rendered content on each request. This offers the most amount of customization with the least amount of code.PersonalizationThere are many personalization tools out there that will show visitors tailored content, but for static sites, all that content still needs to be delivered on pageload. With SSR, that content can simply be omitted.ProtectedIf your page should be hidden behind a protected environment, you can perform authorization checks on the server, and send the client the appropriate response.When to use SSRIf your website has thousands of pages, or protected routes, SSR might be a great fit for you. If your page content updates frequently, such as in cases with lots of content teams editing and updating articles, products, or more, SSR would also be a great fit.HybridThis approach is not available to every framework, but it offers the ultimate flexibility. Provide a combination of both server-side rendered and server-side generated content to handle the widest range of workloads. If you have a page that constantly updates, like a home page, choose the rendered paradigm, if you have a popular article that receives thousands of views per day, choose the generated paradigm.When to use HybridHybrid will always be the best choice \u2013 if your language stack supports it. Not every framework is capable of offering SSR (a space heavily dominated by the Node runtime), and so not every framework will support Hybrid. If you have a language that offers a Hybrid framework to work with, this would be a great choice to look into. Building a Jamstack toolboxWith the considerations mentioned above, here is a guide to identifying your Jamstack stack.Identifying your Jamstack componentsWhen working in a serverless friendly way, identifying the services you need to integrate with is the first step. Some good questions to ask yourself.Is my website asset-heavy?If it is, you\u2019ll benefit from a CDN to deliver cached, and customizable resources.Do I need authentication?Add an auth provider. Choose one that will integrate with modern web frameworks.Do I need custom data?Pick a back-end provider. Ideally, you\u2019ll want one that lets you orchestrate some of your services without too much fuss so you have less overall API surface area to fiddle with.Do I need to run custom business logic?You will need a FaaS provider. Business logic is different from business \u201ctasks\u201d such as booking appointments, for services like calendering, lead capture, etc \u2013 a good SaaS provider would be the place to look in the Jamstack approach. If you need to channel those leads based on arbitrary inputs, then this would qualify as business logic. A stable FaaS is able to handle many different kinds of tasks like this.Identifying your business typeJamstack works for companies of all sizes these days. While the remainder of this article will focus on indie developers or small teams, large enterprises benefit just as much from the majority of the content in this article. The only exception is in terms of a service provider where procurement will play an important role in which services are allowed.Generally speaking, identifying any service provider constraints before any of the remaining stack identification processes will help narrow the scope of available options. Highly regulated or otherwise restricted environments should look for a vendor that offers self-hosting, open-source, or every kind of fortune 500 logo on their website to ensure a smooth flow through the procurement process.Choosing a frameworkPicking a solid website framework will depend largely on the pattern you\u2019ve chosen, the host you choose, and the language you are comfortable working with. An opinionated position of this author is that a Javascript framework will yield dividends in terms of time-to-market as the final compilation of the project will end up in some form of HTML, CSS, and JavaScript, so reducing the amount of compilation and transformation will help ensure your framework stays current for a long time to come.Some excellent Javascript frameworks worth looking at today are:AstroJSAstro focuses on lean development. The idea is to let HTML and CSS hold their own in the Jamstack, which is a stark contrast to many of the frameworks out there.GatsbyJSThe name that built a thousand sites, GatsbyJS can nearly single-handedly take the credit for getting static site generation into popular developer culture. Where many, this author included, were extolling benefits of Static sites to our nerdy friends, Gatsby somehow got static sites a seat at the table. The framework itself has progressed drastically since its first inception and it handles a wide range of use-case, though static is still core to its DNA. A unique selling prop for Gatsby is the content graph, where a rich ecosystem of plugins can source content from nearly any modern data source and render that content to the page. If you like plugin culture, Gatsby has a lot to offer.NextJSNextJS is the workhorse of the JavaScript developer. Handling Hybride site structures, NextJS essentially defined the category of hybrid frameworks. NextJS offers rich abstractions for common use cases but leaves the majority of the opinion to the wayside and lets developers write applications that fit their workload. NextJS is also written by and maintained by one of our recommended hosts and as such, maintains lockstep with at least one friction-free deploy target.NuxtJSNuxt is NextJS for Vue. And that\u2019s a compliment. With all the benefits of the NextJS blend between sensible convention with plenty of configuration, but available to the VueJS audience, Nuxt has a lot to offer with the addition of a very clean API.Remix.RunRemix is the newest entrant on the list but is written by and maintained by an incredibly talented team. RemixRun looks to take the power-feature concepts of NextJS and build on them with an alternative set of abstractions and benefit from being able to start the inspiration gathering from the delta position of many years of category maturation. Nested Routes are of particular interest.SapperSapper is is SvelteJS\u2019s take on NexJS. What makes Sapper unique is Svelte itself - which aims to lean heavily into blazing fast compile times for a generated pattern. Worth noting that the creator of Svelte (and Sapper) has been hired by the team behind NextJS to work on Svelte, boasting good things for the framework and framework diversity as a whole.Choosing a FaaS providerFaaS providers handle the business logic of your code. We\u2019ve gone into some explanation of why you\u2019d want to do this above, but the fact is, this is where the 90% use-case of needing a server is handled. We\u2019ve written about different FaaS providers here at length. Embracing this pattern lets you write domain code and less boilerplate code.Some FaaS providers worth looking into:FlyIoFlyIo specializes in creating one-off services (databases, functions, etc) and deploys them to edge locations. They require a higher degree of technical aptitude but offer a high volume scale for relatively low onboarding effort.Cloudflare WorkersCloudflare Workers is similar to FlyIo in terms of technical acumen required, but what\u2019s interesting about Cloudflare is that it brings additional services to the table such as their flagship service, being a CDN. With a little know-how, deploying to workers again requires considerably low onboard effort.VercelVercel benefits from a tight coupling between the framework they provide (NextJS, now also SvelteJS), and their function hosting service. The integration is so good that you can define your functions under an API path of your NextJS projects and they\u2019ll simply work as a traditional API endpoint, though technically are a service worker.NetlifyNetlify is the most \u201cplatform complete\u201d provider on the list. They provide not only excellent hosting, but they offer \u201cspeciality\u201d function handling for form capture, authentication, and more. Similar to Vercel, they offer IAC through a .toml file that can be included in the core deployment, which then maps to a functions folder in your codebase, allowing you to benefit from local development as well as automated deployments.AutocodeThe first of our low-code entries on the list. For popular services together, Autocode (formerly Standard Lib) takes out a lot of the pain. It\u2019s difficult to prototype faster than with Autocode for FaaS execution.PipedreamPipedream is our second low-code provider that abstracts away a lot of the boilerplate and provides robust logging, error management, and smooth, versioned, deploys. Pipedream has very low warmup times for a low-code tool and can be used for production workloads.Choosing a hostHosting your Jamstack app depends a lot on which platform you intend to use, and in some cases, which framework. If you use Gatsby, you have access to Gatsby Cloud. If you want the best NextJS experience, you\u2019ll use Vercel. If you want the widest flexibility with the easiest deployment strategy and some additional services like auth, then Netlify is the best choice. Though none of these are exclusive.Gatsby CloudGatsby Cloud gets a node because GatsbyJS is still very prevalent in the ecosystem. If you are in the Gatsby ecosystem, enjoy the Gatsby Cloud if it handles your use case.NetlifyNetlify handles additional services like auth, form handling, workflows, and more. As with the three other providers on this list, CI/CD deployment from Github is a game-changing functionality.RenderRender provides a lot of the services a Jamstack site would need. From CI/CD deploys, to one-click database launches, Render is well worth a look for their predictable pricing model and full-featured offerings.VercelVercel handles all the behavior you\u2019d expect from a modern web host, and offers excellent integration with NextJS for obvious reasons. If you are looking for the smoothest NextJS experience, you don\u2019t need to look much further. You\u2019ll be on your own for auth and form capture, though.Choosing an auth providerIf your stack needs authentication, there are a number of good providers available.Auth0Auth0 is the heavyweight that can do everything. If you\u2019re looking for a well-vetted solution, Auth0 is a great choice. After recently acquiring Otka, they\u2019ve only increased market dominance.ClerkClerk offers first-class support for React-based frameworks along with very simple APIs and elegant user management panels.FirebaseFirebase doesn\u2019t need much introduction these days, it\u2019s a quick, can-do integration that plays well enough with others. We\u2019ve kept the other \u201cbig-tech\u201d providers off this list such as Incognito, but Firebase is such a ubiquitous auth provider it gets a mention.Magic LinkMagic link wins the \u201cone thing well\u201d category for providing dead-simple, passwordless authentication with generous pricing. If you want an extremely simple provider, have a look.Choosing a CDNDepending on what you are delivering, a good CDN will save the launch day. Two worth noting.CloudflareFor a general-purpose CDN that will give your sight robust protection against DDoS (intentional or not), Cloudflare is a great choice for delivering your simple static content.CloudinaryFor delivering images from a powerful CDN that also offers image transformations and a host of other features, Cloudinary is about as simple as it gets.Choosing a BackendOrchestrating all of these different FaaS providers, pulling data onto the website, protecting your data, and rate-limiting excessive usage is a critical component to manage when looking into a Jamstack site.Hasura will manage all of those pieces for you. You can connect your FaaS providers to your API, protect them behind access controls, leverage built-in caching, and more by using Hasura Cloud. With one-click deployments of Postgres on Heroku, you can deploy, design, and access your data in less than 5 minutes.TLDR;Jamstack support all kinds of workloads thanks to the rise of serverless computing and a rich, SaaS ecosystem. Jamstack sites help you build faster, more affordable websites with less effort, and are quickly becoming the default way to a modern experience on the web."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Comparing Serverless Functions Providers: Vercel vs Netlify vs Gatsby Cloud Description: For front-end developers, few FaaS providers command as much name recognition as Vercel, Netlify, and Gatsby Cloud. In this post, we'll compare them. Author: Jesse Martin"}, {"role": "assistant", "content": "In a previous article, we looked at a broad range of FaaS providers and their various benefits. The modern, Function as a Service ecosystem has solutions for any scale of workload with nearly any possible runtime. Serverless functions handle everything from artificial intelligence to zipping up files.But the benefits of serverless compute is not just limited to running business logic. Front-end developers, through increasing adoption of modern frameworks that are faster to build and cheaper to run, are also reaching for serverless functions to handle many of the tasks that used to require beefier frameworks. Tasks such as authentication, checkout, and form processing are all moving away from integrated parts of the modern web framework and into the serverless realm, which allows websites to become increasingly static in build, while not feeling static in nature.The goal is not to benefit from a leaner framework with faster development cycles to simply re-allocate the time and complexity into a bunch of serverless functions requiring context shift in runtime, platform, and authentication.Comparing Serverless Functions ProvidersNetlify vs Vercel vs Gatsby CloudFortunately, in front-end deployment, there are a number of hosting providers that are blurring the boundaries between host and FaaS provider which gives front-end developers an ever-expanding toolset without sacrificing focus. Today, the big vendors of note in this space are Vercel, Gatsby, and Netlify. An honorable mention is necessary for Render, Begin, and Cloudflare Workers which are each dramatically scaling up their hosting and serverless deployment options.With a noted exception of Netlify, both Vercel and Gatsby Cloud have their own framework product as well. Gatsby Cloud\u2019s namesake Gatsby, which was a forerunner of the Gatsby Cloud platform, is the newest entrant into serverless hosting. Vercel, formerly Now, has the NextJS framework which has been a Megatron only gaining in speed. Though Vercel have intentionally diversified the frameworks you can run to not just be NextJS, and have even spun serverless functions into a standalone product.What becomes powerful with both Vercel\u2019s NextJS framework and Gatsby Cloud\u2019s Gatsby framework is that writing serverless functions can sit within the same code repository, and execute locally with the same tooling, which creates a force multiplier of developer productivity.And don\u2019t be mistaken to think these frameworks are for less serious websites. The three platforms have raised a combined $300 million in recent fundraising rounds with a TAM of $7.7 Billion in 2021 and at a CAGR of 32%, this is just the start of the dollar train charging into FaaS station.Comparing the IncomparableThe more challenging part of comparing these three platforms is that they are sharing an overlapping problem set with overlapping approaches, but with fairly distinct approaches.Gatsby Cloud is bullish on Gatsby framework, Vercel has their own framework, but is still largely framework agnostic, and Netlify brings additional features that abstract away some of the common cases for FaaS hosting to begin with (form handling, authentication, etc).To properly compare them, let\u2019s consider the problems as they are solved for developers and developer productivity. We\u2019ll let each have a byline about their framework integration, USP for DX, and some quick stats about FaaS execution (runtime, location, timeouts.)Here\u2019s the comparison we ran along with the other FaaS providers.Function Framework\n\n\nProvider\nSummary\n\n\n\n\nGatsby Cloud\nGatsby Cloud let\u2019s you define function handlers in the Gatbsy framework. And that\u2019s about it. Where it might seem limited, this level of simplicity may just handle the 70% use case without needing to complicate things and the rich plugin ecosystem for Gatsby will surely extend to functions as well.\n\n\nVercel\nVercel lets your write functions as both part of the Vercel Function framework, and as part of the NextJS Framework via API routes.\n\n\nNetlify\nWhere Netlify doesn\u2019t have a specific front-end framework to act as a leader, it does provide a framework for writing functions, with any handler under the /netlify/functions path acting as a route handler. Netlify provides their own CLI which enables local development with these functions as well.\n\n\n\nUSP for Developer Experience\n\n\nProvider\nSummary\n\n\n\n\nGatsby Cloud\nGatsby\u2019s USP is that it is simple to integrate with. It scales well and is tailor-made to work with Gatsby websites. While that might sound limited, Gatsby single-handedly owns the brand game around static sites right now, with name recognition for anyone else in the static website space trailing by a mile. For customers dead set on Gatsby for a static site, Gatsby Cloud Functions will let them stay in that ecosystem without adding extra complexity.\n\n\nVercel\nThe Vercel functions USP can\u2019t be properly explained without mentioning the powerhouse of frameworks that is NextJS. If you want to build a website with Javascript and Typescript, you won\u2019t find a concept NextJS can\u2019t handle. It is as simple as that. Being able to code API handlers, webhook handlers, and edge middleware \u2013 all within the same codebase as your website \u2013 with integrated local development, environment management, and deployment CLIs, Vercel is the Apple of FaaS providers. They simply get the ecosystem right.\n\n\nNetlify\nNetlify takes a different approach to the USP by focusing on value-add. On top of serverless functions and edge execution, they additionally offer packaged auth, large object management, and form handling. These are very common use cases for serverless functions that are pre-made, qualifying Netlify as the most \u201cFaaS platformesque\u201d of the three.\n\n\n\nExecutionAll metrics provided will show the free-tier upper bound and the last plan-based upper bound before enterprise.\n\n\nProvider\nInvocation\nLanguages\nMemory Size\nTimeout\nLocation\n\n\n\n\nGatsby Cloud\n25,000 - 250,000\nJavaScript, TypeScript\n128mb - 1gb\n15s\nGatsby Servers\n\n\nVercel\nUnlimited (1000 concurrent - )\nJavaScript, TypeScript, Go, Ruby, Python\n1gb (1024mb) - 3gb (3008mb\n5s-15s (Proxied requests: 30s\nEdge Nodes, Vercel Servers\n\n\nNetlify\n 125k- \n JavaScript, TypeScript, Go\n1gb (1024mb) -\n10s (Sync), 15m (Background)\nEdge Nodes, Netlify Servers\n\n\nThe Hasura AngleWe\u2019ve looked at these popular front-end hosts and their respective frameworks, but how does this impact a Hasura developer? Well, it fits really well with our ethos for building applications. Hasura, and specifically Hasura Cloud, fits into a hosted paradigm of low-ops, managed services where you can configure your data access layer without needing to write a lot of boilerplate code. Leveraging utilities like Events and Actions, you are able to perform just-in-time business logic on API behaviors that don\u2019t need endless servers sitting idle. Putting together a Hasura Cloud instance, and identifying the intended behavior parallel to our front-end code (even if the code will be scoped to sever-side execution) lets us close the loop on developer experience and shared context which let us create better, more resilient applications, faster."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Why GraphQL API is the perfect data layer for your backend Description: This article discusses why readers should adopt GraphQL as their backend data layer, providing relevant context and examples from companies using GraphQL in production. Additionally, it provides actionable insights for readers on how they can get started with GraphQL. Author: Hasura"}, {"role": "assistant", "content": "GraphQL, a query language for reading and modifying data from APIs, is growing in popularity. Data from the State of JavaScript 2020 reports that 44.3% of respondents said they had used GraphQL API in 2020, compared to 5% in 2016. Many Fortune 500 companies use GraphQL API for their backends in production; top companies like Facebook and GitHub use the API technology to improve their product offerings and boost system performance.With the ever-changing technology landscape, you might have doubts about adopting GraphQL API for your backend and be unsure how it can improve your current data needs. However, regardless of what language or architecture you are using, GraphQL API can help simplify data access, supercharge the product development process, improve application performance, and handle complex backend data scenarios.One unified endpoint for your backendOne advantage of GraphQL API is that it provides a single unified endpoint for your backend to meet both client and general API use cases. A single GraphQL API can handle all the client\u2019s requests and forward them to the correct backend service.As companies and their products scale, REST endpoints can become large and incredibly hard to manage. One of the main problems in REST is the N+1 problem, where the client calls the server N+1 times to get N number of resources, mainly because a single REST resource does not provide enough information for the client to operate with.Brennan Saeta, an infrastructure engineer at Coursera, cited centralizing REST APIs for simple consumption by the client as one of the reasons why GraphQL API appealed to them.GraphQL API hides the complexity of the underlying backend system by aggregating the required data from different sources and providing a simple interface that clients can communicate with through a single resource.Improved application performanceGraphQL API improves application performance by reducing JSON payload size and network overhead from multiple API calls. By allowing the client to specify its data requirements, GraphQL APIs eliminate the need for unnecessary generic data being passed from the backend and minimize the number of roundtrips required to retrieve backend data.As companies scale and iterate their products, the data required by different clients and pages changes. In REST API design, when new requirements arise, developers must decide whether to create a new endpoint and have clients make a new request for that data or add more data to an existing endpoint. The first option leads to more server roundtrips, and the second option increases payload size, both of which can hinder app performance.With more applications being accessed over flaky mobile networks and app speed being an important metric for retaining users, minimizing server roundtrips and data transferred over the wire is paramount. Companies like Netflix and Pulley have reported faster page load times and overall app performance after switching to GraphQL APIs.With GraphQL APIs, clients receive only the data they specify from the backend server in a single trip, saving network bandwidth and resources.Improved system reliabilityGraphQL API improves system reliability by ensuring that client queries are valid and in sync with the GraphQL schema specification before query execution. GraphQL uses a type system that describes what type of data can be queried from the backend server and in what form it exists. A strongly-typed schema offers enhanced security over REST APIs by preventing breaches like SQL injection and unintentional data leaks, resulting in a more secure and reliable application.Type-safety ensures that any breaking changes by the backend server would be caught by clients consuming the data, resulting in less downtime. With GraphQL APIs, you can modify and deprecate API fields while still maintaining backward compatibility. It is easier to know which fields in the API are being used by what clients and which ones are not. With GraphQL, API developers can confidently make changes to the existing APIs without breaking the existing functionality.Swiggy, India's largest food delivery platform, discussed how adopting GraphQL API for their backend enabled them to make changes to their existing APIs without breaking the consumer application.Ultimately, GraphQL ensures API consistency, reliability, and security, which results in better system performance.Better developer experienceAdopting GraphQL APIs improves the developer experience for both frontend and backend developers. Backend developers want to build APIs that are intuitive and easy to explore and navigate. GraphQL helps with that by reducing the time spent documenting APIs.Through a concept known as introspection, GraphQL APIs allow clients to query the schema and discover the resources available on it. With excellent tools and libraries like the GraphiQL IDE, the interactivity for GraphQL API endpoints is much more intuitive for the developer.API consumers want APIs that are flexible, easy to integrate, and support a variety of client use cases (Web, iOS, and Android). Because of REST's inflexibility, it is challenging to create a single endpoint that can meet all of the client\u2019s data requirements. Often the same client has to call multiple APIs to get the data it needs from the backend. Additionally, with REST APIs, frontend developers have to rely on the backend for API specifications before building product features, leading to slower development time.Peter Downs, director of engineering at fintech startup Pipe, mentioned how choosing GraphQL API early on cut their development time by 90%. With GraphQL APIs, frontend and UI developers can quickly and easily iterate on features by simply specifying the data requirements in the GraphQL API, thereby reducing the dependence on backend developers to make changes.Getting started with GraphQL APIThe best way to learn GraphQL is by getting your hands dirty and building applications with it. Depending on your language of choice, there are GraphQL libraries in many different languages to help you get started.You can start by:Creating a simple project using GraphQL APIs: We recommend this full-stack tutorial on our learning platform that walks you through building a real-time to-do app using GraphQL APIs.Migrating an existing REST API to GraphQL API: We have an array of tutorials in multiple languages (Node.js, Python, Ruby on Rails, Go, Java) to help you get started with this.Integrating a GraphQL API on the frontend: We put together a list of courses that help you get started with integrating GraphQL in your frontend application.As with any new technology, adopting GraphQL APIs for your backend and building high performant GraphQL backends requires experience, which is not peculiar to GraphQL alone.Suppose your application relies heavily on interconnected data, or you aim to develop an API to be used across several clients, or perhaps you are looking for a high-performant API solution for your backend that minimizes bandwidth usage. In that case, GraphQL API is a perfect data layer for your backend.Hasura is an open-source GraphQL API engine that helps you instantly set up a scalable and real-time GraphQL backend, speeding up development time and making your engineering teams super productive. Sign up here."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: What we learnt by migrating from CircleCI to Buildkite Description: We recently rolled out a new CI system at Hasura and we are writing this post to take you on a journey about why and how we did it. This is a collection of ideas and implementation details which we believe to have helped us arrive at an optimal solution that worked out for us. Author: Vishnu Bharathi"}, {"role": "assistant", "content": "We recently rolled out a new CI system at Hasura and we are writing this post to take you on a journey about why and how we did it. This is a collection of ideas and implementation details which we believe to have helped us arrive at an optimal solution that worked out for us.Why did we roll out a new CI system?TLDR: \"To control CI costs\"We have been using CircleCI for a long time as our primary CI. It had been working out pretty well for us. But when we created our monorepo last December, our CI usage approximately doubled in usage. On top of that our engineering team had grown significantly over the months after that. This ultimately led to fat CI bills. We were able to lower our consumption rate by introducing various optimizations during this period in our old system that used CircleCI. But still we felt that there is space for improvement and a quick napkin math revealed that we could do much better with respect to cost. To give a more practical insight, our annual CI spend was projected to be around 100K USD with the old CI system and our goal was to reach 40% to 50% cost reduction with the new system.Apart from the cost factor, we were also looking forward to stepping up our CI experience in general with some features which weren\u2019t provided by our old system. We will be discussing those features too in this blog post.Choosing a new CI platformWhile we were using CircleCI, we had already been experimenting with GitHub Actions on the side. We really liked the workflow dispatch model in GitHub Action which let us start prototyping our automation code by allowing us to trigger it in manual fashion and then automating them via the GitHub workflow dispatch API endpoint whenever we felt confident. We could never get features like this in CircleCI and on the other hand GitHub Action was lacking in some standard features that CircleCI had - example: Ability to rerun only failed jobs in a workflow.These experiments helped us arrive at a baseline for what our new CI system should look like:The CI configuration would be trackable via source control. Example: In the case of CircleCI, we checked-in a huge YAML file in our source code repositories.The new CI system should allow us to get rid of YAML if possible. Why? If not YAML, then what? (well, wait till you read about it later on this blog post)We were typically looking for self-hosted and semi-self-hosted CI platforms. (because they seem to align more with our cost requirements and also give us more control)Ability to run CI jobs on various platforms. We have a variety of workloads which get scheduled on various targets like Linux x86 and arm64 hosts, docker containers, and macOS machines.Autoscaling build nodes. We didn\u2019t want our build nodes to be always running - instead we would like to bring up and bring down them based on the demand.Various triggering strategies - like trigger a build for commit in a branch/pull request (with pattern matching), trigger a build manually, auto-cancel triggered builds when there were newer commits on the pull request..Reasonable pricing, good support and active community.Our original comparison matrix had about 25 criteria from which I have listed a few of them above for brevity.Based on the list we prepared, we took a look at a variety of CI providers like GitHub Action, TeamCity, Azure Pipelines, Buildkite, Concourse, Jenkins, etc.While listing, comparing, and contrasting each of the options that we explored could be a blog post in itself, I am going to fast-track a bit here and give you the CI that won us - It was Buildkite. From here, we will focus on some of the powers that came with it and how we implemented our new CI system on Buildkite.BuildkiteWe went with Buildkite because it checked most of the boxes we had in our requirements. To start with, Buildkite is a semi-self-hosted option, meaning that it allows us to run builds on our own cloud infrastructure rather than on theirs and at the same time it solves the pain point by hosting the web UI, API, and services (like webhook, scheduling) on their infrastructure. We found it to be a good balance to have for our use-case. Some nice things that came as a result of it was that all our source code and secrets stayed within our own infrastructure and we got to enjoy features like Single Sign-On authentication etc.Buildkite\u2019s abstraction of pipelines, builds, steps, jobs, and teams are easy to understand and operate with. It had both the standard and the modern features that we were looking for. But there were still some missing pieces of the puzzle for us. We had to design, build and maintain our own CI infrastructure (when keeping constraints like cost and time in mind).First thing we didOne of the first things we did while starting to develop the CI infrastructure was actually to set up CI on a newly created repository called ci-infra that was supposed to contain the terraform project and packer scripts that will be used to manage the CI infrastructure. Every commit on the main branch would run terraform apply and every commit on a pull request would run terraform plan. We used (and still use) GitHub Action for CI on this repository. Much like how bootstrapping a new compiler relies on another compiler, we decided to bootstrap our CI infrastructure with another CI.Another thing that happened very early on was that we got a dedicated AWS cloud account for the CI infrastructure. This gives a good separation of concerns from the other things we were doing and also it becomes easy to track the cost incurred for the CI infrastructure.We started using (and still do use) the AWS console in read-only mode - mostly to check the bills everyday. In fact one out of the two persons who worked on this project didn\u2019t even have access to the AWS console until halfway through the project. We heavily relied on terraform and git. Having things version controlled helped us a lot while debugging issues, retrospecting, and during casual discussions over slack and virtual meetings. Setting this up and getting to this workflow was a bit difficult initially and we didn\u2019t see immediate benefits, but as our infra code base grew day by day we found it to be hugely beneficial.One (unwritten) ruleWe just had (and still have) one unwritten rule if somebody is trying to make changes to our ci-infra repository. It is \"Make the main branch to be green before you go to sleep\".We are very aware of the fact that a success in terraform plan might not always result in a success of terraform apply. There are cases where the results could be different like consider this snippet of code.resource \"aws_iam_role\" \"build_node_role\" {\n    name = \"BuildNodeRole\"\n    assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n        Action = \"sts:AssumeRole\"\n        Effect = \"Allow\"\n        Principal = {\n        Service = \"ec2.amazonaws.com\"\n        }\n    }]\n    })\n    managed_policy_arns = [\n        aws_iam_policy.build_agent_setup_secret_read.arn,\n        aws_iam_policy.sample_secrets_read.arn,\n        aws_iam_policy.ci_secrets_read.arn,\n        aws_iam_policy.nomad_client_secrets_read.arn,\n        aws_iam_policy.nomad_service_discovery.arn,\n        aws_iam_policy.build_node_metadata_read.arn,\n        aws_iam_policy.build_node_autoscale_helper.arn,\n        aws_iam_policy.build_node_artifact_store_access.arn,\n        aws_iam_policy.dockerhub_public_repo_read_token.arn,\n        aws_iam_policy.hasura_bot_github_token_read.arn\n    ]\n}\nIf we try to add a new managed policy to the iam role here, terraform plan on the pull request would pass and we will be allowed to merge the pull request. But terraform apply would fail on main because the maximum number of managed policies that could be present in an iam role seem to be 10. Terraform is able to catch this rate limit error only when we are trying to apply the changes.Code reviews definitely helped in this regard. But we generally advise people to not be afraid of breaking the main branch on the ci-infra repository. We could always make it green again by either fixing the root cause or just by reverting the commit. As long as we are doing it before going to sleep, we won\u2019t block merges of any other changes that need to go in and we won\u2019t cause much trouble to the people using the CI system.CI Trigger labelsWe don\u2019t automatically run CI for any pull request that gets opened in our monorepo. This is intentional and it used to be this way even in our old system. There are two reason for this:We automatically shadow every pull request we receive in our graphql-engine oss repository to the monorepo (and it is not safe to run CI on all of them by default - you could understand why by reading this blog post from GitHub).We saw a 30% drop in the number of builds when we explicitly asked people to add a label to their pull requests when it is ready for using CI. So this was also a cost optimization scheme.With CircleCI, we used to have a job at the start of our workflow which checks for this label and fails the workflow if the label was not present. When the label was added by a team member, they needed to visit the CircleCI dashboard and rerun the workflow for that particular pull request.With Buildkite, we got this out of the box. All we had to do was mention the conditional settings (refer screenshot). It even took care of automatically triggering a build for the last commit when the label got added to a pull request. (one less step for every team member!)Dynamic pipelines(a.k.a \"How we burned down 2000 lines of YAML\")Most of the CI platforms provide a YAML DSL to configure them. I think most of them came to the realization that YAML is not scaling for complex use-cases. The solution they all seem to introduce for overcoming this seems to be dynamic configuration.In the case of Buildkite, it is known to be \u201cdynamic pipelines\u201d. This very feature allowed us to define all our CI configurations as a Go program. Yep, you read it right - a real programming language with a compiler. We felt it to be more expressive and helpful while building and modifying pipelines.All our Buildkite pipelines that deal with our monorepo contain this to be the initial config.agents:\n  queue: ops-node\n\nsteps:\n  - label: \":pipeline: generate pipeline\"\n    key: \"pipeline-gen\"\n    plugins:\n      - hasura/smooth-secrets#v1.2.1:\n          # sets up some secrets here\n      - hasura/smooth-checkout#v2.0.0:\n          config:\n            - url: git@github.com:hasura/graphql-engine-mono.git\n    command: \".buildkite/scripts/buildkite-upload.sh\"\nIt just runs a shell script whose content looks like:#!/bin/bash\n \nset -eo pipefail\n \nexport BUILDKITE_PIPELINE_NO_INTERPOLATION=true\nexport PATH=\"$PATH:/usr/local/go/bin\"\n \necho \"--- :Buildkite: build information\"\necho \"BUILDKITE_BUILD_ID = $BUILDKITE_BUILD_ID\"\n \necho \"--- :shell: linting CI shell scripts\"\nshellcheck .buildkite/scripts/{.,**}/*.sh -P .buildkite/scripts\n \necho \"--- :pipeline: generating pipeline\"\ncd .buildkite/pipeline-gen\ngo run main.go\n \necho \"--- :pipeline_upload: uploading pipeline.json\"\n/usr/bin/buildkite-agent pipeline upload pipeline.jsonSo it runs a step which runs a go program to generate the json file which configures the rest of the build.This way we totally avoided the need for fiddling with YAML and we just needed to touch Go code and little bash scripts. As you see, these bash scripts are shellchecked before they get used in the generated pipeline.Why Go?Are you still not convinced about our choice to go with a programming language instead of using a configuration language like YAML, TOML, JSON etc.? I will try to sell it a bit harder here.Consider this snippet of Buildkite configuration written as YAML.steps:\n  - command: echo \"Checks out repo at given ref\"\n    plugins:\n      - hasura/smooth-checkout#v1.1.0:\n          clone_url: https://github.com/<username>/<reponame>\n          ref: <ref>\nIt is a simple snippet that clones a repository at a given ref via a plugin called smooth-checkout which we have pinned to be v1.1.0. Let\u2019s say that we need to upgrade that plugin to a newer version which contains breaking changes (v2.0.0) across a large number of instances of it in the YAML code base.steps:\n  - command: echo \"Checks out repo at given ref\"\n    plugins:\n      - hasura/smooth-checkout#v2.0.0:\n          config:\n            - url: git@github.com:<username>/<reponame>.git\n              ref: <ref>\nThe process would require the developer to search for hasura/smooth-checkout in the codebase and upgrade it one by one very carefully. Even with much care, there is a chance that it will get misconfigured or missed out.Let\u2019s say that we did the same task using Go: The first thing we would do is go to the place where the struct for the plugin is defined.package plugin\n\ntype SmoothCheckout struct {\n\tCloneURL     *string `json:\"clone_url,omitempty\"`\n\tRef          *string `json:\"ref,omitempty\"`\n\tSkipCheckout *bool   `json:\"skip_checkout,omitempty\"`\n}\n\nfunc (SmoothCheckout) Name() string {\n\treturn \"hasura/smooth-checkout\"\n}\n\nfunc (SmoothCheckout) Version() string {\n\treturn \"v1.1.0\"\n}\nAnd change the struct definition like this:package plugin\n \ntype SmoothCheckoutConfig struct {\n URL string  `json:\"url\"`\n Ref *string `json:\"ref,omitempty\"`\n}\n \ntype SmoothCheckout struct {\n Config       []SmoothCheckoutConfig `json:\"config,omitempty\"`\n SkipCheckout *bool                  `json:\"skip_checkout,omitempty\"`\n}\n \nfunc (SmoothCheckout) Name() string {\n return \"hasura/smooth-checkout\"\n}\n \nfunc (SmoothCheckout) Version() string {\n return \"v2.0.0\"\n}\nNow we could effortlessly use the Go compiler\u2019s help. Running go run main.go would lead to a bunch of compiler errors which indicate the spots where the structure of the plugin needs to be updated. If you have a fancy editor setup that integrates with Go tools, your workflow would seem something like this:You could easily click through the \"Problems\" which should get you the spots you are looking for or click on the files denoted in red color to fix problems on that file.And on top of this workflow, it ensures that if you make your Go program to compile locally we won\u2019t have any structural problems.Apart from this, it becomes very easy to refactor common functionalities as funcs and reuse them often. Example: we use this little func across many different pipelines to allow all the steps in a given pipeline to be manually retryable.func makeAllStepsToBeRetryable(p *pipeline) {\n for idx, s := range p.Steps {\n   cmdStep, ok := s.(step.Command)\n   if ok {\n     if cmdStep.Retry == nil {\n       cmdStep.Retry = &step.CommandRetry{}\n     }\n     if cmdStep.Retry.Manual == nil {\n       cmdStep.Retry.Manual = &step.ManualRetry{\n         PermitOnPassed: util.Bool(true),\n       }\n     }\n     p.Steps[idx] = cmdStep\n   }\n }\n}Agent QueuesOur CI jobs varied in the hardware requirements. Example: Hasura\u2019s graphql-engine server is written in Haskell and compiling it in our CI pipeline needed much greater hardware than compiling the Hasura CLI which was written in Go and relatively compiles quicker with much lesser hardware requirements.CircleCI provided us with resource classes to handle this use case. But in the case of Buildkite, we had to introduce a similar setup using a concept called agent queues. Whenever we brought up a build machine which was running Buildkite agent (as a systemd service), we configured it to listen to a particular queue. When new CI jobs are triggered and if they mention an agent queue in its configuration, then Buildkite takes care of scheduling that on a build node where the Buildkite agent was listening to that particular queue. If the build node was busy handling some other job, it would be queued and picked up when the build node got free.Here are the common agent queues that we provide in our setup.Apart from these, we have a queue called ops-node (which should have been called default, but it is too much of a legacy to change right now). It is used for scheduling the first step that generates the rest of the pipeline (you might notice it in the YAML configuration given as the example in the dynamic pipeline section above)Autoscaling build nodesAt any given point in time, if no-one is using our CI system then the number of build nodes that are running would be zero (example: weekends). We spin up build nodes only when needed. We evaluated various systems for achieving this. One of our first choices was Buildkite\u2019s Elastic CI Stack for AWS. It was okay, but it felt overwhelming. On top of that it was using Cloud Formation (which is configured via YAML files) for managing the cloud resources. We wanted our setup to be simpler to understand and operate and fully-managed by terraform (because our team is comfortable with it and it is cloud agnostic). We also experimented by modelling Buildkite jobs as nomad jobs after we came across this, thinking that we would rely on nomad autoscaler for autoscaling the build nodes. But the experiment complicated our setup and the nomad autoscaler was still in its initial stages so we could barely get it to work. So we had to abandon the experiment and pursue a simpler solution.We started digging into the internals of theElastic CI Stack thinking it might lead to a simpler solution. It did! We ended up discovering one of the important components of the Elastic CI Stack, which is buildkite-agent-scaler. It is a Go program which could be run periodically to do the following:Get the number of \"waiting\" jobs from Buildkite API for a given agent queueGet the desired node count of an AWS autoscaling group (in which a build node for a particular agent queue will be spun up)If \"waiting jobs count\" is greater than \"desired node count of AWS autoscaling group\", set a higher desired count, so that enough new nodes come up for handling the waiting jobs.Perfectly simple! We took this program and ran it as an AWS lambda function. Here is how the entire \"scale-out\" process looks likeEach agent queue has a respective AWS lambda function which does the \"scale-out\" computation and scales out the auto scaling group. The lambda functions were triggered using an event rule in AWS EventBridge, which would trigger these functions regularly at an interval of 1 minute. That\u2019s a lot of function invocations per day. Won\u2019t it increase the cost of the system? Well, it seems like it won\u2019t. For an accurate idea, we incurred 16 USD in AWS lambda charges for a month with this setup.Up until now we saw \"scale-out\", but what about \"scale-in\" (the process in which we bring down build nodes when they are not being used)? buildkite-agent-scaler has a flag for it but it is usually turned off both in our setup and in case of Buildkite\u2019s Elastic CI stack. Because scaling-in is just the matter of updating the desired count of the autoscaling group to a lower value and AWS autoscaling group would take out any random node in the group to achieve the desired count. What if that random node is a node that was actively performing a CI build. To workaround this we use this approach that Buildkite suggested here (a little bit of systemd magic)So finally, we have our auto scaling setup which is cheap and is fully managed via terraform!ArtifactsCurrently a single run of \u00a0\"build and test\" pipeline in our monorepo seems to produce close to 10GB of artifacts and we would like to save them at least for a month. Initially we used Buildkite provided artifact store for storing these. All these were going via the internet through our NAT gateway. This in-turn led to skyrocketing costs for the week when we introduced it. When we took a closer look at how we could optimize it, we found the following solution.Step 1: Host the artifacts in an S3 bucket in our AWS CI infrastructure.Step 2: Route the S3 traffic via a VPC Gateway EndpointDoing the second step was crucial because it helped us avoid all the data transfer charges we incurred before - this was massive for us. As a follow up to this implementation we are planning to introduce various retention policies for artifacts which would help us to automatically clean up older artifacts and keep the cost in control.We had a new problem: The artifacts stored in the private S3 bucket were not accessible via the Buildkite dashboard. We would have the list of artifact names like shown in the picture below, but clicking on them would result in \"access denied\" because Buildkite doesn\u2019t have AWS keys which could retrieve them from a private S3 bucket.The solution was to host an S3 proxy behind Cloudflare Access to access it. Currently we use pottava/aws-s3-proxy as the S3 proxy server with basic auth disabled and instead use Cloudflare Access for the auth layer. The proxy runs alongside cloudflare daemon (for tunneling connection to cloudflare) on a t2.micro instance (1vCPU, 1GB RAM). Now when the artifact links in Buildkite are clicked, it redirects to the Cloudflare Access page where we need to sign-on via Hasura email.SecretsHaving personally used hosted CI solutions for a long time, it felt a bit different to know that Buildkite does not provide a managed secrets manager which could be used by the builds. But while building this system, it all made sense!First we experimented with Hashicorp\u2019s Vault for storing our CI secrets. We really liked using it, but bringing up a production ready Vault was time consuming and we were a little doubtful about the uptime guarantees that we could give for the team since it is self-hosted and we didn\u2019t have monitoring setup at all at that point. So we pivoted to using a managed solution: \"AWS Secrets Manager\". It was simple, reliable, and within our budget. The only bottleneck is in-order for adding a new secret to CI setup, a team member would need access to the CI AWS account (and not everyone has it). If it was Vault, we could have used a Cloudflare Access like approach (that we saw above) for members of the team to easily access and upload new secrets. But we are ok with this tradeoff since \"adding a secret\" to CI environment happens rarely.Using AWS IAM for identity management while accessing Secrets Manager was seamless and I would say that this saved us quite a bit of time. To give an idea about the cost incurred: secrets manager had received 1 million+ requests in one month while running builds in the new system. The total cost incurred for this was 20 USD.CachingThis is a place where we are lacking a bit and are actively working on. Most of the managed CI services like CircleCI have some kind of caching features which would help us to run builds faster. How they do it is a blackbox for us. (and we are forced to pay for it)We typically want to cache the following things:We run most of the builds as docker containers. So we would definitely like to cache the docker images which are used for running these CI jobs.Any software programs (like command line tools) that are going to be used during the build.Software libraries (which are installed by package managers like npm, cabal, etc.)All the build nodes use a custom built AMI (which could be built by a GitHub Action present in our ci-infra repository) and all the docker images which are used for running the builds are being pulled and saved in the AMI. This significantly speeds up the builds by not pulling those images from docker hub everytime. (which means we could avoid the NAT gateway data transfer charges leading to cost savings)If we need new software programs for running a build, they could usually be added to the Dockerfile of the image which runs that build. For machine level dependencies, they could edit the packer configuration in ci-infra. In both cases, we will need to build a new AMI for the caching to be effective. This is where we currently lack automation. New docker images are built via a Buildkite job and AMI building happens in GitHub Action. We will need to connect these two for a seamless experience.Software libraries caching is still an ongoing issue. So, I don\u2019t have much to share there.Monitoring and debuggingWe have a very basic monitoring setup from the early days, but we haven\u2019t been using it much. The setup includes running prometheus and alert manager on a t2.micro instance. It currently uses buildkite-agent-metrics for collecting some information from Buildkite. (We plan on using it more)For alerts we have a dedicated slack channel called #cicd-monitor. Apart from the alerts from the alert manager, the channel also listens over a few RSS feeds on services we depend on for CI/CD and internal tooling like Buildkite, GitHub, Vercel, etc.Most CI platforms contain a \"Rerun with SSH\" button for getting SSH access to the build node for debugging purposes. In our experience, this is a rarely used feature but still a valuable one when debugging difficult CI failures. We currently do not have a solid workflow for this: bringing up bastion nodes and adding the SSH key of the team member who needs to the build machine seems to be the way now. But we are hoping to use browser rendered terminal via Cloudflare Access to make this a seamless experience. (that way team members would use Single Sign-On to get SSH access instead of SSH keys)FutureI hope this gave you a good idea about how we tackled different problems while rolling out a new CI system and the tradeoffs we made. There are still future improvements to be done and we are actively working on them.One such future work is the ability to share builds across our mono repo and oss repo to avoid running builds multiple times. To implement this effectively, we would need this feature from Buildkite. We may need to pause CI on our OSS graphql-engine repository (which is currently using the old CI system) to roll out the new CI system for it too. So, we kindly request our OSS contributors to bear with us during that time frame.Some other future work involves solving some ratelimit problems we are facing intermittently during busy hours (if you are looking for an OSS project to contribute to, you could help us solve this), improve our caching setup and workflows, ramp up our monitoring and debugging setup, using Buildkite\u2019s terraform provider to manage pipelines, etc.The new CI system was running on our internal monorepo for about 45 days now. It seems to be quite stable (no major outages during the entire project). While it is hard to tell if we will hit our cost goals (since it is very early to make a prediction), we still feel that we are staying on-track for it - we will let you know how it goes!Finally, I would like to thank our entire engineering team for helping us when we needed some help and being patient throughout the entire time."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: AWS Service Orchestration with GraphQL and Hasura Cloud Description: AWS has tons of services which can be integrated with Hasura Cloud for various functionalities and use cases. In this post, we will look at how to leverage them from a 3factor app architecture and event driven serverless pattern. Author: Praveen Durairaju"}, {"role": "assistant", "content": "AWS has tons of services which can be integrated with Hasura Cloud for various functionalities and use cases. In this post, we will look at how to leverage them from a 3factor app architecture and event driven serverless pattern.Hasura Cloud gives you a GraphQL API instantly. But we need to connect a database to start building the app. Further, we need an authentication service to manage users and setup a permission system. Finally for all sorts of custom business logic, we can make use of serverless functions.AWS Service Orchestration with GraphQL and Hasura CloudBroadly, we will make use ofAWS RDS for DatabaseAWS Cognito for AuthenticationAWS Lambda for Events and Custom Business LogicBefore we get started, lets quickly setup a Hasura project.Create a Hasura Cloud ProjectClick on the following button to create a new free project on Hasura CloudOnce you create a new free tier project, launch the console and head to the Data tab to connect the database. We will look at how to create the database on AWS RDS to complete this step.Create AWS RDS PostgreSQLLogin to the AWS Console.Create a new database with AWS RDS and select PostgreSQL. You can skip this step if you already have an existing database.Allow public access and assign a VPC security group. This ensures that the database can be accessed from the outside world.Configure Hasura Cloud IP in inbound rules. You can fetch the IP details from the project settings of Hasura Cloud dashboard. This is important because, the inbound rule will allow the Hasura Cloud instance to communicate with AWS RDS instance. Of course, if you end up adding the IP as 0.0.0.0/0 the database instance can be accessed by everyone on the internet.The Database URL format postgresql://<user-name>:<password>@<public-ip>:<postgres-port>/<db> can be constructed so that it can be used to connect in the next step. The user name and db will most likely be default values of postgres and the port would be 5432. Primarily the password and the host IP value is the one that gets changed on the AWS dashboard.For detailed steps, please refer to the docs on Using Hasura Cloud with AWS RDS PostgreSQL.Now lets connect the database to Hasura Cloud using the project console.Hasura Cloud Connect Existing DatabaseOnce the database is connected, we can head to table creation. Lets say we create a table called author with columns id, name, email, created_at. As a next step, we will setup an Authentication service with Cognito.Setting up Amazon CognitoAmazon Cognito lets you setup user sign up/sign in for your applications quickly. It also supports social sign-in providers like Google, Apple, Facebook etc among other connectors. You can configure Cognito to generate JWT.Create user poolsAdd an app client and note down the client ID.Configure app client settings, callback and signout URLs. Note that callback URLs can be localhost URLs too.Enable Implicit Grant Flow for JWTChoose a domain name that will be used for the hosted UI page.Navigate to the Hosted UI page - https://your_domain/login?response_type=token&client_id=your_app_client_id&redirect_uri=http://localhost:3000/callbackIn the above URL, replace the parameters like client_id and redirect_uri accordingly. Alternatively you can click on the Launch Hosted UI link on the Cognito Dashboard App Settings to directly land on the UI that facilitates user signup / signin.Once this is done, we need to modify the claims that are used to generate the token. So here we will make use of Lambda to create a serverless function which injects the custom claims.Add Custom JWT Claims for HasuraNavigate to AWS LambdaCreate a new function. Copy the following handler code to generate custom claimsexports.handler = (event, context, callback) => {\n    event.response = {\n        \"claimsOverrideDetails\": {\n            \"claimsToAddOrOverride\": {\n                \"https://hasura.io/jwt/claims\": JSON.stringify({\n                    \"x-hasura-user-id\": event.request.userAttributes.sub,\n                    \"x-hasura-default-role\": \"user\",\n                    // do some custom logic to decide allowed roles\n                    \"x-hasura-allowed-roles\": [\"user\"],\n                })\n            }\n        }\n    }\n    callback(null, event)\n}Note that, we are using default values for x-hasura-role. You are free to write any custom business logic before assigning the role name to the key.In Cognito, under Triggers, configure Pre Token Generation handler and select the lamdba function we just created above. This will ensure that the lambda function is triggerred everytime a token generation workflow is happening in the background.Head to App Client Settings and click on Launch Hosted UI. Signup with a user and copy the id_token portion. Note that you will be redirected to the callback URL, likely the localhost URL that was configured earlier and hence you might get an error. But you can ignore the UI and focus on the URL parameters to extract just the id_token value.Test the JWT in the debugger of jwt.io. Paste the id_token here to inspect the payload and verify the signature.The next step is to facilitate Hasura Cloud to validate the tokens being used for the requests. For this, we need to configure Hasura Cloud with the Cognito configuration.Configure Hasura Cloud ENVCopy the following config for HASURA_GRAPHQL_JWT_SECRET env. {\n    \"type\":\"RS256\",\n    \"jwk_url\": \"https://cognito-idp.<aws-region>.amazonaws.com/<user-pool-id>/.well-known/jwks.json\",\n    \"claims_format\": \"stringified_json\"\n}Substitute the aws-region and user-pool-id from the URL parameters / General settings.Create permissions for the role userWe had created the table author earlier. Now lets setup permissions for this table to allow only authors to query for their own data.Head to the table permissions tab, create a new role called user and apply a filter for id column to map to x-hasura-user-id. Finally we will create a serverless function to perform some custom business logic through async eventing system. We will make use of AWS Lambda for the functions.Set up Lambda for Hasura EventsCreate a simple function on Lambda.Add a route on API Gateway to expose the function outside.Add the endpoint to Hasura events to test an Event Trigger on a database table.There are still more services like the S3 for storage from AWS which fit in well with the GraphQL and Hasura Cloud ecosystem. I would like to understand from you folks on what are the different ways you use AWS services with Hasura Cloud and what do you like to get a more in-depth tutorial on. Do drop in the comments."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Meet Hasura at AWS re:Invent Description: This event is over\n\nIt\u2019s Hasura\u2019s first year at re:Invent and we\u2019re excited to meet everyone from\nthe Hasura Community who\u2019s going to be there!\n\n\nHere are all the ways you can find us during the conference:\n\nStop by our booth for demos, expert consultations and to meet the team that\u2019s\nbuilding Hasura!\n\nWe'll be at booth #402. Stop by for some fun Hasura demos, pick up some of our\nspecial re:Invent stickers and meet our team including our co-founders. We\u2019re\nright by the entrance, but here\u2019s a lit Author: Shraddha"}, {"role": "assistant", "content": "This event is overIt\u2019s Hasura\u2019s first year at re:Invent and we\u2019re excited to meet everyone from the Hasura Community who\u2019s going to be there!Here are all the ways you can find us during the conference:Stop by our booth for demos, expert consultations and to meet the team that\u2019s building Hasura!We'll be at booth #402. Stop by for some fun Hasura demos, pick up some of our special re:Invent stickers and meet our team including our co-founders. We\u2019re right by the entrance, but here\u2019s a little map of where to find us:Hasura will be at Booth 402We\u2019ll also host 1:1 consultations with our technical team at the booth, including our CEO Tanmai. Just to make sure we don\u2019t miss anyone, do sign up in advance here for a personalized 15-minute session to discuss topics like:Your GraphQL API performanceData Federation & creating a unified data layer with HasuraAuthorization, Caching and advanced GraphQL3 factor app architectureGraphQL Strategy for your organization with incremental adoptionHappy Hour with Hasura at re:Invent 2021Since it\u2019s our first year at re:Invent, we\u2019re celebrating it with a Happy Hour at The Wynn. We\u2019d love to have you there! Sip on a drink and chat with the Hasura team about the Hasura roadmap, all things Hasura and your GraphQL & data strategy. It\u2019s also a chance to meet other Hasura users and learn about how they\u2019re using Hasura. Seating is limited so reserve your spot soon, and we\u2019ll be in touch to confirm! Tuesday, November 30, 6:00 pm - 8:00 pm, East Side Lounge at The WynnReserve your t-shirtFinally, what\u2019s a conference without exciting swag? We\u2019ve got limited numbers of some special Hasura t-shirts! Just reserve your t-shirt here and drop by our booth during the conference to pick it up. Hope to see you at re:Invent soon! "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Getting started with React Query and GraphQL Description: React Query is a powerful client that works hand-in-hand with GraphQL for minimum boilerplate and ultimate flexibility. Author: Jesse Martin"}, {"role": "assistant", "content": "In this article, we\u2019ll look at a few strategies to implement GraphQL with React Query. In a previous article, we talked about what React Query is, and how to get started with it. This is the follow-up to that post about how to work with GraphQL specifically.Previously, we left our basic repository fetching GraphQL with the window fetch library. Which works! However, this simple approach doesn't let us unlock the full potential of GraphQL. In this guide, we'll set up our project to take full advantage of asynchronous GraphQL with subscriptions, and auto-generate our hooks using GraphQL codegen with the React Query plugin.Updating our base exampleIn the previous example, we simply fetched some content from Hasura and showed the results. Well, that\u2019s not the most engaging experience. Especially since our model takes advantage of an event-driven paradigm where we can upload HEX colors, and will get a complementary color scheme automatically returned, which means that we have some asynchronous behavior that would benefit from at least mutations (uploading our colors) and subscriptions (updating with our returned colors). If you followed along with the previous example, our new application now has an input field, which lets us provide these colors.We\u2019ve added a couple of additional utilities to help check for color lightness which will let us toggle light or dark text depending on the color we are overlaying on.Invalidating Cache vs SubscriptionReact Query gives us a number of methodologies for updating our content. We can query our colors, which stores the result in the cache. When we mutate (update/add/remove) a color, we can use a success callback to clear that cache and update with the results we got back from Hasura. The other option is to create a subscription if our query has asynchronous content which will allow us to update the results when they become available. We'll provide examples of both approaches.Invalidating CacheWe'll add a mutation with the following code, still using fetch. In order to benefit from our existing query, we are going to update our global cache with the response of our mutation. React Query gives us a couple of methods to update our cache. We can either call a cache invalidation call after we successfully update from the server, or we can update the cache in a success handler of our initial mutation. We'll opt for the later.//Dependency\nimport { useMutation, useQueryClient } from \"react-query\";\n\n// Client Reference\nconst queryClient = useQueryClient();\n\n// Mutation Hook\nconst { isLoading, isError, isSuccess, data, mutate } = useMutation(\n    (hex) =>\n      fetch(\"https://intent-shad-91.hasura.app/v1/graphql\", {\n        method: \"POST\",\n        body: JSON.stringify({\n          query: `\n        mutation InsertColorOne( $hex: String ){\n          insert_color_one(object: {color: $hex}) {\n            color\n            complementary_colors {\n               color\n            }\n          }\n        }\n      `,\n          variables: {\n            hex,\n          },\n        }),\n      })\n        .then((res) => res.json())\n        .then((res) => res.data),\n    {\n      onSuccess: ({insert_color_one}) => {\n        queryClient.setQueryData([\"data\"], ({color}) => {\n          return { color: [...color, insert_color_one] };\n        });\n      },\n    }\n  );\n\n\nCreating SubscriptionsReact Query doesn't support subscriptions natively, which are a crucial part of what makes GraphQL so powerful! However, React Query does provide all the tooling we need to work with subscriptions using whichever fetching mechanism we would want, in this case, we'll write a simple WebSocket wrapper to fetch our content. A big shout out to this user on Github for bulk of the reference code!To refactor for subscriptions, we'll create a useColorSubscription hook. This will implement graphql-ws and on successful message, we'll update React Query's cache. For UI design, I've exported a state manager to let us know when the subscription is attempting to make the initial query, and after we've received the data.import { useEffect, useState } from \"react\";\nimport { useQueryClient } from \"react-query\";\n\nconst url = \"ws://intent-shad-91.hasura.app/v1/graphql\";\n\nexport const useColorSubscription = () => {\n  const queryClient = useQueryClient();\n  const [isSubscribing, setIsSubscribing] = useState(false);\n  const [isSubscribingSuccess, setIsSubscribingSuccess] = useState(false);\n\n  useEffect(() => {\n    const ws = new WebSocket(url, \"graphql-ws\");\n    setIsSubscribing(true);\n\n    ws.onopen = () => {\n      ws.send(JSON.stringify({ type: \"connection_init\", payload: {} }));\n      ws.send(\n        JSON.stringify({\n          id: \"1\",\n          type: \"start\",\n          payload: {\n            // variables: {},\n            extensions: {},\n            operationName: \"GetColors\",\n            query: `subscription GetColors {\n                color {\n                  color\n                  complementary_colors {\n                    color\n                  }\n                }\n              }`,\n          },\n        })\n      );\n    };\n\n    ws.onmessage = (event) => {\n      const msg = JSON.parse(event.data);\n\n      if (msg.type == \"data\") {\n        setIsSubscribingSuccess(true);\n        setIsSubscribing(false);\n        const data = msg.payload.data.color;\n        queryClient.setQueriesData(\"colors\", data);\n      }\n    };\n\n    return () => {\n      ws.send(JSON.stringify({ id: \"1\", type: \"stop\" }));\n      ws.close();\n    };\n  }, []);\n  return { isSubscribing, isSubscribingSuccess };\n};\n\n\nWith this in place, we'll refactor our main application code that loops over this cache. Specifically, we'll be initializing the cache to an empty array, and preventing the useQuery call to refetch as our subscription will handle all our updating logic.  const { data } = useQuery(\"colors\", () => [], {\n    refetchOnMount: false,\n    refetchOnWindowFocus: false,\n    refetchOnReconnect: false,\n    refetchIntervalInBackground: false,\n  });\n\n  const { isSubscribing, isSubscribingSuccess } = useColorSubscription();\n\nNow we can loop over the content in our application.{isSubscribing && <p className=\"text-2xl text-gray-200\">Loading...</p>}\n        {isSubscribingSuccess &&\n          data.map((item, key) => {\n            return ({/* My Code */})\n          }\n        </div>\n    );\n})}\n\nWe can refactor our mutation now as well to not handle any cache updates on its own.const { isLoading, isError, isSuccess, data, mutate } = useMutation((hex) =>\n    fetch(\"https://intent-shad-91.hasura.app/v1/graphql\", {\n      method: \"POST\",\n      body: JSON.stringify({\n        query: `\n        mutation InsertColorOne( $hex: String ){\n          insert_color_one(object: {color: $hex}) {\n            color\n            complementary_colors {\n               color\n            }\n          }\n        }\n      `,\n        variables: {\n          hex,\n        },\n      }),\n    })\n      .then((res) => res.json())\n      .then((res) => res.data)\n  );\n\nSwitching to subscriptions comes with some performance tradeoffs, as well as additional code to maintain, but since this example includes asynchronous code (our complementary color scheme) the trade off is warranted, and React Query gives us easy access to all the internal methods we'd need to work with the cache in a logical manner.One primary pieces is failing us at the moment, and that is that we are still writing fetch calls by hand. This works for a couple of one-off queries, but there's a better way! Let's look at incorporating a GraphQL library to help make this cleaner.Implementing GraphQL CodegenWe'll follow the onboarding guide as outlined by the codegen installation guide. The first step will be to extract our mutation out of our Input component and into a separate file. I'm putting mine in a subdirectory called queries/mutation InsertColorOne($hex: String) {\n  insert_color_one(object: { color: $hex }) {\n    color\n    complementary_colors {\n      color\n    }\n  }\n}\n\n\nFrom there we can invoke the CLI from codegen by calling yarn graphql-codegen init - be sure to have installed the dependencies first!// Dependencies for generator script\nyarn add graphql\nyarn add -D @graphql-codegen/cli\n\n// Init Script\nyarn graphql-codegen init\n\nFor supporting React Query, we'll need to add an extra plugin. You can follow the React Query plugin steps here, but ultimately we'll need to install yarn add -D @graphql-codegen/typescript-react-query and then install the remaining dependencies added from the onboarding script with yarn.In the end, your codegen.yml file should look similar to the following:overwrite: true\nschema: \"https://intent-shad-91.hasura.app/v1/graphql\"\ndocuments: \"./src/queries/**/**.gql\"\ngenerates:\n  src/generated/graphql.ts:\n    plugins:\n      - \"typescript\"\n      - \"typescript-operations\"\n      - \"typescript-react-query\"\n    config:\n      fetcher:\n        endpoint: \"https://intent-shad-91.hasura.app/v1/graphql\"\n  ./graphql.schema.json:\n    plugins:\n      - \"introspection\"\n\nAfter running our codegen, we are able to replace all of our fetch code with a simple h0ok, reducing 21 lines of code in just this one example!// Imports\nimport { useInsertColorOneMutation } from \"./generated/graphql\";\n\n// Instantiate\nconst { mutate, isLoading, isSuccess } = useInsertColorOneMutation();\n\n// Use\n() => {mutate({ hex })}\n\nObviously this project is a bit contrived. But it's pretty to look at and it imparts some helpful patterns, made simple with React Query. Even in the most complicated scenarios such as supporting real-time content with generated GraphQL SDKs, React Query has a robust set of tooling to make it simple to implement by removing massive amounts of boilerplate.If you'd like to have a look yourself, you can find the complete updated example at this Github repo."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Getting started with React Query Description: React Query is a lightweight but powerful data-fetching manager for React. In this tutorial, we'll learn the basics of what it is, and how to get started quickly! Author: Jesse Martin"}, {"role": "assistant", "content": "In this article, we\u2019ll look at the React Query library, how it compares to its alternatives, what its unique product offering is, and how to get started, quickly.This post is meant for beginners or those brand-new to React Query. The documentation for React Query is excellent and you can read in-depth use-cases and pattern guidance at the React Query documentation.What is React QueryReact Query is a data management library for React, as the name implies. It handles all the major use-cases one would expect for a data fetching library in today\u2019s app ecosystem such as desktop and mobile (React/React Native) support, configurable caching strategies, and various methods for SSR, SSG, and dynamic rendering.Worth noting as a special callout, \u201cQuery\u201d here has nothing to do with GraphQL. In fact, any function that returns a Promise is considered a query. As long as the library you are using is able to return a Promise, you are able to work with React Query out of the box. There are some utilities for working with Websockets which we won\u2019t get into here, but this is a great write-up on the topic if you really want to roll GraphQL subscription support for React Query.React Query solves a lot of the problems that React developers experience with an \u201copinionated-first\u201d approach, meaning less overall boilerplate, but with enough escape-hatches for deep customization.I emphasize the word React developer because part of what makes React Query so special is the narrow problem set it sets out to solve.How does React Query compare with Apollo, Urql, et al?React Query, as the name implies, and as mentioned, is for React. That means it\u2019s able to leverage existing React features under the hood like contexts. It knows which mechanisms developers have to work within React and leans into them, providing a hook-driven approach that is by now second nature to most React developers. Let\u2019s consider this comparison more deeply.Scoped to ReactWhereas tooling like Apollo and Urql are able to do much of what React Query can do as well, often you need to write a lot of boilerplate to adjust them to work with React as the tooling is designed to solve for a very wide use-set. With React itself expanding the problem-set it solves, i.e. server-side components, most developers who work with React don\u2019t often need to reach for additional tooling. Having a robust query library tailor-made (or Tanner-made, if you will) for the framework you work with means that the tooling is independently motivated to stay current with your framework of choice.Scoped to data fetchingReact Query only deals with data fetching. Urql and Apollo both concentrate on GraphQL, React Query focuses on data fetching. It\u2019s lower level in the stack, which allows you to embrace your query tooling of choice, like GraphQL Codegen, for example. Scoped to Server StateOne of the primary ways that React Query differentiates from the other libraries is that it relies heavily on \u201cserver-state\u201d as being the global source of truth. All of its paradigms are focused on solving problems around syncing data with a server. Where there are a lot of state management libraries, and a lot of data fetching management libraries, and a lot of libraries attempting to do both, React-Query focuses on solving the sync of data with the server. This again reduces the overall amount of boilerplate needed and is a paradigm that fits really well with Hasura\u2019s own thinking about application development.React Query Basic ConceptsAt the top level, React Query provides a client wrapper that handles global data caching and fetching. function App() {\n   return (\n     // Provide the client to your App\n     <QueryClientProvider client={queryClient}>\n       <Todos />\n     </QueryClientProvider>\n   )\n }\n\nThe library then provides a series of hooks that plug into this global controller for use with any fetching method that returns a promise. Even plain fetch will work!   const query = useQuery('todos', getTodos)\n\nRefetch defaultsAs part of the library\u2019s intention to sync data with a server, queries get automatically run on sensible actions like window focus and reconnect. More information about React Query\u2019s smart defaults can be found here. These can obviously be fine-tuned, especially during development where refocus will happen quite a bit more than in production scenarios.Initial data for hydrationServer-side hydration is becoming a most for a good user experience in many applications and this is a standard functionality with React Query.Dependent queries and success/error event handlersOne of the more innovative features is the ability to set a boolean configuration value for when a query should run, this is a very helpful mechanism to have for cases where sub-components need to wait for an initial query to run. Additionally, you can handle success and error events directly from the query itself for cascading and eventful behavior.Familiar data methodsAs is becoming a familiar pattern for data-fetching libraries, React Query hooks will return a set of \u201cisIdle\u201d, \u201cisLoading\u201d, \u201cisError\u201d, and \u201cisSuccess\u201d. In addition, you\u2019ll all get an error and data object with the actual contents of those two states. Paired with onSuccess handlers, query invalidation, and more, you have a robust toolset for building data-intensive applications.Restating server-state firstIt\u2019s worth restating here as an important concept, but essentially, almost all use-cases where you\u2019d want a helper for fetching your content directly from the server are present. If you\u2019re looking for a local-state heavy utility library, this is not it. But if you want to persist your state on the server and sync the most efficient way possible with an elegant interface and set of utilities, give React Query a try!React Query for HasuraConnecting React Query to Hasura is as simply as making a GraphQL request to the Hasura server. As mentioned before, any library that returns a Promise can be called by React Query and added to it\u2019s smart cache. Let\u2019s look at a simple example.For this project, we\u2019ll use Create React App, and for styles, we\u2019ll utilize Tailwind. Tailwind means we need to use Craco for running our scripts since react-scripts doesn\u2019t let us override the PostCSS settings. None of these implementation details impact how React Query works and are thus out of the scope of this tutorial.For the index.js path, we're going to add our Tailwind CSS file index.css, our QueryClientProvider and the ReactQueryDevtools which is a great utility for debugging our queries.import React from \"react\";\nimport ReactDOM from \"react-dom\";\nimport { QueryClient, QueryClientProvider } from \"react-query\";\nimport { ReactQueryDevtools } from \"react-query/devtools\";\n\nimport \"./index.css\"; // Optional\nimport App from \"./App\";\nimport reportWebVitals from \"./reportWebVitals\";\n\nconst queryClient = new QueryClient();\n\nReactDOM.render(\n  <React.StrictMode>\n    {/* Provider */}\n    <QueryClientProvider client={queryClient}>\n      {/* Dev tools */}\n      <ReactQueryDevtools initialIsOpen={false} />\n      <div className=\"bg-gray-800 h-screen\">\n        <div className=\"container max-w-5xl mx-auto\">\n          <App />\n        </div>\n      </div>\n    </QueryClientProvider>\n  </React.StrictMode>,\n  document.getElementById(\"root\")\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n\n\nAnd that's all we need to add. Everything else is either default CRA or some style helpers.Next, we need to make an actual request for our data. We'll do that in the App.js file.\n// App.js\n\nimport { useQuery } from \"react-query\";\nimport ColorChip from \"./ColorChip\";\nimport ComplementaryColors from \"./ComplementaryColors\";\n\nfunction App() {\n  const { isSuccess, data } = useQuery(\"data\", () =>\n    fetch(\"https://intent-shad-91.hasura.app/v1/graphql\", {\n      method: \"POST\",\n      body: JSON.stringify({\n        query: `\n      {\n        color {\n          color\n          complementary_colors {\n            color\n          }\n        }\n      }\n      `,\n      }),\n    })\n      .then((res) => res.json())\n      .then((res) => res.data)\n  );\n\n  return (\n    <div>\n      {isSuccess && (\n        <div className=\"grid grid-cols-4 gap-4\">\n          {data.color.map(({ color, complementary_colors }, key) => {\n            return (\n              <div\n                className=\"w-full h-64 rounded-sm flex flex-wrap space-between\"\n                key={key}\n                style={{ backgroundColor: color }}\n              >\n                <ColorChip color={color} />\n                {complementary_colors && (\n                  <ComplementaryColors colors={complementary_colors} />\n                )}\n              </div>\n            );\n          })}\n        </div>\n      )}\n    </div>\n  );\n}\n\nexport default App;\n\n// ColorChip.js\n\nimport React from \"react\";\n\nexport default function ColorChip({ color }) {\n  return (\n    <div className=\"flex items-center w-full\">\n      <p className=\"font-bold text-center w-full\">{color}</p>\n    </div>\n  );\n}\n\n\n// ComplementaryColors.js\n\nimport React from \"react\";\nimport ColorChip from \"./ColorChip\";\n\nexport default function ComplementaryColors({ colors }) {\n  return (\n    <div className=\"w-full grid grid-cols-3 h-24 self-end\">\n      {colors.map(({ color }, key) => {\n        return (\n          <div\n            className=\"h-full items-center flex\"\n            key={key}\n            style={{\n              backgroundColor: color,\n            }}\n          >\n            <ColorChip color={color} />\n          </div>\n        );\n      })}\n    </div>\n  );\n}\n\n\n\nThat's all we needed to add. The magical part we queried from our server was right:const { isSuccess, data } = useQuery(\"data\", () =>\n    fetch(\"https://intent-shad-91.hasura.app/v1/graphql\", {\n      method: \"POST\",\n      body: JSON.stringify({\n        query: `\n      {\n        color {\n          color\n          complementary_colors {\n            color\n          }\n        }\n      }\n      `,\n      }),\n    })\n      .then((res) => res.json())\n      .then((res) => res.data)\n  );\n\nSimply passing in a fetch request with a GraphQL body and we get state management for free. Pretty neat!If you'd like to have a look at the whole project, you can find this React Query start template here.Next StepsNot everyone wants to write GraphQL in raw fetch requests, however. In our next article, we'll look at integrating React Query with GraphQL for even better developer productivity!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How to request a GraphQL API with Fetch or Axios Description: We will look at how to make a request to a GraphQL API with JavaScript fetch or libraries like axios. Author: Praveen Durairaju"}, {"role": "assistant", "content": "If you are new to GraphQL, all you need to understand is this - GraphQL is just a query language and popular implementations on the client and server leverage existing JSON standards for requests and responses over HTTP.Now with that basic understanding, we can look at how to construct a GraphQL request in this post. If you have ever done any REST API calls from your frontend using fetch or axios in the past, this should be very quick to grok.What constitutes a GraphQL request?Lets start with the basics. Is a GraphQL request any different from a REST API request? Not really. At least with current server implementations that mostly deals with HTTP and JSON.GraphQL Endpoint - There is only one endpoint for a GraphQL server.Headers - The relevant authorization headers if any will be applicable here.Request Body - This is where you will be sending a JSON body to the GraphQL server. Most server implementations use JSON for request / responses.Lets now look at what the request body JSON looks like:{\n  \"operationName\": \"\",\n  \"query\": ``,\n  \"variables\": {}\n}The JSON body has 3 objects.operationName: This is an optional string and can be used to denote the name of the GraphQL operation being performed. This will be useful for logging and debugging later.query: This is the actual GraphQL query. The query language syntax that we speak of about GraphQL comes in here. This is a string which contains the GraphQL syntax. It can be enclosed in backticks (`) since the query is usually multi-line.variables: This is an optional object and should only be used if a variable is being used within a query. Think of it as passing an argument to a function. The values will be substituted directly. It is hard to construct a string with dynamic values and hence while making the query part, you can make use of variables objects for anything dynamic.Alright, lets look at an example for a basic GraphQL query:query fetchAuthor {\n  author {\n    id\n    name\n  }\n}\nNow lets say the GraphQL server is hosted at https://sweeping-reindeer-45.hasura.app/v1/graphql and it has some Authorization token to access the server.The GraphQL request for the above query in JavaScript Fetch will look like the following:GraphQL Example Request using Fetchconst endpoint = \"https://sweeping-reindeer-45.hasura.app/v1/graphql\";\nconst headers = {\n\t\"content-type\": \"application/json\",\n    \"Authorization\": \"<token>\"\n};\nconst graphqlQuery = {\n    \"operationName\": \"fetchAuthor\",\n    \"query\": `query fetchAuthor { author { id name } }`,\n    \"variables\": {}\n};\n\nconst options = {\n    \"method\": \"POST\",\n    \"headers\": headers,\n    \"body\": JSON.stringify(graphqlQuery)\n};\n\nconst response = await fetch(endpoint, options);\nconst data = await response.json();\n\nconsole.log(data.data); // data\nconsole.log(data.errors); //Sounds familiar? Yes! This is very similar to how you make REST API calls. The only part which you need to worry about is constructing the query object of the GraphQL Query.Now let us look at the same example with Axios.GraphQL API Request with Axiosconst axios = require(\"axios\");\n\nconst endpoint = \"https://sweeping-reindeer-45.hasura.app/v1/graphql\";\nconst headers = {\n\t\"content-type\": \"application/json\",\n    \"Authorization\": \"<token>\"\n};\nconst graphqlQuery = {\n    \"operationName\": \"fetchAuthor\",\n    \"query\": `query fetchAuthor { author { id name } }`,\n    \"variables\": {}\n};\n\nconst response = axios({\n  url: endpoint,\n  method: 'post',\n  headers: headers,\n  data: graphqlQuery\n});\n\nconsole.log(response.data); // data\nconsole.log(response.errors); // errors if anyWith axios, you don't need to stringify the JSON when you are passing the request body to the data key.Handling ErrorsOnce the response is available, you can handle errors with the response object. If you find the errors object in the response, then you can parse that for errors.If you are completely new to GraphQL, I would recommend you check out our open source tutorial on Introduction to GraphQL which goes over the core concepts through examples."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: A new look for Hasura Learn with 25+ courses on GraphQL, Databases & Hasura Description: Hasura Learn comes with 25+ open source tutorials on GraphQL, Databases & Hasura to help developers get up to speed with these technologies.  Author: Rajoshi"}, {"role": "assistant", "content": "One of the earliest products we launched on the heels of launching the Hasura GraphQL Engine, was an educational site called \u201cHasura Learn\u201d to help developers get up to speed with GraphQL. New landing page for Hasura LearnOur first course was for React Developers to learn the concepts of GraphQL while integrating it into an existing React application. We picked a To Do app (we know, but wait for it\u2026), with a twist: realtime collaboration on public todos to show off realtime collaboration made possible with GraphQL subscriptions, while also having a private to dos board. Over the last 2 years, we have added \u00a0over 25 tutorials, as well as Database courses and Hasura courses into the mix. Today Hasura Learn has courses on:GraphQL for frontend developers to get hands-on with GraphQL while working within the comfort of their favorite languages, frameworks and popular GraphQL clients (React, Vue, Flutter, React Native, Elm, Flutter etc)Fullstack GraphQL + Hasura backend courses for a few stacks (NextJS with Hasura) and offline-first applications with RxDB and Unity applications with a Hasura backend.Database fundamentals for frontend and fullstack devs (Postgres, SQL Server and MySQL)Hasura courses: Hasura Basics for those just getting started, Hasura Advanced to experience the production ready features that come baked in, as well as Hasura Auth (building a Slack like permissions system) to help understand the authorization engine that comes baked in with Hasura.Open Source & Community Maintained Tutorials for GraphQL, Databases & Hasura DevelopersThese tutorials are open source and are built and maintained with the help of the amazing GraphQL & Hasura community. They have also been built to help fill in the gaps, and not learn everything from scratch - hence our tagline \u201cfor the busy developer\u201d. For example, the React + GraphQL tutorial assumes React proficiency and goes into the GraphQL fundamentals alone. With this in mind, our shortest tutorials (GraphQL Basics & Hasura Basics) should take you about 30mins to an hour to complete and all the tutorials should be something you can cover in one sitting (~2 hours to complete). What\u2019s new with Hasura Learn Today, we\u2019re very excited to unveil a new look for the site that\u2019s been made to accommodate the expanding number of courses! We intend to invest in this a lot more and build many more courses and keep existing ones up to date to help the community find the best tutorials as they start exploring GraphQL, Databases & Hasura.Along with the new look, we\u2019re also super excited that we now have 2 of our most popular courses (GraphQL basics & Hasura basics) available in Mandarin and Japanese. We will be translating these into more languages as well based on the interest from the community. Hasura Learn courses are available in Japanese & Mandarin in addition to EnglishFinally we have updated our React course to use React18, updated our NextJS course to use the latest version of Apollo Client and Next.js 11 and have brand new SQL Server and MySQL courses. What\u2019s next for Hasura LearnWe intend to build many more courses for other languages & frameworks, newer GraphQL clients, more databases, and many more sample applications with Hasura. In fact we\u2019re hiring a GraphQL & Education Advocate at Hasura for exactly this reason :). We\u2019ll also be looking to work with the community more closely, and we\u2019ll share more about this soon! If you\u2019re someone who\u2019d like to build new courses for your favourite stacks that you don\u2019t find here or help us translate the courses into other languages, reach out to us on our community Discord on the #contrib channel to let us know what you\u2019d like to see and we\u2019d love to work with you (we will compensate you for your time)!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura v2.1.0-beta.1 Description: We are excited to announce the first beta release of v2.1 series with 2.1.0-beta.1 Author: Praveen Durairaju"}, {"role": "assistant", "content": "We are excited to announce the first beta release of v2.1 series with v2.1.0-beta.1.This release largely focuses on making custom business logic more and more flexible with new features around Hasura Actions / Event Triggers and Remote Schema name spacing. And on top of that, we are prepping up for adding support to mutations in Microsoft SQL Server.Highlights in this releaseWebhook transforms for Actions and Event Triggers: Webhook transforms allow you to arbitrarily customize the Hasura request to your endpoint thereby making it super simple to integrate your existing API or 3rd party service with Actions or Event Triggers.Actions enhancements: We\u2019ve added a bunch of upgrades to Hasura Actions such as ignoring extra fields in the webhook response and the ability to return extensions key for detailed errors.Customize your Remote Schemas: You can now customize your remote schema in Hasura by adding a suffix/prefix, a namespace or individually changing the names to prevent conflicts with other schema objectsPreparation for MSSQL mutations and generalized joins: This release also includes internal changes which will support MSSQL mutations and general joins in the upcoming release. Read more about Hasura's support for Microsoft SQL Server.Performance fixes to handle large db schemas: These performance fixes make the console function smoothly when working with large database schemas.Optimize SQL query generation with LIMITs: A significant performance improvement in query execution when using LIMIT clauses has been addedBug FixesThis release also contains a few bug fixes across the server and console.Fix custom-check based permissions for MSSQL - Issue #7429.Fix explicit null values not allowed in nested object relationship inserts - Issue \u00a0#7484.Fix a bug in replace_metadata API which allowed inconsistent objects to persist despite setting allow_inconsistent_object config to false.Fix cross-schema array relationship suggestions on the console.If you are curious on the other improvements and fixes, check out the full changelog for this release.Try it outPlease use the docker target hasura/graphql-engine:v2.1.0-beta.1, try it out, let us know what works and what needs improvement. You can find us on Twitter (@HasuraHQ) and in Github. For ad-hoc conversation, feel free to join us on the Hasura Community Discord.And, if you want to get started now with the latest stable release, check out the deployment simplicity of Hasura Cloud."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Tips & Tricks Contest Description: Announcing the Harvest 2021 Tips and Tricks contest! Share knowledge, learn new things, and win prizes! Author: Jesse Martin"}, {"role": "assistant", "content": "The Harvest EditionRunning from October 1, 2021 - \u00a0Nov 30, 2021Hasura does many things from access control to data federation, and beyond. Teams of all shapes and sizes making apps of all categories and scale are using Hasura to ship products faster.To capitalize on this wide base of knowledge, we want to see your favorite tips and tricks of working with Hasura. From the simplest shortcut to the most complex input filter the world has ever seen! We want to see them all!Oh, and there will be prizes.PrizesYes, there will be stickers\u2026AND\u2026Everyone\u2019s a winner!Everyone, EVERYONE, that submits, gets some swag. Sometimes you just gotta grab some swag by the horns.Top 5 SubmissionsThe top 5 submissions, as chosen by the Hasura team, will gain access to 6 months of Hasura Cloud Standard, worth $600.Most Creative, Most Unique, & Most Popular Submissions!Again, as determined by the Hasura team, we will give an extra prize each for the most creative, most unique, and most popular all-around submission.$300 Amazon Gift Card(If you win and Amazon is not available where you live, let us know!)What does a submission look like?There\u2019s a number of ways to submit to the contest, and we want to make it possible for everyone to join in!All Submissions need:Required: Describe your favorite tip/trick about app development with Hasura up to 140 characters or less than 2 minutes in video length. If you can Tweet it, it\u2019s valid!Required: Publicly accessible URL where the content can be viewed.Could be a Tweet, blog post, TikTok, etc. See Below.Bonus Points: Record a quick video and add it to the submission for bonus points.Submitting via TwitterThrow in the hashtag #HasuraTipsAndTricksSubmitting via DiscordSubmit in the channel #hasura-tips-and-tricksSubmitting via Blog, Youtube Channel, or other Social platformNotify us through either DM on Twitter, Discord, or E-Mail where to find your post!What does an example tip look like?Tweet contest entryTweet with short videoWhat should it be about?Anything that would teach users about the unique way that you are using Hasura! Either in the console, a unique IDE tool, a cool database feature exposed in Hasura, etc.What should it not be about?Anything that does not relate to using Hasura.Anything that negatively names another company, person, or group of people.We can\u2019t wait to see what you all share with us!We want you to win! \ud83d\ude80We can\u2019t wait to see what you come up with and what you have to offer the community as a whole.Like making content for the community? Did you know we pay cash for technical articles? Maybe make a tip and turn it into a post!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Implementing Authorization for Organization-based, Team-based, or Tenant-based Apps in Hasura Description: This blog post outlines how to model a common organization-based permission system in Hasura. Author: Gavin Ray"}, {"role": "assistant", "content": "Table of Contents\n\nIntroduction\nExample Data\nRelationships\nPermissions\nQuery Results\n\nIntroduction\nThis blog post is the first in a 2-part series that aims to outline how to model a common organization-based permission system in Hasura. Keep your eyes peeled for the second part for a walkthrough of a clonable sample repo that you can use to model similar permissions systems.\nThe problem being solved is:\n\n\"In my application, I have a 'group' and entities that belong to group(s).\"\n\"Each entity in the group should only be able to see/interact with other entities in the same group(s).\"\n\nThis is a common pattern in applications and in this article we'll take a look at one of the ways you can solve it. Our solution will use nothing but Hasura's standard permissions system and authorization rules.\nThis  approach and principle works for any \"group\"-like structure, such as:\n\nTeams\nOrganizations\nTenants in a Multi-tenant app\nChat group-messages, & more!\n\nLet's assume that you have some table structure like the following:\n\n\n\nTable Name\nColumns\nForeign Keys\n\n\n\n\nuser\nid, name, email\n\n\n\norganization_user\nid, user_id, organization_id\nuser_id -> user.id, organization_id -> organization.id\n\n\norganization\nid, name\n\n\n\n\nExample Data\nHere we have example users:\n\nJohn & Jane belong to Organization ID 1\nFrank belong to Organization ID 2\n\nWe create the following database records:\n\nuser\n\n\n\norganization_user\n\n\n\norganization\n\n\nRelationships\nWe create the following relationships on our data:\n\nuser\n\n\n\norganization_user\n\n\n\norganization\n\n\nPermissions\nAnd provision the permissions like such:\n\nuser\n\n\n\norganization_user\n\n\n\norganization\n\n\nQuery Results\nNow when we query with our X-Hasura-User-Id set as User 1 and 3 respectively, we can see only those users in our own organizations:\n\nQuerying as John (User ID 1, Org 1)\n\n\n\nQuerying as Frank (User ID 3, Org 2)\n\n\nVoila! We've successfully implemented an organization-based authorization system using Hasura!\nThe previous article in our authorization cookbook explains how to implement a hierarchical Google Drive-style permission system in Hasura using ACL.\nIf you have any questions or want to share your thoughts on the topic, reach out to us on Twitter / Discord / GitHub Discussions.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top ways to write a custom GraphQL Server with production ready features Description: Open source implementations of the GraphQL spec are now available in most popular languages. In this post we will look at the most popular libraries for each language for anyone to get started with and ways to add production ready features like Authorization, Rate Limiting, Caching etc. Author: Praveen Durairaju"}, {"role": "assistant", "content": "GraphQL has evolved over the years and now has wide support in the community across languages and frameworks. Open source implementations of the GraphQL spec are now available in most popular languages. In this post we will look at the most popular libraries for each language for anyone to get started with and ways to add production ready features like Authorization, Rate Limiting, Caching etc.The common interface to try out GraphQL requests can be GraphiQL, irrespective of how your API is built. If you are looking to test your GraphQL APIs, here is the list of GraphQL IDEs that you can try out.The GraphQL runtimes mentioned in each of the languages are spec compliant and in some cases even have draft implementations of newer spec that is work in progress.Write Custom GraphQL ServerNode + GraphQLLets start with the origins. The original implementation of GraphQL spec was written in Node.js by the Facebook team and it was released as a reference implementation graphql-js.This was followed by various GraphQL runtime implementations by the community in other languages. But JavaScript community has had the most number of projects, both client side and server side.GraphQL Runtime(s)graphql-jsAuthorizationYes, with graphql-shield moduleCachingResponse caching with envelop response-cache pluginCode GeneratorYes, with graphql-code-generatorClients@apollo/client, urql and more*Subscriptionsgraphql-subscriptions module (Pub/Sub) - graphql-wsRelay SupportYes, can be added with graphql-relay-jsRate LimitingYes, with graphql-rate-limit moduleTracingYes, with apollo-tracing extension*If you are curious on the clients, here is a great resource to check out fluent graphql clients.Note that, this may not be a comprehensive list of modules available for each feature implementation. Do let us know in the comments, if you feel there are other modules or plugins which might be more suited for specific use cases.There are many server implementations which add more value on top of graphql-js. Ben Awad had done a nice benchmark suite to test the performance of different Node based GraphQL runtimes.Python + GraphQLPython has a few options in the GraphQL space. Whether you are building a full fledged app using Django or just building a GraphQL server from scratch, you got it covered.GraphQL Runtime(s)grapheneAuthorizationLimiting field access, Global filteringCachingData Loader batching and caching*Code GeneratorNAClientsgql, python-graphql-clientSubscriptionsYes, Django channels with graphql-wsRelay SupportYes, with graphql-relay-pyRate LimitingUsing django rate limit configurationTracingNA / CustomThe graphene framework gives you a headstart but if you are using the django integration, then you can add features like rate limiting more easily. graphql-code-generator doesn't seem to support Python yet and it looks like code gen is lacking.On the client side, there's one maintained by Prisma, which is the python-graphql-client.Golang + GraphQLThe reference implementation for GraphQL on Golang is available at graphql-go/graphql. It supports queries, mutations and subscriptions and is spec compliant as it follows the graphql-js implementation.GraphQL Runtime(s)graphql-goAuthorizationCustomCachingData Loader batching onlyCode GeneratorYes, with graphql-code-generatorClientsshurcooL/graphqlSubscriptionsYesRelay SupportYes, with graphql-relay-goRate LimitingNA / CustomTracingYes, with graphql-goNow this list can be extended to other languages and frameworks like PHP, Haskell, Scala etc. Let me know in the comments if you want them to be added to this list.One thing which seems to be clear is, you can get started with a spec compliant runtime or an enriched framework but there are certain production ready features that require adding plugins.Ruby + GraphQLThe Ruby implementation of GraphQL is open-source at graphql-ruby which is widely used in enterprises like GitHub and Shopify.You can install it by adding the graphql gem to your gemfile.If you are looking at Authentication with JWT, you can check out the graphql-auth gem.GraphQL Runtime(s)graphql-rubyAuthorizationYes, with graphql.pro plugin(paid), CachingResolver level caching with graphql-cache pluginCode GeneratorYes, with generatorsClientsgraphql-client by GitHub and graphql-ruby-clientSubscriptionsYes with graphql.pro plugin (pusher)Relay SupportCustomRate LimitingYes, with graph_attack gem TracingYes, with graphql tracing module and apollo-tracing extensionJava + GraphQLThe reference implementation for GraphQL in Java is available at graphql-java.There's a more full fledged framework called the DGS Framework, built by the folks at Netflix that has more production ready modules ready to be added to the GraphQL server.GraphQL Runtime(s)graphql-javaAuthorizationField level visibilityCachingQuery caching with java-dataloaderCode GeneratorYes, with graphql-code-generatorClientsGraphQL Java Client DGS FrameworkSubscriptionsYes, reactive streams with RxJavaRelay SupportCustomRate LimitingQuery Depth Limiting availableTracingInstrumentation with DGS FrameworkIf you are looking for more awesome resources to get started with Java and GraphQL, check out this awesome-graphql-java list..NET + GraphQLIf you are in the Microsoft .NET platform, the open source GraphQL server Hot Chocolate is the go to choice. It is much more than just a reference implementation as it gives you the ability to build a GraphQL Gateway for a federated data graph.Hot Chocolate GraphQL PlatformYou can use the Hot Chocolate server as a standalone ASP.NET Core Server or as a Serverless Azure Function that serves a GraphQL endpoint.The serverless support opens up use cases where you can build event driven applications using the 3factor app architecture.GraphQL Runtime(s)graphql-dotnet and Hot ChocolateAuthorizationYes, role based, @authorized directiveCachingData Loader batching and caching, document based cachingCode GeneratorYes, with graphql-code-generatorClientsgraphql-clientSubscriptionsYes, through the use of IObservableRelay SupportYes, with relay toolsetRate LimitingBuilt-in with graphql-dotnet and HotchocolateTracingYes, with apollo-tracingAdd custom GraphQL to HasuraAny of the above custom servers, (or any GraphQL endpoint) can be added to Hasura via Remote Schema. All you need is an endpoint and the relevant auth headers to be able to aggregate these types into Hasura. This lets you query your custom GraphQL API from your frontend client via Hasura's unified API endpoint.AuthorizationWhen it comes to security, especially Authorization for your API, you need to add more plugins on top of these servers.Some of them are role based, some provide field level filtering etc. There are naunces here to understand what the plugin offers. But by far, graphql-shield in the Node.js ecosystem seems to offer a more rounded set of features for adding Authorization to a GraphQL server.Relationships between DataExtending your custom server to interact with other GraphQL servers is going to be critical since modern apps require data aggregated from various places and some of which are related.Some frameworks in the above list support schema stitching to aggregate different GraphQL schemas. A few support federated style where you can establish relationships between different types and schemas.Hasura for existing GraphQL APIsIf you are looking to leverage Hasura for all your existing APIs and need features like Authorization, Caching, Rate Limiting, Monitoring and Analytics, then check out this guide - Hasura for existing GraphQL APIsMost of the above servers are reference implementations of the spec with added nicety to make it easy to start writing code. This boilerplate doesn't necessarily involve features like Authorization, Caching, Monitoring out of the box. Mostly it is restricted to defining the GraphQL schema and custom types associated for your API with the right resolvers for the same. You will need different plugins for each production ready use case and wire it up all together to run a production ready GraphQL API. As the community evolves, we will see more matured solutions in all languages for a lot of these features. Do let us know in the comments - What's your go to language / framework for writing a custom GraphQL server?"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Choosing a FaaS (Function as a Service) provider in 2021 Description: Serverless computing is the new home for domain code. In this post, we compare the major players to help you decide where to run your business logic. Author: Jesse Martin"}, {"role": "assistant", "content": "Functions as a service, FaaS, is most likely the most interacted with serverless product on the market. Function execution platforms really gained rise as more people got tired of setting up full containerized environments for simple code execution. The costs, overall maintenance, and cascading consequences of failures meant more people were looking for a lean context to execute one-off workers in the cloud.The industry really rose to notoriety when AWS introduced AWS Lambda, a lambda being an anonymous function, and the service being that promise, as a service - these are anonymized functions that are not bound to a calling context, but respond to discrete events and take discrete inputs. They\u2019re simple to reason about, simple to call, and most of all, they\u2019re drastically more affordable to run, lending themselves really well to \u201cpay for compute time\u201d, not \u201cpay for standby\u201d which containers need to do.The pros and cons of FaaSOutlined above, there\u2019s a few specific advantages and a few disadvantages (though decreasing by the day) to opting for a FaaS provider.Pro: MaintenanceFaaS providers allow you to design, deploy, and execute functions \u2013 and only functions. You don\u2019t need to maintain additional infrastructure. Setting up an environment to run a function is akin to running a restaurant to make yourself a cup of coffee from time to time.Pro: CostFunctions have obvious execution signatures which lend themselves to detailed billing. The more fine-tuned your billing, the more exact your cost to benefit analysis will be, and more often than not, that leads to cheaper costs on the whole.Pro: UpgradableYour functions can be independently updated, optimized, and measured. Scoping domain logic to independent functions allows for working in an iterative manner.Cons: Higher LatencyWhile many tools have risen to solve this problem, serverless functions are subject to slow cold-start times and overall more latency than a dedicated execution environment.Cons: PortabilityMany FaaS providers have proprietary route handling logicChoosing a FaaS ProviderUp to a few years ago, the question about choosing a FaaS approach versus maintaining a dedicated execution environment was much more nuanced. Today, the developments of the FaaS ecosystem can handle the vast majority of all cloud execution needs. Choosing a provider, however, has simply adopted the former nuances.When choosing a FaaS provider, the two most important questions to answer are related to workload and latency.Workload dependent criteriaWhile all major FaaS providers are capable of handling simple workloads, not all of them are capable of offering the same breadth of services. Computationally intensive tasks including machine learning or resource intensive tasks like rendering all need robust service providers supporting those needs.Some features such as ML processing is only even supported by a smaller number of service providers to begin with.Latency dependent criteriaAs the number of FaaS providers has increased, their product differentiation in solving common problems has also increased. One area of differentiation is where the code actually executes, either in centralized regions or at the edge.Edge computing is a growing trend, set to explore in the coming few years. At the moment, 10% of enterprise data is currently created/stored outside of standard cloud center. By 2025, that is estimated to reach 75%. Edge computing replicates the code that is to be executed to smaller servers closer to the individual customers, which means that the amount of time, network resources, and energy required to execute my code is all reduced dramatically.This is, of course, a hard problem to solve, thus not all providers are capable of offering these benefits. CDN providers, having solved many of these challenges in different contexts, are some of the first to embrace these patterns.Other criteriaSome honorable mentions for things to consider when choosing a FaaS provider also include pricing, speed of prototype, and working with what you have. If you are running serverless workloads with hyper cost-sensitive business requirements, you may find yourself naturally limited to a certain set of providers.If you are working on setting up \u201cquick and dirty\u201d prototypes, you\u2019re not going to have the time to onboard with an account manager, high-tough, FaaS provider \u2013 and there\u2019s plenty of those.Lastly, if you have existing GCP or AWS infrastructure with billing accounts in place and stakeholder approved, it\u2019s typically best practice to keep serverless services within a single chain of command for both credentialed access, unified billing, and observability tooling.Comparing FaaS providersWith so many providers popping up, it\u2019s helpful to have a short analysis of where the different ones excel, and how they rank. Beyond the provider\u2019s individual ability to offer support for the widest array of workloads, at the fastest speeds possible, at a reasonable cost, we need to consider developer efficiency, which has knock-on impacts on operational costs, as well.DX considerations for FaaS providersDeveloper experience for FaaS focuses around design, development, and deployment.DesignLocal simulation of a production environment allows you to ensure that your local code you tested will execute with some degree of predictability in a production environment. This is one of the most useful features for designing serverless functions where layers between your code and the hosting provider may introduce instability.CLI based templating allows developers to quickly scaffold functions with consistency and best practices built in.Code first configuration also allows for provisioning resources and designing serverless functions with higher degree of predictability and consistency, benefiting from version control and static analysis tooling.DevelopLanguage selection is one of the most important factors when choosing a FaaS provider as different languages lend themselves best to different workload types. Being able to have a range of selection allows executing all serverless workloads with the same FaaS provider. Having to adapt operational code to a different language or runtime just because of FaaS requirements is a substantial hit on developer productivity.Portable code, or generally abstracted code that is able to run equally across more than one vendor is another critical metric. If each vendor is implementing their own networking interfaces, this will also slow down developer productivity. Code designed for one execution environment should be able to be used with multiple providers.Tangentially related, but when provider specific APIs are offered or required, providing strong typings for those APIs increases productivity even more, allowing developers to reduce context switching from IDE to documentation and back again.DeployContinuous deployment is perhaps the highest development improvement, allowing versioning and roll-backs as an integrated part of the development process.Where continuous deployment is not available, or even where it is, CLI based deployment and log tailing is another critical piece of developer experience improvement that reduces time to production, deployment errors, and onboarding of new team members.FaaS comparison criteriaAbstracting all our results above, we\u2019ll evaluate some of the more popular FaaS providers with the following criteria.Plugins, Integrations, and LibsEdge ReadyCostLocal production simulationLanguage selectionInfrastructure as code (IaC)CLI availabilityContinuous deploymentComparing the VendorsBelow is a non-exhaustive list of FaaS providers. Our list doesn't include options such as Knative, or OpenFaaS which are platforms for running FaaS solutions themselves. The distinction turns grey quickly, but for the purposes of a general overview, we\u2019ll be looking at providers offering a \u201chosted function\u201d product proper.Comparison of FaaS providers in 2021Traditional providersAWS LamdbaYou can\u2019t really begin talking about FaaS providers without talking about Lambda, or really even AWS as a whole. Seen by many as the forerunner of cloud computing at scale, Lambda is the 800 pound gorilla in the room. Plugins, integrations, and libsSomething for everyone, including ML APIs, rendering services, messaging queues, and more.Edge readyYes - Lambda@EdgeCostMonthly with scaled usage pricingFree tierYes (Credit based)Local production simulationnoLanguage selectionAnything that compiles to Javascript or WASM.Infrastructure as codeMinimal, routing and execution parametersCLI availabilityYesContinuous deploymentNoGoogle Cloud functionsGCP will of course have their own cloud computing options. Where raw computing resources may lag behind some alternatives, the knowledge APIs and libraries are the best of the best. Plugins, integrations, and libsSomething for everyone, including ML APIs, rendering services, messaging queues, and more.Edge readyNoCostMonthly with scaled usage pricingFree tierYesLocal production simulationNoLanguage selectionNode, Python, Go Runtime, Java, .NET, Ruby, PHPInfrastructure as codeWith TerraformCLI availabilityYesContinuous deploymentNoAzure functionsMicrosoft is sure to bring their own FaaS solutions to the table, and with one of the most diverse set of supported languages, as well. No matter how you write your business logic, Microsoft's rich ecosystem and pre-existing billing approval with many companies make it a great place to host serverless workloads.Plugins, integrations, and libsLimited, but growingEdge readyNoCostVarious plans, scaled usage and dedicatedFree tierFree limit with billing accountLocal production simulationNoLanguage selectionC#, JavaScript, F#, Java, Powershell, Python, TypeScript (Transpiled to JavaScript)Infrastructure as codeYesCLI availabilityYesContinuous deploymentYesVercelVercel has long been a favorite for rapid deployment in the JavaScript community. Having taken a developer obsessed focus to product offerings and being the makers of the wildly popular NextJS framework, they\u2019ve been the gateway to serverless computing for many.Plugins, integrations, and libsDeployment triggers, collaborative editing, and analyticsEdge readyYesCostMonthly with fixed capsFree tierYesLocal production simulationYesLanguage selectionNode, Go, Python, RubyInfrastructure as codeMinimal, routing and execution parametersCLI availabilityYesContinuous deploymentYes (Github, GitLab, Bitbucket)Gatsby Cloud functionsFor those with front-end experience, Gatsby is a familiar name, having extolled the promises and benefits of static sites for years. Born of the static first approach is the need to still have a few processes running server side. Gatsby Cloud functions is a great place to see just how far the frontier of static plus serverless has really gone. Plugins, integrations, and libsNoneEdge readyYesCostMonthly with hard capsFree tierYesLocal production simulationNoLanguage selectionJavascript and TypescriptInfrastructure as codeNoCLI availabilityNo (not for cloud functions)Continuous deploymentYes (Github, GitLab, Bitbucket)Netlify functionsAnother well loved tool by front-end developers, Netlify is an all-inclusive solution for hosting web applications, cloud functions, and more. For those looking for less overhead than the big cloud providers, Netlify is a great option that can still perform as well as the rest of them.Plugins, integrations, and libsObservability and operations integrationsEdge readyYesCostMonthly with incremental overage pricingFree tierYesLocal production simulationYesLanguage selectionJavaScript, TypeScript, GoInfrastructure as codeDeployment and routing configurationCLI availabilityYesContinuous deploymentBetaCloudflare WorkersCloudflare workers are a newer entrant into the cloud computing ecosystem, but with the extensive experience of Cloudflare in edge networks, they\u2019ve risen quickly in prominence.Plugins, integrations, and libsLibs for traditional web processing like KV stores, caching, and more.Edge readyYesCostMonthly with scaled usage pricingFree tierYesLocal production simulationYesLanguage selectionAnything that compiles to Javascript or WASM.Infrastructure as codeMinimal, routing and execution parametersCLI availabilityYesContinuous deploymentNoFly.ioFly.io does a lot more than cloud computing, including multi-cloud, distributed databases. Their cloud computing, however, is why they\u2019re in this list, so the stats are:Plugins, integrations, and libsLibs for traditional web processing like KV stores, caching, and more.Edge readyYesCostMonthly with scaled usage pricingFree tierYesLocal production simulationYesLanguage selectionAnything that compiles to Javascript or WASM.Infrastructure as codeMinimal, routing and execution parametersCLI availabilityYesContinuous deploymentNoOracle FunctionsRounding out their long standing strong feature set, oracle has surprisingly one of the more open and portable FaaS solutions available, allowing the product to be wholesale picked up and transferred into alternative cloud environments.Plugins, integrations, and libsLimitedEdge readyYesCostMonthly with scaled usage pricingFree tierYes with billing accountLocal production simulationNoLanguage selectionNode, Java, Python, Ruby, GoInfrastructure as codeYesCLI availabilityYesContinuous deploymentNoIBM FunctionsOf course big blue wouldn\u2019t be left out of the cloud revolution. As part of a massive push into the cloud, IBM has kept lock step with all the major cloud developments while maintaining the enterprise features and trust they\u2019ve come to be counted on for by so many of the world\u2019s biggest companies.Plugins, integrations, and libsYesEdge readyYesCostMonthly with scaled usage pricingFree tierYesLocal production simulationNoLanguage selectionJS/Node, Swift, Java, Python, Ruby, Go, PHP, .NET, CustomInfrastructure as codeYesCLI availabilityYesContinuous deploymentNoTencent CloudTencent is the juggernaut cloud you either hear everything or nothing about. Dominating the asian market as well as powering many of the popular apps arrayed across your phone, Tencent has a robust offering worth adding to your price comparison analysis.Plugins, integrations, and libsYes, many optionsEdge readyNoCostScaled usage pricingFree tierYes - 6 MonthsLocal production simulationNoLanguage selectionPython, Node, Go, PHP, Java, CustomInfrastructure as codeYesCLI availabilityYesContinuous deploymentNoAlibaba CloudAnother of the asian giants, Alibaba is more than the everything store of the east, it boasts a growing and competitive cloud offering as well. Alibaba has a multitude of additional developer tools available including integrated IDEs and more.Plugins, integrations, and libsYes, many optionsEdge readyNoCostScaled usage pricingFree tierYes - 12 MonthsLocal production simulationNoLanguage selectionC#, Python, Node, Go, PHP, Java, CustomInfrastructure as codeYesCLI availabilityYes, multipleContinuous deploymentNoRedhat Open ShiftOpen Shift is a strong contender in cloud computing and robust development environments. Their FaaS support is an early beta preview, but worth keeping an eye on for all the benefits Redhat has to offer.Plugins, integrations, and libsNoEdge readyYesCostMonthly with scaled usage pricingFree tierYesLocal production simulationNoLanguage selectionNode, Python, Go, Quarkus (Java), TypeScriptInfrastructure as codeYesCLI availabilityYes - exclusivelyContinuous deploymentNoNo-Code / Low-Code providersThese last providers don\u2019t warrant an analysis block in that much of their core product offering is to abstract those implementation details away anyways. But for speed of prototyping, and, in many cases, aptly handling a large number of use cases, the low-code / no-code providers make the list.TinyFunctionA very new entrant onto the scene, TinyFunction is, without a doubt, the fastest way to get a function executing at an endpoint. And that\u2019s all it does. If you\u2019re looking for a quick prototype method, look no further.PipedreamPipedream is a robust platform that performs well under load. They provide a large number of integrations from Fitbit to finance. While still allowing for breakout code environments, they opt for a configuration over code interface design that allows anyone to drag-and-drop glue code with minimal effort.AutocodeAutocode allows you to write any type of serverless code and implement with their large standard library of external service providers. With helpful log handling, team account management, and an intuitive online IDE, Autocode is a great solution to bridge the gap from pseudo-code to production.Using FaaS with HasuraWhere does FaaS fit into the Hasura story? Hasura has two specific features that pair really well with serverless execution: actions and events.With actions, you can design serverless workloads that execute behind a custom query or mutation field. If you need to perform custom authentication logic, this is a perfect use case for actions.With events, you can also define serverless workloads that execute in reaction to events that happen in your database. Let\u2019s say a user uploads a PDF to your storage engine, which in turn writes that file reference into your database, an event could begin processing that PDF with OCR for content extraction. Paired with subscriptions, this is a great way to execute long running processes.A bonus opportunity for cloud execution is for micro GraphQL services that perform limited functionality that can then be added as remote schemas. Performance restrictions will limit the number of cases this should be used in production, but for some problems this can be a good solution!ConclusionServerless execution is here to stay. With the rough 80/20 analysis method, serverless will be a dominant strategy for many companies that want to control costs, embrace low-op approaches to infrastructure, and focus on domain logic, not managing their own servers. Embracing FaaS providers is a great way to begin the serverless journey for incremental adoption and observing the benefits that it provides."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL & the Data Mesh -  developer productivity in an age of exploding data Description: The promise of GraphQL and the architectural concept of the data mesh seem to be perfectly suited to solve the application development challenges of the modern enterprise - with an extreme focus on developer productivity and enablement of the application developer.  Author: Rajoshi"}, {"role": "assistant", "content": "GraphQL has now been around for over 5 years, and has been adopted across companies as varied as GitHub, Airbnb, New York Times, Philips, government organisations and some of today\u2019s fastest growing startups. GraphQL is an API specification created at Facebook that took the developer world by storm. The core of GraphQL\u2019s popularity was the developer experience and productivity it provided to the frontend/full stack developer, who no longer needed to wait on a backend developer to build an API for them. The ability to dynamically fetch whatever data is required is a productivity dream that GraphQL seemed to be in the best place to enable. However the realities of enabling GraphQL inside of an organization are multifold. GraphQL promises one single API for all your application development needs. For this to indeed be true, the GraphQL API needs to be able to fetch data across multiple domains. This opens up further architectural challenges to account for performance, security and scalability as well as organisational challenges of how teams should be architected - where is authorization handled, who is responsible for performance, how is the GraphQL API maintained, how does the workflow change for each microservice author, and so on. Meanwhile, in an adjacent data universe of analytical / static data, the idea of a data mesh was born. What is a Data Mesh?The idea of data mesh was introduced by Zhamak Dehghani (Director of Emerging Technologies, Thoughtworks) in 2019. Zhamak describes Data Mesh as \u201can architectural paradigm that unlocks analytical data at scale; rapidly unlocking access to an ever-growing number of distributed domain data sets, for a proliferation of consumption scenarios such as machine learning, analytics or data intensive applications across the organization. Data mesh addresses the common failure modes of the traditional centralized data lake or data platform architecture, with a shift from the centralized paradigm of a lake, or its predecessor, the data warehouse. Data mesh shifts to a paradigm that draws from modern distributed architecture: considering domains as the first-class concern, applying platform thinking to create a self-serve data infrastructure, treating data as a product and implementing open standardization to enable an ecosystem of interoperable distributed data products.\u201dCan the Data Mesh concept be applied to operational data? Operational data is becoming increasingly fragmented - data is stored across multiple databases depending on the nature of the data. Relational and transactional data could be in Postgres/MySQL (and increasingly in distributed and scale-out flavours of these databases!), search data or materialized data could be in a document stores like Mongo & Elastic, and workload specific data like timeseries could use another type of databases. In addition to this, data that\u2019s required to be accessed by applications could come from 3rd party SaaS services and CMSes. Although the data is spread across these sources, they\u2019re still potentially inter-related and fulfil the goals of a domain.Similar to what the data mesh tries to address with analytical data, distributed operational & transactional data can also be made self-serve for application developers by creating a single semantically unified API so that frontend / full stack developers can access any data fast to build applications and experiences for their users.The promise of GraphQL and the architectural concept of the data mesh seem to be perfectly suited to solve the application development challenges of the modern enterprise - with an extreme focus on developer productivity and enablement of the application developer; both of which are themes very dear to our hearts at Hasura! The Enterprise GraphQL Conference:In the second edition of the Enterprise GraphQL Conference, we\u2019re exploring these concepts with technical & business leaders who are at the forefront of these changes - either implementing it within their organization or working with organizations who are trying to solve these challenges at scale. The 3-hour mini-conference has a power-packed line up with:Tanmai Gopal, Hasura\u2019s CEO, will be kicking off this conference with an introduction to the concepts behind the Data Mesh. He will also talk about how GraphQL is an ideal choice for building a Data Mesh.Rachel Stephens, a Senior Analyst at RedMonk, will talk about how databases have evolved to solve different use cases over the years. She\u2019ll also give us a peek into what we can expect from database technologies in the future.Arun Vijayvergiya, Staff Engineer at Airbnb: \u00a0Last year, Airbnb introduced Viaduct \u00a0- data-oriented service mesh - at EGC. This year, Arun will talk about how it\u2019s evolved to serve most of Airbnb\u2019s traffic and its impact on how they create and operate services.Engineers from Nutrien Australia, will share their experiences and everything they learnt building and operating a Data Mesh powered by GraphQL, Typescript, Hasura, and React Native Web.Zhamak Dehghani, the creator of the concept of the Data Mesh, in a fireside chat with our CEO Tanmai Gopal on porting the data mesh over to operational data.If you\u2019re a technical leader exploring ways to amplify developer productivity in your organization, or an architect looking to understand modern architectures to deal with the proliferation of data sources that your teams need to access or a developer looking to understand the use-cases of GraphQL at scale, this conference is for you! We look forward to having you join us on the 28th October from 9AM PST - 12PM PST for 5 power-packed sessions on this theme! Register here \u2192 https://hasura.io/enterprisegraphql/"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Celebrating Open-source with Hasura: Hacktoberfest & more! Description: Every year, Hasura participates in the Hacktoberfest organised by Digital Ocean. And we are back at it in 2021 making it our 4th year in a row with our own flavour of the event! Author: Vishwa Mehta"}, {"role": "assistant", "content": "Every year, Hasura participates in the Hacktoberfest organised by Digital Ocean. And we are back at it in 2021 making it our 4th year in a row.This time around, there are 2 ways you can participating in celebrating open-source with us:Contribute to the Hasura GraphQL EngineHasura Tips & Tricks ContestHere\u2019s how!Hacktoberfest: The Hasura EditionAs part of this year\u2019s Hacktoberfest,, we have labelled a bunch of issues in the Hasura GraphQL project on GitHub. We will be accepting contributions for the open issues till 14 Nov. If you are new to Hacktoberfest, you can read all about it here.Note: Valid PRs made after 31 Oct will be eligible to win Hasura swag. However, they will not be considered for the official Hacktoberfest event by Digital Ocean (since it ends on 31 Oct).\ud83e\udd1d How to participateSubmit a PR for one of our issues mentioned belowWe will review your PR within 12 daysIf it is valid, we will either merge it or label it as \u201chacktoberfest-accepted\u201d\ud83d\udc55 Winning SwagAll valid contributions get sent a swag boxPlease make sure you have your email address mentioned in your GitHub profile so that we can send you the swag form on acceptance of your contributionThis year, the following components of the hasura-graphql repo and a few other projects on our GitHub have curating a list of issues that you can contribute to:Learn HasuraGraphqurlCommunityDocsBefore you get started, read through the contributing guide of the component you\u2019d like to work on:DocsCommunityLearn Hasura\u23f0 DeadlineIf you want to qualify for Hacktoberfest, you need to submit your PR before 31st Oct.However, we will be accepting PR submissions for the open issues till 14 Nov and will ship you swag for an accepted PR.\u2705 What is a Valid PR for HacktoberfestA PR that gets labelled as \u201chacktoberfest-accepted\u201dA PR that gets merged\u274c PRs that will not be accepted for HacktoberfestWe will not accept:Typo fixesGrammatical fixesContributions to issues not tagged with \u201cHacktoberfest\u201dIf you have any questions or want to contribute to an existing issue without the hacktoberfest label, please reach out to us on Twitter. We will see if we want to open it up for Hacktoberfest.\ud83d\udcdd GuidelinesHere are a few guidelines that will help make the Hacktoberfest experience a fun experience for everyone!Working on an issue \ud83d\udd0dHacktoberfest is a great way to encourage open source contributions and help beginners get started. To ensure that everyone gets a fair chance to contribute, we won\u2019t be assigning issues to a single contributor. To show that you're working on a certain issue, comment on the issue of interest and make sure to reference it in your PR so that other participants can see the progress.Code of conduct \ud83d\udcc4Open source is all about learning with and giving back to the community. To facilitate this, it is pivotal for us to create and maintain an inclusive space so that everyone can learn and grow together.Please go through to our Code of Conduct thoroughly and adhere to it. Any violations of the Code of Conduct will not be tolerated and will further lead to disqualification from the challenge.Quality over quantity \ud83d\udc96As the old saying goes, the more the merrier \u2014 we truly appreciate, and hence, encourage you to contribute to as many issues as you can. However, we also believe that quality trumps quantity and thus, abide by the official Hacktoberfest quality standards while reviewing PRs. PRs that do not match these quality standards will not be considered as eligible for completion of the challenge.Hope to see some awesome contributions from you!Hasura Tips & Tricks \ud83c\udfa9We\u2019re hosting a contest for sharing your favourite tips & tricks with us! And we\u2019ve got some special prizes and swag for winners and participants.\ud83e\udd14 How can I enter?Follow us on Twitter to stay connected during the contestLike our announcement tweet embedded belowShare your favourite tips & tricks below the announcement tweet as a comment\ud83d\udce3 Announcing the Hasura Tips & Tricks Contest! \ud83c\udfa9Share tweet-sized tips & tricks about using Hasura in your current stack and get a chance to win awesome prizes! \ud83c\udf81\ud83d\udcc6 Open till 14 Nov\ud83d\udcddhttps://t.co/0NMmeLSGgd#TipsAndTricksWithHasura pic.twitter.com/xzCNFsV0qA\u2014 Hasura (@HasuraHQ) October 12, 2021\n\n\ud83d\udcdc What does my submission look like?Describe your favourite trick/hack about app development with Hasura that fits in a single tweetRecord a quick video and add it to the tweet for bonus pointsThrow in the hashtag #Tips&TricksWithHasuraAnd voila! You\u2019ve entered the contest! \ud83c\udf89\ud83d\udd16 Here\u2019s a quick example of what your submission could look like:Tweet onlyTweet with short video\u2705 What can it be about?Anything \u00a0that would teach users about a unique way that you are using Hasura!\u274c What can it not be about?Anything that does not relate to using GraphQL or Hasura.\ud83c\udf81 What do winners get?Participants get something too: We will ship some Hasura swag to you if you enter the contest with a valid submission. Submissions that do not align with submission guidelines will not be accepted.Additionally, we will send free Hasura Cloud credits to the top 5 entries.And lastly, the most creative, most popular, and most unique entries get special prizes.\ud83d\udcc6 How long will the contest run?The contest will be running from the time of announcement till November 14. We will be announcing the winners shortly after that.Can\u2019t wait to see what you all share with us!We want you to win! \ud83d\ude80We are super stoked to have you all participate and celebrate open-source yet again with the Hasura Community!Follow us on Twitter to keep an eye out for updates and join us on our Discord to come hang out with the rest of the Hasura community."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura for existing GraphQL and REST APIs: An Architect's Guide Description: Typically you would connect a database to Hasura and get GraphQL and REST APIs. But what if you already have existing GraphQL / REST APIs outside of Hasura? How do you reuse them without re-writing every functionality from scratch? In this post, we will talk about that and more. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Hasura gives you instant GraphQL and REST APIs for all your data. Typically you would connect a database to Hasura and get these APIs. But what if you already have existing GraphQL / REST APIs outside of Hasura? How do you reuse them without re-writing every functionality from scratch? Where does Hasura fit in this whole architecture? In this post, we will look at ways to architect an app to incrementally add GraphQL and Hasura.Architect Guide for using Hasura with existing GraphQL and REST APIs\nMaking your GraphQL APIs production ready\n\nAdd your existing server as Remote Schema\n\n\nSecure your GraphQL API\n\nAdd Authorization\nEnable rate limiting and query depth limiting\n\n\nHigh Performance GraphQL\n\nCaching\n\n\nMonitoring and Observability\nJoin data with other sources\nReusing Existing REST APIs with Hasura\n\nMaking your GraphQL APIs production readyThe hardest part about GraphQL APIs is not just writing the custom resolvers but also optimising them for high performance, caching and securing them. Making your GraphQL API production ready involves a lot of heavy lifting. In this section, we will look at how toSecure your API without writing any codeAdd GraphQL caching to reduce response timesMonitor and observe APIsJoin your response data with any sourceHasura with custom business logicHasura supports adding custom GraphQL servers via Remote Schemas. You need to plug your GraphQL endpoint along with the relevant auth headers and Hasura will introspect your API and add that schema to Hasura's existing GraphQL schema.Hasura is designed to sit directly between your application and data sources.Add your existing Server as a Remote SchemaOnce you are on the Hasura Console, Head to the Remote Schemas tab of the console and click on the Add button.Add a remote schema to HasuraThe GraphQL endpoint can be written in any language / framework of choice. Your GraphQL server should enable introspection for Hasura to be able to connect and add that schema.This is the first step to bring all your existing GraphQL APIs to Hasura and create a unified endpoint.Secure your GraphQL APINow lets dive in to Security. Securing your GraphQL API needs to be approached differently. The typical REST API approach of rate limiting endpoint access and preventing attacks wont work with GraphQL, since you know, GraphQL API has a single endpoint and is schema based. Lets see how Hasura helps you here.Add Authorization to your GraphQL ServerWriting authorization rules with GraphQL will deeply involve the GraphQL schema and potentially writing custom directives. Hasura lets you define these rules declaratively, without writing code. You can use the Console UI or the Metadata yaml to create these rules programatically. In the end, you get the benefits ofNo codeDeclaratively add permission rulesRole basedField level granularityApply token claims as filtersOnce you have added your custom GraphQL server as a remote schema, you need to enable permissions for remote schemas. You can do so by adding the following environment variable.HASURA_GRAPHQL_ENABLE_REMOTE_SCHEMA_PERMISSIONS: \"true\"What you see below is the permissions UI of the Hasura Console. This lets you browse the GraphQL schema of your custom server and define role based permission for each query type.Role based permissions UIEnable Rate Limiting and Query Depth LimitingDefining authorization rules prevent unauthorized access to your data. But that doesn't prevent anyone from hitting your API and eventually your database/datastore. You would always want genuine requests to hit your database to serve the users of your app. This leads us to think about Denial of Service (DoS and DDoS) attacks, either volumetric or application level security.To prevent attacks and API abuse, Hasura lets youLimit requests to your API per min/hourApply limits based on user identity and session variablesPrevent scraping and abusing with Allow ListsConfigure role based API LimitsHigh Performance GraphQLWith the right API limits configured, you prevent un-intented hits to the database. If you are using Hasura to connect your database(s), you don't need to worry about the typical N+1 problem. Optimising for performance though is not limited to the database but to the actual API being consumed.This brings us to Caching GraphQL APIs on the Server Side. Typically your queries could be slow due to various parameters like size of the response, location of your server (thereby latency) and no of concurrent API calls. You want to keep the response times of your queries as low as possible.Writing your own caching solution for GraphQL involves maintaining a low latency key-value store like Redis at the edge. Cache invalidation logic needs to be defined and how you cache authenticated data will usually require lots of boilerplate and logic specific code.Cache your existing GraphQL ServerHasura Cloud has support for caching your existing GraphQL server. You canJust use the @cached directive to existing GraphQL queryGet automatic edge caching for free*Cache authenticated data\nSince Hasura has metadata about the different GraphQL types used and the corresponding Authorization rules defined per role, it is able to offer an end to end caching solution.Cache invalidation can be configured with the @cached directive arguments like ttl. You also get the flexibility of API access to manually clear cached data.Once you hit your custom GraphQL server through Hasura with the cached directive, the HTTP response will include a Cache-Control header, that indicates the maximum number of seconds for the returned response to remain in the cache. The infrastructure of maintaining and invalidating the cache is taken care by Hasura Cloud.*Support for Edge caching is coming soon to Hasura Cloud. Right now caching is limited to the region where the Hasura project was created.Monitoring and ObservabilityGetting the performance and security benefits helps get your APIs production ready. But when the GraphQL API is running in production, you need true visibility into the system to diagnose issues or learn about user behaviour. At any point of time, you should be able to ask any arbitrary question about how your application works.Hasura Cloud Console comes with the Monitoring tab which gives you detailed insights into your API availability and performance. You canMonitor errors and get detailed breakdown of individual requestsDistributed tracing for your APIIdentify slow queriesWebsocket monitoring for realtime use cases\nSince you are working with custom GraphQL servers, you want visibility into how the API request traverses through different components of the architecture. First your client hits the Hasura endpoint. Hasura parses the query to decide if it is a database query or a remote schema query and proxies the API call to the right connection. Now the performance could be slow in certain components of this architecture and you want to get to the root of the problem whenever an issue arises. Distributed tracing attempts to give a unified view about the various performance characteristics of this architecture.\nJoin API response with other data sourcesTypically your custom logic and data from external sources will have some relationship with the rest of your API, either through database connection or through other external APIs.Remote Joins in Hasura extend the concept of joining data across tables, to being able to join data across tables and remote data sources. Once you create relationships between types from your database and types created from APIs, you can then \u201cjoin\u201d them by running GraphQL queries.Establish relationships between other data sources via Remote JoinsJoin with database or any other GraphQL source\nYou can look at examples of Remote Joins to get an idea of setting up custom GraphQL servers and extending Hasura.Get GraphQL over existing REST APIWriting a custom GraphQL server for business logic is a feasible option if you are starting from scratch. But what if you already have an existing REST API and want to reuse the logic? Now again, this could be re-used inside your GraphQL resolver. Hasura actually makes this easy by letting youMap your REST API to GraphQL typesGenerate boilerplate with codegen\nActions work well in the 3factor architecture where the HTTP handlers can be deployed as serverless functions.Read our series of posts on Turning your REST API to GraphQL using Hasura Actions.Get started with production ready GraphQL today!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Modern GraphQL examples with strings, compilers, and SDKs Description: Learn all about the modern ways to query with GraphQL with strings, compilers, and SDKs! Author: Jesse Martin"}, {"role": "assistant", "content": "Every few years or so it seems relevant to review the current state of GraphQL, see which trends have caught on with the industry, what the current best practices are, and make some speculation about what\u2019s coming next. We recently reviewed the current state of GraphQL directives, which you can read about here. Today, we\u2019ll have a refreshed (2021) look at the query syntax itself.Are you new to GraphQL? This article will cover details on the current state of querying content with the GraphQL query syntax. For an introduction to GraphQL, have a look at this article, here.Types of OperationsOne of the confusions people face with GraphQL is \u201cwhat is a query\u201d. This confusion comes from the fact that a query is a query. The syntax that is sent to the server to fetch data is a query, but a query can have one of three operation types, query, mutation, or subscription.If you\u2019re looking for a more concise intro to GraphQL guide, have a look at our core concepts guide from our learning resources.The name immediately following the operation type is the operation name. These are often used for by server-side tooling to know exactly which query was requesting that data since the query itself can have nearly any shape. Hasura Cloud uses these names to support features like observability metrics, caching, and allow lists.To complicate matters, you\u2019ll often see the query operation assumed by default because most GraphQL servers (as allowed by the specification) will assume a \u201cquery query type\u201d. It\u2019s an accepted query shorthand.With that bit of confusion out of the way, let\u2019s look at the remaining parts of thequery syntax.Parts of the queryA query, regardless of type, is composed of a handful of components to create the final document sent to the server.FieldsFields, sometimes referred to as leaves of the query, are the actual pieces of data you are wanting to fetch from the server. As the GraphQL specification requires, the data responding from the server must return valid JSON, and so the fields are the keys to the JSON object.Some have argued that this type of readable syntax has led to the mass adoption of GraphQL. It\u2019s considerably readable for anyone looking to understand what data is being requested for. It\u2019s also very expressive for its simplicity. Have a look at what a relational join query looks like.It\u2019s very clear what we are requesting, and what we expect will return from the server.AliasesA limitation to GraphQL fields, since it returns valid JSON, is that keys need to be unique, and so you can only request fields one time. But there are cases you may want to request the same field for a different reason. Or you may want to rename the key returning from the server. For that, we need to use Aliases.ArgumentsWith a readable syntax, we can begin to see the benefits of GraphQL, but without the ability to pass in search parameters, we are missing a very important part of any query language! That\u2019s where we get arguments. Arguments are defined as an Input Type, and allow us to make requests for subsets of our data. Every vendor implements arguments differently in terms of the accepted syntax. Some follow a mongo-inspired theme, others follow a more traditional SQL-like naming convention. Some frameworks and tooling like Relay requires a guaranteed set of arguments to support automatic pagination and other framework-specific features. For the purposes of our examples, I\u2019ll be using the same syntax that Hasura uses, which means you can use these examples in your own queries.When combined with aliases that we saw above, we can begin to see some interesting use cases.FragmentsLooking at syntax like this, we can already see a problem arising. This is not DRY syntax! That\u2019s where fragments let us define sub-selection of fields that we can re-use across our queries. Fragment syntax starts by declaring a fragment, providing a name for the fragment, and specifying which type the subset of fields will be executed on.VariablesWe may want to declare a specific query format, but there may be some variable input from our users. For that, we can specify variable input syntax. Variables are declared with the `$` syntax followed by the variable name and a declaration of what type it is. Variables can represent any number of complex inputs that may be needed.We can parameterize the above query by simply accepting the integer as a variable.Or we can provide a more complex input, replacing the entire filter statement that we generate from our application code.The variables themselves are sent as a separate parameter in the request to the GraphQL endpoint. The final body payload represents the following shape:It\u2019s important to indicate which of the input arguments are required per the schema definition. `$price: Integer` and `$price: Integer!` have very different implications for our resolving logic on the server! If you declare a variable in your query, you must provide, and conversely, if you provide a variable, you must use it, otherwise, the query will not validate in the query planning stage.DirectivesDirectives allow us to provide variable execution for our queries based on small syntactical changes. We wrote about GraphQL directives at length in this article. An example of a directive in Hasura Cloud is providing a `cached` directive.Ways to QueryAll GraphQL servers accept two inputs in the body as we saw with variables above, the `query` and `variables` parameters. Variables can be of any valid JSON type, with all declared variables needing to be provided, and all provided variables needing to be declared. The query will always be a string type.In the following sections, we\u2019ll look at common examples of a compiled query, but there are a number of frameworks and tools now that provide helpers around generating queries into their final format. The days of string concatenated queries are long behind us!String QueryAs mentioned above, all queries will finally be sent to the server in a string format. In languages that support string definitions with line breaks, it can be quite helpful for declaring our query.Regardless of how the string comes together, below we\u2019ll look at some common patterns for putting queries together.Examples of GraphQL QueriesBasic Queryquery GetItems {\n  items {\n    name\n    value\n  }\n}\n\nQuery with Inputsquery GetItems {\n  items(where: {\n    value: {\n      _lt: 50\n    }\n  }) {\n    name\n    value\n  }\n}\n\nQuery with Variablesquery GetItems($value: Integer) {\n  items(where: {\n    value: {\n      _lt: $value\n    }\n  }) {\n    name\n    value\n  }\n}\n\nExamples of GraphQL MutationsBasic Mutationmutation CreateItem {\n  insert_item_one(object:{\n    name: \"New Item\",\n    value: 50\n  }) {\n    name\n    value\n  }\n}\n\nMutation with Variablesmutation CreateItem($item: item_insert_input!) {\n  insert_item_one(object: $item) {\n    name\n    value\n  }\n}\nExamples of GraphQL SubscriptionsBasic Subscriptionsubscription GetItems {\n  items {\n    name\n    value\n  }\n}\n\nSubscription with Inputssubscription GetItems {\n  items(where: {\n    value: {\n      _lt: 50\n    }\n  }) {\n    name\n    value\n  }\n}\n\nSubscription with Variablessubscription GetItems($value: Integer) {\n  items(where: {\n    value: {\n      _lt: $value\n    }\n  }) {\n    name\n    value\n  }\n}\n\nExamples of Fragments, Aliases, and DirectivesFragment examplefragment ItemDetails on Item {\n  name\n  value\n}\n\nquery GetItems {\n  items {\n    ...ItemDetails\n  }\n}Alias Examplefragment ItemDetails on Item {\n  name\n  value\n}\n\nquery GetItems {\n  set1: items { # <---\n    ...ItemDetails\n  }\n  set2: items { # <---\n    ...ItemDetails\n  }\n}\n\nDirective Examplesquery GetItems @cached { # <---\n  items(where: {\n    value: {\n      _lt: 50\n    }\n  }) {\n    name\n    value\n  }\n}\n\nTry out more GraphQL queries here.Fluent GraphQLWhile all queries will end up as strings, the path to get there can be whatever you want! An entire series of clients have popped up under the umbrella of \u201cFluent GraphQL\u201d - the ability to write queries as abstracted concepts. We\u2019ve written a previous round-up on the various providers here. For the purposes of this example, we\u2019ll use GraphQL Zeus which is the client we used for our Haura Super App which is a large reference architecture.Whichever client you use, the pattern is fairly similar. You provide your endpoint to some form of CLI, and an SDK is generated that allows you to provide object-like query structures. The benefit of this approach is that you can use typical object manipulation such as spread operators, optional subsets, and more.Examples of GraphQL Queries with GraphQL ZeusBasic QueryGql.query({\n    items: [{}, {\n        name: true,\n        value: true\n    }]\n})\n\nQuery with InputsGql.query({\n    items: [{\n        where: {\n            value: {\n                _lt: 50\n            }\n        }\n    }, {\n        name: true,\n        value: true\n    }]\n})\n\nQuery with Variables# import the $ helper from the generated client\n\nGql.query({\n    items: [{\n        where: {\n            value: {\n                _lt: $`value`\n            }\n        }\n    }, {\n        name: true,\n        value: true\n    }]\n}, {\n    value: 50\n})\n\nExamples of GraphQL Mutations with GraphQL ZeusBasic MutationGql.mutation({\n    insert_item_one: [{\n        object: {\n            name: \"New Items\",\n            value: 50\n        }\n    }, {\n        name: true,\n        value: true\n    }]\n})\n\nMutation with Variables# import the $ helper from the generated client\n\nGql.mutation({\n    insert_item_one: [{\n        object: $`item`\n    }, {\n        name: true,\n        value: true\n    }]\n}, {\n    item: {\n        name: \"New Item\",\n        value: 50,\n    }\n})\n\nExamples of GraphQL Subscriptions with GraphQL ZeusBasic SubscriptionGql.subscribtion({\n    items: [{}, {\n        name: true,\n        value: true\n    }]\n})\n\nSubscription with InputsGql.subscription({\n    items: [{\n        where: {\n            value: {\n                _lt: 50\n            }\n        }\n    }, {\n        name: true,\n        value: true\n    }]\n})\n\nSubscription with Variables# import the $ helper from the generated client\n\nGql.subscription({\n    items: [{\n        where: {\n            value: {\n                _lt: $`value`\n            }\n        }\n    }, {\n        name: true,\n        value: true\n    }]\n}, {\n    value: 50\n})\n\nExamples of Fragments, Aliases, and Directives with GraphQL ZeusFragment example#js Example\n\nconst itemDetails = {\n    name: true,\n    value: true,\n}\n\nGql.query({\n    items: [{}, {\n        ...itemDetails #They're simple objects!\n    }]\n})\n\nAlias ExampleGql.query({\n    __alias: {\n        set1: {\n            items: [{}, {\n                name: true,\n                value: true\n            }]\n        },\n        set2: {\n            items: [{}, {\n                name: true,\n                value: true\n            }]\n        }\n    }\n})\n\nDirective ExamplesGraphQL Examples with GraphQL Code GeneratorGraphQL code generator is a tool provided by the Guild, a consortium of open source developers that provide open tooling for the GraphQL ecosystem. What sets GraphQL Code Generator apart is the ability to generate an SDK from GraphQL documents. Where you write your queries, even testing them in GraphiQL, but still benefit from the SDK-like behavior in your application. Paired with a tool like Hasura Cloud\u2019s allow list, you get a locked-down API with an explicit handshake between your SDK and your API.What this means, is that if we take a query such as:query getItems {\n  items(where: {\n    value: {\n      _lt: 50\n    }\n  }) {\n    name\n    value\n  }\n}\n\nAnd you import it into your project, you can access it using an SDK style accessor such as:const { items } = await sdk.getItems();\n\nEverything is exactly the same as writing traditional style GraphQL queries, mutations, and subscriptions. You just get a wrapper with invocation and authentication built in.SummaryThe GraphQL ecosystem is growing with newer and better tools being created every day. The community is vibrant and actively pushing the specification maintainers to expand what anyone thought was possible just a few years ago, making GraphQL, and learning how to query with GraphQL, a must have skillset for any developer today."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Implementing a Google Drive Style Hierarchical Authorization System in Hasura Description: A guide to implementing a hierarchical, role-based ACL system pattern similar to Google Drive, with individual permission overrides and permission inheritance. Author: Gavin Ray"}, {"role": "assistant", "content": "Table of Contents\n\nIntro\nOverview\n\nDatabase Schema\n\n\nHow to accomplish this\n\nIntro\nHasura is an open-source platform that provides a realtime GraphQL and REST APIs on top of data sources. These data sources can be any mixture of databases, REST API's, or even other GraphQL API's. Hasura generates a unified, federated schema and data-graph, and enables defining virtual relationships between data that lives in different places. This allows to transparently query across data sources as though they were one entity.\nAuthorization and access control is critical when dealing with data-access. Hasura has a role and permissions system to handle this that is capable of most any scenario.\nSome scenarios require a bit of engineering between the database and Hasura's permission system though, and can be tricky to get right.\nAn auth model which has come up several times now is one based around hierarchical roles (IE manager, should have permissions of both manager and employee) AND dynamic access controls (IE user with id = 3 can READ/WRITE thing with id = 5)\nThis recipe gives an example of implementing such an auth pattern -- similar to the one you'd find in an environment like Google Drive.\n\nNote: For further reference, you can check out our other post which contains a similar guide at:\n\nhttps://hasura.io/blog/authorization-rules-for-multi-tenant-system-google-cloud/\n\nOr the tutorial on our \"Learn\" resources at:\n\nhttps://hasura.io/learn/graphql/hasura-auth-slack/introduction/\n\nWe're going to be publishing more posts on AuthZ patterns over this month. If you have any requests, message me on Discord / Twitter: @GavinRayDev\n\nOverview\nThe scenario represented in the application is:\n\nAn architecture project for a house\nCollaborators are broken up into \"teams\"\nTeams have been assigned parts of the project (and access to those folders)\n\nUsers may be assigned to one or more teams\nIndividual users may have folder permissions overriden/specified\n\n\nBeing assigned access to a folder grants access to all child folders\n\nUnless an overriding rule is present\n\n\n\nTo visualize, you might imagine something like this:\n\n\n\n\nDatabase Schema\nThe schema for the database is composed of seven tables, and a few views used to calculate permissions:\n\nCREATE TABLE user (\n  id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  name text NOT NULL\n);\n\nCREATE TABLE folder (\n  id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  name text NOT NULL,\n  parent_folder_id int REFERENCES folder (id)\n);\n\nCREATE TABLE team (\n  id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  name text NOT NULL\n);\n\nCREATE TABLE user_team (\n  user_id int NOT NULL REFERENCES user (id),\n  team_id int NOT NULL REFERENCES team (id),\n  PRIMARY KEY (user_id, team_id)\n);\n\nCREATE TABLE permission_folder_user (\n  folder_id int REFERENCES folder (id),\n  user_id int NOT NULL REFERENCES user (id),\n  access_type text NOT NULL\n);\n\nCREATE TABLE permission_folder_team (\n  folder_id int REFERENCES folder (id),\n  team_id int NOT NULL REFERENCES team (id),\n  access_type text NOT NULL\n);\n\nCREATE TABLE user_folder_blocklist (\n  folder_id int REFERENCES folder (id),\n  user_id int NOT NULL REFERENCES user (id),\n  reason text NOT NULL\n);\n\nHow to accomplish this\nThe end goal is to wind up with a flat table, containing the:\n\nFolder ID\nAccess Level\n\nFor every user and folder they are allowed to access.\nSomething like:\n\n\n\nfolder_id\nuser_id\naccess_type\n\n\n\n\n1\n1\nread\n\n\n2\n1\nwrite\n\n\n3\n1\nwrite\n\n\n3\n2\nread\n\n\n\nModeling this directly with Hasura's permission clauses isn't possible because the depth of the relationships is unknown ahead of time -- the parent-child tree could be arbitrarily deep.\nBecause of this, we need to use SQL to dynamically generate the \"flattened\" view of the relationships and folder <-> user <-> access level records.\nWhile not the focus of this walkthrough, an implementation of that can be found below:\nCREATE\nOR REPLACE VIEW \"acl\".\"permission_folder_user_and_team\" AS\nSELECT\n  DISTINCT ON (q.folder_id, q.user_id)\n  q.folder_id,\n  q.user_id,\n  q.access_type\nFROM\n  (\n    SELECT\n      pfu.folder_id,\n      pfu.user_id,\n      pfu.access_type,\n      0 AS specificty\n    FROM\n      permission_folder_user pfu\n    UNION ALL\n    SELECT\n      pft.folder_id,\n      ut.user_id,\n      pft.access_type,\n      1 AS specificity\n    FROM\n      (\n        permission_folder_team pft\n        LEFT JOIN user_team ut ON ((pft.team_id = ut.team_id))\n      )\n  ) q\nORDER BY\n  q.folder_id,\n  q.user_id,\n  q.specificty,\n  q.access_type DESC;\n\nCREATE\nOR REPLACE VIEW \"acl\".\"folder_ancestor\" AS WITH RECURSIVE flattened_folder(\n  folder_id,\n  folder_name,\n  ancestor_id,\n  ancestor_name,\n  depth\n) AS (\n  SELECT\n    folder.id AS folder_id,\n    folder.name AS folder_name,\n    folder.id AS ancestor_id,\n    folder.name AS ancestor_name,\n    0\n  FROM\n    folder\n  UNION ALL\n  SELECT\n    ff.folder_id,\n    ff.folder_name,\n    f.parent_folder AS ancestor_id,\n    f2.name AS ancestor_name,\n    (ff.depth + 1)\n  FROM\n    flattened_folder ff,\n    folder f,\n    folder f2\n  WHERE\n    (\n      (ff.ancestor_id = f.id)\n      AND (f.parent_folder IS NOT NULL)\n      AND (f2.id = f.parent_folder)\n    )\n)\nSELECT\n  flattened_folder.folder_id,\n  flattened_folder.folder_name,\n  flattened_folder.ancestor_id,\n  flattened_folder.ancestor_name,\n  flattened_folder.depth\nFROM\n  flattened_folder\nORDER BY\n  flattened_folder.folder_id,\n  flattened_folder.depth;\n\nCREATE\nOR REPLACE VIEW \"acl\".\"flattened_folder_permission\" AS\nSELECT\n  DISTINCT ON (q.folder_id, q.user_id)\n  q.folder_id,\n  q.user_id,\n  q.access_type\nFROM\n  (\n    SELECT\n      ff.folder_id,\n      ff.folder_name,\n      ff.ancestor_id,\n      ff.ancestor_name,\n      ff.depth,\n      pfu.user_id,\n      pfu.access_type\n    FROM\n      (\n        acl.folder_ancestor ff\n        JOIN acl.permission_folder_user_and_team pfu ON ((ff.ancestor_id = pfu.folder_id))\n      )\n  ) q\nORDER BY\n  q.folder_id,\n  q.user_id,\n  q.depth;\n\nCreating the view mentioned above should give us the table we need, with data that looks something like this:\n\nNow what we want to say is:\n\nFor a folder:\n\nAllow them to view a folder if:\n\nThe <current users id> is present in <the flattened folder permissions view>\n\n\nAllow them to update the folder if\n\nThe <current users id> is present in <the flattened folder permissions view>\nThe <current user id> record in the <the flattened folder permissions view> has an access type of write\n\n\n\n\n\nThe way we phrase the first permission in Hasura is like:\n{\n    \"permissions\": {\n        \"user_id\": {\n            \"_eq\": \"X-Hasura-User-Id\"\n        }\n    }\n}\n\n\nAnd to configure the permissions for restricted update capabilities, is like:\n{\n    \"permissions\": {\n        \"_and\": [\n            \"user_id\": {\n                \"_eq\": \"X-Hasura-User-Id\"\n            },\n            \"access_type\": {\n                \"_eq\": \"write\"\n            }\n        ]\n    }\n}\n\n\nAfter adding these two rules, congratulations!\nWe've now successfully implemented a hierarchical, role-based ACL system with individual permission overrides and permission inheritance!\nIf you have any questions or want to share your thoughts on the topic, reach out to us on Twitter / Discord / GitHub Discussions.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL directives you should know about Description: Learn all about GraphQL directives in this article where we talk about specification directives, Hasura directives, and community defined directives for architecting the perfect API. Author: Jesse Martin"}, {"role": "assistant", "content": "What are GraphQL directives?From the specification, \u201cDirectives provide a way to describe alternate runtime execution and type validation behavior in a GraphQL document.\u201d Essentially, directives in the GraphQL specification are annotations that can be used to alter the behavior of a query or mutation. They\u2019re easy to recognize because they are always prefixed with the \u201c@\u201d character. The GraphQL specification itself states that all spec-compliant schemas need to implement @skip, @include, @deprecated, and @specifiedBy \u2013 but more on those, and the upcoming standard directive implementations, later. Where can GraphQL directives be used?Directives can only be used in the places their specification allows for, namely, what\u2019s defined after the \u201con\u201d statement of a directive definition. For example, from the GraphQL schema specification, this is what a \u201cdeprecated\u201d directive definition looks like.directive @deprecated(\nreason: String = \"No longer supported\"\n) on FIELD_DEFINITION | ARGUMENT_DEFINITION | INPUT_FIELD_DEFINITION | ENUM_VALUE\n\nNote that the locations defined after \u201con\u201d are where this directive can be used. Beyond that, the specification is very open where directives can be used! As defined in the literature, directives can be created on either executable or type system locations. Type system directive locations are: SCHEMA, SCALAR, OBJECT, FIELD_DEFINITION, ARGUMENT_DEFINITION, INTERFACE, UNION, ENUM, ENUM_VALUE, INPUT_OBJECT, and INPUT_FIELD_DEFINITION. Executable directive locations are QUERY, MUTATION, SUBSCRIPTION, FIELD, FRAGMENT_DEFINITION, FRAGMENT_SPREAD, and INLINE_FRAGMENT. That means it\u2019s open for implementation as each API provider seems fit. Directives can live on fields, inputs, operations, and more.Example of an operation directive:query GetItems @cached {\n    id\n}\n\nExample of a field directive:query GetItems($shouldNotGetNames: Boolean) {\n    id\n    names @skip(if:$shouldNotGetNames)\n}\n\nWhat do GraphQL directives do?Returning again to our opening statement that directives allow us to modify the behavior of our queries and mutations, let\u2019s look at the default behavior of the standardized directives, the behavior of the Hasura specific directives, and some generally creative implementations we\u2019ve seen in the larger GraphQL community.Current Specification Directives@deprecatedThe @deprecated directive defines fields that are no longer supported or will likely be replaced soon. Given the iterative nature of a GraphQl endpoint, being able to let consumers know that a specific field they are using will be replaced, along with a reason and direction on what to use instead is an essential method for communicating changes to the API design.@skipThe @skip directive lets you specify when the following field should not be returned based on a truthy condition.@includeThe @include directive lets you specify when the following field should be returned based on a truthy condition. This is the opposite of the @skip directive but likewise allows for choosing when a field should or should not be returned.@specifiedBy@specifiedBy provides type references as to where the specific field type is defined in the literature. If you want to explain which date type or what the specifics around the use of a UUID type are, you can link to their official reference documentation.Future Specification DirectivesGithub RFC for reference: https://github.com/graphql/graphql-js/issues/2848@defer with argsWhen fetching data from data layers where not all content priority should be treated equally, the @defer directive lets you explain when it\u2019s ok to resolve data early. As the HTTP/2 spec becomes more widely implemented, we\u2019ll see an uptick in best practices recommending the use of @defer. This argument takes a label input that labels the response for front-end orchestration@stream with args@stream lets the user define how many items the API should respond with before the initial response. Where there may be a few hundred results from the server, the @stream directive lets you define a minimum required number before getting the first batch of results sent to the server. This is also technology-dependent and as we see specifications like HTTP/2 making headway in adoption, we\u2019ll see these directives gain more precedence. This input takes an initialCount argument for defining what the threshold is along with a label argument for identifying the follow-up payload from this server.Hasura defined Custom Directives@cachedThe Hasura API enables an additional directive called @cached that enables automatic caching on Hasura Cloud. The arguments for this directive are ttl (time to live), defined in seconds with an upper bound of 300 seconds, and a boolean refresh argument to declare if the cache should be automatically cleared or not. You can read more about the @cached directive and test it out with an interactive example here.Community defined Custom Directives@restPerhaps a more common use-case among API designers would be implementing a directive like this @rest directive. Its arguments take a REST endpoint and allow for resolving the data that lives at the specified endpoint. This would be quite common for data-federation architectures.@dbquerySimilar to the data resolver pattern seen in the @rest design, @dbquery resolves data at the specified database endpoint with the specified query syntax. It\u2019s a query within a query. While the overhead may be problematic for long-term maintenance, this brute-force approach helps prototype GraphQL endpoints quickly.@materializerThis query directs the requested fields to be executed against a secondary GraphQL endpoint, combining the end result into a single API.@externalA similar pattern as @materializer, @external indicates that the requested data is owned by an \u201cexternal\u201d service.@requires(fields: _FieldSet!)For services requiring annotated APIs for federation, the @requires directive lets a service identify the needed data to resolve fields from an external dataset.@provides(fields: _FieldSet!)Let a consuming service know which fields are guaranteed to be in the response and thus, allows the consuming service to make predictable guarantees about that data shape.@key(fields: _FieldSet!)When federating data sources, it\u2019s important to track uniqueness across datasets, and this directive allows for declaring which fields or which product of fields will guarantee that uniqueness.@_Perhaps one of the most interesting examples of directive usage, this concept creates a breakout utility set for accessing lodash style utility helpers on queries and query responses. If formatting data at the request stage is something you need to accommodate for, this might just be the tool you have been looking for."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura and The Low-code Ecosystem Description: In this article, we'll have a look at the current state of the low-code ecosystem, how its evolving, and what it means for users of Hasura. Is low-code right for you? Come and find out. Author: Jesse Martin"}, {"role": "assistant", "content": "Over the course of a series of tutorials, we explored the booming ecosystem of low-code tools. We built mobile, desktop, and admin applications. We also built workflow automation to help manage the flow of data.The tutorials:Mobile app development with Draftbit.Web-app development with Bubble.Internal tooling with Retool.In surveying the current ecosystem, one that is estimated to reach $13 billion by the end of 2021, there are some key observations to make.Less code, but more thoughtAs with any tool that increases the speed of change, more thought is required for getting the intended results. Low-code is no different. If you are able to add a table to an interface, attach some buttons, and create reactive listeners within a small handful of clicks, knowing what you intend them to do becomes the majority of effort applied.Well-documented application flows lead to well-communicated production timelines, lead to well-defined outcomes.Less code, not less programmingWhile the widgets abstract the implementation details of the code written, thinking like a programmer (logic handlers, algorithm implementation, reactivity) is still very much required. Low-code lends itself very well to long-term maintenance by someone with technical acumen and logical thinking, without needing to learn the specific syntax of the programming language du jour.Less code, but more appsOf the observations that stood out, one that stood out the most was just how well these tools play with something like Hasura. When coupled with a hosted tool like Hasura Cloud, you have a closed circuit that\u2019s able to iterate quickly on both the data and presentation layer \u2013 delivering more \"app interfaces\" for teams needing to act on and respond to your businesses changing data environment.The Hasura specific features that stood out are:Fine-grained access controlsThe permissions layer allows you to determine which tools can access which data and be very specific about your data security.REST EndpointsBeing able to leverage the dominance of REST with the descriptive nature of GraphQL lets you both fine-tune exactly what data any specific client will be requesting, as well as provide flawless interoperability without the need for a proxy wrapper.The Hasura Cloud features that stood out are:API ObservabilityUsing the monitoring tab it was simple to see if the queries were malformed, the database was under too high of load, or whatever else may be the cause of an API-side error. Sometimes the low-code tools would emit opaque messages about failing to load data, and the Hasura Cloud functionality to see what was failing was helpful.Instant CacheMany of the low-code tools are not optimized for query performance, specifically around call frequency. Being able to sprinkle cache on as needed allowed for quick query performance, but more importantly, helped keep these tools from DDoSing our server.The low-code space is one worth watching if not outright embracing. With significant offerings from major name brands in the enterprise system, down to the classic \u201cprosumer\u201d product offerings, virtually any company could benefit from the efficiency and improved time-to-value that low-code offers.Let us know if you have any questions! You can hit us up on github discussions / discord if you need any help!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcement: Hasura Cloud Achieves SOC2 Type 2 Certification Description: Hasura Cloud is now SOC 2 Type 2 certified, enabling more companies to place their critical workloads on Hasura Cloud and have the best of both enterprise security and first-class developer experience. Author: Jesse Martin"}, {"role": "assistant", "content": "Following on from our last SOC2 Type 1 announcement in December 2020, we are happy to announce that Hasura Cloud has achieved SOC2 Type 2 certification. This further emphasizes our continued commitment to security for our users and our community to enable mission-critical workloads to be run on Hasura. If you'd like to read more about our SOC2 Type 1 certification (and HIPAA compliance attestation) you can find that post at this link.About the certification:The SOC (Service Organization Controls) 2 Type 2 examination was conducted by an independent, third-party accounting and auditing firm, which evaluated Hasura's processes, procedures, and controls for security, availability and confidentiality (together known as the Trust Services Criteria).The examination was conducted by an independent third party accordance with the American Institute of Certified Public Accountants (AICPA) attestation standards and assurance standards issued by the International Auditing and Assurance Standards Board (ISAE 3000) and can be trusted by US and international customers.What does this mean for our customers?This certification means that Hasura Cloud has been independently verified to have implemented a secure platform, and has a verified track record of maintaining this implementation.By undergoing this rigorous scrutiny of our security practices, we demonstrate our commitment to protecting our customers and the trust you place in us. You can read more about our security practices at hasura.io/security/.Paving the way for platforms built on Hasura CloudAn additional benefit for customers building their own SaaS offerings with Hasura Cloud \u2013 SOC2 Type 2 is within your grasp, too! Secure platforms need to be built with secure tools, and this latest certification removes that extra hurdle on your own way to delivering exceptional security to your customers.Whether you are building in FinTech, Health Tech, SaaS, PaaS, or more \u2013 Hasura Cloud provides the stability and security you need to deliver secure apps, faster.Customers with compliance requirements may request Hasura\u2019s SOC 2 Type 2 report as part of their compliance strategy. The report will be shared under an NDA.Let us know if you have any questions! You can hit us up on github discussions / discord if you need any help!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Tutorial: Get instant GraphQL APIs on BigQuery with Hasura Description: In this tutorial, we learn how to get instant, dynamic APIs on top of BigQuery with Hasura. Get a BigQuery API in just a couple of clicks! Author: Jesse Martin"}, {"role": "assistant", "content": "IntroductionWe recently launched BigQuery support for Hasura (to get a full list of supported databases, look here.In this guide, we\u2019ll show how to connect BigQuery to Hasura Cloud, set default limits, and run a few queries.This guide will follow in two parts, feel free to skip to the part that is more interesting for you.Part I: Connecting BigQuery (you are here)Part II: Building a Dashboard with BigQuery, Optimizing PerformanceTo get started with Hasura on BigQuery you can head straight to the documentation:\u201cGetting started with BigQuery\u201d.BigQuery with HasuraBigQuery provides a rich ecosystem of utilities and services for wide-column datasets that have made it a market leader in use-cases that need to track many points of detail with high-transaction counts.Hasura excels in the space between BigQuery and the front-end architects. Hasura makes really fast, dynamic, APIs, provides descriptive access controls, and if you are using Hasura Cloud, enables automatic rate limiting, caching query depth controls, and more.Combining Hasura with BigQuery provides a fast and descriptive API for BigQuery that can be meshed with the rest of your data-layer, opening up a wider set of use-cases for the massive amounts of data that BigQuery typically stores.Features of note are:Dynamic GraphQL APIs that are generated based on the model in your database.Fine-grained authorization controls that ensure appropriate data ownership practices.An @cache directive for annotating queries out of BigQuery for instant cache.Setting API rate limits and query depth limits to ensure efficient and affordable database utilization.Allow-lists that let you observe your actual query usage, and save those to an approved list, preventing unintended use of the application.Utilizing these features you free up your developers to explore the dataset and surface new and creative ways to work with data you have while still maintaining good data hygiene, ownership hierarchy, and privacy best practices. Write less proxy code and more domain code by integrating BigQuery with Hasura.Early users of the BigQuery integration have used Hasura to build instant, dynamic APIs on top of BigQuery as well as speed up application development timelines. With Hasura, frontend developers on the team are able to do the work of data engineers thereby decreasing the overall time to market.Connecting the APIsIn this first video, we\u2019ll walk through connecting BigQuery to Hasura, tracking tables, defining custom relations and setting reasonable security defaults. In the followup video, we\u2019ll look at accessing that content from a front-end application.ClosingIn the next video, we'll cover adding permission controls and query optimizations as we load the data into a front-end application."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Rapid Prototyping with Hasura: Fifteen minutes to a published app with zero lines of code Description: In this quick guide, we'll go from data model to rendered UI in 15 minutes with zero lines of code. Author: Jesse Martin"}, {"role": "assistant", "content": "As developers, we are already familiar with \u00a0benefit of low-ops/no-ops tooling. We enjoy having the questions of scale, guaranteed uptime, and dedicated support as features we can toggle on and off.In this quick video, I want to demonstrate how these tools support rapid-prototyping. With low-ops, you're applications is already built on a resilient stack from day-one, and is ready to scale and mature with your business objectives.In this video I'll run through database modelling with visual feedback, database procurement and adding an API layer with Hasura Cloud, which we'll then finish off with iterating over an admin interface for backend tooling.Our featured stack will be:DBDiagram.io for our database modelling, utilizing their open-sourced DSL for descriptive modelling.Heroku provides our Postgres database. We'll specifically be utilizing the integration with Hasura itself to provision the database and take our transformed DSL from DBDiagram to Postgres SQL statements.Mockeroo is a mock-data generator that allows us to create prototype content quickly and descriptively.Hasura provides the generated API for our content and is the basis of our application.Retool is the hosted \"internal tool builder\" that lets us quickly iterate on ideas with a visual-first approach, deploying both development versions and production-ready releases.ChaptersFor a short-hand guide to the following video, you can jump to the following topics on the timeline.00:00 Introduction to rapid prototyping00:58 Creating a project in Hasura01:19 Working with DBDiagram.io05:50 Creating our tables in Postgres08:10 Creating mock data with Mockeroo10:00 Importing mock data11:20 Working with Retool 14:10 OutroLet me know if you have other rapid prototyping tools that you're working with alongside Hasura. Join our community on Discord and you can find me there @jesse \u2013 thanks for reading!Let us know if you have any questions while trying these out! You can hit us up on github discussions / discord if you need any help!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Multi-tenancy: A History Quick Take & How-to with Hasura Description: Look at various ways to setup multi-tenant applications ranging from a data-centric approach to an architectural perspective. The article wraps up with short summary and an example of a multi-tenant setup in a Hasura instance across schemas using a multi-tenant data based example. Author: Hasura"}, {"role": "assistant", "content": "The first notion of multi-tenant applications started in the 1960s. You read that correctly, they are indeed that old. Companies would rent time, processing times and work time, on mainframes. In that time mainframes were expensive, individuals didn't own computers, and the idea that one day individual people would have more computing power in their hand than a mainframe was a wild dream. Thus this idea to share costs, share resources, and gain the advantages of a mainframe computer was born. The concept then was identified as time sharing.\nYears went by and variations of this theme continued even amidst individuals starting to purchase computers, termed personal computers or as we still term them today, \"PCs''. Even with these and other computer variants, time sharing continued as a prominent computing style for mainframes and supercomputers.\nThen in the 1990s a new creation appeared that was termed application service providers, or ASPs. They hosted applications for their customers and similarly to mainframes, these applications were made available to multiple customers. This form was short lived but existed to serve and inform the next decade's revolutionary computing change that erupted with cloud computing. Which in many ways, cloud computing is effectively what ASPs had hoped to become, but the bandwidth and resources in that era's computing just wasn't up to the task.\nToday, a solid decade plus worth of years into cloud computing we now have many different types of multi-tenancy based providers out there. Almost anything called something another -as-a-service is multi-tenant in some way. The cloud providers themselves are vast examples of multi-tenant services. Many companies you know; Salesforce, Google Docs, Office 365, Spotify, Slack, Steam, and other services are all multi-tenant.\nPedantic Additions\nLet's get pedantic now about multi-tenancy and pick apart some of the specifics.\nconflations...\nThere are many things that are termed multi-tenant that provide an element of what is needed to provide multi-tenant services but they aren't really multi-tenant themselves. For example, hyper-visor virtualization services are not multi-tenant. It's a tool that is regularly used to provide multi-tenant services but it is merely a tool. The same goes for the popular containerization tools such as Docker, it's a tool, and isn't particularly multi-tenant.\ndifferentiations...\nWhen talking about multi-tenancy one can often twist a single-tenant description into multi-tenancy and things can get confused. Single-tenant can mean a single tenant with a multi-tenant service or a singular tenant within a multi-tenant offering. Generally in a single-tenant environment we can think of each customer sharing software, servers, databases, or at least parts of the software, server, or databases. In a multi-tenant environment each customer would get their own software, services, or databases, however that line can get real muddied real fast!\nHasura Multi-Tenancy\nFor this section I\u2019m going to break out a few options around using Hasura from several multi-tenant perspectives;\n\nMulti-tenancy by service/API.\nMulti-tenancy by schema & data.\nMulti-tenancy by database.\n\nEach of these perspectives can be mixed with the other, to provide various abstractions and separations of data, services, and geographic location of compute or storage. All things that might physically be needed but also might be mandated for an architectural need from a legal perspective.\nMulti-tenancy by Service / API\nOne possible way to break out services for different customers is to geographically locate the GraphQL API at the nearest geographic point of presence. Another situation might be a need for the access of API calls to occur within a particular state by legal requirement. In the latter situation one might spin up a Hasura API in AWS in the US-West region located in California. This would give California residents access to the API endpoint that is geographically located in their state, thus meeting the respective requirement.\nThe other possible solutions would be to split service by geographic location by customer location. An example of this situation might be that the point of access for a customer is the east coast, something detected when they spin up their instance, so the instance uses that automatically detected location and spins up the API in US-East.\nBelow is a sample Azure deployment where several databases from different virtual networks. These are accessed via a Hasura API service that connects from another virtual network. All of which could or would be geographically located or set up to provide tenancy that is separated between databases.\n\nThe summary of multi-tenancy through individual deployments of the Hasura API server instance can be brought into architectures in numerous ways, keeping them independent or bringing together multiple database servers altogether under a single instance. All of this together, to borrow a relational database term, is very much a many to many, one to many, or many to one relational architecture design.\nMulti-tenancy by Schema & Data\nOne way that can provide multi-tenancy support within applications and systems, is to break out customer tenancy by schema and data design within the database itself. The key limitation with this model is limited geographic options to break out the data itself within a cloud provider or otherwise. However some of the benefits around this option is to manage tenancy through the application itself instead of systems deployments, and keeping the application aware of its own tenancy. This is in effect very much how tenancy is managed in the big web properties like Twitter, Facebook, and similar.\nIn this model one can take a schema per customer or use data to delineate different tenancy and break out queries and related assets based on the schema. This provides a hard differentiation to ensure no cross-over tenancy conflicts or the like. The following is a simple breakout for a schema  that could be used to provide tenancy per customer while using a singular database. With additional authorization and other capabilities inherent in the Hasura server this provides a wide range of flexibility to provide customer tenancy per application just from the perspective of the data in a database.\n\nThe summary of schema and data design focuses around the various ways and limitations of the underlying database can be split by schema, the automation of said schema, and further breaking out elements to the actual data, segmentation and segregation of data in the system itself, and bringing those together to split according for each customer tenant.\nMulti-tenancy by Database\nThe final form, which is another flexible multi-tenancy model, is to split customers, services, or other application collateral across databases. These could be databases in one region or multiple, split not just across geographic boundaries but also across groupings of customer class, rank, or other division. Some may even divide data among databases for sharding or gamification. The options are wide ranging.\nThe basic database division in Hasura is enabled by connecting multiple databases to a singular Hasura instance as shown below.\n\nTo see additional details how to add multiple databases check out this post with this video.\nAn Example by Data Schema Derived Multi-Tenancy\nThe following is an example of keeping tenants divided by data and schema usage in Postgres and Hasura. More concisely, an example of tenancy divisions by schema. In this example the tenants are broken out by schema within the database. As shown with tenant1 and tenant2 schemas.\n\nTo bridge the data across the schemas, to query against specific tenants, we would then create a view in the public schema. The create code, for copy and paste friendliness,would look like this.\nCREATE\nOR REPLACE VIEW \"public\".\"tenant_mps\" AS\nSELECT\n  'tenant1' :: text AS tenant,\n  mps.data,\n  mps.date\nFROM\n  tenant1.mps\nUNION ALL\nSELECT\n  'tenant2' :: text AS tenant,\n  mps.data,\n  mps.date\nFROM\n  tenant2.mps;\n\nSpecifically, here is the same code in the console.\n\nWith that in place we would now see data, populated as shown, showing each of the tenants and providing access via a single query.\n\nThe advantage of this model is that we can now set up security authorization using Hasura across those tables, that this query would then automatically filter as per authorization rights.\n\nSummary\nThere are multiple ways that multi-tenancy can be a feature need, from customer needing physical tenancy separation, to data just being cleanly available to one or another customer from an application use perspective. Whatever the case, GraphQL combined with Hasura provides a significant advantage in developing the tiers of an application (like Twitter or an ERP), or system of applications (like Google Docs) that need to have a multitude of databases federated together via an API.\nWe'd love to hear from you, about the systems you're building, or things you're interested in, from architecture to tactical deployments, join us on Discord and let's discuss!\nLet us know if you have any questions while trying these out! You can hit us up on github discussions / discord if you need any help!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Product Updates: August Round-up Description: We just wrapped up our August monthly community call, and ICYMI, here is all of the product updates & launches for you! Author: Jesse Martin"}, {"role": "assistant", "content": "We just wrapped up our August monthly community call, and ICYMI, here is all of the product updates & launches for you!In this post\n\ngraphql-ws\nComputed fields supported on remote schema joins:\nCaching on Hasura Cloud free tier:\nVPC Peering on Hasura Cloud\nSelf-signed HTTPS Webhooks\nRemote Schema Customisations\n\ngraphql-wsgraphql-ws is a new protocol for GraphQL subscriptions aiming to become the de facto standard for GraphQL. This has been a highly upvoted issue on our github repo and we\u2019ve finally merged this! It is now available to you on Hasura CE, Cloud & EE. Here are the docs to get started.Here's an overview:Computed fields supported on remote schema joins:Last month we announced enhancements to our computed fields feature: support for where and order clauses in the GraphQL query and in permissions, and also in join conditions for remote schema relationships. Computed fields are now also supported in remote schema joins.Here's the video once more for how they work in action.GitHub Integration Upgrades on Hasura Cloud:This is a feature we launched last month and we've been working very closely with our early access users! Quick recap: with the new GitHub Integration, you can now deploy your local migrations to staging and production by a simple git push.This integration is now available to everyone on Hasura Cloud! We have some upgrades we\u2019ve made in the UX for better error handling and the migration status tab to check the history of migrations applied on the database. These are the docs to try this out on your Hasura Cloud project. Here's the summary video:Caching on Hasura Cloud free tier:We have now enabled caching on the free tier on Hasura Cloud. You can cache authenticated data as well as cache any existing remote GraphQL API by adding it as a remote schema. Here are the docs to get started with caching. Here's a feature overview of caching with Hasura.VPC Peering on Hasura CloudSeveral of our users have requested for a dedicated VPC and we wanted to make this workflow easier to access! You can request for a dedicated Hasura VPC to connect to your own VPCs from the Hasura Cloud dashboard.Self-signed HTTPS WebhooksSometimes, you may have HTTPS endpoints (actions, event triggers, etc) connected to Hasura which are secured via self-signed TLS certificate and could throw an invalid CA error. You can now add custom domains to Hasura metadata to allow using webhooks with self-signed TLS certificates. We have also added support for insecure TLS allowlist.Here are the docs to help you get started. Here's the video sneak peak if you missed the community call.Remote Schema CustomisationsWe recently added support for customising the way graphql-engine exposes the GraphQL API of a remote schema - namespacing the root fields, prefixing and suffixing types. You can find the demos from the community call in the recording on YouTube. Apart from these, we also went over handling complex authorization patterns beyond role-based permissions, and discussed the parameterized query spec.Join us for the next community call on the 30th of September for more updates & discussions with our product & engineering teams! If you\u2019d like these updates in your inbox monthly, sign up for our monthly product updates newsletter."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Tutorial: Integrating Hasura with Microsoft Power Automate\u2014A low-code tool for business workflow automation Description: In this tutorial, we'll learn how to use Hasura APIs with Microsoft Power Automate to create flexible workflow automation for critical business tasks. Author: Jesse Martin"}, {"role": "assistant", "content": "Continuing our series of low-code application development, we are moving to a more familiar ecosystem with perhaps an unfamiliar application. Microsoft Power Automate, previously Microsoft Flow, is a low-code tool for building business processes that respond to events, fixed timings, and manual interactions.The interfaceCompared to our previous examples, there is no traditional user interface! Our interface will be a connection between a cron job (a daily trigger at 5am), our Hasura API via a data connector, and our email client. Our intention here is to create a daily digest to be made aware of any flagged uploads that need to be checked.The TutorialPrerequisites:Familiarity with our base tutorial. See the diagram below for reference.A working, accessible Hasura project (not running on localhost) with existing data.A Microsoft Office 365 Account for Work or School (you\u2019ll need this for the HTTP request plugins).It could be argued that a cleaner interface for our purposes would be to persist our data behind a REST endpoint, but for the purposes of educating for more use-cases, we\u2019ll show how to enable GraphQL queries.Given the nuances involved in this tutorial, we've recorded the steps in the following video.Power Automate provides all integrations you'd expect from an enterprise grade workflow automator. Using low-ops tooling like Hasura Cloud provides a smooth path to create automated tooling solving your time-intensive or time-critical tasks.Let us know if you're using any low-code tools or platforms for automating your business workflows. Tweet at us with questions or tell us which low-code tool you'd like us to write about next in our low-code series.For more such content and discussions:Follow us on TwitterStart a discussion on GitHubJoin our Discord communityGetting started with Hasura Cloud takes 30s. Try it out!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Open-Source GraphQL Projects We Love Description: A lot of credit for The GraphQL Engine's success goes to the original team at Facebook, the GraphQL foundation, and developers all over the world.\n\nTo pay it forward, we are sponsoring open-source GraphQL projects that add value to the ecosystem. Author: Harsha"}, {"role": "assistant", "content": "Note: This post was updated on 15/11/2021 to include 3 new project sponsorships.Not a lot of people know this, but the original version of Hasura didn\u2019t use GraphQL. It used a JSON DSL that we had built to address the issues we had with REST. When we discovered GraphQL, we were totally blown away - it did exactly what we were trying to do with our custom DSL and then some! We quickly integrated it into Hasura and launched The Hasura GraphQL Engine.The response was phenomenal! In 2 years, the GraphQL Engine grew to over 300 million downloads.A lot of credit for The GraphQL Engine's success goes to the community: The original team at Facebook for developing and open-sourcing GraphQL and to the GraphQL foundation for steering the project. However, the lion\u2019s share, in our opinion, goes to the thousands of developers all over the world who carried the GraphQL torch. From speaking at workshops, meetups, and conferences to building a huge ecosystem of tooling - these devs laid the foundation for GraphQL\u2019s popularity.So we wanted to pay it forward. Starting this month, we will be sponsoring open-source GraphQL projects that, we think, add tremendous value to the ecosystem.Our goals with the sponsorships are:To financially support the developers/teams behind the projects so that they can continue with their fantastic workTo shine a spotlight on these projects so that more people discover and use themTo encourage developers to build more open-source projects that will help and grow the adoption of GraphQL and the community at largeHere they are and why we chose them:graphql-flutter: Maintained by Zino App and the community, graphql-flutter is a GraphQL client for flutter which combines the benefits of streams in Dart to bring out a high performing modern client. Our flutter tutorial uses this awesome client for building out the realtime todo app.graphql-dotnet: This is the implementation of the official GraphQL spec in .NET. Maintained by the community with over 100+ contributors, this reference implementation becomes the base for .NET projects to embrace GraphQL. We believe that such projects are essential for the community to collaborate on to expand the ecosystem beyond JavaScript / Node.gqless: If you\u2019re looking for a simple way to query GraphQL from the frontend without worrying about writing special strings check out gqless by Sam Denty. It works by generating queries at runtime based on the data your app consumes. gqless uses ES6 proxies and when properties are accessed it converts the paths into GraphQL queries. Here\u2019s a detailed look at it if you want to know more. This is also a lightweight alternative to existing GraphQL clients for React and is one of the fluent GraphQL clients about which we wrote a while ago.graphql-code-generator: Maintained by Dotan Simha, graphql-code-generator is a CLI tool that lets you automatically generate types from your GraphQL schema and operations. While making our GraphQL Tutorial for TypeScript, we wanted a way to automate the type system for React components using GraphQL. graphql-code-generator worked like magic and it was a breeze converting an existing React app with GraphQL to Typescript.elm-graphql: Built and maintained by Dillon Kearns, elm-graphql gives type-safe GraphQL queries for the entire schema. Our GraphQL tutorial for Elm uses this client for generating types :). On a related note, GraphQL and Elm make a great match thanks to the strong typing of Elm and schema-based definitions in GraphQL.We've also begun sponsoring open source projects that are useful to the Hasura community at large. Here's the first project sponsorship:nestjs-hasura: Jesse Carter's Hasura packages enable you to turn any function in a NestJS app into a Hasura Event Trigger, or Scheduled Event handler.It can also automatically generate and manage the metadata for these handlers, and comes with security functionality to verify that payloads are secure and sent only from your Hasura app.If you like any of these projects, give it a shout-out on Twitter, tell a friend, or go ahead and sponsor it yourself!We\u2019ll sponsor new projects each month and add them to this list. Follow us on Twitter to keep an eye on it. If you\u2019re working on an open-source GraphQL project, or know one we should support, reach out and let us know on Twitter or Discord and we\u2019ll take a look!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Tutorial: Integrating Hasura with Retool\u2014A low-code tool for building B2B workflows & tools. Description: In this tutorial, we'll learn how to build powerful internal tools, fast, with Hasura's advanced permission controls and Retool's intuitive UI builder for enterprise-grade apps. Author: Jesse Martin"}, {"role": "assistant", "content": "In this tutorial, we\u2019ll be expanding on our previous low-code builds of both mobile and web apps, to now implement a moderator interface to flag images and train tag accuracy. We\u2019ll be creating a special set of permissions in Hasura for our moderator role, then mapping those to some read, update, and delete operations in Retool. Retool has great support for GraphQL so we\u2019ll be able to leverage the best of Hasura without wrapping our services in any kind of REST handler first. The Hasura features we\u2019ll use are:\nRole permissions for updating tag ratings, deleting tag associations, and updating images.\nGraphQL queries and mutations.\n\nThe interfaceRetool provides elegant components for all the UI primitives you need to build robust B2B dashboards. We\u2019ll create a tabbed view that views all images, and all flagged images, a table to view those results, and action buttons to handle our tag training and deletion as well as our image flagging/unflagging.The TutorialPrerequisites:\nFamiliarity with our base tutorial See the diagram below for reference.\nA working and accessible Hasura project (not running on localhost) with existing data\nA Retool account\n\nModifying the data layer shape and access controlsWe\u2019ll begin with modifying our data model to support a \u201cflagged\u201d column.Next, we need to define a new role called \u201cModerator\u201d which will be allowed to delete our errant tag associations, change the confidence interval, flag/unflag the uploads.Beginning with flagging, we add the moderator role under the permissions tab of upload. Then we enable select on all columns (for read operations), but on update, we\u2019ll only allow updating the value on the flagged column. We\u2019ll disallow insert and delete entirely.We\u2019ll enable updating the confidence interval on the upload_tag model as well as delete entries.We can review these permissions in the permission summary tab.With these permissions in place, we can navigate to retool to create our interface. Implementing the InterfaceFirst, we begin with creating a new, blank template project.Then we create a new resource to be used across the project. We\u2019ll provide our x-hasura-admin-secret header to be shared across all the uses of this resource.Lastly, as all the requests will be for the moderator role, we\u2019ll add that to the header.With our application created, we\u2019ll add a table component. This will immediately ask us for a data source to iterate over for the rows of the table.We\u2019ll choose the resources we created earlier, and define a query for fetching the contents of the table.In the Inspect panel, with the table selected, you can define the columns you want to show up. We\u2019ve added three:\nName\nImage\nFlag\n\nIn each of the column settings, you can define the calculation being used. In general, retool allows a {{}} style data assessor method to read data off of the currentRow\u2019s properties. To handle the images here in a table, we need to get a little creative with Retool\u2019s formatting. We\u2019ll define a text column, which we\u2019ll tell Retool to display as HTML, then we\u2019ll create an image tag to be inserted into that column.For the flag column, we\u2019ll choose the button column type, use a flag emoji for the value, and define a new GraphQL query for the flagging behaviour.Let\u2019s inspect the flag query. We can see that we are mutating content through Hasura and passing in the ID of the currently selected row.Applying what we\u2019ve learned, we can create additional queries as seen in the screenshot above for deleting tags, training our confidence interval, and getting all flagged uploads.It\u2019s helpful to test these in the Hasura console, though the Retool interface also provides API introspection which helps formulate functional queries. An important feature recently introduced was the concept of resource triggers, that allow you to define success actions after a query successfully runs. In the case of our flag query, we will re-initiate our upload fetch which is filtering out the now recently flagged entries. This feature enables highly complex workflows.In our final UI, we\u2019ll add a second table to display the tags of the selected image row. Dragging a table next to the image table, we\u2019ll choose the data source as the tags property of our selected row with the following syntax.The final UI would look like below:We\u2019ll make the \u201cTrain\u201d and \u201cDelete\u201d columns run queries (mutations), mapping to statements we already defined. Our delete action would look like the following:Our interface is nearly complete, four our final touch, let\u2019s create a tabbed container that will let us show \u201cunflagged\u201d images in one column, and flagged images in the other. Adjusting the size of the image to assist in the moderation process, our \u201cflagged\u201d image shows as the following.Retool allows us to preview, share, and publish these \u201capps\u201d for use by others outside of the Retool login flow. Sharing this link, we get the following look.And with that, our application is completed.Retool allows us to build business applications for complex workflows with minimal effort to both create and maintain.It only takes 30s to get started. Try it out with Hasura Cloud!\nLet us know if you have any questions while trying these out! You can hit us up on github discussions / discord if you need any help!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build Fullstack Apps with NestJS, Hasura, and GraphQL APIs Description: In this post, we'll give a general overview to NestJS, how it works in the context of Hasura and how GraphQL APIs can be built with these together. Author: Gavin Ray"}, {"role": "assistant", "content": "NestJS and Hasura\n\nNestJS and Hasura\n\nAcknowledgements\nAn Invitation\nIntroduction\n10,000ft View\n\nDependency Injection (DI)\nNest's Module Structure\nNest's Request -> Response Lifecycle\n\n\nSetting up our Application\n\nScaffolding with the Nest CLI\nA few educational alterations\nRunning the app\n\n\nBuilding our Application\n\nREST\nGraphQL\n@golevelup/nestjs-hasura\n\n\n\n\n\nAcknowledgements\nAs we tend to forget at times -- a human being wrote this article.\nAnd that human being has other human beings he would like to thank as a first order of business.\nWithout them this article (and following workshop) would not have been possible.\nI owe a great deal of gratitude to:\n\nJesse Carter, @WonderPandaDev, for explaining many of the concepts and fundamentals relayed in this article to me oh-so-many moons ago. Incidentally also the author of the @golevelup/nestjs-hasura modules, and one of the most talented TypeScript developers I've ever met.\nJay McDoniel, @jmcdo29, core member of the NestJS team and also fielder of many, many of my questions early on. Quite possibly one of the most knowledgeable people about Nest on Earth, and also un-endingly helpful.\n\nAn Invitation\nJoin us on 2nd September at 9AM PST for a live, hands-on workshop on how to build a fullstack using NestJS, Hasura & GraphQL\nIntroduction\nNestJS is a TypeScript application framework that has been rapidly gaining popularity in recent years. It's garnered quite a reputation, particularly in Enterprise circles -- as much of it's architecture and design patterns are modeled after another well-known Enterprise framework: Java's own Spring Boot.\nUnder the \"Who's using Nest?\" section, adoption is seen by the likes of:\n\nRoche Pharmaceuticals\nAdidas\nSociete Generale\nAutodesk\nNeo4j\nBytedance (TikTok)\nMany, many others\n\nIn their own words, Nest is:\n\n... a framework for building efficient, scalable Node.js server-side applications. It uses progressive JavaScript, is built with and fully supports TypeScript (yet still enables developers to code in pure JavaScript) and combines elements of OOP (Object Oriented Programming), FP (Functional Programming), and FRP (Functional Reactive Programming).\nUnder the hood, Nest makes use of robust HTTP Server frameworks like Express (the default) and optionally can be configured to use Fastify as well!\nNest provides a level of abstraction above these common Node.js frameworks (Express/Fastify), but also exposes their APIs directly to the developer. This gives developers the freedom to use the myriad of third-party modules which are available for the underlying platform.\n\nIn this post, we'll give a general overview to NestJS and it's concepts for those unfamiliar, and then turn towards what practical usage looks like in the context of a Hasura application.\nSpecifically, we'll look at how to consume a set of modules @golevelup/nestjs-hasura tailor-made for this!\n10,000ft View\nNestJS is not the easiest platform to onboard to.\nUnlike many other JS/TS frameworks, it requires more specialized knowledge than just general understanding of the language.\nThere are two major concepts that have to be addressed before you can reasonably dive into the code side of things:\n\nDependency Injection (commonly abbreviated to \"DI\")\nNest's application conventions & request/response lifecycle\n\nSo we'll make our passes over these concepts and attempt to explain them as succinctly as possible.\nIf you're already familiar with both of these things, please feel free to skip ahead to the hands-on section below.\nDependency Injection (DI)\nThere are a lot of ways to attempt to explain Dependency Injection.\nThe easiest way probably is through code.\nFor example's sake, let's look at this contrived bit below:\nclass Car {\n  constructor(wheels: Wheels) {}\n}\n\nclass Wheels {\n  constructor(tires: Tires) {}\n}\n\nclass Tires {\n  constructor(rims: Rims, treads: Treads)\n}\n\nclass Rims {\n  constructor() {}\n}\n\nclass Treads {\n  constructor() {}\n}\n\nIf we want a Car, the way we'd ask for one is like:\nconst car = new Car(new Wheels(new Tires(new Rims(), new Treads())))\n\nNow, I don't know about you, but I don't think that's particularly pretty -- or maintainable!\n\nWhat happens if we want to swap out what instance of a Tires class is provided everywhere, or for a particular scope?\nHow fragile and prone to refactoring errors is this sort of code?\nIs this code flexible and easily testable? Are we able to provide alternative/mocked implementations during our tests?\n\nThe answers are not looking so hot \ud83d\ude2c\nBUT! Dependency Injection is here to save the day!\nDI says:\n\"Hey, well uhh, I know that Car needs a Wheels. And that Wheels needs Tires. And that Tires, need Rims and Treads. So if all you're concerned about is being handed a working Car, how about I take care of the in-between stuff for you?\"\nAnd you go: \"Wow -- yeah DI, that sounds pretty sweet! You'd really do that for me? D'awww, you're the best!\"\n\nThe fancy-pants term for this sort of thing I believe has to do with \"Directed Acyclic Graphs\", but: who cares, am I right?\n\nDependency Injection doing you this solid usually looks something along the lines of:\n// \"Constructor Injection\"\n// Recommended pattern, regardless of language/framework due to flexibility\nclass NeedsACar {\n  constructor(@Inject private car: Car) {}\n}\n\n// \"Field (or Property) injection\"\n// https://docs.nestjs.com/providers#property-based-injection\nclass NeedsACar {\n  @Inject\n  private car: Car\n}\n\n// \"Setter Injection\"\n// (Does not exist in NestJS, but is the third and final common way to perform injection)\nclass NeedsACar {\n  @Inject\n  public setCar(car: Car) {\n    this.car = car;\n  }\n}\n\nThe exact syntax varies from language to language, but in general it's something very close or nearly identical.\nIn fact, Java's JSR 330: Dependency Injection for Java standard @Inject would be a near verbatim translation of this.\nThere are a lot of benefits (and some arguable drawbacks) to Dependency Injection, but for the sake of this article we won't be able to cover a comprehensive tutorial.\nWhat we need to know to conclude our mini-DI series is that:\n\nNestJS uses Constructor Injection by convention and allows for Field/Property Injection (though discourages it, for good reason)\nDependency Injection provides (among other things):\n\nAbstracting away of implementation details from the consumer of an end-class or interface\nAllowing for looser coupling, which provides more flexibility to do things like swap implementations\n\n\nDependency Injection really shines when it comes to tests, in particular\n\nNest's Module Structure\nNest is more of a \"convention over configuration\" framework. Not necessarily to the point of say, Rails, which literally won't work unless laid out exactly as prescribed.\nBut there are a set of idioms that the community attempts to follow in regards to structure of files.\n\nNote: Credit for this diagram goes to https://blog.mimacom.com/a-quick-dive-into-nestjs\n\nA rough outline for what a single Nest.js module looks like, may be something like the below:\n\nNest's Request -> Response Lifecycle\n\nNote: Credit for this diagram goes to Dmitri Moore (@demisx1)\n\nThis is for a slightly older version of Nest, but fundamentally the request flow hasn't changed, and the diagram is fantastically made to use as a discussion point.\n\nThe lifecycle of a single client request looks like:\n\nClient Request is made\nAny class which implements the NestMiddleware interface and has been registered will process the request.\n\nThese middleware, much like Express middleware, CAN alter the request/response objects, or set contextual values\nNestJS even has support for Express middleware libraries\nThis is the layer at which you'd typically handle something like reading a JWT token from a header and setting user data on the request\n\n\nAfter middleware, any class which implements the CanActivate interface is called.\n\nThese are \"guard\" interfaces, and they determine whether a request will be accepted/rejected.\nWith the JWT Auth example, this is the point at which you would read from the previously set user data that the middleware had put into the ExecutionContext, and check to see if it matched what you ought to allow for that particular endpoint.\nIf not, you can deny the request. Otherwise, it may be permitted through.\n\n\nNext, classes which implement the NestInterceptor interface fire.\n\nThese have callbacks that are run both before and after the handler response\nUsually, interceptors are a good place to put things like loggers/metrics that need both the before + after context of a request, or if you need to transform the input/output shape.\n\n\nThe last step before the actual request handler is called are classes which implement PipeTransform\n\nAs the name suggests, these are transforms on arguments into the requests.\nBuiltin Pipes are ValidationPipe, ParseIntPipe, etc.\nThese are generally used to format/validate arguments that are going to be given to the controller route handler\n\n\nFinally, the @Controller route handler is called.\n\nThis is your typical API endpoint handler\nLikely going to call DB/persistence methods, or communicate with other service API's\n\n\nThe response is sent back to the client (not forgetting it'll be handled by any NestInterceptors \"after\" handlers, if they've been registered)\n\nPhew! Okay, on to actually making something we go.\nSetting up our Application\nScaffolding with the Nest CLI\nThe NestJS CLI is a central tool when developing apps with it.\nWe'll begin by scaffolding the app.\nLater we'll see how to use the CLI to generate resources that integrate entities + services with REST and GraphQL handlers.\n$ npm install -g @nestjs/cli\n$ nest new my-nest-project\n\nNow, that'll prompt you a few questions, and should generate a directory structure which looks like this:\n.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 nest-cli.json\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 app.controller.spec.ts\n\u2502   \u251c\u2500\u2500 app.controller.ts\n\u2502   \u251c\u2500\u2500 app.module.ts\n\u2502   \u251c\u2500\u2500 app.service.ts\n\u2502   \u2514\u2500\u2500 main.ts\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 app.e2e-spec.ts\n\u2502   \u2514\u2500\u2500 jest-e2e.json\n\u251c\u2500\u2500 tsconfig.build.json\n\u251c\u2500\u2500 tsconfig.json\n\u2514\u2500\u2500 yarn.lock\n\nA few educational alterations\nFor the sake of explanation, we're going to make one alteration to the generated sources, so that things are perhaps more understandable.\nIn src/app.controller.ts, you'll see the content is:\n@Controller()\nexport class AppController {\n  constructor(private readonly appService: AppService) {}\n\n  @Get()\n  getHello(): string {\n    return this.appService.getHello();\n  }\n}\n\nWe're going to change that to:\n@Controller()\nexport class AppController {\n  constructor(@Inject('AppService') private readonly appService: AppService) {}\n\n  @Get()\n  getHello(): string {\n    return this.appService.getHello();\n  }\n}\n\nAnd then change the contents of app.module.ts from:\n@Module({\n  imports: [],\n  controllers: [AppController],\n  providers: [AppService],\n})\nexport class AppModule {}\n\nTo:\n@Module({\n  imports: [],\n  controllers: [AppController],\n  providers: [\n    {\n      // When @Inject() asks us for an \"AppService\"\n      provide: 'AppService',\n      // Use an instance of the AppService class as the provided value\n      useClass: AppService,\n    },\n  ],\n})\nexport class AppModule {}\n\nThe reasoning here is that there is a lot of \"magic\" that Nest does under the hood for us.\nWith these changes, hopefully you can see that:\n\nIn the AppModule, we are providing the AppService class when any @Inject()ion asks for the string: \"AppService\"\nIn the AppController constructor, we say @Inject(\"AppService\") private readonly appService, which will look in the module's provider hierarchy for a provider that is resolved by the string \"AppService\", and then provide it's value (in this case -- an instance of the AppService class itself)\n\nNow, hopefully that's understandable.\nYou won't see this syntax used in Nest and you're free to change it back if you like.\nThe reasoning is that, if the TYPE of a constructor parameter matches the TYPE of a provider, Nest will auto-magically wire up the @Inject() for you.\nIE:\n@Module({\n  providers: [AppService],\n})\n\n@Controller()\nexport class AppController {\n  // \"AppService\" class was in the module \"providers\", so it'll \"auto-magically\" be @Inject()'ed here\n  constructor(private readonly appService: AppService) {}\n\nRunning the app\nNow, if we run yarn start:dev, we should see:\n[2:18:06 PM] Starting compilation in watch mode...\n[2:18:09 PM] Found 0 errors. Watching for file changes.\n\n[Nest] 9804  - 08/24/2021, 2:18:17 PM     LOG [NestFactory] Starting Nest application...\n[Nest] 9804  - 08/24/2021, 2:18:17 PM     LOG [InstanceLoader] AppModule dependencies initialized +24ms\n[Nest] 9804  - 08/24/2021, 2:18:17 PM     LOG [RoutesResolver] AppController {/}: +5ms\n[Nest] 9804  - 08/24/2021, 2:18:17 PM     LOG [RouterExplorer] Mapped {/, GET} route +2ms\n[Nest] 9804  - 08/24/2021, 2:18:17 PM     LOG [NestApplication] Nest application successfully started +2ms\n\nAnd in main.ts, we can see that it's being bootstrapped on port 3000:\n// src/main.ts\nasync function bootstrap() {\n  const app = await NestFactory.create(AppModule);\n  await app.listen(3000);\n}\n\nSo if we visit http://localhost:3000 then we should see our AppController's getHello() method fired, and returned as a web result:\n// @Get() without a string path in the parens means it's resolved at the root of the controller's route\n@Get()\ngetHello(): string {\n  // Calls the @Inject()'ed AppService\n  return this.appService.getHello();\n}\n\n\nAlright, now we're cookin'!!\nWe finally know enough to do something useless.\nThat's better than nothing =)\nNow we can dive into how to integrate Nest with Hasura.\nBuilding our Application\nThe outline of what we'll be building is:\n\nA single Hasura table, user, which we can use to link information to\nA REST module for payments, integrated into Hasura via Actions\nA GraphQL module for houses, integrated into Hasura with Remote Schemas\nWebhooks for subscribing to changes, integrated into Hasura with Event Triggers via @golevelup/nestjs-hasura\nScheduled/recurring jobs, integrated into Hasura with Scheduled Triggers via @golevelup/nestjs-hasura\n\nREST\nUsing the Nest CLI, we can use it's \"generate\" function to create the boilerplate for what we need. It will even install the new resources we create into the top-level app module for us.\nWe want to create a:\n\nPayment Module\n\nPayment Controller (HTTP request handler for Hasura Actions)\nPayment Service (holds the business logic, to keep the controller code slim)\n\n\n\nTo do this, we can run:\n$ nest generate module payment\nCREATE src/payment/payment.module.ts (84 bytes)\nUPDATE src/app.module.ts (441 bytes)\n\n$ nest generate controller payment\nCREATE src/payment/payment.controller.spec.ts (499 bytes)\nCREATE src/payment/payment.controller.ts (103 bytes)\nUPDATE src/payment/payment.module.ts (178 bytes)\n\n$ nest generate service payment\nCREATE src/payment/payment.service.spec.ts (467 bytes)\nCREATE src/payment/payment.service.ts (91 bytes)\nUPDATE src/payment/payment.module.ts (261 bytes)\n\nWhat we should have at this point is:\nsrc/payment/\n\u251c\u2500\u2500 payment.controller.spec.ts\n\u251c\u2500\u2500 payment.controller.ts\n\u251c\u2500\u2500 payment.module.ts\n\u251c\u2500\u2500 payment.service.spec.ts\n\u2514\u2500\u2500 payment.service.ts\n\nNow, what we're going to do is set up a fake payment handler that takes a user_id, product_id, and quantity, and then pretends to process a payment as a Hasura Action.\nTo do that, first, lets edit the payment.service.ts to have the methods we need:\nimport { Injectable } from '@nestjs/common';\n\n@Injectable()\nexport class PaymentService {\n  private static products = [\n    { id: 1, name: 'Milk', price: 2.5 },\n    { id: 2, name: 'Apples', price: 1.25 },\n    { id: 3, name: 'Eggs', price: 0.99 },\n  ];\n\n  public calculateTotal(params: { product_id: number; quantity: number }): number {\n    return PaymentService.products.find((it) => it.id == params.product_id).price * params.quantity;\n  }\n\n  public processPayment(params: { total: number }): boolean {\n    console.log(\"This is where you'd call a payment processor, and charge the customer for \" + params.total);\n    return true;\n  }\n}\n\nAnd then set up payment.controller.ts to take a payload from a Hasura Action and call this service, returning total, paymentResult, and a fake receiptNumber:\nimport { Body, Controller, Post } from '@nestjs/common';\nimport { PaymentService } from './payment.service';\n\ninterface HasuraActionsPayload<Input extends {} = {}, Session extends {} = {}> {\n  action: {\n    name: string;\n  };\n  input: Input;\n  session_variables: Session;\n}\n\ninterface CreatePaymentForUserArgs {\n  user_id: number;\n  product_id: number;\n  quantity: number;\n}\n\n@Controller('payment')\nexport class PaymentController {\n  constructor(private readonly paymentService: PaymentService) {}\n\n  @Post('/createPaymentForUser')\n  createPaymentForUser(@Body() payload: HasuraActionsPayload<{ params: CreatePaymentForUserArgs }>) {\n    const total = this.paymentService.calculateTotal(payload.input.params);\n    const paymentResult = this.paymentService.processPayment({ total });\n    return {\n      total,\n      paymentResult,\n      receiptNumber: 1234567,\n    };\n  }\n}\n\nNow if we boot up our app again, with yarn start:dev again, we should see in the output:\n[Nest] 21396  - 08/24/2021, 3:30:42 PM     LOG [RouterExplorer] Mapped {/payment/createPaymentForUser, POST} route +1ms\n\nAnd if we try to make a request to that endpoint giving it the same payload it would receive if it were called by Hasura, as a Hasura Action, like this:\nPOST http://localhost:3000/payment/createPaymentForUser HTTP/1.1\ncontent-type: application/json\n\n{\n  \"action\": {\n    \"name\": \"createPaymentForUser\"\n  },\n  \"input\": {\n    \"params\": {\n      \"user_id\": 1,\n      \"product_id\": 2,\n      \"quantity\": 10\n    }\n  },\n  \"session_variables\": {}\n}\n\nThen you should see output like this:\n\nThis means everything is good for us to hook into our Hasura app as an Action!\nTo do this, what we need to do is open our Hasura console at:\n\nhttp://localhost:8080/console/actions/manage/add\nOr Cloud, if running on a Cloud app\n\nNow, add the following Action definition:\n\nFrom the GraphiQL tab on the console, we now should be able to make the following mutation:\nmutation {\n  createPaymentForUser(params: {\n    user_id: 1,\n    product_id: 2,\n    quantity: 10\n  }) {\n    total\n    paymentResult\n    receiptNumber\n  }\n}\n\n\nTa-da! We've integrated a Nest.js REST API into Hasura as an Action. If we wanted, we could do a Remote Relationship between payment.user_id to user.id in our table to connect the data back to our graph.\nGraphQL\nIn a similar vein to the way we started above, we can begin by using Nest's CLI to scaffold our resources.\nWe'll do:\n$ nest generate resource house\n? What transport layer do you use? \n  REST API \n> GraphQL (code first) \n  GraphQL (schema first) \n  Microservice (non-HTTP) \n  WebSockets\n\n$ nest generate resource house\n? What transport layer do you use? GraphQL (code first)\n? Would you like to generate CRUD entry points? (Y/n) y\n\n$ nest generate resource house\n? What transport layer do you use? GraphQL (code first)\n? Would you like to generate CRUD entry points? Yes\nCREATE src/house/house.module.ts (224 bytes)\nCREATE src/house/house.resolver.spec.ts (525 bytes)\nCREATE src/house/house.resolver.ts (1134 bytes)\nCREATE src/house/house.service.spec.ts (453 bytes)\nCREATE src/house/house.service.ts (637 bytes)\nCREATE src/house/dto/create-house.input.ts (197 bytes)\nCREATE src/house/dto/update-house.input.ts (247 bytes)\nCREATE src/house/entities/house.entity.ts (188 bytes)\nUPDATE src/app.module.ts (448 bytes)\n\nAnd now, we should have:\nsrc/house\n\u251c\u2500\u2500 dto\n\u2502   \u251c\u2500\u2500 create-house.input.ts\n\u2502   \u2514\u2500\u2500 update-house.input.ts\n\u251c\u2500\u2500 entities\n\u2502   \u2514\u2500\u2500 house.entity.ts\n\u251c\u2500\u2500 house.module.ts\n\u251c\u2500\u2500 house.resolver.spec.ts\n\u251c\u2500\u2500 house.resolver.ts\n\u251c\u2500\u2500 house.service.spec.ts\n\u2514\u2500\u2500 house.service.ts\n\nWe'll immediately encounter errors, due to lacking the @nestjs/graphql module and a few others, so we'll yarn add graphql @nestjs/graphql apollo-server-express@2.x.x to fix that.\nFinally, we need to actually set up the GraphQL server and schema to be served.\nTo do this, modify src/app.module.ts with the changes you see below -- namely just importing GraphQLModule and adding GraphQLModule.forRoot({ autoSchemaFile: true }) to the @Module imports:\nimport { Module } from '@nestjs/common';\nimport { AppController } from './app.controller';\nimport { AppService } from './app.service';\nimport { HouseModule } from './house/house.module';\nimport { PaymentModule } from './payment/payment.module';+\n+ import { GraphQLModule } from '@nestjs/graphql';\n\n@Module({\n-  imports: [PaymentModule, HouseModule],\n+  imports: [PaymentModule, HouseModule, GraphQLModule.forRoot({ autoSchemaFile: true })],\n  controllers: [AppController],\n  providers: [\n    {\n      provide: 'AppService',\n      useClass: AppService,\n    },\n  ],\n})\nexport class AppModule {}\n\nStart the server again with yarn start:dev and ensure that on http://localhost:3000/graphql you have a GraphQL playground instance that holds the schema to your House entity and resolver methods:\n\nAfter this, we should be ready to proceed.\nIn Hasura, go to create a new Remote Schema at:\n\nhttp://localhost:8080/console/remote-schemas/manage/add\n\nAnd add http://host.docker.internal:3000/graphql, as below:\n\nYou should see this, Remote Schema Added Successfully:\n\nNow, because in the house resolver, the HouseService call doesn't actually return the proper object that the schema requires, any calls to it will fail at the moment:\n// Resonse types need to be an object with an \"exampleField\" that's an Int\n@ObjectType()\nexport class House {\n  @Field(() => Int, { description: 'Example field (placeholder)' })\n  exampleField: number;\n}\n\n@Resolver(() => House)\nexport class HouseResolver {\n  constructor(private readonly houseService: HouseService) {}\n\n  @Query(() => House, { name: 'house' })\n  findOne(@Args('id', { type: () => Int }) id: number) {\n    return this.houseService.findOne(id);\n  }\n}\n\n@Injectable()\nexport class HouseService {\n  findOne(id: number) {\n    return `This action returns a #${id} house`;\n  }\n}\n\nWe can see that we are able to call it and see it from Hasura's schema, and if we modified return 'This action returns a #${id} house'; to be return { exampleField: 1 } for instance, it would work:\n\n@golevelup/nestjs-hasura\nThe final piece of today's tutorial is perhaps the most exciting.\nThe @golevelup/nestjs set of modules are a multi-functional toolset that span several domains.\n\nhttps://github.com/golevelup/nestjs\n\nHasura is one area the tools cover. Another offers integration with Stripe. A third with RabbitMQ. Other modules offer much more general application functionality than a single targeted usecase. There's likely to be something useful in those modules for just about everyone.\nToday we'll only be discussing the @golevelup/nestjs-hasura module though.\nThis module allows you to annotate functions, which should act as Event Triggers or Scheduled Trigger handlers in Hasura.\nEven more impressively, it optionally allows you to export the metadata.json to automatically configure Hasura so that these are integrated for you.\nTo install it, we'll need to:\n\nyarn add @golevelup/nestjs-hasura\n\nAnd then to configure it, we need to do two things:\n\nAdd it into our app.module.ts\nActually set up some Event Triggers or Scheduled Trigger handlers\n\nTo add it to our app, in the module file.\nAt this point, our app.module.ts should look like:\nimport { HasuraModule } from '@golevelup/nestjs-hasura';\nimport { Module } from '@nestjs/common';\nimport { GraphQLModule } from '@nestjs/graphql';\nimport { AppController } from './app.controller';\nimport { AppService } from './app.service';\nimport { HouseModule } from './house/house.module';\nimport { PaymentModule } from './payment/payment.module';\nimport { UserService } from './user/user.service';\nconst path = require('path');\n\n@Module({\n  imports: [\n    PaymentModule,\n    HouseModule,\n    GraphQLModule.forRoot({ autoSchemaFile: true }),\n    HasuraModule.forRoot(HasuraModule, {\n      webhookConfig: {\n        /**\n         * The value of the secret Header. The Hasura module will ensure that incoming webhook payloads contain this\n         * value in order to validate that it is a trusted request\n         */\n        secretFactory: 'secret',\n        /** The name of the Header that Hasura will send along with all event payloads */\n        secretHeader: 'secret-header',\n      },\n      managedMetaDataConfig: {\n        metadataVersion: 'v3',\n        dirPath: path.join(process.cwd(), 'hasura/metadata'),\n        secretHeaderEnvName: 'HASURA_NESTJS_WEBHOOK_SECRET_HEADER_VALUE',\n        nestEndpointEnvName: 'NESTJS_EVENT_WEBHOOK_ENDPOINT',\n        defaultEventRetryConfig: {\n          intervalInSeconds: 15,\n          numRetries: 3,\n          timeoutInSeconds: 100,\n          toleranceSeconds: 21600,\n        },\n      },\n    }),\n  ],\n  controllers: [AppController],\n  providers: [AppService, UserService],\n})\nexport class AppModule {}\n\nNow, let's generate the UserService to put our Event Trigger and Scheduled Trigger handlers on:\n$ nest generate service user\nCREATE src/user/user.service.spec.ts (446 bytes)\nCREATE src/user/user.service.ts (88 bytes)\nUPDATE src/app.module.ts (1543 bytes)\n\nAnd inside of the src/user/user.service.ts, let's put:\nimport { HasuraInsertEvent, HasuraUpdateEvent, TrackedHasuraEventHandler } from '@golevelup/nestjs-hasura';\nimport { Injectable } from '@nestjs/common';\n\ninterface User {\n  id: number;\n  email: string;\n}\n\n@Injectable()\nexport class UserService {\n  @TrackedHasuraEventHandler({\n    triggerName: 'user-created',\n    tableName: 'user',\n    definition: { type: 'insert' },\n  })\n  handleUserCreated(evt: HasuraInsertEvent<User>) {\n    console.log('A new user was created!');\n    console.log('User info:', evt.event.data.new);\n  }\n\n  @TrackedHasuraEventHandler({\n    triggerName: 'user-updated',\n    tableName: 'user',\n    definition: { type: 'update', columns: ['email'] },\n  })\n  handleUserUpdated(evt: HasuraUpdateEvent<User>) {\n    console.log('handleUserUpdated was called, due to user.email changing and Hasura sending us a webhook!');\n    console.log('email was changed from', evt.event.data.old.email, 'to', evt.event.data.new.email);\n  }\n}\n\nThis should log out user info when a user is created, and also log when a user's email is updated.\nIf we run yarn start:dev again, we should see the HasuraModule detect this in the logs:\n[Nest] 30244  - 08/24/2021, 5:49:11 PM     LOG [RoutesResolver] EventHandlerController {/hasura}: +1ms\n[Nest] 30244  - 08/24/2021, 5:49:11 PM     LOG [RouterExplorer] Mapped {/hasura/events, POST} route +1ms\n[Nest] 30244  - 08/24/2021, 5:49:11 PM     LOG [HasuraModule] Initializing Hasura Module\n[Nest] 30244  - 08/24/2021, 5:49:11 PM     LOG [HasuraModule] Discovered 2 hasura event handlers\n[Nest] 30244  - 08/24/2021, 5:49:11 PM     LOG [HasuraModule] Automatically syncing hasura metadata based on discovered event handlers. Remember to apply any changes to your Hasura instance using the CLI\n[Nest] 30244  - 08/24/2021, 5:49:11 PM     LOG [HasuraModule] Registering hasura event handlers from UserService\n\nNow if we look inside of nest-hasura-example\\hasura\\metadata\\databases\\default\\tables\\public_user.yaml, we should see the automatically created Hasura metadata matching these TypeScript NestJS handlers:\ntable:\n  name: user\n  schema: public\nevent_triggers:\n  - name: user-created\n    definition:\n      enable_manual: false\n      insert:\n        columns: '*'\n    retry_conf:\n      num_retries: 3\n      interval_sec: 15\n      timeout_sec: 100\n    webhook_from_env: NESTJS_EVENT_WEBHOOK_ENDPOINT\n    headers:\n      - name: secret-header\n        value_from_env: HASURA_NESTJS_WEBHOOK_SECRET_HEADER_VALUE\n  - name: user-updated\n    definition:\n      enable_manual: false\n      update:\n        columns:\n          - email\n    retry_conf:\n      num_retries: 3\n      interval_sec: 15\n      timeout_sec: 100\n    webhook_from_env: NESTJS_EVENT_WEBHOOK_ENDPOINT\n    headers:\n      - name: secret-header\n        value_from_env: HASURA_NESTJS_WEBHOOK_SECRET_HEADER_VALUE\n\nAmazing! Now we just need to run hasura metadata apply so the configuration is applied.\ncd hasura to change into the Hasura console directory, and run the above command:\n$ hasura metadata apply\nINFO Metadata applied\n\nAnd finally, to test it, we just need to create and then edit some users, while we watch our Nest app's logs =)\nFirst, let's create a user, giving it the email user1@site.com.\nThen, checking our Event Trigger invocations for the user-created trigger:\n\nhttp://localhost:8081/console/events/data/user-created/processed\n\nWe can see we have a webhook that went out! Was it successful?\n\nIt was! Let's check our NestJS console output and see if our handler logged what we expected...\n\nWow, awesome, everything is working as intended. That's the easiest way to set up secure Event Triggers I've ever seen!\nTo conclude this saga, let's go ahead and update our users email, it can be anything you want:\n\nAnd now check the processed events for the user-updated Event Trigger:\n\nhttp://localhost:8081/console/events/data/user-updated/processed\n\n\nStellar, looks like it fired off successfully here too!\nAnd lastly, just checking our NestJS console one final time:\n\nTa-da!\nLet us know if you have any questions while trying these out! You can hit us up on github discussions / discord if you need any help!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Local Dev to Staging to getting Production-Ready with Hasura! \ud83d\ude80 Description: In this post, we will look at how to approach various stages of development with Hasura, from localhost to staging -> going live to production and finally performing maintenance updates. Author: Praveen Durairaju"}, {"role": "assistant", "content": "In this post, we will look at how to approach various stages of development with Hasura, from localhost to staging -> going live to production and finally performing maintenance updates.Local DevelopmentLet's begin with local development. We will look at different aspects of local development like running the server/database, managing migrations, testing out Actions and Events with Hasura.Running Hasura via docker-composeThe simplest setup to run Hasura locally is to use the docker-compose setup to run both graphql-engine and postgres as docker containers. In case you have an existing Postgres setup running locally through a different environment, like the native Postgres Mac app or the Postgres binary installed on Linux, you can configure the HASURA_GRAPHQL_METADATA_DATABASE_URL to point to the right postgres connection string and remove the Postgres container from the docker-compose setup.Note: Hasura can connect to other databases like SQL Server, BigQuery etc, but still requires a PostgreSQL database for managing metadata.Managing schema migrationsHasura needs the Postgres schema and the metadata to create the GraphQL API. We will see how to manage migrations for both.Database migration filesThe state of your PG database is managed via incremental SQL migration files. These migration files can be applied one after the other to achieve the final DB schema.DB migration files can be generated incrementally and can by applied in parts to reach particular checkpoints. They can be used to roll-back the DB schema as well, provided the right down migration files are created.Hasura metadata filesThe state of Hasura metadata is managed via snapshots of the metadata. These snapshots can be applied as a whole to configure Hasura to a state represented in the snapshot.Hasura metadata can be exported and imported as a whole.Once you have the containers (graphql-engine and postgres) running, you have an option to manage the database schema migrations, depending on your current database state.Start using Hasura Console via CLI to enable database migrationsDatabase Migrations tab on Hasura ConsoleIn case you missed doing the above, you can create a one-off migration file through CLI once the initial schema is ready.Install Hasura CLIFor doing the above, you need the Hasura CLI installed. You can check out docs for installing the Hasura CLI. Once installed, initialise a hasura project with hasura init and this will scaffold a project directory with migrations, metadata and config.New database: If your database is clean without any existing schema, you can start using the console via CLI ( hasura console ) , connect to a database, modify the database schema and the CLI will take care of creating the up and down migration files.Existing database + Hasura setup: In case you have an existing database schema and metadata from a Hasura Cloud project, you can use the CLI to initialise the migration for that schema using the following command:hasura migrate create init --sql-from-server --endpoint <hasura-project-url> --admin-secret <admin-secret>\nThis will take a pg_dump of the public schema (can specify other schemas through flags) and create an up migration to get started. Alternatively if you have a SQL file with all the DDL statements you can also specify that as an argument:hasura migrate create init --sql-from-file <path-to-file.sql>Now head to the migrations directory to check the version of the migration generated.cd migrations/defaultNote down the directory name, which would look something like this 1627564572630_init. Just copy the version number without the _init parts of the name.hasura migrate apply --endpoint <hasura-project-url> --admin-secret <admin-secret> --version 1627564572630 --skip-executionIt is important to add the skip-execution flag since that marks the migration as applied. This is to ensure that the schema that is already created on Hasura Cloud project is not getting applied again, which would result in a conflict.Additionally to pull metadata from the existing Hasura server, execute the following:hasura metadata export --endpoint <hasura-project-url> --admin-secret <admin-secret> \nNow head back to Hasura Console and add a database source with the pre-configured env PG_DATABASE_URL. Finally, sync these project changes to your actual local Hasura project by running.hasura migrate apply --database-name default && hasura metadata applyOnce you set this up, you can continue to use Hasura Console served via CLI and make any schema changes and migration files will be automatically created as you work along.Squashing MigrationsDuring local dev, we typically modify schema iterating it over multiple times. This leads to a large number of migration files being created over time. If you are building a schema for a specific feature and you really don't want to rollback or manage the smaller iterations in between, you can squash the migration files into a \u00a0single file for easier organisation.hasura migrate squash --from <version>Migrations are also versioned with timestamps. At any point of time, using the migrations, you should be able to (re)create the schema and apply metadata to replicate the project quickly.Note: Using Hasura's migration system is optional. In case you are comfortable or familiar using other database migration tooling, you can continue to use that to manage your database schema. To disable Hasura's migration, you can do so via the Console served by the CLI. Head to to the Data -> Migrations tab and switch off the toggle \u00a0Allow Postgres schema changes via console. You can also create a project on Hasura Cloud for development. All the steps required to sync between dev and staging would remain the same. Typically, the webhook URL handlers need to be exposed to a public endpoint that Hasura Cloud can access and hence they cannot be localhost urls. We recommend using something like ngrok to expose a local server running for Actions/Remote Schemas/Events to a publicly accessible endpointMetadataHasura stores some metadata to create the GraphQL API over databases and provide other functionalities like Remote Schemas, Events etc. All the actions performed on the console, like tracking tables/views/functions, creating relationships, configuring permissions, creating event triggers and remote schemas, etc. can be exported as a JSON/yaml metadata file. You can use hasura metadata export anytime to export the current state of Hasura.The metadata directory of your Hasura project should be put in your version control system like git, so that you can rollback corresponding changes later, if required.Alright, so far we have taken care of managing migrations and metadata for our local dev. Now let's move on to business logic.How do I develop and test business logic?Hasura lets you write business logic in a flexible way. If you are comfortable writing your own GraphQL server from scratch for the business logic parts, you can add them as a Remote Schema. If you are interested in (re)using REST API endpoints, you can map GraphQL types with Actions.Adding either a Remote Schema or Action will become part of Hasura's metadata. It's configuration after all. When you are adding either of them, you will have to give the HTTP handler / URL so that Hasura can communicate with that endpoint. Assuming that handler is also running on your local machine, you will need to give an endpoint that is accessible from inside the docker container of Hasura.Typically for Mac, this will be host.docker.internal and for linux where the containers are running in host mode, it will be localhost. Configuring Handlers via Environment VariablesThe recommended way to configure these handler URLs is via environment variables, irrespective of the environment (local/staging/prod).ActionsActions can have a base url through ENVs, something like \u00a0{{ACTION_BASE_URL}}. So for example if all your REST API endpoints are running in a single server, you can configure the env with the host name. {{ACTION_BASE_URL}}/createUserwhere the {{ACTION_BASE_URL}} will typically have values like http://myserver.com or when running in localhost, it will look something like http://localhost:3000. All of this will be passed to the graphql-engine server as ENVs.Forwarding Headers can be configured using ENV. This will be applicable when you are sharing a common secret between your Action handler or passing in some Authorization tokens etc.Remote Schemas / EventsThe GraphQL server URL can come from an env var. Similarly any additional headers can be configured that can have values from env.Configure Remote Schema URLs and Headers with EnvSimilarly while creating Events, you can specify the URL for event handler via Env.Debugging in local devDuring local dev, you may want to look at errors in detail so that you can fix the underlying issue. For a GraphQL request that results in an error, Hasura may provide additional information for each object in the extensions key of errors. The internal key contains error information including the generated SQL statement and exception information from Postgres. This can be highly useful, especially in the case of debugging errors in action requests.Enable the Dev mode debugging via HASURA_GRAPHQL_DEV_MODE environment variable.Moving to StagingOnce you are done with local dev, moving to a different environment (staging/production) will primarily revolve aroundSetting up CI/CDConfiguring Environment variablesApplying migrations and metadataRunning testsSetting up CI/CDGenerally, when you are done developing your app locally, you would push it to your upstream version control system like Github or Gitlab. You can trigger CI/CD workflows when a push is made to your upstream repository. When you want to deploy your changes to staging, you may push your latest code to a special branch or push a new tag which updates your staging environment.The process for CI/CD with Hasura instances is essentially a mirror of the manual local workflow you would use. The CI/CD instance should download or be given the CLI as an artifact, and then run the series of commands you\u2019d like to integrate. This generally means \u201chasura migrate apply\u201d and \u201chasura metadata apply\u201d, and could also mean \u201chasura pro regression-tests run\u201d.You can make use of hasura/graphql-engine:vX.X-cli-migrations images to do this manually for yourself in your self hosted setup. You can auto-apply migrations/metadata when the graphql-engine server starts.GitHub DeploymentIf you would like to automate the migration and metadata updates and you are using GitHub to version control your migrations/metadata and Hasura Cloud to host your GraphQL API, then you can make use of our GitHub integration. Once you connect your repo and configure a branch, you can simply do a git push to your branch and trigger a deployment of migrations/metadata to your Hasura Cloud project.Configuring Environment VariablesThere are various components of Hasura metadata which are dependent on environment variables. This allows environment specific runtime without changing the metadata definition. If an environment variable is being used by some part of metadata and isn't available in an environment, the metadata application won't succeed. Before applying migrations/metadata, we need to ensure that the configuration is correct. Additionally you can check for the following:The GraphQL Endpoint needs to be secured. You will need to add an HASURA_GRAPHQL_ADMIN_SECRET env. This is added by default on Hasura Cloud, but on local development using docker, you will have to ensure it gets added for security.Environment variables for various entities like Actions/Remote Schemas/Events are configured.Applying Migrations and Metadata ManuallyMigrations can be manually applied to any Hasura instance through:hasura migrate apply --endpoint <graphql-engine-endpoint> --admin-secret <admin-secret> --database-name <database-name>This will apply only migrations which have not been already applied to the instance.Metadata can be manually applied via:hasura metadata apply --endpoint <graphql-engine-endpoint> --admin-secret <admin-secret>Running Tests - Regression TestingA good development workflow would require that tests be run 1) early in the dev process, and 2) automatically with changes, to ensure changes to the schema don\u2019t break functionality.As you keep making schema changes, running regression tests on Hasura Cloud will ensure you are not making unwanted breaking changes.Regression Testing with HasuraRead more about Regression Testing with Hasura.Going Live / ProductionHasura Cloud is the recommended hosting solution for Hasura as it takes care of Infrastructure management automatically (like auto-scaling), apart from providing analytics/rate limiting and other advanced features.Like staging, the migrations/metadata workflow needs to be repeated. The docs cover a Production Checklist for going live.. But broadly they are Secure endpoint with admin secretDisable Console - so that nobody will be able to modify schema/data directly.Disable APIs - Except the GraphQL API, you don't need access to other APIs like pg_dump, config and metadata etc.Disable dev mode - You don't want expanded detailed internal error messages in production.Restrict CORS domains - Allow only specific domains to make requests.Allow lists - If you know the exact GraphQL queries that would be made in the app, enable allow lists to deny any other request.Maintenance / updatesAfter going live, you can continue to use the same migrations/metadata workflow via CLI as part of incremental app building.Updating Hasura VersionThis depends on where your Hasura container is hosted. But broadly what we need to update is the docker image hasura/graphql-engine:<version> where the <version> will be replaced with the latest version. (v2.0.7 for example).There are update guides for popular vendors and tools like Heroku / Docker / Kubernetes / DigitalOcean. If you are using Hasura Cloud, this is taken care automatically.Updating ENV via APIHasura Cloud exposes GraphQL APIs to update environment variables or even create projects from scratch. For example, to update a few environment variables, you can make a mutation like via the API. Read more in the API reference.mutation updateTenantEnv {\n  updateTenantEnv(\n    tenantId: \"7a79cf94-0e53-4520-a560-1b02bf522f08\"\n    currentHash: \"6902a395d70072fbf8d36288f0eacc36c9d82e68\"\n    envs: [\n      {key: \"HASURA_GRAPHQL_ENABLE_CONSOLE\", value: \"false\"},\n      {key: \"ACTIONS_ENDPOINT\", value: \"https://my-actions-endpoint.com/actions\"}\n    ]\n  ) {\n    hash\n    envVars\n  }\n}What's your workflow for managing local dev, staging and production with Hasura look like? Do share in the comments :)"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Moving from Self Hosted Hasura OSS to Hasura Cloud Description: If you are running Hasura OSS, it's very easy to migrate to Hasura Cloud. We will look at why and how to migrate in this post. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Migrating from OSS to Hasura CloudWe recently announced the launch of Hasura Cloud that takes care of managing the GraphQL Infrastructure automatically for you, so that you can focus on building your apps and not worry about Infrastructure. If you are running Hasura OSS, it's fairly straigtforward to migrate to Hasura Cloud. We will look at why and how to migrate in this post and the benefits it brings along.Why migrate to Hasura Cloud?Hasura Cloud uses the OSS graphql-engine as the base and provides a cloud offering on top of that. Broadly there are two benefits to move your OSS instance to Hasura Cloud.Infrastructure concerns like number of instances, cores, memory, concurrent users, high-availability, realtime monitoring and tracing are taken care of, automatically for you. \u00a0GraphQL Server concerns like data caching, rate-limiting, regression testing, error monitoring are some of the features that will be useful at scale.With Hasura Cloud, GraphQL becomes the invisible piece in your architecture. Auto-scaling kicks in if you have a sudden traffic rush and eventually the only bottleneck will be the database. To optimise for database bottlenecks, Hasura Cloud gives you monitoring and tracing features that help you understand database query performance and give you insight into what to do.You will also have the ability to add collaborators to your team per project and give restricted access to the console and server operations among other benefits.Hasura Cloud Monitoring OverviewRead more in detail about the benefits of Hasura Cloud in our announcement post.Now that you know the benefits of migrating, let's quickly look at the steps required in performing the migration. The process depends on where your current Hasura instance is running and where your database is hosted. Do remember that, the database will still remain hosted the same way as before and doesn't need any migration apart from a potential configuration update like IP whitelisting. It is only the graphql-engine OSS server that needs to be migrated to Hasura Cloud.Migration StepsNote: If you are running Hasura OSS in production, you can follow all of the steps below on your staging server first and recommend moving your production workload to Hasura Cloud once it reaches general availability.This guide also assumes that your current Hasura version is atleast v2. If you are running versions prior to v2, for example, v1.3.3, you will need to follow the steps here to migrate to v2.0 before proceeding.Step 1: Update your current Hasura OSS GraphQL server to the latest version. Any new project created via Hasura Cloud runs on the latest version of Hasura OSS. At the time of writing this guide, the latest version of Hasura OSS is v2.0.7 and the docker image for the same is hasura/graphql-engine:v2.0.7. Change yours accordingly. Before migrating, we need to ensure OSS server is on the latest version to avoid any inconsistencies in metadata and configuration. Read more on how to update your current Hasura instance.Step 2: Take a database backup and metadata snapshot.Although updating to Hasura Cloud will only make use of the internal schema that the server maintains in the database, it's a good practice to take a backup before any migration. Use pg_dump CLI tool or the PG Dump API to take a backup of your database and take a snapshot of your metadata using the Hasura CLI. ie. hasura metadata export. Read more on metadata export.Step 3: Find the connection string for your existing database(s).We need to keep the database connection URL handy before creating a project on Hasura Cloud. This would look something like postgres://username:password@hostname:port/dbname for a PostgreSQL database.But if the database is not exposed to the outside world and the server is hosted on the same cluster as the database, accessing db via internal network, then you need to expose the database on a port which is accessible from outside that server. You can add a VPC which lets you access database not exposed over internet.For example, if you are running Hasura OSS on a DigitalOcean droplet, usually created via the One-Click integration on their marketplace, then the database instance is not exposed outside. In this case, you will need to modify the docker-compose file to run the db and expose on a port (for example: 5432). DigitalOcean has Firewall Rules that can be setup to limit who can access what ports.Below is an example from Google Cloud Platform to whitelist an IP to authorize access.We will find what IP to whitelist after creating the project on Hasura Cloud.The whitelisting IP step varies depending on the cloud vendor where your database is hosted currently and I would recommend checking official docs of the Cloud provider to configure this.Step 4: Keep the HASURA_GRAPHQL_* environment variables readyDepending on where Hasura OSS is hosted, fetch all the Hasura environment variables and their values used for the server. If it's a docker-compose file, then all envs used would be present there. If it's a kubernetes cluster, check the deployment.yaml for the env used.Note: If you are passing server flags to the OSS server, make sure to find the equivalent environment variable from the docs and keep them handy.Step 5: Configure necessary database permissionsThis step is optional and applicable only if your database is running in a controlled environment, where you might need to configure Hasura Cloud to use a specific database user that your DBA gives you.Read more about Postgres permissions required for running Hasura Cloud.Step 6: Create a new project on Hasura Cloud.Now that all configurations are ready, we can move on to the project creation step. Click on the Deploy to Hasura button below, signup for free to create a new project.In the new project page, enter the project name.Create new Hasura Cloud ProjectOnce you click on Create project, your project will be initialised and ideally after a few seconds you should get the Launch Console button enabled for the project.Step 7: Configure Environment variablesOnce the Cloud project is initialised, click on the Env vars tab on the project detail page.Environment Variables on Hasura CloudTo add a new environment variable, click on New Env Var and it should open up the following screen:Add ENV Hasura CloudAdd all the environment variables obtained from Step 4 to your Cloud project. You can ignore the HASURA_GRAPHQL_ADMIN_SECRET environment variables since that's already configured. In case you are using the old admin secret in other backend servers / APIs, you can update that env here so that those APIs don't break.Make sure to add the right envs for database connection strings. The default one in your local docker-compose comes with PG_DATABASE_URL, but you could be using any env for any data source that you have connected.Step 8: Add Custom DomainThis is the final step of the migration process. Hasura Cloud lets you configure your own domain for the API endpoint instead of the auto-generated hasura.app endpoint. To add a custom domain, navigate to the Domains tab on the project detail page and click on New Custom Domain at the top.\nEnter your custom domain and click on Add.\nAfter adding a custom domain, a pop-up similar to the one below, will appear.\nAdd the hasura domain as a CNAME record to your DNS. Depending on your DNS provider, this can take time.Apply metadata and migrationsOnce your Hasura Cloud project is configured like the above, the next step is to apply the metadata and migrations from your existing OSS project. Execute the following commands against your newly created Hasura Cloud Project.hasura metadata apply --endpoint <hasura-project-url> --admin-secret xxx\n\nhasura migrate apply --database-name <database-name> --endpoint <hasura-project-url> --admin-secret xxxVerify and test the migrationThat's it! You should be able to access the Hasura Console and make all your API requests to the new endpoint from your app. You can monitor the requests and see if queries are failing from the Monitoring tab of the console.Local to CloudWhat if there is an instance of Hasura running on local environment without migrations already? Migrating this to Hasura Cloud will involve additional steps, primarily around setting up the database itself. (the schema)You need to setup the database somewhere (Either on Heroku Postgres via One-Click integration on Hasura Cloud, or via other cloud vendors).All of the steps mentioned above for migration would apply for local to cloud migration too.Once the cloud project is created, if you have the migrations and metadata ready for your local instance, apply them to the cloud project using the Hasura CLI via hasura metadata apply and hasura migrate apply --database-name <database-name> commands.If you don't have the migrations ready or didn't use the Console via CLI for migrations in your local dev, you can make use of the PG Dump API to export the local database schema and apply it to the new database created as the initial migration file. The metadata can be exported from your local db by using hasura metadata export.Follow the Production Checklist to ensure your local instance is ready to go live.Things to Note:Moving from Hasura OSS to Hasura Cloud (and vice-versa) is reversible as long as the server version connecting to the database remains the same on both.You can create the project, apply the environment variables via an API too. Read more on the API reference.Let us know how the migration went through! In case you want help to troubleshoot/debug, feel free to join our Discord server and post your queries over there."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How our marketing ops team built a realtime lead-to-account matching solution with Hasura, BigQuery, Zapier and Salesforce. Description: Hasura\u2019s Martech stack includes Marketo, Salesforce and BigQuery primarily, and several other tools to fill in the missing pieces of the puzzle. One part of the process is mapping leads to accounts in Salesforce. Author: Yuko Takagi"}, {"role": "assistant", "content": "Hasura\u2019s Martech stack includes Marketo, Salesforce and BigQuery primarily, and several other tools to fill in the missing pieces of the puzzle. One part of the process is mapping leads to accounts in Salesforce. For example, if we have several folks aggregated in Salesforce from company X, they would need to be tied into Company X\u2019s Account, and not appear as individual leads. As we use information in Salesforce as well as outside of Salesforce, we have the logic stored in Google BigQuery as a view, and it was uploaded into Salesforce at a monthly cadence for us to be able to map every relevant lead to the matching account.With Hasura\u2019s recently announced support for BigQuery, our Marketing Ops team was able to automate this workflow by using Hasura\u2019s instant GraphQL & REST APIs on BigQuery and powerful querying abilities and make this process real-time.In this tutorial, we will walk through how this was set up using Hasura Cloud! Follow along, and do let us know if you have any questions. If you\u2019re looking to try Hasura out on your BigQuery database, get started instantly here. It takes <1 min to get a GraphQL API on your existing data! TutorialSetup Access for BigQueryOn Google Cloud Console, create a service account.Follow steps on pre-requisites.Go to IAM & Admin > Service Accounts > Create Service AccountGive it a name, and under roles, and grant these 3 roles: BigQuery Metadata Viewer, BigQuery Data Viewer and BigQuery Job User.Click on the created service account, Keys > ADD KEY > Create New Key > JSON > Create. This will download a service account file on your computer.Salesforce SetupCreate a lookup field to Account on Lead object. Create a BigQuery ViewCreate a view in BigQuery with lead id to update with a potential matching account. At Hasura, we use website addresses to match leads to accounts. Additionally, we prefer if the lead's country name matches the account country. Tip: NET.REG_DOMAIN to standardize the website address when you are doing the matching.So if we have an account for Hasura (country = United States) and also account Hasura (India), and if we find a lead with hasura.io website in the United States, we associate this lead to Hasura account in the United States, not in India. Also since one company may have multiple website addresses, we maintain a BigQuery table with 2 columns. 1 column with main account website, and another column with website address that Hasura\u2019s sales team consider as the same account/company. (ie. disneyplus.com is part of the Disney account.)Tip: One thing to keep in mind if you are setting up something similar, if you create a BigQuery table from Google sheet or using an external BigQuery table, you will not be able to read this table from Hasura.Set it so that query result of this view contains Lead ID for the Salesforce record and Account ID that Lead is matched to.Hasura Database ConnectionGo to Hasura Cloud dashboard, create a project (if not created yet), then within the Console, go to Data tab. Database Display Name: Name the databaseData Source Driver: BigQueryConnect Database Via: Connection ParametersService Account Key: Copy and paste the json you got from creating a service account in BigQuery.Copy and paste Project Id, Datasets from BigQuery.I left Global Select Limit as 1000 which was the default value.Set it to track the view you have created in BigQuery.Go to the API tab then find the table that was created from the BigQuery view in Explore, then select the columns with the result. (Salesforce Lead ID and matched Account ID.)Click on \"Rest\" button then Create Endpoint.Name the Endpoint, add description and use location field to set the webhook URL to be used in Zapier. Leave \"GET\" checked as Methods.Set up ZapierCreate a Zap that\u2019s triggered by Retrieve Poll.Set up a trigger.URL is the webhook URL created in Hasura REST Endpoint.Key is this part of query result.Test the trigger then you will see the result of the query.Turn the Zap on then it will run every hour if there is a new match to the query.Then add a step to update record in Salesforce.Map Lead id to Record to Update field, then Account ID to the field that was created as lookup field.Success!It checks the result of the query every hour and update records if they find new entry in the query. (It will only run for the new result added to the query.)ResultWith this setup, we are able to answer questions likeHow much activity do we have from this prospect account?We can see activities from both Leads and Contacts, without converting all the leads into an account.Who is the right rep / SDR for the lead?We are able to make decisions on lead routing with combined information, not just the information on the lead."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Tutorial: Build low code apps with Bubble and Hasura APIs Description: This tutorial covers fetching and uploading content with Hasura from Bubble.io\u2014A low-code platform for building any kind of app you can imagine. Author: Jesse Martin"}, {"role": "assistant", "content": "Hasura + Bubble = Powerful low code appsIn this tutorial, we\u2019ll be expanding on a previous tutorial we completed where we integrated a Hasura project with another low-code tool, Draftbit. In this continuation, we\u2019ll be building out a website presence for our application using Bubble. Bubble has an expansive community offering support and plugins for almost any use case. We will build on the REST integration we created in the previous tutorial and now include an additional endpoint that allows us to upload content to Hasura.\nThe Hasura features we\u2019ll use\nThe Interface\nThe Tutorial\nWorkflow Summary\nHow Hasura Cloud Helps\nClosing\n\nThe Hasura features we\u2019ll usePreviously, we used Hasura\u2019s ability to create a REST endpoint for a saved GraphQL query in the Hasura console. We\u2019ll re-use this functionality and add the Pipedream serverless endpoint from our API to upload images and persist them to Hasura. You can read more about that in the original tutorial.The InterfaceWe\u2019ll continue the trend of minimum viable interfaces with this series and present just enough UI to functionally display the images, tags, and a file upload interface.The TutorialPrerequisites:Familiarity with our base tutorial. See the diagram below for reference.A working, accessible Hasura project (not running on localhost) with existing data.A Bubble account.Scroll to the bottom of the page from the welcome screen, pass the onboarding tutorials, and click \u201cCreate a New App.\u201dFill in the default information for your application.To keep our UI lean, we will choose \u201cStart with a blank page.\u201dAfter that, press \u201cLet\u2019s get started.\u201dYou\u2019ll be asked what you want the title of the page to be (what displays in the tab across the top), what the default language of the page will be, and what you want the favicon to be.The next two questions are the most important, you can skip adding the payment plugin, but we\u2019ll want to add the \u201cAPI Connector\u201d plugin in the question following that.You\u2019ll be presented with a marketplace where you can install the plugin you want; we\u2019ll choose the one created by Bubble themselves.Install, then press \u201cDone.\u201dFrom here, you can choose to take the application tour or simply close the assistant. For this tutorial, we\u2019ll go straight to editing the application.Once we are viewing the empty screen, we will navigate to our Plugin tab on the left sidebar of the application, choose the API plugin, and press \u201cAdd another API.\u201dWe\u2019ll provide the default connection parameters for the API call, including a generic x-hasura-admin-secret for now. Note: Best practice would be to incorporate a user-auth flow to protect the interactions on this page.The individual call details are obscured behind the \u201cexpand\u201d button pointed out in the screenshot.At this point, we can map the parameters to our saved REST endpoints. Next, we will define a Get List call, ensuring that this will be used as \u201cdata\u201d on the page and that the response is JSON. Initialize the call to ensure that it works.Bubble provides a helper utility to map the response types from our server to use them with appropriate components and utilities when building our interface.Here we\u2019ll define that our created_at field is a date and that our image_url is an image.While in our plugin details, we\u2019ll define the \u201cGet Single\u201d API call as well, this time using Bubble\u2019s \u201c[]\u201d annotation to indicate where the URL parameterization should occur.And as before, we\u2019ll map the response types for Date and Image where applicable.One last adaption to our API is to add a Date/Time header to our API calls which will allow us to bust Bubble\u2019s cache proxy for requests to the server.As before, make sure to leave the \u201cPrivate\u201d option unchecked, which allows us to define this dynamically at the time of calling.We\u2019ll return to the Design tab with the APIs defined and add a \u201cRepeating Group\u201d container to the page.We\u2019ll need to identify the data source and the type of content the repeating blocks expect to see. We\u2019ll start with the data source, choose \u201cGet data from an external API,\u201d and then choose the Hasura - Get List\u201d API call. From the secondary menu.From there, we\u2019ll choose the \u201cGet List upload\u201d response type from the \u201cType of content\u201d dropdown.Draw a Text component into the first grid of the repeating list; this will prompt the appearance panel to show. Next, click inside the \u201c...edit me\u2026\u201d text area, which will show the \u201cinsert dynamic data\u201d window.Bubble will present a unique but intuitive property accessor interface where you select a natural language parse of the values you are trying to view. For example, the resulting string will be the \u201cCurrent cell\u2019s Get List upload\u2019s image name.\u201d Let\u2019s preview our results.It\u2019s working! We are iterating content in Bubble from Hasura.Let\u2019s replumb the component structure to put a top-level group in our repeating field, behaving as a click target for our next bit of UI. First, remove the text box, then add a group container to the component repeat field. With that in place, define its data source as we did previously with the text box, then add a child text box which will receive the final state of props.Let\u2019s add an image component to the page to view the URL of the clicked-on list item. I\u2019ve added a border under Appearance to help visually track its place in the UI for now.Now we need to create a workflow to handle click events on the group we added previously.We will tell the workflow to set the state on an element on our page.In this case, we are setting a custom state called \u201cImage URL\u201d on \u201cImage A\u201d to the value of the current cell\u2019s image URL property.In our Image A component, we will tell the image to be the value of that new Image URL state property.Let\u2019s test this behavior.That works!Let\u2019s try to show the tags associated with this image.We will add another repeater list next to the image component. Then, we\u2019ll add another \u201cset state\u201d flow to assign the ID of the selected item as \u201cUpload ID\u201d on the internal state of the new repeater field. The result should look like this.Adding another text box to this repeat, we can drill down to the tag name property being passed in from our API call.If we preview this gain, we can see that it works, if not in need of a loading state!To upload an image, we need to define another API call. We handle binary data through an external service that stores the file for this particular application, then calls Hasura to persist the data to our backend. You can read more about that architecture here.We need to call a different API and define that this endpoint will receive a field parameter identified as a dynamic entry when we call the API. To allow this parameter to be called later, we need to indicate that this is not a private field.With that in place, we need to create a custom upload workflow trigger to handle a manual API call based on arbitrary events.Now we need to create a trigger for this workflow from our UI. We\u2019ll use the \u201cUpload image\u201d component, and I\u2019ve opted to place mine inside of a popup. The implementation details are less critical on which UI primitive to use. Note: Ideally, you\u2019d use a file picker and send this to our API; as it stands, the UI components from Bubble automatically persist these files to a storage bucket, so we are in effect doubling our storage. An alternative in the meantime would be to write a custom form handler using the HTML block. To show the workflow architecture, I\u2019ve opted to stick with the default components for now.I\u2019ve layered an image preview and an image select. We\u2019ll be hiding the image select and reveal the image previewer on a successful response from our API.Back in our workflow editor, we\u2019ll define a listener for property changes on this image upload component, which will trigger our custom API call.This is how that looks in the editor.When the upload is done, we\u2019ll trigger our follow-up request using the returned Upload ID to request our \u201cGet Single\u201d API endpoint, which will also return our tags once the backend processing is completed.The custom event is defined to set the state of our Tags list once data returns.The last component we need to include is a polling API call that will run at a fixed interval (5 seconds) that will only make an API request once the Upload ID is present, the total list of returned tags is 0, and the popup is open.We introduce the third check because we need to create our tag count check in a round-about way to get around a minor bug in the platform. We will only show the tag list if the count is greater than zero, which offers predictable behavior; we then proxy this truthy condition in our API poll to check if the tag list is visible. The problem arises when the popup is dismissed and the checklist is hidden once more, returning its \u201chidden state\u201d back to true. Our third conditional is the popup open, mitigates this limitation. No code often requires some edge-case thinking where convenience features may be fighting against a presupposed mental pattern.The steps triggered by this event will now introduce our cache-busting Date header.Workflow SummaryThe workflow logic can be challenging to follow; here is an outline of what is happening behind the scenes. On page load, we fetch content from Hasura with the images. When we select one of them, we make another request for the tags of that image.When we upload an image, we persist that image to storage, save its reference to Hasura, and return that to the client. Once that round trip is complete, we request the tags for that image and display them in the UI.How Hasura Cloud helpsDebugging serverless workflows like this can be difficult as we are waiting for asynchronous behaviors to execute in the background. Fortunately, Hasura Cloud has a monitoring solution that allows us to see which APIs are being called, what data was passed in, and how well they are performing.Functionality like this gives us complete observability of our low-code stack to see what\u2019s happening in our front-end and the backing data layer.ClosingWe\u2019ve consumed an AI-powered image tagging service without writing any custom code, persisted our objects to storage, and navigated the content. Best of all, tools such as Bubble allow a wider range of architects to analyze the business use-case of a customer and implement a functional and effective solution without a lot of front-end specific knowledge.Low-code will continue to be a mainstay as we see the continual offloading of business logic and corporate IP handled at an abstracted data layer and thinner and more agile front-ends simply consuming these serverless behaviors."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How to Build a Simple, Secure Social Media Site with Magic and Hasura Description: Learn how to build a simple yet secure social media site where you can share photos of your kitties and see other people's kitten galleries! Uses Magic Auth and Hasura GraphQL Author: Praveen Durairaju"}, {"role": "assistant", "content": "QuickstartCreate a new account and application with Magic. Keep this tab open! You\u2019ll need both the Publishable API and Secret Keys soon.Create a new project with HasuraAdd a new environment variable called HASURA_GRAPHQL_JWT_SECRET in your Hasura project\u2019s Settings > Env vars like so:{\n  \"key\": \"abcdefghijklmnopqrstuvwxyz1234567890\",\n  \"type\": \"HS256\"\n}4. Keep your Hasura project Settings > General tab open, you\u2019ll need the GraphQL API url soon as well.5. Run npx make-magic --template hasuraNow that the app is running, you can sign up with an email address and username. However, since we haven\u2019t set up the backend to store and retrieve the data we need, the app will look pretty empty! If you want to complete the app, continue reading.IntroLearn how to build a simple yet secure social media site where you can share photos of your kitties and see other people's kitten galleries!Magic Auth with Hasura and Next.jsPart 1: Build Heroku PostgreSQL DatabaseStep 0. Update Hasura EnvRemember how you ran npx make-magic --template hasura to create a template for this project? If you take a look at the .env file, you\u2019ll find that we\u2019ve generated a new JWT_SECRET for you. Make sure to go back to your Hasura project\u2019s Settings > Env var tab, and replace the key in HASURA_GRAPHQL_JWT_SECRET with the newly generated JWT Secret.Step 1. Set up HerokuFirst thing we\u2019re going to do is set up a database. Thanks to Hasura, we can do this a lot quicker and easier than doing it from scratch. All we have to do is create an account and a new project with Hasura, launch their console, go into the Data tab and then create a Heroku PostgreSQL database.Connect a Database to HasuraStep 2. Create users tableNext we\u2019ll create our first table where we\u2019ll store the user\u2019s information. Namely, their issuer, publicAddress, email and username. Set the issuer as the primary key so that we can create a relationship with another table in a bit.Users tableTo control access to the users table, we\u2019ll create a user role with a few limitations.Users PermissionUsers should be able to see other profiles, so they have full access to select from the users table.Users select permission2. Users are only able to create, update or remove their own profile. This means we need a rule that requires the request to the GraphQL API to include a session variable called X-Hasura-User-Id.Users insert permissionX-Hasura-User-Id means the user is able to insert data if and only if it is an authenticated request. Later on in this guide you\u2019ll see how to use the JWT token to meet this rule.Step 3. Create kitties tableLet\u2019s create another table and call it kitties. It\u2019s where we\u2019ll be storing all of the photos people submit of their kittens. The data each row will have is: id, pic_url and user_id.We also want to control access to this table, so let\u2019s set up a user role again. The rules are going to sound similar to the users table.Users should be able to see other people\u2019s kittens.Kitties select permissions2. Users should only be able to upload or delete photos of their own kitten.Kitties insert permissionBecause we\u2019ll need to run nested queries on both the users and kitties tables, we first have to establish a relationship between them. We do this by setting the user_id as the foreign key that is mapped to the primary key of the users table (namely, the issuer).Kitties table modifyStep 4. Create an array relationshipSince there is a relationship between these two tables, we can create an array relationship which lets us do a nested query on the users table. In our case, an array relationship means a user can have multiple photos of kittens.Users array relationship with KittiesNow that we\u2019ve set it up, we\u2019re able to run a nested object query on the users table and are able to see all of the kitty pics every user has shared.query users and kittiesStep 5. Create an object relationshipLastly, we\u2019ll create an object relationship which lets us do a nested object query on the kitties table. This means every photo of a kitty has exactly one owner.kitties object relationship with userHere\u2019s how a nested object query looks.query kitties and userPart 2: Build Next.js ClientNow that we\u2019ve got our database set up, let\u2019s move onto the frontend and explain what our core UI components do!<Signup />Boxi SignupThis is where the user signs up with their email address. Because I want to display people\u2019s usernames on their profile and dashboard, I\u2019ve also added an extra field for it.Note: The expected flow is for the user to 1) sign up, and if they ever log out, to get back into their account they\u2019ll need to 2) log in. To keep the code simple, there is no logic in place to check whether or not the user is already registered.<KittyDashboard />Enter profileOnce the user logs in, they land on the dashboard. This is where they\u2019ll be able to see all of the kitty pictures people have shared. It\u2019s also where they can share photos of their kittens as well.To create a profile for every user, we\u2019ve implemented Next.js dynamic routes.<UserKittyPics />Boxi GalleryIf we click into a person\u2019s profile, we\u2019ll be able to see their profile which includes a gallery of their kittens.<Profile />If we click into our own profile, we\u2019ll also see a gallery along with our personal information.Part 3: Plug in Magic AuthNow let\u2019s understand the core parts that make authentication with Magic possible.Signing up or Logging insignup.js is the main file that handles the user onboarding flow. ../lib/magic.js is where import the Magic library and initialize the Magic SDK. When the user logs in, we call magic.auth.loginWithMagicLink which triggers the magic link to be emailed to the user.Once the user is authenticated by Magic, a Decentralized Identity (DID) token with a default 15-minute lifespan is generated. We send this token to our serverless function at /api/login to validate it, pull the authenticated user\u2019s information to save in a JWT, and then store this JWT in a cookie to persist user sessions.Ensure User is Logged inIf we take a closer look at signup.js we\u2019ll see that we\u2019re calling useuser(). This function checks whether or not the user is logged in, which helps us route the user to the appropriate page.Logging outWhen the user logs out, our app calls a serverless function at /api/logout. This function clears the cookie that contains the JWT and then calls magic.users.logouByIssuer() to log the user out of their session with Magic.Part 4: Build GraphQL APINow that we\u2019ve got our database, client, and auth all set up, it\u2019s time to create a GraphQL API.<Signup />When the user signs up, we want to create a new user in our users table. Here\u2019s the query to make this happen:<KittyDashboard />When a user shares a photo of their kitten, we\u2019ll want to add a new row to the kitties table. The query for this is addKittyQuery:<UserKittyPics />To display the kitty galleries owned by other users, we\u2019ll need to fetch all of the photos for the specified user. Here is the query:<Profile />Boxi Profile Magic AuthLastly, for the logged in user to see their own kitty gallery as well as their personal information, we\u2019ll need to fetch these data as well. The query to get the user\u2019s username is:And the query to get the kitty gallery is the same as in <UserKittyPics />. The only difference is that we\u2019re not passing userId as a prop into the component.Try MagicThere you have it. A simple, yet secure social media site where you can share and see photos of other people\u2019s kitties! Here is the open source repo, and a link to the demo.If you need an auth solution that\u2019s easy to use, try Magic today. For a limited time, you\u2019ll get your first 10,000 logins free. Sign up today at https://go.magic.link/hasura.About the AuthorMaricris Bonzo - Maricris has been a Developer Advocate for a variety of tech companies for 4+ years. She loves to learn about modern infrastructure and tech that either simplifies or elevates the developer experience. When she's not advocating for passwordless auth at Magic, she is building a community in public called Blockchain Ladies Club ( \ud83d\udee1\u26d3\ud83d\udc1e\u2663.y.at). Follow her on Twitter - https://twitter.com/seemcat"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Your Guide to GraphQL with TypeScript Description: Type safety is an enormous benefit for developers when embracing the GraphQL ecosystem. This tutorial guides you through a practice known as generative type tooling which allows you to unlock the full potential of GraphQL types in the front-end. Author: Gavin Ray"}, {"role": "assistant", "content": "Type safety is an enormous benefit for developers when embracing the GraphQL ecosystem. This tutorial guides you through a practice known as generative type tooling which allows you to unlock the full potential of GraphQL types in the front-end.\n\nWhy Type Safety\nWhy isn\u2019t everyone embracing type-safe languages?\nTutorial\n\nStack\nDefining \u201cGenerative\u201d\nPrerequisites:\nStep One: Clone and configure the code repository\nStep Two: Run Hasura Console\nStep Three: Generate the typings\nStep Four: Consume the SDK\nStep Five: Iterate on the pattern\n\n\nTutorial 2 (using GraphQL Code Generator)\n\nOverview\nSetup\nDevelopment Process\n\n\n\nWhy Type Safety\nThe GraphQL specification guarantees that GraphQL APIs are strongly typed. On its own, this means that a query can be validated for syntactical correctness and validity before executing, which eliminates runtime errors. Strongly typed schemas also allow us to fully know the shape of a thing, which has led to the movement of rich API tooling in the GraphQL ecosystem \u2013 a fact often referenced in explaining GraphQL\u2019s rapid rise to popularity.\nType safety, the name given to the benefit of using strongly typed languages and schemas, helps developers write code faster and with fewer errors. But type safety on the API alone doesn't capture all of the benefits that can be provided. Other benefits include:\n\nCallsite safety\nImproved developer efficiency\nIntegrated documentation\nEnhanced workflow\n\nPaired with a \u201ctype-safe\u201d programming language for back-end and front-end development, a type-checking toolchain can remove virtually all syntactical errors from occuring, both in development and production.\nHowever, when paired with tooling generated from a type-safe API, not only can that type-checker validate our data access patterns, it can also help us \u201cpeek inside\u201d our API to know what data is available, make guarantees around those access patterns that support features like lazy loading, and help us ship code with considerably improved speed and accuracy.\nWhy isn\u2019t everyone embracing type-safe languages?\nSo if type safety is such an improvement, why isn\u2019t everyone using it? One could look at the historical coupling of GraphQL and JavaScript, a language without types, as being one of the reasons (note, you don\u2019t have to use JavaScript to use GraphQL, it\u2019s language agnostic!). Another reason could be that enabling type-safe applications often requires a lot of boilerplate. The typical type-safe project needs a typing layer over the API (typically written by hand), setting up all the type tooling to actually validate type-safe code in development, and lastly, but still not the least important, teaching developers to embrace the new syntax of a typesafe language.\nWell, with GraphQL and generative tooling we\u2019ll get the manual typings for our API. With Next.js we\u2019ll get a typesafe runtime for TypeScript out of the box. And in this tutorial, we\u2019ll show you how. Let\u2019s begin.\nTutorial\nStack\nWe\u2019re going to assume that if you\u2019re reading this, here on this blog, you\u2019re already familiar with GraphQL. The other two parts of our stack may be less familiar to you.\n\n\nTypescript is a JavaScript flavored language that compiles to JavaScript. It has the benefit of funded development from Microsoft, and for better or worse, has become the path to type-safe development for most JavaScript developers. Thanks to the historical linking of React (JavaScript) and GraphQL, many GraphQL users are also familiar with JS.\n\n\nNext.js is an application framework with built-in support for TypeScript. It reduces a large amount of the tooling needed to write, validate, and compile TypeScript. It also provides a very simple path to deploying your application on either the Vercel platform (creators of Next.js) or other serverless runtimes.\n\n\nDefining \u201cGenerative\u201d\nGenerative tooling simply refers to having our code written for us by the machine. Given a provided input (I), we get predictable output, written to the filesystem (O). In the case of our GraphQL tooling, we\u2019ll provide an API with a couple of parameters, and get typings and an SDK in return. For our project, we\u2019ll use a CLI called GraphQL Zeus for this part of the chain.\nPrerequisites:\nDocker installed and running on a non-ARM machine or a Hasura Cloud account.\nNode 12+ installed.\nStep One: Clone and configure the code repository\nYou can find all the code for this tutorial on this Github repository:\n\nhttps://github.com/GavinRay97/typescript-gql-workshop-demo\n\nClone the repository to your local filesystem and either:\n\nA) Use docker compose to run a local image of Hasura GraphQL Engine\nB) Swap the URL value at typescript-gql-workshop-demo/utils/constants.ts from localhost:8080 to your Hasura Cloud app's URL\n\nFor A) the process is:\n$ docker-compose up -d # Start Hasura + Postgres in Docker\n$ cd hasura\n$ hasura migrate apply # Apply DB table migrations\n$ hasura metadata apply # Apply metadata for tables\n$ cd ../\n$ yarn install\n$ yarn dev\n# Now visit http://localhost:3000\n\nFor B) open typescript-gql-workshop-demo/utils/constants.ts and modify the file contents:\nexport const CONSTANTS = {\n  HASURA_URL: \"https://my.hasura.app\"\n}\n\nAnd then run a similar set of commands as you see above, instead passing your Cloud app endpoint and credentials:\n$ cd hasura\n$ hasura migrate apply --endpoint https://my.hasura.app --admin-secret secret # Apply DB table migrations\n$ hasura metadata apply  --endpoint https://my.hasura.app --admin-secret secret # Apply metadata for tables\n$ cd ../\n$ yarn install\n$ yarn dev\n# Now visit http://localhost:3000\n\nStep Two: Run Hasura Console\nAs part of this tutorial we'll want to make changes to our data model in a realistic manner.\nIn the real world, when we do this, generally we want those changes to be tracked somewhere.\nWhile we COULD directly alter our tables or columns from the \"normal\" Hasura web console, instead we will launch the specific console instance which is capable of automatically creating migration and metadata files based on our changes.\nTo do this, all we have to do is run:\n$ cd hasura\n# With Hasura OSS:\n$ hasura console\n# Or, if you're using Hasura Cloud:\n$ hasura console --endpoint https://my.hasura.app --admin-secret secret\n\nThis should launch a new web console available at http://localhost:9695\nNow, any changes we make there will be recorded and tracked.\nStep Three: Generate the typings\nLets start by generating the TypeScript SDK for our GraphQL API operations.\nIn the project, there's an NPM package.json script which is an alias that does this for us. For conveniences' sake, that command is:\n\nyarn generate-gql-client\n\nIf we look in our package.json, we can see what that command is actually doing, under the hood:\n{\n  \"scripts\": {\n    \"dev\": \"next\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"post-update\": \"yarn upgrade --latest\",\n    \"generate-gql-client\": \"zeus http://localhost:8080/v1/graphql ./utils/generated --ts\"\n  }\n}\n\nIt's calling zeus http://localhost:8080/v1/graphql ./utils/generated --ts. This command says:\n\nRun graphql-zeus on the API that exists at localhost:8080\nPlace the generated file(s) in the directory ./utils/generated\nUse TypeScript (--ts) for the SDK. (Alternatives: JS, ESM)\n\nThere is one more, fairly important piece of info that this command doesn't use.\nThat is: how to attach headers. If our API has an admin-secret set, then we need to append:\n\n-H 'X-Hasura-Admin-Secret: mysecret\n\nStep Four: Consume the SDK\nAssuming all has gone well, you should have an SDK generated.\nTo test it out, we can start by creating some file, say utils/generated/sandbox.ts and import utils/generated/graphql-zeus.\nimport { Gql } from \"./graphql-zeus\"\n\nIf you hover over this object, you'll notice it has three properties -- query, mutation, and subscription. If we ctrl + click it, we can see that the definition is:\nexport const Gql = Chain('http://localhost:8080/v1/graphql')\n\nexport const Chain = (...options: fetchOptions) => ({\n  query: ((o: any, variables) =>\n    fullChainConstruct(apiFetch(options))('query', 'query_root')(o, variables).then(\n      (response: any) => response\n    )) as OperationToGraphQL<ValueTypes[\"query_root\"],query_root>,\nmutation: ((o: any, variables) =>\n    fullChainConstruct(apiFetch(options))('mutation', 'mutation_root')(o, variables).then(\n      (response: any) => response\n    )) as OperationToGraphQL<ValueTypes[\"mutation_root\"],mutation_root>,\nsubscription: ((o: any, variables) =>\n    fullChainConstruct(apiFetch(options))('subscription', 'subscription_root')(o, variables).then(\n      (response: any) => response\n    )) as OperationToGraphQL<ValueTypes[\"subscription_root\"],subscription_root>\n});\n\nSo what this Gql object is, is just a typed wrapper around fetch() requests.\nSome TypeScript magic is used to map objects of input arguments and objects of selected fields, onto a schema and schema types.\nIf we do Gql.query({}) and press Ctrl + Space to autocomplete with our cursor inside of the curly braces, we'll see we get a list of every operation in our GraphQL API, along with the corresponding TypeScript types:\n\nWith the API of GraphQL Zeus, the function args are a tuple of [queryArgs, selectionSet]\nFor example, the GraphQL query:\nquery {\n  user(where: {\n     username: { _eq: \"person \" }\n  }) {\n    id\n    username\n    user_todos {\n      todo {\n        id\n        description\n        is_completed\n      }\n    }\n  }\n}\n\nWould be written as:\nconst result = Gql.query({\n  user: [\n    {\n      where: {\n        username: { _eq: \"person \" },\n      },\n    },\n    {\n      id: true,\n      username: true,\n      user_todos: [\n        {},\n        {\n          todo: {\n            id: true,\n            description: true,\n            is_completed: true,\n          },\n        },\n      ],\n    },\n  ],\n})\n\nIf we do this, we see that the result type is dynamically calculated from the fields we select (it contains ONLY those fields, and their correct types):\n\nBecause the core of the SDK is essentially just a way to map objects to GraphQL queries and their types, we can use it without fetch() and write custom wrappers for other tools.\nOne such example is as a wrapper for Apollo Client.\nAn implementation of that could look something like this:\nimport {\n  ApolloClient,\n  gql,\n  LazyQueryHookOptions,\n  MutationHookOptions,\n  MutationOptions,\n  NormalizedCacheObject,\n  QueryHookOptions,\n  QueryOptions,\n  SubscriptionHookOptions,\n  SubscriptionOptions,\n  useLazyQuery,\n  useMutation,\n  useQuery,\n  useSubscription,\n} from \"@apollo/client\"\n\nimport {\n  MapType,\n  mutation_root,\n  query_root,\n  Selectors,\n  subscription_root,\n  ValueTypes,\n  Zeus,\n} from \"./generated/graphql-zeus\"\n\nexport function useTypedQuery<Q extends ValueTypes[\"query_root\"]>(\n  query: Q,\n  options?: QueryHookOptions<MapType<query_root, Q>, Record<string, any>>,\n) {\n  return useQuery<MapType<query_root, Q>>(gql(Zeus.query(query)), options)\n}\n\nexport function useTypedLazyQuery<Q extends ValueTypes[\"query_root\"]>(\n  query: Q,\n  options?: LazyQueryHookOptions<MapType<query_root, Q>, Record<string, any>>,\n) {\n  return useLazyQuery<MapType<query_root, Q>>(gql(Zeus.query(query)), options)\n}\n\nexport function useTypedMutation<Q extends ValueTypes[\"mutation_root\"]>(\n  mutation: Q,\n  options?: MutationHookOptions<MapType<mutation_root, Q>, Record<string, any>>,\n) {\n  return useMutation<MapType<mutation_root, Q>>(\n    gql(Zeus.mutation(mutation)),\n    options,\n  )\n}\n\nexport function useTypedSubscription<Q extends ValueTypes[\"subscription_root\"]>(\n  subscription: Q,\n  options?: SubscriptionHookOptions<\n    MapType<subscription_root, Q>,\n    Record<string, any>\n  >,\n) {\n  return useSubscription<MapType<subscription_root, Q>>(\n    gql(Zeus.subscription(subscription)),\n    options,\n  )\n}\n\nexport function useTypedClientQuery<Q extends ValueTypes[\"query_root\"]>(\n  apollo: ApolloClient<NormalizedCacheObject>,\n  query: Q,\n  options?: QueryOptions<MapType<query_root, Q>, Record<string, any>>,\n) {\n  return apollo.query<MapType<query_root, Q>>({\n    query: gql(Zeus.query(query)),\n    ...options,\n  })\n}\n\nexport function useTypedClientMutation<Q extends ValueTypes[\"mutation_root\"]>(\n  apollo: ApolloClient<NormalizedCacheObject>,\n  mutation: Q,\n  options?: MutationOptions<MapType<mutation_root, Q>, Record<string, any>>,\n) {\n  return apollo.mutate<MapType<mutation_root, Q>>({\n    mutation: gql(Zeus.mutation(mutation)),\n    ...options,\n  })\n}\n\nfunction useTypedClientSubscription<Q extends ValueTypes[\"subscription_root\"]>(\n  apollo: ApolloClient<NormalizedCacheObject>,\n  subscription: Q,\n  options?: SubscriptionOptions<\n    MapType<subscription_root, Q>,\n    Record<string, any>\n  >,\n) {\n  return apollo.subscribe<MapType<subscription_root, Q>>({\n    query: gql(Zeus.subscription(subscription)),\n    ...options,\n  })\n}\n\nNow these wrappers such as useTypedQuery() can be used wherever useQuery() would have been -- and same with useTypedClientQuery() and apollo.query(), etc.\n\nStep Five: Iterate on the pattern\nNow that you have a typed SDK for your GraphQL API, and you understand:\n\nHow to generate it\nHow it works under the hood\nHow to adapt it to your needs by using the underlying type definitions\n\nThe final piece is to discuss how to continue to iterate when using these sorts of SDK's.\nIn order to keep your codebase type-safe and in sync with your schema, whenever you make schema changes you should:\n\nRe-run the SDK generator (in our case, yarn generate-gql-client)\nRestart all consuming clients of the SDK (in our case, only Next.js)\nConfirm that:\n\n\n\nThe TypeScript compiler passes typechecks (IE that you have not made an incompatible signature change)\n\n\n\n\nThat all services still function normally (Typically done via test suites)\n\n\n\n\n\nTutorial 2 (using GraphQL Code Generator)\nOverview\nNot covered in the text above, yet incredibly popular is another tool called GraphQL Code Generator. This tool serves the same purpose, but does so in a different way.\nOne of the benefits of GraphQL Code Generator is that it isn't limited to generating TypeScript types -- it has generators for both frontend and backend, in several languages including C#, Java, Kotlin, and many others.\nThe primary difference between GraphQL Code Generator and tools like graphql-zeus, is that GraphQL Code Generator generates operation-specific types, rather than a single SDK containing all possible operations.\nSetup\nThe process for configuring and running GraphQL Code Generator is similar to the process above, albeit wih some minor differences:\n\nWe will need a .yml file containing configuration\nThe generation process will need to be run either continually, or each time we change the queries we use in our codebase\n\nFor this demo, we'll look at how to use one of the many plugins available:\n\nhttps://www.graphql-code-generator.com/docs/presets/gql-tag-operations\n\nThe installation and configuration process, from zero, goes something like this:\n# Create a new package.json\nyarn init -y\n# Add GraphQL Codegen dependencies, and GraphQL \"TypedDocumentNode\" typings package\nyarn add -D @graphql-codegen/cli @graphql-codegen/gql-tag-operations-preset @graphql-typed-document-node/core\n# Add GraphQL itself, and URQL (used as a demonstration)\nyarn add graphql urql\n# Initialize Typescript\ntsc --init\n\nNext, we need to config our codegen.yml file. This is the file GraphQL Code Generator uses to produce the generated code from our schema and types.\nLet's create a file with contents like these:\n# Put the URL to your Hasura app here\nschema: http://localhost:8080/v1/graphql\n# schema: ./schema.graphql  <-- Alternatively, you can use a GraphQL schema file\ndocuments:\n  - 'pages/**/*.tsx'\n  - '!utils/gql/**/*'\ngenerates:\n  ./utils/gql/:\n    preset: gql-tag-operations-preset\n\nWhat we've said here is to:\n\nRead the schema using an introspection query from the API hosted at http://localhost:8080/v1/graphql\nUse every file matching ./pages/**/*.tsx as a document to check for GraphQL fragments/queries\n\nOmit every file under ./utils/gql/**/* -- don't check these (this is where we are going to put our generated files, we don't want to double-generate)\n\n\nFor generated files:\n\nin the directory ./utils/gql/\n\nuse the plugin gql-tag-operations-preset on the documents that were found\n\n\n\n\n\nFinally, the last step is to run it!\nDevelopment Process\nYou need to ensure that you have GraphQL documents/operations in the documents directory listed.\nFor the sake of this demo, there's a sample application you can view along with hosted here:\n\nhttps://codesandbox.io/s/agitated-sun-ghb1m?file=/pages/index.tsx\n\nHere, we have an index.tsx page which has a GQL operation:\nimport { gql } from \"../utils/gql\";\nimport { useQuery } from \"urql\";\n\nconst UsersQuery = gql(/* GraphQL */ `\n  query UsersQuery {\n    user {\n      id\n      email\n      name\n    }\n  }\n`);\n\nThe code generator will pick this up, and generate the corresponding type for it\nTo run the code generator, the command is:\n\nyarn graphql-codegen\n\nThe output should be something like this:\n\nNow, we hover over the type of UsersQuery, we can see that it contains a type specific to that particular GQL type and selection of fields:\n\nAnd the same is true for the type of useQuery({ query: UsersQuery })\n\nThe way you'd more commonly use this during development would likely be in \"watch\" mode, where it picks up file changes and re-generates automatically.\nTo do that, instead run:\n\nyarn graphql-codegen --watch\n\nThen, as you below, it should stay synced with your operation definitions as you alter them:\n\n\n\n    Sorry, your browser doesn't support embedded videos.\nWe hope you found this guide useful. If you have questions, you can reach out to us on Discord or Github Discussions. If you're new to GraphQL & Hasura, we have a 2 hour tutorial for Typescript developers to get hands-on with GraphQL and a Hasura Basics course to get you up and running with Hasura. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: A Tutorial using Hasura with Draftbit, a low-code tool for building mobile apps Description: In this tutorial we will integrate Hasura with the low-code mobile app development tool Draftbit. We will scaffold a basic application with Draftbit, map the required data to a couple of GraphQL queries, and save those queries behind idiomatic REST endpoints in the Hasura console. Author: Jesse Martin"}, {"role": "assistant", "content": "In this tutorial we will integrate Hasura with the low-code mobile app development tool Draftbit. We will scaffold a basic application with Draftbit, map the required data to a couple of GraphQL queries, and save those queries behind idiomatic REST endpoints in the Hasura console. As mobile-first development continues to lead many of today\u2019s go-to-market strategies for new initiatives, tools like Draftbit allow teams to iterate on layouts visually, with native code running in the background.The Hasura features we\u2019ll useWe will be using Hasura\u2019s ability to generate instant REST endpoints to integrate with the UI components that Draftbit makes available to us. The primary glue to integrate Hasura with Draftbit is creating idiomatic REST endpoints for GraphQL queries in the Hasura console. This will allow us to define a query for Draftbit (which doesn\u2019t support GraphQL), on the Hasura side and save that behind a RESTful endpoint. More on that later.Our data modelWe\u2019ll be re-using an existing API for our project from an auto-tagging image service. If you\u2019d like to follow along you can find the original tutorial here. The concepts involved are universal, so you can feel free to simply adapt the use-case for your individual project.As a point of reference, our model reflects the following pattern.The exact models vary, and include an upload join table with both photo and tags associated with a user.The interfaceFor the purposes of learning we won\u2019t be creating anything detailed with the interface. We\u2019ll be using stock components from Draftbit and create an interface that let us view all the photos in a list, then view the details (tags) for an individual photo.The TutorialPrerequisites:A working, accessible Hasura project (not running on localhost) with existing data. See the diagram below for reference.A Draftbit account.From DraftbitCreate a new project from the Dashboard view.Provide some details about your application.And for our demo we\u2019ll be choosing \u201cPosts / Content\u201d and \u201cFeed\u201d as our template starters. Allow the project to finish initializing, then click \u201cStart Building\u201d.The interface is quite intuitive with standard looking design and preview functionalities. You should be seeing an initial screen with a pre-filled feed of some data. Let\u2019s see how far we can get with these default settings by simply replacing the source content with our API. To do that, we need to select the data tab from the right-hand menu, and choose a custom API for \u201cREST\u201d.Many tools including low-code tooling are optimized for REST consumption. This is exactly why Hasura has support for creating REST endpoints \u2013 for handling the widest range of use-cases possible.And the best part is you still get to leverage your GraphQL experience!We will be creating a base resource here that we can use throughout the application. In this case, I will simply name my resource \u201cHasura\u201d \u2013 and for simplicity, I will provide my Hasura Admin Secret to authenticate the requests. You could use a user context for requests later on. If you would like to see user authentication with low-code, let us know!Side note: You can create Global variables under the settings tab for the project to avoid pasting string values into the configuration here. These are loaded into the environment during runtime.Returning to our REST API dashboard, we need to configure three critical parts of the resource definition.We need to provide the base API path for REST requests to Hasura. This is the part of your API before the \u201cv1/graphql\u201d path segment. Then add \u201capi/rest\u201d at the end.Secondly, we need to define the Hasura admin secret header (x-hasura-admin-secret) and the value - which I have defined in a global variable called \u201cHAS\u201d.Once you have saved the resource, you need to define some endpoints which will be available to the application. But before we do that, we need to now turn to the Hasura console to define what those will be. Open your Hasura project in a new Tab so we can hop back-and-forth as needed.The default UI from Draftbit\u2019s feed component included a date, image URL, and title. Let\u2019s fetch the equivalent data from our API. We won\u2019t worry about pagination in this tutorial. I\u2019ll create a named query operation that will request this data. It\u2019s important for the next step that this be a named operation. It\u2019s also a good best practice to name operations in general for tracking down performance leaks in our application.With our query working, we\u2019ll now navigate to the power feature available to us in Hasura which enables this entire integration.From the REST interface, we can name our operation, choose our protocol, and create our endpoint. See the image below as a reference.We\u2019ll return to Draftbit and add an endpoint.Skip step three.Save that endpoint and click done on the API resource window. We\u2019ve got our first resource from Hasura fetching data!Now to iterate over our content, we need to tell our UI to use this new data source. Click on the \u201cFetch\u201d component in our Layout tree on the left side of the screen.From the data properties pane on the right side of the window, we\u2019ll need to configure our new data source as the feed provider.The blank screen means that it is working! Almost\u2026Now we need to choose the List component in the Layout tree on the left and ensure we\u2019re still viewing the Data properties pane on the right. For \u201cData\u201d we need to choose the response from our API that is of an Array type - in our case, it will be the first key in the response called \u201cupload\u201d.You\u2019ll see the list component update now to look as though it\u2019s iterating over \u201citems\u201d - though they are all invisible at the moment. Now we\u2019ll update which pieces of data will be fed into the UI and at which point.Select the \u201cimage background\u201d component in the Layout tree. From Data, we\u2019ll choose the Source dropdown of the Setup section. We\u2019ll look for \u201cimage.url\u201d from the dropdown.We have live art on the page!Let\u2019s move on to the date and title.It seems that our date is coming back in an undesired format, let\u2019s create a custom transformer to handle that.Click on \u201ccustom\u201d in the toolbar across the top. We\u2019ll write a \u201clow-code\u201d transformer that runs on this bit of text.Under the \u201cFunctions\u201d menu item, click \u201c+ Add\u201d to create a new function.Under the information for the function, provide a name and input parameters. For my date, I\u2019ll do a simple Locale String parse. You can run any native Javascript and access some libraries if needed.Select the new transformer and now we have nicely formatted date and time.Let\u2019s repeat these steps for our filename.The code for the second transformer simply replaces some repetitive text in the titles.function niceFormat(text) {\n    return text\n        .replace(\"image-from-rawpixel-\",\"\")\n        .replace(\"-jpeg.jpg\",\"\");\n}\n\nPlay around with some of the formatting options for the different components. In my case, I wanted the posters to be full-size. The end effect looks like this.Now it\u2019s time to handle individual details. We\u2019ll tell our \u201cTouchable\u201d component to pass the id of this box to the detail view screen.With that configured, we can now navigate to the Article View screen.Looking at the top-level fetch for this view, under the Data panel, we can see that it passes \u201carticleId\u201d as a parameter to the URL structure of the request.What this means is we will pass a variable with the value of the ID of an individual upload to a specified REST endpoint. Let\u2019s create that endpoint in Draftbit now. Note, we can provide a test variable that will be run in the case of a non-existing URL parameter, as is the case in development mode.If we test that now, however, it will fail, since we have not defined the endpoint in Hasura. Let\u2019s do that now. First, we test the query we\u2019d like to persist to an endpoint.Second, we need to save that as we did with our listing API call. Using the syntax :variable lets us define where we\u2019ll accept inputs in the REST API parameters.Back to Drafitbit, we can save that endpoint now that we have a successful Test case run.Returning to our detailed view, we can select our top-level Fetch in the Layout browser, choose our saved Hasura service under the data tab, select the Get Single endpoint, and choose the articleId we passed in from our list-view (also defined in our test case).We can see by the partial UI loading, that we are at least receiving some form of data. Let\u2019s connect our pieces of data as we did in the previous screen.With a little bit of cleanup on the components we need to display our data, re-applying some text transforms, and adjusting the styles, we end up with a rough shape of the following:However, we\u2019ve done nothing with our tags! Let\u2019s add a way to loop over tag elements. Based on what we\u2019ve seen so far, we can assume we\u2019ll need some combination of another \u201cView\u201d box - typically as a container primitive, some form of a \u201cList\u201d component to iterate over an array of data, and the final form of our tag element.Implementation details will vary, but with these primitives we were able to loop over our tags, and output them on the screen.But we\u2019ll notice that\u2019s a lot of tags! And tags like \u201cmeat\u201d don\u2019t seem relevant to our pictures of fruit! Fortunately, our auto-tagging API includes the confidence level of these tags. Let\u2019s adjust the response data from Hasura.Here, we are limiting our tags to only those with a confidence greater than 50.Once we save that data, we can fetch again from Draftbit to see what we get.That\u2019s a considerable improvement! It\u2019s time to preview this app in context, and play with the different pages.Pressing the play button gives us a scannable QR code to preview the app directly on our phones, you\u2019ll need to download the Expo app, first.With just a few functions, some powerful features from both Draftbit and Hasura, we\u2019ve been able to create a native app from our API with very low coding, indeed.Publishing this app is out of scope for this tutorial, but you can follow the guides at Draftbit and use the built-in UI to deliver your app to users around the world. To interact with this demo, use this link.Build something with your API today and let us know what you made!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Setting up webhooks in Hasura using Next.js API routes Description: Learn how to configure Hasura Event Trigger webhooks that can call a Next.js API Route for performing custom logic. Author: Hasura"}, {"role": "assistant", "content": "This article was written by Chidume Nnamdi as part of the Hasura Technical Writer Program. If you would like to publish an article about Hasura or GraphQL on our blog, apply here.I have worked with Hasura for a long time and have realized that developers find it hard to understand what webhooks are in Hasura and how to set them up. In this blog post, I will be explaining how a webhook in Hasura works and how we can set it up to call an API endpoint in a Next.js app.Data Models & APILet's say we have a PostgreSQL database food with the tables:food tableidname1riceConnecting our Postgres database will expose the GraphQL API endpoints and we can now perform CRUD operations on the table via queries and mutations:Mutationsmutation {\n    delete_foods(where: {})\n    delete_foods_by_pk(id: 10)\n    insert_foods\n    insert_foods_one(object: {})\n    update_foods(where: {})\n    update_foods_by_pk(pk_columns: {id: 10})\n}Queriesquery {\n    foods\n    foods_aggregate\n    foods_by_pk(id: 10)\n}We can perform the queries and mutations on http://localhost:8080/v1/graphql locally.Webhooks in HasuraA webhook is a database event trigger that calls a web URL when an action is performed on a database table.Let's see it this way... We have a database table named food. Now, we can create an event trigger on the food table and set it up to be triggered when any of the following actions are performed:an item is inserted into the tablean item is deleted from the table.an item was edited or read from the table.The event has an event handler\u2014A web URL that is called when the database event is triggered.In the next section, we will learn how to set up the Hasura GraphQL engine in our local environment and create a webhook.Setting up HasuraFirst, create a folder where everything will be housed:mkdir hasura-prj\ncd hasura-prjNow, let's fetch the Hasura docker compose file. Run either of the below commands:wget https://raw.githubusercontent.com/hasura/graphql-engine/stable/install-manifests/docker-compose/docker-compose.yamlORcurl https://raw.githubusercontent.com/hasura/graphql-engine/stable/install-manifests/docker-compose/docker-compose.yaml -o docker-compose.ymlThis will download the docker-compose.yaml file to the hasura-prj folder.Next, run the Hasura GraphQL engine along with a Postgres database.docker-compose up -dThis command will pull the container and run it. Let's open our browser and navigate to localhost:8080/console. This will open up the Hasura admin UI. This admin UI is where we can connect our SQL database, create data in our database, test the data, and do many other things.We will connect a Postgres database to our Hasura. To do that click on the Data, and on Manage and last on Connect Database.Fill in the inputs, leave the Database Display Name as default. Type PostgreSQL in Data Source Driver. In the Database URL type in postgres://postgres:postgrespassword@postgres:5432/postgres.Finally, click on the Connect Database button on the form. This will connect Hasura to the Postgres database.Create a tableNow, let's create a table users. This table is where we will register database events to trigger our webhook URL when a record is inserted into it.Navigate to Data -> Create Table, enter the fields to look like below:After that click on the Add Table button. It will create the table and also the mutations and queries for the table.Now, we need to create an event trigger on the users table. Before that, we need to create a Next.js project and deploy it.Setting up Next.js projectLet's set up a Next.js project.npx create-next-app hasura-webhookORyarn create next-app hasura-webhookThis will create a Next.js project in hasura-webhook folder.In Nextjs, the API endpoints are housed in the pages/api folder. The files and folders in this pages/api folder map to an endpoint.For instance:pages/api/hello.js maps to http://DOMAIN/api/hello.pages/api/index.js maps to http://DOMAIN/api/.So we saw how the mapping works!Now, this API endpoint from our Next.js app is what we want to call when an event is fired from our Hasura webhook.We already have hasura-webhook/pages/api/hello.js generated by Next.js. Let's set it as the webhook URL to be called by our Hasura event.The code inside the file is:// Next.js API route support: https://nextjs.org/docs/api-routes/introduction\n\nexport default (req, res) => {\nconsole.log(\"triggered\");\nres.status(200).json({ name: \"John Doe\" });\n};Woohoo! We now have our Next.js app being served locally. Let's go ahead and deploy it to the cloud.Deploying our Next.js projectIn this tutorial, we will be using ngrok. Let's install it via Yarn in our project.Run the following command:yarn add ngrokNext, let's add an NPM script to our package.json file, as follows:\"scripts\": {\n//...\n\"ngrok\": \"ngrok http 3000\"\n},This command will start an HTTP tunnel on port 3000. Run the command below:yarn ngrokWe can this cascade on our terminal as follows:ngrok by @inconshreveable                        (Ctrl+C to quit)\n\nSession Status                online\nVersion                       2.3.40\nRegion                        United States (us)\nWeb Interface                 http://127.0.0.1:4040\nForwarding                    http://474a28b0fd1a.ngrok.io -> htt\nForwarding                    https://474a28b0fd1a.ngrok.io -> ht\n\nConnections                   ttl     opn     rt1     rt5     p50https://474a28b0fd1a.ngrok.io is the URL of our deployed Next.js project. Not that when you try this out, your URL will be different from mine. So copy it because it will be your webhook URL.Now, our api/hello API endpoint will be https://474a28b0fd1a.ngrok.io/api/hello. So this is our webhook URL.Creating an event triggerNow, let's go back to our Hasura admin UI. We will now create our first event trigger!Clicking on Events > Create will open up a form.Add the Trigger Name to be insert_users. This will be the unique name of the Event Trigger. Select the Schema to be public and the Table to be users. This table is where the event trigger will be created.On Trigger Operations check the Insert box, this will make the event be triggered on the users table when a record is inserted into it.On the Webhook URL type in http://474a28b0fd1a.ngrok.io/api/hello, this URL will be called with the event payload when a record is inserted in the users table.Test runNow, let's go to API tab and run the following mutation:mutation {\n    insert_users_one(object: { name: \"David\" }) {\n    \treturning {\n\n\t\t}\n\t}\n}The insert_users event will be called along with our API at https://474a28b0fd1a.ngrok.io/api/hello.Go to Events > Invocation Logs and you will see an entry that is made there. That is the details of the event insert_users that has been fired.ConclusionIn this blog post, we briefly learned about Hasura and how it works. Next, we learned what a webhook is and how it functions. Then, to fully demonstrate how to set up a webhook in Hasura, we created a Hasura instance and deployed a Next.js project using ngrok. We then created a webhook and pointed the event handler to our Next.js API endpoint. Finally, we performed a mutation in our table to fire the webhook! Hurray! \ud83c\udf89About the AuthorChidume Nnamdi is a software engineer with over six years of experience. He has worked with different stacks, including WAMP, MERN, and MEAN. My language of choice is JavaScript; frameworks are Angular and Node.js"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Authorization with Hasura: How to prevent mutations after a time interval Description: A guide on how to use PostgreSQL Triggers and Column Comparison operators in Hasura to prevent users from updating rows after a time interval. Author: Gavin Ray"}, {"role": "assistant", "content": "Recently, a user got in contact with us to ask about how to solve a particularly challenging issue they were facing:\n\nI have a requirement to disallow my users to create/update/delete a record in a table after a certain period of time. To be more specific, I have an application that allows my users to predict who will win a sports tournament, which needs to be made before the tournament starts. Obviously I want to prevent them from changing their prediction after the tournament starts.\n\n\nAs far as I am aware it is not possible to do this with Permissions because you can only check against \"static\" values (or hasura session variables), but I need to check if the start_date of the Tournament is <= today, where today obviously is NOT static. I guess my first question is if this assumption that this cannot be done with Permissions is correct?\n\nFortunately for this person, their assumption was NOT correct!\nIn this post I cover three approaches to solve this problem:\n\nApproach #1 (Postgres Triggers)\nApproach #2 (Hasura's \"Column Comparison\" permission operators)\nBonus Approach (Postgres \"Check Constraints\")\n\nGeneral Table Layout\nThe table structures we're assuming for the purposes of this post will be as follows:\nCREATE TABLE football_tournament (\n  id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  start_date timestamptz NOT NULL,\n  name text NOT NULL\n);\n\nCREATE TABLE football_tournament_prediction (\n    id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n    football_tournament_id int NOT NULL REFERENCES football_tournament (id),\n    prediction text NOT NULL\n);\n\nApproach #1 (Postgres Triggers)\nThis solution uses native Postgres triggers to validate row input during creation and when updating.\nDepending on your familiarity and comfort level with SQL, this approach may seem more or less appealing than the approach which uses Hasura's features.\nWhat we'll do is ask Postgres to run a function every time that a football_tournament_prediction row is created or updated.\nThis function will check that the football_tournament the prediction is attached to, has a start_date which hasn't happened yet.\nWriting that in pl/pgSQL looks something like this:\nCREATE FUNCTION ensure_predication_tournament_date_is_before_now() RETURNS trigger AS $$\n    DECLARE\n      tournament football_tournament;\n    BEGIN\n        SELECT INTO tournament * FROM football_tournament WHERE id = NEW.football_tournament_id;\n\n        IF tournament.start_date <= now() THEN\n            RAISE EXCEPTION 'Tournament is happening or has already happened';\n        END IF;\n        \n        RETURN NEW;\n    END;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER ensure_predication_tournament_date_is_before_now_trigger\n    BEFORE INSERT OR UPDATE\n    ON football_tournament_prediction\n    FOR EACH ROW\n    EXECUTE PROCEDURE ensure_predication_tournament_date_is_before_now();\n\nIf we take these statements, and run them from the \"SQL\" page (/console/data/sql):\n\nAnd then we create a football_tournament row which has a date in the past, let's say 01-01-2020:\n\nThen finally, we attempt to create a prediction for this football tournament:\n\nTa-da! \ud83c\udf89 It won't let us. Which we're happy about!\n\nNote: If you're following along and plan to try both options out, at this point you should delete the Postgres trigger so that you can configure the second option without interference:\nDROP TRIGGER ensure_predication_tournament_date_is_before_now_trigger ON football_tournament_prediction;\n\n\nApproach #2 (Hasura's \"Column Comparison\" permission operators)\nAs opposed to the DB-centric approach given above, this variant uses Hasura's permission operators to emulate a similar sort of constraint.\n\nNOTE: Off the bat, it may be useful to note that this type of constraint can ONLY verify integrity within the bounds of Hasura's permission and roles system.\nThat means that it does not prevent ill-formed or invalid records from being inserted (intentionally or accidentally) by admins or roles which don't have access restricted.\n\nWhat we will do is go to the \"Permissions\" tab of the football_tournament_predictions table, and for the role (here we use user) say that no records may be inserted/updated if the result of the SQL now() function is less than the football_tournament.start_date.\nTo do this, configure the permission as below:\n\nNow, if we attempt to insert a new football_tournament_prediction as role user, we can see it has failed the constraint check:\n\nBonus Approach (Postgres \"Check Constraints\")\nIf the value we were checking was a column on the same table, it would be possible also to use a third (and simpler) option. Below we'll show an example of how this could be used, just so that you know how to use this feature in Hasura.\n\nNOTE: For this example, we will use a somewhat nonsensical scenario\nThat we don't want to allow football_tournments to be created if they already happened (IE, no historical records). While this MAY be something you might model in real situations, it's probably less realistic than the other two.\n\nThis solution will be using the Check Constraints feature of Postgres to validate changes to rows at a database level.\nIf you are't familiar with Check Constraints don't worry. We'll assume zero-knowledge and we'll walk through it step-by-step.\n\nNote: If you'd like to read more about Check Constraints in Postgres, you can do so at\n\nhttps://www.postgresql.org/docs/current/ddl-constraints.html#DDL-CONSTRAINTS-CHECK-CONSTRAINTSs\n\n\nWhat we will be doing is asking Postgres to ensure that the value of this row is never allowed to be anything but BEFORE the start_date of the sports game.\nTo accomplish this, the steps we would take are:\n\nAdding \"created_at\" and \"updated_at\" from the \"Frequently Used Column\" button, from the Table -> Modify page:\n\nThis will also create Postgres triggers to set updated_at to \"now()\" when the row changes, and created_at = now() by default\n\n\n\nUnderneath the table columns, there's a section called \"Check Constraints\". Here you can visually add SQL constraints that have to hold true for any row insert/updates.\n\nIn \"Check Constraints\" we can add a new constraint, \"created_at_before_tournament_start\" with the following condition:\n\n\n\nNow, if we attempt to insert a new record into \"football_tournament\" which is being created NOT before the start_date of the tournament, we get:\n\n\n\n\n\nIf you'd like to learn more about Hasura's authorization system, we have a tutorial here.I hope you found these solutions helpful! If you have any further questions or interesting use-cases that you'd like to share with us, hit us up on Discord / GitHub Discussions."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Observability with Hasura Description: When you're trying to diagnose why a particular operation is taking a long time\nto complete, or just want to better understand what's going on\n\"behind-the-scenes\" in your queries, the ability to debug and analyze metrics\nfrom your Hasura instance is a critical factor.\n\nIn this post we'll take a look at how to do this by:\n\n * Walking through the tools we have available\n * Showcasing how to use these tools, with follow-along images and guided\n   descriptions\n * Discussing \"gotchas\" and useful tips Author: Gavin Ray"}, {"role": "assistant", "content": "When you're trying to diagnose why a particular operation is taking a long time to complete, or just want to better understand what's going on \"behind-the-scenes\" in your queries, the ability to debug and analyze metrics from your Hasura instance is a critical factor.\nIn this post we'll take a look at how to do this by:\n\nWalking through the tools we have available\nShowcasing how to use these tools, with follow-along images and guided descriptions\nDiscussing \"gotchas\" and useful tips as we go, so that you're better equipped to avoid pitfalls and common issues\n\nOutline\nThere are two distinct areas to keep in mind when approaching performance issues.\nThe line that separates these areas is the network boundaries:\n\nThis diagram highlights the boundaries we'll be covering -- the ones separating the network from the application.\nNetwork Boundary\nNetwork timing information between a Client and Hasura is easily profiled and gathered.\nInvestigating this phase can be useful to rule out a problem with the actual network between the client and server.\nOne straightforward way of doing this is with the inbuilt browser devtools.\nTo understand what the request timeline looks like, you can emulate a query made from your client to Hasura by using fetch() or GraphiQL.\nUsing the Network pane, we can look at the timing information for each segment of the request.\nWhat happens inside (or behind) Hasura is not visible to us here, but one fairly important piece of info is -- the time that the request isn't actively being processed:\n\n\n\n\nFrom the above images (timing info for a query, taken once in Edge and once in Firefox) we can see that the time spent not connected and actively waiting for a response is relatively brief in comparison to the time spent waiting for first-byte.\nUsing this info, we can likely conclude that there aren't any network issues happening during these requests. If there were, the timing information would be skewed, with significant portions of it spent in the phases outside of waiting (the connections and handshake phases).\nApplication Boundary (Business Logic)\nThough coverage of debugging and profiling techniques for what occurs in application business logic is somewhat outside of the scope this article, there is one critical piece of information that can be of use here.\nWithin the Hasura Cloud docs, there is a section relevant to Hasura's implementation of OpenTracing and how to configure this end-to-end for your application.\n\nhttps://hasura.io/docs/latest/graphql/cloud/tracing.html#\n\nUnder the Trace Propagation subsection, there is some info outlining how we utilize tracing information to display timings in metrics.\nAs noted, likely the easiest way to get this most out of this feature if you aren't trying to implement custom/advanced tracing functionality is simply to forward the request headers from Hasura:\n\nIf you are unsure how to implement B3 propagation in your own web service, the simplest thing to do is to read these headers and pass them along to any HTTP services you call which also support B3 propagation, including Hasura itself.\nIn particular, if an event trigger webhook or action handler propagates these B3 headers back to Hasura, we will be able to trace the entire interaction.\n\nApplication Boundary (Database)\nWhen it comes to gaining insight about what's happening in the database layer, we have a few tools available to us.\n\nHasura's ANALYZE feature to display the compiled SQL query for an operation\n\nRunning EXPLAIN ANALYZE on the resulting query to get a Postgres plan for it\nUsing visualizers like https://explain.dalibo.com to help graphically examine the plan\n\n\nThe Cloud \"Metrics\" dashboard's Operation modal, which provides timing spans for PG operations as part of it's information\nUsing the new Query Tags feature along with a tool like https://pganalyze.com to deeply investigate operations or overall database health/activity\n\nLet's look at how we can use each of these tools in order to better help us understand how our application is performing.\nDatabase Walkthrough, Pt. 1 (\"ANALYZE\" feature)\n\nUsing Hasura's ANALYZE feature to display the compiled SQL query for an operation\n\nOne useful feature Hasura has is the ability to provide you with the compiled SQL query and a non-executed (theoretical/hypothesized) query plan for a GraphQL operation.\nThere are multiple ways to get this information. The compiled query is available on the Metrics dashboard, as part of operation info on several of the sub-pages, but another way we can get it is by simply pressing the ANALYZE button on the GraphiQL dashboard:\n\nThis will open a modal that contains:\n\nThe SQL Query itself\nAn execution plan (the query is not actually run to produce this)\n\nIf we copy the compiled query, we can use this to produce a report via EXPLAIN ANALYZE that can be very helpful when searching for performance-related issues or checking for proper use of indices, etc.\nYou can copy the query by pressing the COPY icon in the bottom right of the execution plan -- marked here by the red area circled in the image below:\n\nIf we paste the output of the compiled query into the query-editor on the DATA page, prepended with:\nEXPLAIN (ANALYZE, COSTS, VERBOSE, BUFFERS, FORMAT JSON)\n\nAnd run it, a report will be produced:\n\n\nWe can then copy that plan, along with the original generated SQL query, and input it at https://explain.dalibo.com to get an incredibly helpful visualization of it:\n\n\nFrom the results, one thing we notice immediately is that the grand majority of the operations processing time appears to be spent on a Seq Scan for allowed books.\nThe reason behind this (in this demonstration/example) is almost certainly due to the row count being too low to utilize any indices (unless we run set enable_seqscan=false on our DB to force this). It is easy to see how this kind of visualization can be tremendously helpful in real-world situations.\nDatabase Walkthrough, Pt. 2 (Cloud Metrics dashboard)\nAnother source of information we can use is the Metrics dashboard. There is more than one place that contains information we may be interested in, but the most directly related to the database is the spans/timing information.\nThat information can be found by navigating to an individual operation's details modal.\nTo get there, first decide on an operation you want to examine. Let's say it's something like this:\nquery QueryForExplaining {\n  ...\n}\n\nAssuming this query has been run sometime relatively recently (if it hasn't you can run it from the GraphiQL tab), navigate to the \"Metrics\" tab, and then select the \"Operations\" panel from the left-hand menu:\n\nThen, find the operation you're looking to inspect, and click on the \"Magnifying Lens\" icon under the Actions column. This icon is circled in red in the image above.\nThis will open a modal containing detailed information about that particular operation.\nOne of the pieces of info available to us here is the waterfall-style request timeline. Using this, we can see where the operation spent each portion of it's time, relative to the total processing time:\n\nIn the image above, circled in red is the TIMING portion of the modal. Here the breakdown of request timing is available.\nYou can click on segments of the span to zoom in or out, or hover your mouse over them to view a popup containing the precise millisecond timing and relative percentage of the total that span counted for (pictured).\nThis plot shows its usefulness particularly when:\n\nA) Trying to get a brief overview of where an operation is spending it's time, so you can decide which part ought to be investigated\nB) Visualizing the execution of queries that contain one or more calls to Actions/Remote Schemas/Remote Joins\n\nIn the case of B, the reasoning for that may be more apparent if visualized.\nHere's what the trace looks like for an operation that calls out to a Remote Schema and joins on the return value:\nmetrics-dashboard-3\nDatabase Walkthrough Pt. 3! (Query Tags)\nThe most recent feature added to the Hasura observability/debugging arsenal is a feature called \"Query Tags\".\nWhat Are Query Tags?\nQuery Tags are SQL comments which are made up of key=value pairs that are appended to generated SQL statements.\nWhy do we need Query Tags?\nWhen the following query is sent to Hasura\nquery GetChild {\n  child {\n     name\n  }\n}\n\nThe SQL generated is something like:\nSELECT * FROM child;\n\nBut when we are looking at Postgres logs and trying to correlate the generated SQL statement with the actual GraphQL request, it quickly becomes difficult.\nHad we had more information such as operation_id/tracing info etc, along with the SQL statement, it would make the mapping easier.\nThe way we can provide such information alongside the generated SQL statement is by appending a comment. And this is what the Query Tags are.\nWith Query Tags, whenever we issue a Query or Mutation - The generated SQL has some extra information.\nIn the case of above example, with the query tags the generated SQL would look like:\nSELECT * FROM child /* request_id=487c2ed5-08a4-429a-b0e0-4666a82e3cc6, field_name=child, operation_name=GetChild */\n\nThe metadata present in these comments can be used by some tools to analyze DB load and track/monitor performance.\nOne such tool is https://pganalyze.com\nIn the guide below, we'll walk through how to integrate pganalyze with Hasura using the new Query Tags feature and show the process of tracking an actual query operation.\nConsuming Query Tags with pganalyze\n\nNote: The below process assumes you are using a Hasura Cloud app connected to a Heroku database. If this isn't the case, you will need to select one of the other provider instructions from the pganalyze dashboard to suit your scenario.\n\n\n\nCreate an account at pganalyze and log in to the dashboard (the 14 day free trial is sufficient for this purpose).\n\n\nFrom the \"Getting Started\" page, select the Heroku Postgres environment:\n\n\n\n\nOn the following page, click the Deploy to Heroku button to deploy the required collector app on your Heroku account. The notes say that the $7 hobby tier are required (due to collector shutting down if the dyno sleeps), and while this is true you may also use the free tier if you're interested in experimenting just to see how it works.\n\n\n\nThe installation guide contains the Heroku CLI commands that you can use to set up the environment programmatically. In the interest of offering an alternative, configuration using the Heroku web dashboard will be shown below.\n\n\n\nOpen your Heroku Dashboard, and navigate to the Resources page of the app that contains your Hasura Cloud Postgres. We need to \"attach\" our Heroku Postgres database (that our Cloud app is using) to the collector app. To do this, start by clicking X Attachments on the Heroku Postgres add-on, and then from the flyout menu, click Manage Attachments:\n\n\n\nThis will open a slide-out form on the righthand side of the screen. In the form, search for the name of your collector app, and then click to add it (in the image, it's already been added):\n\n\n\nNow that the database has been attached, we can continue with the installation process. The next step is to verify that the installation was successful. You can use the Heroku CLI command pictured, or you can follow along using the images below and the web dashboard:\n\n\n\nFrom the pganalyze collector app on Heroku, click the button that says More on the righthand side, and then select Run console from the dropdown:\n\n\n\nFor the console command, enter collector --test:\n\n\n\nThis should run successfully, and you should see command output with Test successful at the bottom:\n\n\nCongratulations! You've now integrated Hasura with pganalyze.\nAs a concluding task, let's make a query from our Cloud app, and then look it up in pganalyze to view more information about it.\nHere's a nonsensically named query for a sample table that I've created in my Cloud app:\nquery asdadf {\n  sample_table(where: {\n    id: { _eq: 1} \n  }) {\n    id\n    text\n    created_at\n    updated_at\n  }\n}\n\nIf we run this query from GraphiQL on our Cloud app console, it will eventually become viewable in the pganalyze dashboard.\n\nNote: This process can take a significant amount of time. IE, 10-20 minutes. Don't be alamred if nothing shows immediately.\n\nHere on the dashboard, all of the operations are visible:\n\nSelecting that operation, we can see the details of the query, as well as a lot of other information:\n\nTo quickly locate operations, you can use the Request ID that can be found from the Metrics dashboard on Cloud.\nHere is an example workflow of back-tracking from a known operation in the Metrics dashboard, to it's analytics inside of pganalyze:\n\n\nGrab the Request ID from Operations tab on Metrics dashboard:\n\n\n\n\n\nSearch pganalyze query log by the Request ID\n\n\n\n\n\nOpen the matching query to view it's analytics and other relevant data\n\n\n\n\n\nLet us know if have any questions while trying these out! You can hit us up on github discussions / discord if you need any help!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: July Roundup of Hasura\u2019s Product Launches: BigQuery, Citus Support, Template Gallery & Database monitoring Description: Hasura releases 8 new product updates at the monthly community call including support for new databases, GraphQL security & observability features, git-push to deploy, CLI support for ARM and enhancements to computed fields. Author: Rajoshi"}, {"role": "assistant", "content": "At Hasura, we have held monthly community calls since early 2019 to share what we\u2019re working on with the community. These start off with Tanmai (CEO, Hasura) giving us an overview of our product roadmap, followed by folks from our product and engineering teams showcasing demos of features we\u2019re working on, sharing RFCs and answering your questions.We have many of our community regulars as well as new members of the Hasura community join us, as well as a customary demo by a member of our community.Hasura is releasing 7 new features at July's community callStarting July, we are changing the format just a little bit to make it a little more exciting by adding product launches into the mix! We ship updates very frequently, and have decided as a team to make the community call the one place for you to join us monthly to learn about everything new that\u2019s been shipped! We will be announcing new product features that you can try out immediately after the call, and of course get any questions about these answered during the call from the folks who have been working on it directly! We will continue to showcase upcoming work, RFCs and have our community demos :) \u00a0So expect a power packed 60 min call for all things Hasura and our lovely community! The community call happens on the last Thursday of every month at 9am PST on 29th, Thursday. If you're reading this before 9am PST, you can still join us for the call here. Here are all the announcements for July: Support for BigQuery on Hasura: In addition to Postgres and SQL Server, we now have support for BigQuery, as our first OLAP database. With Hasura\u2019s support for BigQuery, you can get instant GraphQL & REST APIs on new or existing BigQuery workloads to build online applications. The engineering team has done some fabulous work on the implementation for our first OLAP database to ensure that the performance that you get with Hasura is unparalleled. Read the docs on how to get started. Support for Citus/Hyperscale on Hasura: Deepening our support for the Postgres family of databases, we now also have out-of-the-box support for Citus / Hyperscale (distributed Postgres), in addition to Yugabyte, Timescale and Aurora, and of course all the managed vanilla Postgres offerings on all the cloud providers. Citus support is also available on our OSS version as well as on Hasura Cloud and Hasura EE and could be a great fit for you if you have data models that need the kind of scale-out and sharding that Citus supports. With Hasura 2.0 you can easily use vanilla Postgres and Citus simultaneously making it easy for you to move different types of data workloads to the best place it can be managed! Head to the docs to get started. Template Gallery: We first previewed the template gallery at HasuraCon (introduced as schema sharing) and the reactions from the community was very encouraging. Template Gallery will have installable samples of permissions, relationships, and advanced Hasura features; as well as data models, logical implementations, and extension example, so that you and your team will be able to not only get up to speed with Hasura, but also be able to embrace the power of the database that runs through your API. We are extremely excited at the potential of the Template Gallery to ease onboarding for new Hasura users! In the next iteration, we will make it easy for the community to share their templates with each other. We want to get to a world where all questions about Hasura can be answered with a template :) Check out the templates to get started with on this repo.Computed fields enhancements: As one of the most upvoted issues on the Hasura Github, with this release we land some much awaited enhancements to computed fields! Computed fields are virtual values or objects that are dynamically computed and can be queried along with a table/view\u2019s columns. You can now use computed fields in where and order clauses in the GraphQL query and in permissions, and also in join conditions for remote schema relationships. Read the docs to try it out.New GraphQL Security features on Hasura Cloud: In addition to support for depth limits, rate-limiting and allow-lists, we now have 2 new security features on Hasura Cloud and Hasura EE, along with an easily discoverable Security tab: Node Limit, and the ability to disable GraphQL Introspection per role. You can read about all of the security features in this blogpost. Database Monitoring with Query Tags for debugging performance issues with your application: With the new Query tags feature on Hasura Cloud you can trace GraphQL operations to their SQL queries easily as they\u2019re running in the database! Query tags are applied to SQL statements which get logged in the database. You can then use this to analyze performance directly in the database, and then trace it back to the corresponding GraphQL operation to monitor what operations are responsible for the most amount of load on your database. Read our guide on GraphQL observability to help you debug any performance issues you have with your application. Github integration on Hasura Cloud: You can now deploy your local migrations to staging and production with a simple git push. Give this a try on your Hasura projects with this starter kit. With the release today, we\u2019ve enabled Github integration for all those of you who requested early access! Here is a starter kit , and here are the docs to get you started.Hasura CLI ARM Release: The Hasura CLI now has support for ARM64 architecture that we recently released for Linux and M1 macOS. You can follow the instructions on the CLI installation docs on your arm machine.All these are live and ready for you to try out! Let us know if you have any questions on Github Discussions or on Discord. Take these new features for a spin on your Hasura projects. If you\u2019re new to Hasura, it takes 30 seconds to spin up a project on Hasura Cloud. Get started here."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL API Security with Hasura Cloud Description: Hasura Cloud is a fully managed, globally available, auto scaling version of\nHasura that also includes features specifically designed to make it easier for\nyou to run Hasura in production. These features range from the addition of \nmonitoring [https://hasura.io/graphql/monitoring/]/analytics features for\nincreased observability to built-in caching [https://hasura.io/graphql/caching/] \nfor improved performance, to a suite of security features that help you\nsafeguard your production Hasura applica Author: Jesse Martin"}, {"role": "assistant", "content": "Hasura Cloud is a fully managed, globally available, auto scaling version of Hasura that also includes features specifically designed to make it easier for you to run Hasura in production. These features range from the addition of monitoring/analytics features for increased observability to built-in caching for improved performance, to a suite of security features that help you safeguard your production Hasura applications. All of these are hard problems that all GraphQL Servers run into in production - with Hasura Cloud, these features are available out of the box with simple and intuitive configurations!In this post, I will walk you through the security features available on Hasura Cloud, including a set of new enhancements launched in the latest Cloud release v 2.0.3. These features help you secure your production application, and safeguard your app against injection attacks, DDoS attacks, excessive data exposure, and more, whether malicious or accidental.Global and Role-based API controls under the Security tab [new]Hasura Cloud has a new Security tab under the API section of the console to make it easy to access API security settings.We\u2019ll focus our attention on five key security features that Hasura Cloud adds to the production checklist, and describe the use-cases those features are solving for.Depth limitGraph-like querying structures are susceptible to recursive querying patterns, both accidental and malicious. Without checking the level of nesting, it\u2019s possible to generate large and expensive queries. While it is possible to create proxy services to a self-hosted GraphQL server to handle this problem, Hasura Cloud makes this a click-and-set configuration. Using our shapes and sets analogy, we might end up with the following example:{\n  shapes {\n    set {\n      name\n      shapes { # duplicate reference!\n        name\n      }\n    }\n  }\n}\n\nSetting a depth limit will surface this problem for us.Node limit [new]In addition to depth limits, node limits are another \u201cgotcha\u201d to be aware of. Every field in GraphQL is a node, regardless if that\u2019s nested five levels deep in a query structure or at the root level. GraphQL in particular has additional mechanisms that can quickly lead to excessive node usage if left unchecked. For example, creating reusable fragments and aliases is great for code cleanliness, but, when used incorrectly, you could end up with excessive egress from your database.{\n  set1: shapes {\n    name\n    set {\n      name\n    }\n  }\n  set2: shapes {\n    name\n    set {\n      name\n    }\n  }\n  set3: shapes {\n    name\n    set {\n      name\n    }\n  }\n}\n\nSetting a limit on the total number of nodes helps ensure terse requests that don\u2019t unintentionally inflate your database bill.Rate limitingHasura supports a wide variety of workload types, from heavily cached responses to real-time feeds. The majority of use cases will often end up somewhere in-between and having the ability to rate-limit requests is a great way to balance spiking user load. It\u2019s also a vital and effective piece in the mitigation strategy against DDoS attacks or when a single API consumer is flooding your service with unoptimized requests. Rate limiting can also be applied conditionally to matching IPs or request parameters. This allows for throttling suspicious activity without restricting organically spiking usage.Disabling Introspection [new]The GraphQL specification states that all GraphQL endpoints must be introspectable, which has become the bedrock of much of the powerful tooling in the GraphQL ecosystem. However, persistent and immediately introspectable APIs have been the topic that security experts have included in nearly every formal review of a GraphQL endpoint. The reason behind this critique of an introspectable API is that it falls foul of the OWASP guidance to avoid excessive data exposure/disclosure. Imagine if an introspectable API from a public company now had a root level model for a vertical they\u2019d previously not been associated with, this would lead to significant repercussions in the markets. Fortunately, Hasura Cloud makes disabling introspectable schemas as simple as a click.Allow ListsOne method for ensuring only approved content leaves your database is to require approval for the operations that can be used in production. This list of approved methods is then aptly named, the \u201callow list.\u201d Hasura Cloud makes this a simple process of viewing the previously requested operations, selecting the ones that should be allowed in production, then adding them to the list. This becomes an effective \u201cfinal-form\u201d of the production application, where you can do a final review of all the operations making requests against the API, then add them to the allow list, thus blocking all non-sanctioned requests to your database.Support for SSL certificates on the clientMany of the leading cloud providers offer SSL-based DB connections for the highest security compliance. While not alone in the requirement, Google Cloud Platform (GCP has long required SSL based database authentication \u2013 and Hasura Cloud now makes it simple to use Google\u2019s robust database offerings.Hasura Cloud is able to offer a lot of this functionality as it tightly integrates with the route handling and logging functionality at the server level. Try these out on Hasura Cloud and let us know if you have any questions.Useful reads:Migrating from Hasura CE to Hasura CloudProduction Checking for Hasura CESecurity and Compliance on Hasura Cloud"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Comparison of Managed PostgreSQL Providers: AWS RDS vs Google Cloud SQL vs Azure PostgreSQL Description: We\u2019ll be talking about managed Postgres cloud providers and measure them on various parameters like High Availability, Auto Scaling, Versions and Extensions support, Automated Backups, Monitoring and Metrics, Security and finally the costs. Author: Hasura"}, {"role": "assistant", "content": "Comparison of Managed PostgreSQL ProvidersHey there! My name is Gunar. I\u2019m a Software Development consultant from London. I\u2019ve been a happy user of Hasura for many years now and I was happy to collaborate with Hasura for this blog post. We\u2019ll be talking about managed Postgres cloud providers.Hasura gives us instant realtime GraphQL & REST APIs on new and existing Postgres databases. While building a new application with Hasura, I have often had to go through the choices of which managed cloud provider to pick. This is a post that is meant to ease that decision making process where I compare some of the popular providers based on the vectors I have used for my clients.That is, service providers that perform all maintenance tasks for us. These services allow us to have a reliable database on the cloud, without having to worry about applying updates, managing servers, and keeping a big infra team just to maintain our database layer.We\u2019re going to be looking at six cloud providers:AWSGCloudAzureDigitalOceanHerokuElephantSQLTable of Contents\nOverall Comparison\nHigh Availability\nAuto-scaling\nPostgres Versions\nPostgres Extensions\nAutomated Backups\nMonitoring and Metrics\nSecurity\nCosts\nConclusion: Which provider should I pick?\n\nOverall ComparisonThought I\u2019d start off with the comparison table right off the bat. And let you get a general overview before diving into the deets. As one might\u2019ve expected, AWS is setting the bar, with GCloud and Azure tied for a close second. Then we have slightly simpler solutions like DigitalOcean, Heroku, and ElephantSQL\u2014not too far behind either!Databases are complex, multi-faceted creatures so we'll be looking at multiple characteristics. Which are auto-scaling, monitoring and metrics, costs, versions, extensions, backups, high availability, and security. If you really want to get a good picture of these services, I\u2019d recommend diving in and reading each section.I couldn't comment on the price as different providers offer different benefits. However, the costs section will give us an idea of how much a single server database might set us back with each of these providers.Generally speaking:If you\u2019re an indie hacker or small startup, a simpler solution like DigitalOcean, Heroku, or ElephantSQL might be a good fit. These providers have simpler cost structures and their offerings come in pre-defined packages.If you\u2019re an enterprise company, you\u2019ll want the stability and guarantees of big players such as AWS, GCloud, and Azure.Might be good to remind you that Hasura and Hasura Cloud work with all providers.AWSGCloudAzureDigitalOceanHerokuElephantSQLAuto-scaling\u2705\u23fa\ufe0f\u2705\u274c\u274c\u274cMonitoring and Metrics\u2705\u2705\u2705\u2705\u23fa\ufe0f\u2705Costs\ud83e\udd37\ud83e\udd37\ud83e\udd37\ud83e\udd37\ud83e\udd37\ud83e\udd37Versions\u2705\u2705\u23fa\ufe0f\u23fa\ufe0f\u2705\u2705Extensions\u2705\u2705\u2705\u2705\u2705\u2705Backups\u2705\u2705\u2705\u2705\u2705\u2705High Availability\u2705\u2705\u2705\u2705\u23fa\ufe0f\u2705Security\u2705\u2705\u2705\u2705\u2705\u23fa\ufe0fHigh AvailabilityHigh Availability speaks to data redundancy, availability, and reliability. All providers offer this capability, through read-only followers and automatic failover.As you can see below, most providers make the transition in the background so that you don\u2019t have to tell the application layer to point to a new Postgres URL. All but Heroku. Heroku will update the environment variable DATABASE_URL for your Dynos automatically (which triggers a restart) but this obviously won\u2019t work if you have services hosted on non-Heroku servers.Please note that Google Cloud SQL does NOT trigger a failover event during maintenance. Both the primary and failover instances go down for maintenance.Also, quick shout out to the Azure team. They\u2019ve taken High Availability one step further in simplifying and hiding the complexity away. For example, even the Single Server plan, in fact, stores \u201cthree copies of data within a region to ensure data redundancy, availability, and reliability.\u201d So you get High Availability for the price of a single server and that\u2019s pretty cool.Static IP/endpointAutomatic failoverAWS\u2705\u2705GCloud\u2705\u2705Azure\u2705\u2705DigitalOcean\u2705\u2705Heroku\u274c\u2705ElephantSQL\u2705\u2705Auto ScalingAuto-scaling is a relatively new capability of managed databases. Currently AWS and Azure support auto-scaling of both the storage and server, and Google Cloud supports auto-scaling storage only.Auto-scaling storage means the provider will automatically provision more disk space as required by the database.Auto-scaling servers means the provider will automatically scale up the database, either vertically or horizontally. The most common operation is spinning up more read-only replicas, but beefing up the primary instance is possible too.Auto-scaling is opt-in but make sure it's properly configured. Automatically-scaling capacity also automatically scales costs. Also, it's usually not possible to scale storage down afterwards to match your actual usage and needs.Auto-scaling storageAuto-scaling serversAWS\u2705\u2705GCloud\u2705\u274cAzure\u2705\u2705DigitalOcean\u274c\u274cHeroku\u274c\u274cElephantSQL\u274c\u274cPostgres VersionsPostgres is an actively maintained open-source project. As such, there are multiple major versions that are stable. For simple applications versions 10 or 11 might be enough, however you might have a legacy system which depends on 9.6, or you might need the newest features included in version 12 and 13 (e.g. incremental sorting).On this aspect, I was a bit disappointed by Azure. It\u2019s the year 2021 and Azure still doesn\u2019t support versions 12 and 13. This not only prevents us from using those sleek, cost-saving new features but also tells us that they\u2019ll be slow to implement further new major versions as they\u2019re released.Hasura GraphQL Engine supports Postgres versions 9.5 and above, and Hasura Actions are supported from Postgres 10 and above.9.610111213AWS\u2705\u2705\u2705\u2705\u2705GCloud\u2705\u2705\u2705\u2705\u2705Azure\u2705\u2705\u2705\u274c\u274cDigitalOcean\u274c\u2705\u2705\u2705\u274cHeroku\u274c\u2705\u2705\u2705\u2705ElephantSQL\u2705\u2705\u2705\u2705\u2705Postgres ExtensionsPostgres extensions allow your database to do more. Examples of popular extensions are pg_crypto (for crypto functions like hashing) and PostGIS (for spherically-aware geographical calculations).When using a managed Postgres provider, you mostly don\u2019t have to worry about the availability of extensions. Most providers offer a wide range of extensions which most likely include the ones you\u2019ll be needing. That being said, if you still want to check for specific extensions please follow the links below.ExtensionsAWS\u2705GCloud\u2705Azure\u2705DigitalOcean\u2705Heroku\u2705ElephantSQL\u2705Automated BackupsManaging your own backups can be a stressful and thankless job. It\u2019s easy to make mistakes, forget to check your backups, or perhaps even overwrite them. Also, you have to make sure the backup server doesn\u2019t run out of disk space.This is where cloud providers shine. You can rely on the big cloud providers\u2019 infrastructure to keep database backups for you. You\u2019ll be able to sleep at night knowing that your data is secured with the same infrastructure as NASA\u2019s.All providers offer automated backup solutions, including point-in-time-restore. By using a combination of routine snapshots and incremental logs providers are able to recover data as it was on a specific date and time. Also, I\u2019ve found Google\u2019s approach to be quite interesting and thought worth sharing: \u201cGoogle Cloud SQL backups are incremental. When the oldest backup is deleted, the size of the next oldest backup increases so that a full backup still exists.\u201dAutomated BackupPoint-in-time restoreAWS\u2705\u2705\u00a0GCloud\u2705\u2705Azure\u2705\u2705DigitalOcean\u2705\u2705Heroku\u2705\u2705ElephantSQL\u2705\u2705Monitoring and MetricsObservability is crucial. Would you be able to drive a car with blindfolds on? All providers analyzed here offer similar monitoring capabilities. Common metrics across these providers are network throughput, number of connections, I/O read/write, disk space, CPU, and memory. The only outlier was Heroku which requires you to choose and install an addon.MonitoringAWS\u2705GCloud\u2705Azure\u2705DigitalOcean\u2705Heroku\u23fa\ufe0f*ElephantSQL\u2705* Requires you to install an addon.SecuritySecurity is multi-faceted. You can\u2019t just generically say \u201cprovider X is secure.\u201d Creating a secure infrastructure is a mix of technical and human systems that must work harmoniously.So I\u2019ve looked at some specific, objective metrics:Encryption in transit is expected of course, it\u2019s the year 2021 and HTTPS/SSL is ubiquitous.Encryption at rest means the data is never stored as cleartext (i.e. unencrypted) in any servers ever. This has the obvious benefit that if someone were to physically enter the datacenter (or perhaps an ill-intentioned employee) and steal the HDDs, they wouldn\u2019t be able to access your data. However, that\u2019s very unlikely and really only a box to check in order to be compliant with statutes like SOC2.SOC 2 \u201cis an auditing procedure that ensures your service providers securely manage your data to protect the interests of your organization and the privacy of its clients.\u201d It\u2019s a minimum requirement for pretty much any kind of enterprise operations.Encryption in transitEncryption at restSOC2AWS\u2705\u2705\u2705GCloud\u2705\u2705\u2705Azure\u2705\u2705\u2705DigitalOcean\u2705\u2705\u2705Heroku\u2705\u2705\u2705ElephantSQL\u2705\u23fa\ufe0f*\u2705* ElephantSQL says data \u201ccan be encrypted [\u2026] at rest.\u201d (emphasis mine) so it seems that it\u2019s not the default option and that you have to explicitly request it.CostsAs mentioned in the intro, it\u2019s tough to compare costs as you\u2019re often comparing apples to oranges. Different providers offer different benefits. Some providers allow you to be very precise in the specs you want to hire\u2014like allowing you to pick exactly how many vCPUs you want\u2014while others sell packages which won\u2019t allow you to even choose disk size.My suggestion is, use the other parameters of this research to narrow down your choices, then calculate and estimate costs for each provider for your use case.\tWith this caveat, I\u2019ve picked a fixed spec for comparison across providers and did my best effort to try and match this specific spec:8GB RAMSSD storage2 vCPU (dedicated)Servers in East USSingle availability zone (no follower)Using a managed cloud database, there are multiple items you\u2019ll be paying for:Compute timeNetwork data egressData storage spaceBackup storage spaceThere are basically two groups. The \u201cbig three\u201d AWS, GCloud, and Azure charge you per unit consumed, whereas DigitalOcean, Heroku, and ElephantSQL offer packages with limited customization.Compute timeNetwork data egress\u2021Data storageBackup$/hour$/mo$/GB-mo$/GB-mo$/GB-moAWS$0.245 *$178.85 *$0.09$0.115$0.095GCloud$0.1386$101.18$0.19$0.170$0.080Azure$0.176 \u2020$128.48 \u2020$0.0875$0.115$0.10DigitalOcean$0.17113$124.92$0.0125 GB \u00a7$0.02Heroku$0.2740$200.002TB limit \u00a7256 GB \u00a725 backups \u00a7ElephantSQL$0.2726$199.00N/A250 GB \u00a7Daily \u00a7* 7.5GB instead of 8GB RAM (db.m1.large)\u2020 10GB instead of 8GB RAM\u2021 Generic Internet egress (i.e. any region)\u00a7 Included (it\u2019s a package)Conclusion: Which provider should I pick?We\u2019ve analyzed six providers from multiple angles. They are all pretty competitive and overall exhibit the same features, however sometimes with different names and conceptual models.All things equal, there are only three major differences that I\u2019ve noticed:You might need a postgres version that a given provider does not offerYou might expect your database load to vary greatly and a given provider might not offer auto-scalingYou might need high availability and hence failovers need to keep a static IP so that applications can automatically reconnectOf course, at the end of the day choosing a cloud provider is also an economic question. How do you get the most bang for your buck? As you\u2019ve seen in the \u201ccosts\u201d section above, estimating costs generically is imprecise as sometimes you\u2019d be comparing apples to oranges. So I suggest using the three bullet-points above to narrow down your choices, then running cost estimates for your specific application with each of the providers.That\u2019s all for today. Let me know on twitter or in the comments below if you have any questions, suggestions, or feedback! Looking forward to hearing from you."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: Industrial IoT Platform Machine Metrics Increased Developer Velocity by 10x Using Hasura Description: Machine Metrics started exploring Hasura to build their GraphQL powered data access layer. \n\nThe result? Fast, reliable, secure data APIs which also enabled their teams to increase developer productivity by almost 10x! Author: Tyler Hannan"}, {"role": "assistant", "content": "The team at MachineMetrics has shared deeper information about their offering, their usage of Hasura, and the benefits they enjoyed in a recent recording !MachineMetrics Industrial IoT Platform who just announced their $20M Series B funding round, is a company that transforms manufacturing shop floor data from insights into actions with powerful apps that reduce machine downtime, increase throughput, and maximize profitability driven by their customers\u2019 manufacturing equipment. Their platform automates machine data collection and production analytics to deliver data-driven insights in a matter of minutes. \u00a0Collecting metrics on each machine such as load, temperature, speed, and state, this data is uploaded to their cloud service for analysis, presenting dashboards used on the production floor, and even predictive maintenance to reduce machine failures and downtime.\n\u201cHasura has increased our development velocity by 10X.\u201d\n\nMachine Metrics chose TimescaleDB to handle their time series workload. Hasura\u2019s seamless integration with TimescaleDB was a big part of why Machine Metrics started exploring Hasura to build their GraphQL powered data access layer. The result? Fast, reliable, secure data APIs which also enabled their teams to increase developer productivity by almost 10x, faster feature addition enabled by rapid prototyping of new features, and better customer retention!Other advantages include:Increased developer productivity and far less middle-tier codeIncremental adoption of GraphQL while migrating away from their legacy APIs and REST endpointsProviding a secure data API for their end customersData integration with their existing systemsMachineMetrics\u2019 ChallengeMachineMetrics found themselves needing to modernize their architecture to meet their customers\u2019 needs to directly access data, as well as to create new innovative features and reduce time to market for their new offerings. \u00a0Machine Metrics was already using TimescaleDB to manage their time series data and needed a product that was compatible with it.Incremental AdoptionHasura enabled MachineMetrics to make the transition to a GraphQL-based architecture without requiring an disruptive, all-or-nothing switch. They were making a transition from REST to GraphQL, and from a variety of NoSQL data stores to Postgres, and needed to do this without disrupting operations, and while continuing to provide a consistent user experience on their frontend Machine Metrics gradually added Hasura into their systems while continuing to use their legacy APIs and REST endpoints. Over time they plan to refactor their existing REST services to be served by Hasura as well.Auto-Generated GraphQL Data AccessMachineMetrics consolidated their data to TimescaleDB, which provides a time-series SQL database. \u00a0Hasura enabled them to instantly auto-generate all the GraphQL schemas, resolvers, and API endpoints to query the database - hyper accelerating their development and eliminating tedious ongoing maintenance.Integrating Existing APIs With Remote Schemas and Remote Joins\n\u201cHasura\u2019s remote schemas and joins saves us hours of work to get to that data faster without having to go to different places\u201d\n\nUtilizing Hasura\u2019s support for remote schemas, MachineMetrics was able to integrate systems that were separated and siloed in the past. \u00a0Now they can derive insights from operational, time-series, and 3rd party data. An example of this is the integration with their IoT Edge Device fleet management platform into their graph to seamlessly query information about the devices and manage upgrades.Security & Access ControlData access requests come from many different sources - internally from their web frontend, their reporting and analytics systems, as well as external requests from their customers. Hasura\u2019s integrated, granular access control and authorization capabilities provide peace of mind that sensitive data is only accessible to the right people internally and with their customers. Configurable down to the individual database column and row level, authorization extends to other systems integrated via remote schemas connected via Hasura.One of the interesting use cases is the combination with Grafana. MachineMetrics have set up Grafana to use their OAuth flow and have the GraphQL Data source forward a JWT token to a proxy endpoint. This exchanges the token for a JWT used for authentication with Hasura. All the benefits of Hasura\u2019s permission system using Grafana!10X ProductivityHasura has radically increased the productivity and time to market for the engineering team. \u00a0Creating a new feature only requires a single frontend developer now; in the past this would have required the assistance of a data engineer and a backend/API engineer.Using Hasura has reduced their reliance on full-stack and backend engineers - the data engineering team performs more direct work on GraphQL API endpoint creation, and the frontend team is able to create custom APIs without having to wait on another team to build it for them.\n\u201cHasura and GraphQL have created an explosion of new ideas for new product innovation. In a 2-day hackathon, we were able to accomplish with Hasura what would have taken 2 weeks\u201d\n\nThe initial PoC with Hasura showed the potential of GraphQL & Hasura to discover and access all of their data. This led to a flurry of ideas and internal projects to leverage the data. Some of the projects are now in production or on our product roadmap.This newfound agility has also let MachineMetrics create Labs - experimental new features that customers can opt into using.Increased Customer Engagement & Retention with GraphQL Data APIsMachine Metrics recently started providing direct access to data to their customers so that they can provide direct access to data to their customers via Hasura and can create custom queries for their own analysis. This has helped them earn new customers who appreciate this flexibility, and has increased customer retention and engagement.Customizable Reporting & AnalyticsHasura\u2019s GraphQL endpoints provide even more value to MachineMetrics\u2019 customers by also enabling them to create custom reports, data visualizations, and analytics with tools like Grafana, providing an easy to use reporting and observability UI layer.Operational ExcellenceMachineMetrics utilizes Datadog for monitoring of their systems; Hasura\u2019s Datadog support enables them to export all the Hasura logs so they can centralize all their app monitoring on one system and quickly diagnose any issues that occur.What\u2019s Next for MachineMetricsMachineMetrics plans on utilizing even more Hasura features in the future and using it to support multiple database backends simultaneously, including MySQL. \u00a0Currently used for data querying, MachineMetrics plans on increasing the role of Hasura for data mutations in the future. \u00a0Hasura migrations will also enable them to make consistent metadata and data schema changes as they develop new features and put them into production.Thank you to our friends at MachineMetrics, and Jochen Rau - Lead Data Engineer for sharing their story with us!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura vs Apollo Server vs Prisma 2 Description: How does Hasura compare to Apollo and Prisma 2? We've taken a look at the functionality of these tools, where they live in an application and their relationships with each other! Author: Gavin Ray"}, {"role": "assistant", "content": "Introduction\nA question we've commonly been asked is how \"Hasura compares to Prisma 2\", or \"How Hasura compares to Apollo (Server)\".\nTo answer this question, the ideal scenario is probably to discuss:\n\nThe functionality each of the tools provides\nWhere each piece typically lives in an application, and their relationships to each other\n\nHasura\nHasura is a web service that connects to one or more data sources, and exposes a GraphQL (and optionally, REST) API for the data source.\nIt allows you to federate/compose/stitch other REST or GraphQL API's into this data-source graph, to create a unified data-access hub.\nA configurable authorization layer allows you to set up permissions for access to both datasources (at the schema/table/column/row levels) and REST/GraphQL API endpoints or resolvers that you've pulled into Hasura.\n\nApollo Server\nApollo Server is a GraphQL server framework for Node.js. It allows you to write your own GraphQL server implementation.\nLet's look at the example provided from the Apollo Server docs:\n\nhttps://www.apollographql.com/docs/apollo-server\n\nBelow, we create a GraphQL server that has one query, called books, which returns a mocked, static array of book objects.\nTypically besides boilerplate CRUD operations on database objects, you also have resolvers that do things like:\n\nProcess a payment with Stripe/Braintree\nSend a welcome email to users when they sign up\nPerform authentication by comparing a form-filled password against the securely-hashed password stored in the DB, then signing a JWT token if they match\nEtc.\n\nconst { ApolloServer, gql } = require('apollo-server');\n\n// A schema is a collection of type definitions (hence \"typeDefs\")\n// that together define the \"shape\" of queries that are executed against your data.\nconst typeDefs = gql`\n  # This \"Book\" type defines the queryable fields for every book in our data source.\n  type Book {\n    title: String\n    author: String\n  }\n  # The \"Query\" type is special: it lists all of the available queries that\n  # clients can execute, along with the return type for each. In this\n  # case, the \"books\" query returns an array of zero or more Books (defined above).\n  type Query {\n    books: [Book]\n  }\n`;\n\nconst sampleBooks = [\n  {\n    title: 'The Awakening',\n    author: 'Kate Chopin',\n  },\n  {\n    title: 'City of Glass',\n    author: 'Paul Auster',\n  },\n];\n\n// Resolvers define the technique for fetching the types defined in the\n// schema. This resolver retrieves books from the \"books\" array above.\nconst resolvers = {\n  Query: {\n    books: () => {\n      return sampleBooks\n    },\n  },\n};\n\nconst server = new ApolloServer({ typeDefs, resolvers });\nserver.listen().then(({ url }) => {\n  console.log(`\ud83d\ude80 Server ready at ${url}`);\n});Prisma\nPrisma 2 is a library that provides you access to your database when writing Node.js code. It falls into the category of \"ORM\" tools, similar to ActiveRecord, EntityFramework, TypeORM, SQLAlchemy, or others you may be familiar with.\nIt connects to a data source, and gives you the ability to write code that can query the data source -- this code lives inside of your business logic (API endpoints, internal scripts/tooling, wherever).\nWhat does this mean?\nAs you may have concluded at this point, these tools aren't so much alternatives to each other, as they are orthogonal libraries solving different sets of problems.\nWhen building applications, you almost always need some sort of custom business logic for handling particular tasks.\nHasura can't write this code for you (it may be a bit magical, but not it's not that magical!). What this means is that you're likely going to have to write this custom code, and ultimately need a way of integrating it with Hasura.\nOne of the ways you could integrate your custom business logic with Hasura is through the use of a Remote Schema, which forwarded requests to Apollo Server and called your resolver functions there.\nOr perhaps you have an Express.js API with Prisma already integrated into it, accessing data as part of carrying out your business logic.\nAttaching that REST API endpoint to Hasura, using Actions would be a way of integrating both Express and Prisma into your Hasura stack.\nConclusion\nWe hope that this post has given you a better understanding of all technologies discussed, and that the concrete examples were useful in demonstrating how these tools can be complementary if needed.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: How Nexlab used Hasura to digitize Daikin Vietnam\u2019s AC servicing business unit Description: Daikin Vietnam worked with Nexlab on a project to modernize their legacy technology stack. Nexlab chose Hasura for this initiative, and the end result was a project delivered ahead of schedule and at a 50% lower cost than planned. Author: Rajoshi"}, {"role": "assistant", "content": "Daikin Vietnam worked with Nexlab on a project to modernize their legacy technology stack. Nexlab chose Hasura for this initiative, and the end result was a project delivered ahead of schedule and at a 50% lower cost than planned.Nexlab is a software development agency based in Vietnam that specializes in building custom software solutions as well as providing software outsourcing services for enterprise customers. Their customers are based in Singapore, Hong Kong, Vietnam, and the USA. Nexlab was founded in 2016 and now has 40 employees based in Vietnam. Nexlab moved to use Hasura primarily to be able to deliver projects within a tight timeline and deliver complex enterprise applications confidently. In 1 year, Nexlab has made Hasura an essential part of their stack and has observed a 50% reduction in their development cost and the ability to confidently deliver within timelines.\u201cHasura saves us a lot of time, by reducing our cost, our team size as well as the talent that we need to hire. Overall, Hasura has reduced our development cost by 50% and has made the team confident that we can provide better time estimates to clients as well as deliver on time\u201d.Other advantages highlighted in this case study include:Reduce the dependency on the backend developer for API design and development.Empower frontend developers to be full-stack developers with better domain understanding and visibility into the application architecture.Using best of breed tooling to build cloud-native scalable applicationsIncremental migration to ensure that new systems can stay in sync with legacy applications within the enterpriseChallenges faced by NexLab:Nexlab is a technology partner to Daikin Vietnam, one of the largest Air Conditioning companies globally, and was approached by them to help digitize their entire system. In this case study, we will be focussing on the service management system built by Nexlab for Daikin Vietnam. This system helps users easily register an air conditioning service request on an app and be assigned a technician by Daikin to fulfill the request. This service was originally being managed manually: downloading a lot of data from their legacy systems - ERP software, CRM systems as well as access control software, into several excel sheets and assigning technicians manually. This legacy system was not fast enough for the real-time requirements of the business unit and needed a lot of human intervention for completing each request. Furthermore, there was also no way to monitor and measure the business impact of this service and the teams.Nexlab was approached in September of 2019 to digitize the entire servicing business unit of Daikin Vietnam. Nexlab was to build all the applications required to take this business unit online, as well as integrate with their legacy systems so that reporting could continue to be centralized. Given the business importance of launching this before the busy summer months of April to June, there was significant pressure to deliver the product on time. This project required 3 applications to be built:Mobile applications for the user to book a service and see the allocation and fulfillment by the technicianMobile applications for the technicians to receive and confirm the requestWeb dashboard for the operations team to manage and monitor these requests - for example, to understand if service requests are being fulfilled on time. These systems also had to interact with their existing systems: their CRM, ERP, and their existing call center software.Choosing HasuraNexlab was traditionally a NodeJS and Postgres shop working mostly with REST APIs. The time constraints of the Daikin project got them exploring other solutions that could help reduce their development time. Hasura was recommended by a consultant they worked with as a solution that could not only reduce their development time but was also very performant, and plug and play to set up. The team building these 3 applications consisted of 1 backend developer, 1 web developer for the admin dashboard, and 4 mobile developers, across Android and iOS, for the 2 mobile applications. With their older NodeJS stack, there was a lot of pressure on the backend team to build the multiple APIs required by the front-end team for the admin portal. With Hasura, the frontend teams are able to do this by themselves by using GraphQL to query the backend directly. The single backend developer on the project was responsible for the database design which automatically set up the GraphQL endpoint (Hasura automatically creates the GraphQL server, without the backend developer needing to build it), as well as building a simple NodeJS service that converted Hasura\u2019s GraphQL APIs to REST for consumption by the mobile app teams.\u201cUsing Hasura has helped the front-end developers on our team become full-stack developers by understanding more about database design and using GraphQL to query the database directly. This has been very easy to learn, and the frontend developers no longer have to wait on the backend team to design and build APIs for them to consume\u201dUsing Hasura accelerated the development time significantly while reducing the number of people required to build these applications. The frontend team now also gets involved in the domain design to understand how to write more efficient queries making the overall team highly efficient.Hasura Architecture & UseDiakin Hasura Architecture DiagramWhat Hasura features do you most benefit from: \nHasura Console: The Hasura Console is a convenient way for the team to manage all the systems connected to Hasura. It is used to view and edit data, as well as manage migrations.\nEventing System (Triggers, Schedule Cron): The event system is used extensively across a variety of use cases:\n\nTo listen to data changes and execute background processes for notifications and emails\nCron triggers are used to calculate aggregate data, schedule marketing messages as well as export files\nTo keep a NodeJS server that is used to serve REST APIs for the mobile frontend in sync with the database.\nTo keep the data in sync with their legacy ERP such that weekly and monthly business reports can be generated.\n\n\nThis apart, other features of Hasura are also used extensively:\n\nGraphQL queries, mutations, and subscriptions APIs over Postgres: the Daikin management system has many complicated features for CRM, HRM, warehouse, etc that require a large number of CRUD APIs. Hasura helps us save development time.\nRole-based Authorization: the authorization engine is flexible enough for RBAC role permissions for many user types: customer, staff, admin, manager, etc.\nRemote Schemas and Actions are both used for custom logic requirements built with graphql-server-express (now renamed to apollo-server-express) and Node.js.\nCLI / Schema Migrations: The Hasura console is frequently used. The cli-migration image is also useful for migration automation\n\n\n\nSummaryWhat started as a way to win a client with a tight timeline has now become a business differentiator for Nexlab. As Nexlab still uses their NodeJS and Postgres stack on older projects, a cost assessment done internally has shown that projects using Hasura are at least 50% more cost-effective to build. Nexlab has now turned this into a business advantage to maintain their promise of on-time delivery and win more projects."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top PostgreSQL Flavours and Extensions that work with Hasura Description: In this post, we will look at the different flavours and extensions of Postgres that cater to specific use cases and how Hasura supports them natively. Author: Praveen Durairaju"}, {"role": "assistant", "content": "PostgreSQL Flavours and ExtensionsPostgreSQL is open source and pretty extensible. It allows for products and utilities to be created on top providing the fundamental features of Postgres but offering something specialised on top of it.In this post, we will look at the different flavours and extensions of Postgres that cater to specific use cases and how Hasura supports them natively.TimescaleDBTimescaleDB is a time series system optimised for time series data with relational capabilites of Postgres. It is an open source project with support for complex data types that aid in various use cases related to dates and times.TimescaleDB can be set up as a postgres extension.It supports massive ingestion rates and Postgres functions that make it easy to work with time series data.Timescale supports partitioning across time and space (hash partitioning) attributes. It supports ACID properties by default.Here's a talk by a Developer Advocate at Timescale - Visualizing TimescaleDB compression status in real-time with Hasura.Read more about how to configure Timescale Cloud with Hasura Cloud.YugaByteYugaByte is a distributed, high performance SQL database (compatible with PostgreSQL) for global, internet scale applications.Yugabtye is an open source project that serves PostgreSQL compatible APIs that can then be connected to Hasura via the connection string. It's a standalone service that you can run.It supports partitioning by Hash and Range Sharding. Since it's a distributed database, we will look at the protocol being used. YugaByte's replication is based on Raft distributed consensus protocol and there are atleast 3 replicas running for high availability.It supports distributed ACID properties with Serializable and Snapshot isolation. The data persistence characteristics is based on RocksDB.Quoting YugaByte's blog:YugabyteDB + Hasura: making it easier than ever to build highly available, massively scalable, geo-distributed applicationsRead more about how to configure YugaByte Cloud with Hasura Cloud.Aurora PostgresAmazon Aurora is a Postgres compatible relational database service that has an increased throughput performance over vanilla Postgres version.Though it is not open source, scaling on both compute and storage is the key reason why you might choose Aurora Postgres. The infra for database hosting is taken care automatically with higher performance and hence it's more of a fully managed Postgres solution.It supports horizontal partitioning and is also ACID compliant.But before deciding on whether Aurora is for you, do check this post - AWS Aurora: Is it for you?If you are convinced, read more about how to configure Amazon RDS Aurora with Hasura Cloud.ZomboDBZomboDB is a postgres extension that adds full text search capabilities. It's an open source project that has its use cases around search and analytics data with Postgres.It is fully compatible with Postgres' query plan types. ZomboDB communicates with Elasticsearch via a RESTful interface. Indexing and syncing from elasticsearch happens in the background asynchronously. This way, you can use the power and scale of Elasticsearch with PostgreSQL.Read more about how to configure ZomboDB with Hasura Cloud.PostGISPostGIS is an open-source project that adds support for spatial and geographic objects to the PostgreSQL object-relational database. This allows realtime location queries to be run in SQL.PostGIS can be added as an extension to existing Postgres database. It supports utility functions such as distance, area, union, and intersection among other things. It is also ACID compliant.You can build realtime apps by joining data across multiple data sources such as this one - Using GraphQL to join data across Google Places API and Postgres to build location based apps.Read more about how to configure PostGIS with Hasura Cloud.CitusCitus is an open source postgres extension that transforms Postgres into a distributed database.It supports partitioning by sharding and is ACID compliant.Although Hasura doesn't fully support Citus yet, basic queries against a configured database should work. We are announcing full native support soon!Track the github issue for Hasura support of Citus.What are your favourite postgres flavours and extensions among the above? Do you have any other tool which you think should be supported by Hasura or deserves a mention here? Let us know in the comments below :)"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing The Hasura Contributor Program Description: Community is core to the identity of Hasura.\nWhether you are joining an event, getting the newsletter, writing code, answering questions, or otherwise, you are welcome here. Today, we are pleased to announce a programme to recognise your contributions to Hasura! Author: Tyler Hannan"}, {"role": "assistant", "content": "Community is core to the identity of Hasura.Our team is bigger than just our employees. Whether you are joining an event, getting the newsletter, writing code, answering questions, or otherwise, you are welcome here. And, today, we are pleased to announce a programme to recognise your contributions to Hasura!The Hasura Contributor program was created to acknowledge, recognise, and publicly demonstrate appreciation for the work of our contributors. Contribution takes many forms. It can be code contributions, speaking engagements, coordinating meetups, answering difficult (and easy) questions in our community channels, creating tutorials, translating our content into regional languages, and more.A subset of contributors will receive the special recognition of \u2018Hasura Champion\u2019. Each Hasura Champion brings a unique voice, perspective, and set of capabilities to the community.Let us know what you are working on!What Do Current Champions Have To Say?We have quietly been running the Contributors Program for quite some time now. In particular if you participate in Discord you have seen a number of users with the \u2018Champion\u2019 role. These are some of our most active, and most helpful, community contributors.Perhaps the best way to contextualise the programme is to share the perspective of a Champion directly:The Hasura community is an incredible mix of technical mastery, curiosity and innovation. The combination of a visionary technical breakthrough in data at the center and the pioneering spirit of makers and hackers shaping the future of platform design is a truly special, and an incredibly potent combination.- Ben Ford, Founder, Commando DevelopmentHow do I Participate?Tell us what you are up to! Show us how you are helping to tell the story of Hasura!The Hasura team actively monitors our GitHub, Discord, Discussions, Social channels, etc. and we do our best to identify those who are actively contributing to the Hasura community. However, time zones are a reality and we have a discrete number of humans. And, as a result, we may have missed you!Where do I Participate?So...where do you begin engaging and interacting with the Hasura community?Discord - A real-time, synchronous relationship building chat with other Hasura users including quick discussions, watercooler, and language specific (japanese, spanish, etc) chat.GH Discussions - Need to troubleshoot, initiate a long-form technical discussion, show off your work, or interact with our engineering team? This is where to start.Hasura Community Newsletter - Hasura community & product highlights once monthly. It isn\u2019t spam, or a marketing list, we promise. We highlight Community members monthly, so do let us know what you want to show-off.Hasura Community Call - A monthly \u2018what\u2019s on the roadmap\u2019, early feature demos, and product updates from our engineering team. Having our Champions share their projects on each Community Call is a desire. Let us know what you want to demonstrate for the broader community.And, of course, in your social platform of choiceBut, most importantly, we want to enable you to tell the story of Hasura in your own community. Whether a meetup, a company brown-bag lunch, a speaking engagement at an event, a deep engagement in language (or regional) specific tooling...your contributions matter!RoadmapNo programme would be complete without a view to the future. Some of the ideas we have discussed include:A gallery of recognised Hasura Champions on the websiteHasura sponsorship of Champion organised eventsSupport, and feedback, from the Hasura team for content creation (tutorials, live streams, speaking engagements, etc)Special recognition, discounts, etc. at Hasura eventsA Champion specific meeting with the Hasura teamWhat other things should we be including? Do reach out and let us know. We will shape the present, and future, of this program together as a community.Diversity StatementOur commitment is a diverse and welcoming Contributor Programme for everyone, always.We are committed to participants and Hasura Champions from diverse backgrounds, multiple geographic regions, and underrepresented groups in technology. For more information, or if you have any questions, reach us at community@hasura.io."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: HasuraCon'21 - Cross Database Joins with GraphQL and all the other exciting announcements Description: Celebrating 250M Downloads and 2.0 in stable, HasuraCon'21 starts off with major announcements such as Cross Database Joins with GraphQL, schema sharing and better git-bases CI/CD workflows on Hasura Cloud. Author: Rajoshi"}, {"role": "assistant", "content": "If you haven\u2019t heard, today is the start of HasuraCon (our annual user conference). Tanmai Gopal (CEO and Co-founder) has just dropped all the major announcements as a part of his keynote address entitled I want data; I want it now. The future of application development.In the keynote, Tanmai talked about the evolution of data access over the years and our mission at Hasura to make data access simple, fast and secure (irrespective of where your data lives), and with a developer experience that you will love!Here is everything we are announcing today, and we have detailed demos for each of the product announcements over the 2 days of our conference. Hasura 2.0 is stableCross Database Joins with GraphQLSchema SharingGit based CI/CD workflows on Hasura Cloud Oh, and lest we forget, Hasura GraphQL Engine just crossed 250,000,000 (yes, that is millions!) downloads in less than 3 years!2.0 Is StableFirstly, and with all gratitude to our community, we have officially released Hasura version 2.0.0. This marks the \u2018stable\u2019 release of the 2.0 train. If you want to get started right away, the 2.0 upgrade guide is in the docs. Here is a recap of what Hasura 2.0 brings: Connecting multiple databases simultaneously:Hasura 2.0 allows bringing in multiple databases simultaneously to a single instance of Hasura GraphQL engine (or a scaled up cluster of Hasura instances). Instead of having to supply a single env var, you can now add and remove database sources ON THE FLY on a running Hasura instance!Hasura 2.0 also marks a major underlying architectural change to set up the foundation of adding more databases to Hasura. Today Hasura supports Postgres, SQL Server and Big Query (beta), and multiple instances of any combination of these. You can read about the engineering behind this here.2. \u00a0Support for REST, in addition to GraphQL, for the user facing APIOrganisations have REST APIs, and these aren\u2019t going away anytime soon. With the support for REST, you get the best of both worlds - GraphQL\u2019s advantage of self-serve consumption of data with a single API endpoint, as well as the ability to convert the perfect GraphQL query to an idiomatic REST endpoint such that you can integrate with your existing REST tooling, support external vendor APIs and not have to maintain multiple API engines forever.Create a REST endpoint from a GraphQL query template3. Authorization engine enhancements:Hasura contains a sophisticated authorization engine - very similar to RLS style authorization in the database world. With 2.0, we added 2 key capabilities to our authorization engine:Authorization on remote GraphQL services (your own GraphQL services or external GraphQL APIs you don't own).Support for inherited roles, to compose multiple roles (fine-grained RLS or ABAC style policies) together on the fly.Cross Database Joins with GraphQLData is everywhere and is usually stored across multiple databases and data sources. Different teams in an organization might have their own database. Different types of workloads could mean that you have specialised databases for specialised use cases. In enterprises there is often a mix of legacy databases as well as modern ones.We introduced Remote Joins last year to join across databases and remote GraphQL servers.With Cross Databases Joins at the API layer, we have taken Remote Joins to the next level. All you have to do is configure Hasura to connect to your databases and, with no upstream changes to the underlying database, you can start querying across them! With a single GraphQL query, Hasura will let you fetch related data from multiple databases with authorization baked in. Hasura federates the GraphQL request to the underlying databases giving you the experience of all the data being in a single database!A single GraphQL query can be used to fetch data across multiple databasesYou can try it out on this preview link. Join the detailed talk about this feature at 10:20 PST: https://hasura.io/events/hasura-con-2021/talks/the-evolution-of-remote-joins/Schema SharingWhat did you do when you loaded up your first Hasura project? Maybe you created a sample table, were following a tutorial, or tried implementing a feature. With schema sharing we're bridging the gap for beginner -- and power -- \u00a0Hasura users by bringing installable samples of permissions, relationships, and advanced Hasura features; as well as data models, logical implementations, and extension examples. With these installable schemas you and your team will be able to not only get up to speed with Hasura, but also be able to embrace the power of the database that runs through your API. Schema Sharing in ActionMake sure you attend Martin\u2019s talk at 10:50 am PST today to see this in action: https://hasura.io/events/hasura-con-2021/talks/hasura-schema-sharing/Git-based Workflows for CI/CD on Hasura CloudFrom development to staging to production, we all want to automate everything that can be automated. \u00a0With this feature on Hasura Cloud, \u00a0you can deploys your local migrations to staging and production by a simple git push. You can also get preview apps for every main pull request on your git repository.Automatic GitHub deployment with Hasura CloudAttend Rishi\u2019s talk at #HasuraCon tomorrow at 9:30 am PST for all the details & the demo: https://hasura.io/events/hasura-con-2021/talks/ci-cd-and-hasura/Stories about Hasura in ProductionWe have a super packed agenda with many technical deep dives from our engineering and product teams! We also have users & customers of Hasura talking about how they\u2019re using Hasura and their experience in production! If you join HasuraCon (and there is still time) you too can find out how Hasura is powering applications at startups & Fortune 500 organizations.Delivering real-time statistics for the 2021 Olympics by Erik Reyna, Engineering Lead at The Washington PostInternationalization with Hasura: How to Make Your Backend (Almost) as Smart as Google Translate by Brittany Roddy, Lead Software Eng. & Architect at CurrentDeskNoOps for Enterprise Applications at McDonald's and Coca-Cola by Sebastian Ferrari, the CTO and co-founder of TallerBuilding Off Script on Hasura Cloud from Day 0: what we learned by Gus Fune, CTO at Off Script Building & Scaling Pipe: In this fireside chat, Rajoshi will be speaking with Peter (Director of Engineering) & Yasmin (VP of Marketing) from Pipe about their growth from the lens of building & scaling their tech, team and culture.Modernizing a 30 year old stack at the US House of Representatives: We have Andrew, Adam and Glenn from the Office of the Clerk at the US House of Reps speaking with our co-founder & COO Rajoshi about how they have modernized a legacy system with GraphQL & Hasura. Here is the full agenda for the 2 days of talks at the conference! Join the conversation using the #HasuraCon."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Things you wish you knew about PostgreSQL for building an app Description: I had posted this question on platforms like Twitter, HackerNews and Reddit and got some interesting responses. In this post, I'm collating a bunch of these discussions to help frontend or full stack developers work better with PostgreSQL for their app. Author: Praveen Durairaju"}, {"role": "assistant", "content": "I had posted this question on platforms like Twitter, HackerNews and Reddit and got some interesting responses. In this post, I'm collating a bunch of these discussions to help frontend or full stack developers work better with PostgreSQL for their app.PostgreSQL TricksAs the app grows in size and user base, database typically becomes a bottleneck for various reasons. And I'm not even talking about resource consumption, memory, connection limits etc which will haunt admins at some point. I'm actually going to talk about things which are more fundamental at the data modelling layer, some smaller naunces about Postgres, many of which are applicable to any SQL database.Note that this is not an exhaustive list and is purely based on experiences from various users running Postgres in production, including myself. Alright, lets dive in.Data TypesUsing the right data type for a column is arguably the most important thing to worry about when designing data models, even before starting to use Postgres.Choosing string typeA common mistake is to choose a restrictive data type that doesn't scale in the future. For example: A column containing some string can be of 3 types in PostgreSQL - varchar(n), character(n), text. When choosing the former two, you are restricted to the number of characters you can use. Choosing text type for data is usually harmless and is scalable to accomodate more characters in the future. And if need be, you can add check constraints to restrict data input.Primary key dilemmaThe never ending debate about what is the right primary key type? (unless of course you really have a unique column to mark as primary). The id column can have auto-incrementing integers (preferabbly bigint) and this seems to be the most widely used solution. The collision problem: To be honest, before these discussions on Reddit, I have always used auto-incrementing integers for my demos in the past but with an occassional mention that it might not scale well. This was purely because of the issues I have faced in the past. When you are migrating, import/export of data using CSV, or interacting with multiple databases or data sources, there is a small possibility of collision of id values. So I tend to use uuid as the primary key. But there's a possibility of collision there too, albeit a rare one.Using anything other than auto-incrementing integers for id column should be justified. Most apps can stick to this philosophy.And hey, you can also create your own data types.JSON supportPostgres supports JSON type from v9.2. It has added many operators and functions for data manipulation of JSON. The operator -> returns JSON object field by key. The operator ->> returns JSON object field by text.Does this mean, I can use this type for my NoSQL db use cases? Yes! With support for wide variety of operations, this is a good reason for staying in the relational DB ecosystem.Most columns should be NOT NULLUnless required, most columns in your database should not be nullable. Apart from ensuring data consistency and reliability, null values often cause a lot of issues in the frontend. Especially with an app that is not strongly typed, null values might require extra conditional checks and making the app rendering complex. You really don't want the red screen of death! Default TimestampsIt is always better to have the two timestamptz fields created_at and updated_at columns with default now() value for created_at for tables that store some form of dynamic data. Storing what time a record was created or modified would be very useful in the future when going over some analytics or reporting. Audit logs might be required and timestamps are key.Make use of schemas in PostgreSQLPostgreSQL has support for mutiple schemas. We all typically end up using just the public schema for most use cases. But multiple schemas are a nice way to namespace your different application use cases since joining of data and establishing relationships between schemas is fairly easy.I tend to use schemas for reporting views for analytics and keep my primary table schema (public) noiseless.Make use of extensions in PostgreSQLPostgreSQL is pretty extensible. Extensions in Postgres lets you load it into the database and behave like built-in features. Some of the popular extensions are PostGIS, TimescaleDB, pgmemcache, pg_cron etc. They are designed for use cases and saves a lot of time for what it is intended to be used for. It is driven by the community and there's something written for most use cases.IndexesThis is probably the first thing to add to improve performance of your queries. Postgres adds indexes on primary keys and unique constraints. Indexes won't be created automatically on foreign keys, but as noted from experience, it is typically required in most cases.Be judicious on indexes since there is a performance overhead if there are too many.Database Users and SessionsDefault configurations of postgres comes with superuser postgres. But it is recommended to create new user with specific GRANT types to allow access to certain databases and operations.For example, your custom code server that is connecting to Postgres purely for querying and inserting data, just needs the GRANTs for SELECT and DML statements like INSERT, UPDATE and DELETE. For DDL statements, there can be a separate user with CREATE, ALTER and DROP permissions.Connection StringGenerating a connection string with the right host value is often where things go wrong. libpq libraries use the URI based format to make it easy to connect to.Connection strings are of the format:postgres://user:secret@localhost:5432/mydatabasenameThe value localhost will vary based on the OS being used and the way Postgres is installed and set up. For example, using docker on macOS would change this to host.docker.internal, instead of the localhost which works for native macOS app installation.If you are using a secure SSL installation of Postgres, you need to add the ?sslmode=require parameter to the connection string.ViewsViews can be leveraged to transform data so that queries are easier on the client. It is recommended to not do data transformations or multiple queries on the client, when it could have been solved with one View. Views are powerful and execute everytime you run the query. The data is dynamic and will likely be more performant than client side processing for most use cases.Version control the schemaAs you keep developing your app, the schema changes incrementally. But you would want to keep a log of what changed when, not just from an audit perspective but more so from an ability to rollback perspective.Database schema migrations are key to version control and rollback the schema in case things go wrong with a specific feature deployment.Transactional DDLThere are cases where you might want to execute both DDL and DML statements in a transaction. This is called Transactional DDL. Consider the following use case:postgres=# BEGIN;\nBEGIN\npostgres=# create index on table (status);\nCREATE INDEX\npostgres=# analyze table;\nANALYZE\npostgres=# explain select * from table where status = 'FOO';\nQUERY PLAN ------------------------------------------------------------------------------------\n...some output\npostgres=# rollback;\nROLLBACKThis lets you perform complex updates, inserts, deletes, then perform selects to verify the data matches expectations, then commit or rollback before the changes are visible to users.Join the discussionThere's more to using Postgres in production and advanced things to take care and we all wished we knew earlier. But as I mentioned earlier, this is not an exhaustive list but would be updated as I get more inputs from users and from my own experience.Reddit r/PostgreSQLTwitterHackerNews"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura 2.0.0-beta.2 Description: We continue our progress towards 2.0 stability with today\u2019s release of 2.0.0-beta 2. \n\nThis beta features fixes across datasources, server, and numerous console improvements. Author: Tyler Hannan"}, {"role": "assistant", "content": "Announcing Hasura 2.0.0-beta.2With beta.1 available for about two weeks, and an immense amount of helpful feedback from our community, we are pleased to continue our march towards 2.0 final with the release of 2.0.0-beta.2.Hasura 2.0.0-beta.2 docker quickstartHasura 2.0.0-beta.2 changelogGetting Started with Hasura CloudWhat is a \u2018beta\u2019?Let us explain what a \u2018beta\u2019 is to Hasura.At its simplest, when a release achieves beta it means:The code is feature frozen and no more features will be added (unless absolutely necessary)All breaking changes are incorporated, documented in prior release notes (or addressed), and there will not be more surprises to comeThe code is in a state to undergo final performance & security reviewThis does not, by any means, mean that the value of our community is limited to alpha stages. In fact, now is the perfect time to test the beta releases and give us your feedback as we move towards the 2.0 stable release.A Summary of ReleasesAs mentioned in recent release posts, we have historically announced our releases via Discord and updates to the GitHub release page. As adoption continues to increase, we believe this is insufficient for the future. To that end, we will begin to blog for each release so that you can track releases via blog, twitter (@HasuraHQ), Discord, and the Github Release page. In these posts, we will highlight a few specific elements of each release, but the exhaustive list of features and improvements is always in the changelog.OverviewWhile we continue squashing final bugs in datasources (i.e. BigQuery) and the server, much of this release has been focused on console functionality. This includes:Adding foreign key CRUD functionality to the MS SQL Server tablesAllowing the tracking of custom SQL functions having composite type (rowtype) input argumentsAllowing input object presets in remote schema permissionsThese are, of course, just the highlights. All other fixes are included in the changelog.ConclusionPlease use the docker target `hasura/graphql-engine:v2.0.0-beta.2`, try it out, let us know what works and what needs improvement. You can find us on Twitter (@HasuraHQ) and in Github. For ad-hoc conversation, feel free to join us on the Hasura Community Discord.And, if you want to get started now with the latest stable release, check out the deployment simplicity of Hasura Cloud. If you want to begin your upgrade journey with 2.0, and want to learn a bit more about how Hasura is being used in the enterprise, do feel free to contact us to set up a call with our solutions team.Previous PostsFor previous release announcements in the 2.x series, please review:Announcing Hasura 2.0.0-beta.1Announcing Hasura 2.0.0-alpha.11Announcing Hasura 2.0.0-alpha.10Announcing Hasura 2.0.0-alpha.9Announcing Hasura 2.0.0-alpha.8"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Cloud: the easiest way to run & manage your Hasura applications in production Description: We explore the features that come out of the box with Hasura Cloud that drastically reduce the ops effort required to run and manage your Hasura applications in production.  Author: Jesse Martin"}, {"role": "assistant", "content": "Teams building with Hasura know and love the powerful features available at their disposal - instant GraphQL APIs, remote schema joins, granular authorization, eventing system, and more. \u00a0Hasura is available for use as multiple product offerings - Hasura CE, Hasura EE, and the focus of this blogpost, Hasura Cloud. Here, we will explore the features that come out of the box with Hasura Cloud that drastically reduce the ops effort required to run and manage your Hasura applications in production. \u201cWe have gotten a lot done as a small team. While looking at other companies of our size, we have a much smaller engineering team, and that\u2019s largely because of Hasura and Hasura Cloud\u201d \u2014 Asa Miller, CTO & co-founder of Brave CareBrave Care is a rapidly growing YC backend healthcare startup. Prior to Hasura Cloud, Brave Care was hosting Hasura themselves on a Kubernetes cluster. They\u2019ve now moved their stack over to Hasura Cloud so as to not worry about operating & scaling Hasura themselves. Read the case study here.Teams worldwide and across all industries have come to depend on and love Hasura Cloud to handle all the idiosyncrasies of scaling applications. Instead of focusing on resource allocation, failovers, caching, API throttling, and more \u2013 Hasura Cloud lets you focus on your business, not on scaling cloud infrastructure and a cloud infrastructure team.The best part of Hasura Cloud is that it is easy to migrate to from any functioning Hasura instance. So if you have a running application and are considering the next level of scale, or want to implement some form of automation, or even just add observability \u2013 Hasura Cloud is just a few steps away. Let\u2019s take a high-level survey of the features that let Hasura Cloud reduce your operations burden and speed up your time-to-market. To make this comparison, we look at three critical aspects of running your application in production that Hasura Cloud makes simpler \u2013 reliability, scaling, and observability.ReliabilityMaking any change to the underlying schema of your application, updating your application, and introducing new components risks breaking the build and introducing regressions. Hasura has several features that make production releases stable, well tested, and guaranteed to work.Managed UpgradesThe Hasura product is continually evolving with new features added and new databases supported. When it\u2019s time to upgrade your instance, Hasura Cloud handles all the migrations and updates for you, eliminating downtime and ensuring a seamless upgrade experience.Regression testsRegression tests let you quickly generate a list of tests based on real-world usage of your application. First, check to see which queries are coming into Hasura, select the ones you want to add to your regression test suite. Then whenever you make changes to your schema, the tests will run to ensure that the queries representative of your production usage will continue to work. Instead of creating multiple unit tests for individual parts of your schema, Hasura makes it simple to enable complete \u201cend-to-end\u201d integration tests that ensure your application performs as you\u2019d expect.Adding regression tests to a project.API limitsThe API limits feature on Hasura Cloud lets you set reasonable defaults for how users can interact with your application by simply configuring a few parameters. Some of the configuration options provided include passing in control checks on user IDs or IP addresses and more advanced configurations such as setting maximum query depth \u2013 a particularly tricky problem to solve for in GraphQL servers that you get out of the box.Adding API limits to a project.Scaling your Hasura applicationsHasura Cloud allocates the optimal hardware required for hosting your Hasura application and ensures that extra resources are available as and when needed by your application.Hasura takes care of scalingHasura Cloud ensures that the API gateway, caching, event processing, and more are ready to scale to millions of users on demand. You can still configure Hasura precisely the same way you would for Hasura CE, and then be rest assured that the hardware running your Hasura instance is ready for any scale.Built-in cache layerHasura Cloud includes a caching directive for the GraphQL API. If you add @cached to a query, it will immediately cache the results and send the appropriate response headers with the time to live, the reset key, and more. You can instantly leverage faster queries with this \u201cout-of-the-box\u201d cache layer from Hasura Cloud just by adding seven characters to your existing code!Cacheing expensive queries.Rate limitsWith the built-in API limit controls of Hasura Cloud, you can look for, scope against, and limit excessive traffic from IPs, users, and other header flags that are causing a drain on your app\u2019s performance.ObservabilityObservability is the ability to ensure that the application is running according to the parameters you defined. Hasura Cloud has specific features to help with this task.MonitoringHasura Cloud generalizes monitoring across five top-level categories, errors, usage, operations, websockets, and subscription workers.The operations tab allows you to track every individual request across your entire Hasura Cloud instance. You can drill down onto specific details about those requests and filter on a wide array of indices to find the particular query you want to monitor.Operations tab overview.Under the Errors tab, you can see the usage filtered by errors and grouped by the offending error. In addition, you can see how often the error surfaced, the error response, and the percentage of total errors this particular operation accounted for. This is useful for triaging the cause of a widespread problem.Errors tab overview.The Usage tab also groups requests by operation name and surfaces how many requests of that operation resulted in an error. This view helps provide a contextual overview of your application and surfaces insights into where users may be seeing unexpected behavior or crashes in your application.Usage tab overview.The Websockets view shows all concurrent connections to your Hasura instance, surfacing if there\u2019s an abnormal number of connections not disconnecting, or simply to monitor what kinds of usage is requested in real time.Websockets tab overview.Lastly, the Subscription Worker view shows you which queries are actively executed by the subscriptions inside of Hasura, including variables, error rate, starting time, and more metrics that help you look for outliers in content consumption.Subscription workers tab overview.Distributed tracingHasura supports standard distributed tracing protocols which means that not only do you see details about the query planning, data resolving, and API access stages of a request, but you also see tracing information sent by any external tools such as service providers or bespoke databases. This tracing functionality effectively collocates your entire stack trace into a single point of reference where you can monitor the health of your application.Distributed tracing for queries.LogsHasura Cloud includes detailed logs for all the events in your application. Continuing with the theme of monitoring without maintaining your application, you can see everything happening under the hood on your Hasura Cloud console.SummaryHasura helps you build applications with flexible schemas, descriptive data access patterns, reliable eventing, and much more. Hasura\u2019s fully managed cloud offering not only hosts your Hasura application for you but also controls the everyday operational tasks around reliability, on-demand scaling, and observability that you\u2019d be required to set up for your application in production. Looking for even more information about what Hasura Cloud has to offer? Read here about how Hasura Cloud provides enhanced security for your applications out of the box.Hasura Cloud comes with a generous free tier for you to get started. Get started in under 30 seconds here."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Design Patterns: Content Enhancement with Reactive Programming Description: In this Hasura Patterns guide, we\u2019ll look at how we can build an end-to-end reactive API, in a scale-ready, event-driven architecture with Hasura. We\u2019ll build a Pinterest-like feature for auto-tagging user images as a working example of this reactive programming pattern! Author: Jesse Martin"}, {"role": "assistant", "content": "TL;DR:Modern applications deliver increasingly complex features, powered by machine learning, artificial intelligence, and even more complicated architecture in a way that feels magical if not expected.In this Hasura Patterns guide, we\u2019ll look at how we can build an end-to-end reactive API, in a scale-ready, event-driven architecture with Hasura. We\u2019ll build a Pinterest-like feature for auto-tagging user images as a working example of this reactive programming pattern!Intro- our reactive programming plan:In this post, we\u2019ll do an architectural walkthrough of an end-to-end, asynchronous data flow with Hasura. Specifically, we\u2019ll see how Hasura\u2019s native support for subscriptions, reliable eventing, and its flexible GraphQL API allow us to create cloud-scale features with NoOps. We'll touch on event based programs, event driven systems, data flows, the responsive experience, and more. While not diving into the logic handling code, this guide will serve as a functional pattern to create similar features.Why do we care about reactive programming or async in general?Reactive programming architecture, something that listens to the response of asynchronous events, or data streams, is great for many types of modern user experiences. When a user uploads content with a variable workload required to process that content, we don\u2019t want our interface to be blocked while waiting for a response. For simple actions, this may only be in the low dozens of milliseconds, but what happens when we exceed 500ms or more? That\u2019s a good time to consider a reactive pattern or reactive system as a whole.Synchronous vs Asynchronous data flows.The feature we are building today is an auto-tagging image service that leverages an AI object recognition service. Our user will upload an image, the image will be persisted to storage, the image will be auto-tagged, and our user will be notified of the result.This feature pattern of ML/AI-assisted data enrichment is common in my other programming paradigms. When a user uploads a PDF for OCR processing, it\u2019s not uncommon for that process to take well over a minute. When a user uploads a video to a streaming service, transcoding that data can take tens of minutes! That\u2019s an unacceptably long time to be staring at a loading spinner \u2013 enter reactive programming.Why should we use Hasura for reactive programming?As explained here and in many other places, the benefits of asynchronous, reactive programming are well documented. So why doesn\u2019t everyone build asynchronous models all the time? The problem is asynchronous is hard to build.Specifically, asynchronous requires an atomic and reliable eventing system which itself requires some kind of queue management. The events need to have reliable and flexible triggers and equally be able to listen to changes. The services consumed need to be idempotent and capable of handling events out-of-order.Specifically, building an end-to-end asynchronous workflow requires plumbing across the following:An eventing system that can trigger, queue, and deliver events with the right reliability guarantees so that events aren\u2019t dropped or missed when data changesA realtime API that allows API consumers (web or mobile apps) to subscribe to an async data flow as and when it becomes availableDistributed eventing diagram.With Hasura, we get all these primitives to build asynchronous features, out-of-the-box. Hasura can attach HTTP triggers to database events and guarantees atomic capture as well as at-least-once delivery. On the API side, Hasura provides a GraphQL API to subscribe to any changes to the data flow, in a way that\u2019s secure and scalable. Our effort effectively boils down to writing our business logic, not building plumbing and maintaining infrastructure for our ideal reactive systems.Reactive programming with HasuraThe Hasura WayWhen designing features the Hasura way, we think of our function as three different service layers.First, we consider the read layer - what are the read models our API needs to have, and what are the authorization rules on those read models.Second, we consider our write layer(s), namely, which content will be written synchronously and which content will be written asynchronously (evented). Let\u2019s begin.The feature specificationTo begin our specification, we\u2019ll consider the feature story from a UX writing perspective and then review the actual architecture stack diagram. In an ideal world, these two would align from both a semantic and functional perspective.The user storyLet\u2019s discuss our user flow again. A user uploads content to our service, receives the image back immediately, and then, eventually, receives an auto-updated list of recommended tags. Typically a feature like this might account for custom tags that are user-provided, but we\u2019ll leave this out of our specification for simplicity. \u00a0The key here is that our user receives a synchronous response quickly for our short-lived processing (object storage and database persistence) and then asynchronously receives updates from our long-running process so as to be non blocking. In short, this is our reactive application.That looks something like this.Feature function diagram.Technical reactive architectureOur actual data flow will be relatively similar. An image gets uploaded to a custom API endpoint, which we will persist in a file/object-store. Our custom endpoint will then write that file store\u2019s response into our user database, and finally, return our database entry reference plus the URL of our object store back to our user. Round trip, this should be around 20ms. Our object store actor and user database are combined through loose coupling of the single API endpoint.As soon as the data is written into our user database, we\u2019ll trigger a second data flow that uploads the URL to our object recognition service. That service will then write those new tags to our user database, and, through our subscription API read layer, will update the user interface with the new tags, without any interaction needed from our user. This eventing model allows us to compose asynchronous functionality.If we design our system right, we should have a resilient architecture that scales horizontally, requires low DevOps, with no ongoing maintenance.Architectural feature diagram.Defining the read layer specificationIn defining our read layer, we must first identify the models to be read. In short, we have an \"upload\" that is owned by a \"user\", and that upload can consist of either a \"video\" or an \"image\" asset. The uploads themselves have multiple tags with an associated weighting, and a canonical reference to normalize our database. Refer to the diagram below.Relational data model.These models will need two read permission sets. The first permission control will be per user and be used to limit access to the User, Upload, Image, and the Upload_Tag (join-table) rows.We need a second permission check that ultimately acts as unfettered access to our database for service layer integrations where the majority of our asynchronous operations will happen. This will have unlimited access.Lastly, we\u2019ll leave our normalized Tag reference as public for reuse among the other user uploads.In overview:Per-user permissions: User, Upload, Image, Upload_TagService level access: UnlimitedPublic: TagsThe read layer will be served by two endpoints. First, a REST endpoint for integrating with our image recognition service, which, under the hood, uses a generic GraphQL endpoint to resolve the data. Second, an asynchronous API that will respond to database activities via subscription. Hasura provides both of these out of the box.A note on the REST endpoint and why we\u2019d opt for the REST endpoint when we have access to GraphQL. In many cases for serverless consumption, systems are still optimized for REST read/write access. Because our actions handler is one of these types of services, it\u2019s useful to have that as an option to provide faster integration with our logic handlers.Mapped to our chart, we\u2019ve now covered a large portion of our technical architecture.Architectural feature diagram read layer coverage.Defining the write layer specificationOur feature has three touchpoints for writing content in the system.Service one, effectively our storage API, takes a binary payload, writes that data into a persistent CDN, takes the returned URL, and writes that data into our user database, which finally resolves back to the client with the URL of the stored asset, and an ID of the user database entry that we\u2019ll use for our read layer\u2019s subscription.Called as: Synchronous REST Request APIInput: Binary dataResponse: Database entry ID, URL from the CDN.Service two, when called via a trigger, uploads the URL that it fetches with our previously defined REST endpoint to an image recognition service, waits for the results, and then writes the response (tags) into our user database with the trigger identifier as an insert filter.Called as: Asynchronous Event TriggerInput: Upload IDResponse: 200 okBehavior: Inserts upload_tag references for the Upload ID.Note that an event trigger configuration is required here. The asynchronous message becomes our bridge between the short-running process and the long-running process in our feature.How much of our feature is now accounted for through these two layers in our programming model? Right, we have full coverage! Now we can go on to implement the specification as designed.Architectural feature diagram write layer coverage.Reactive system buildWe'll need to build out each layer of our feature. We'll create the data model, the read permissions, and relations for our different tables.We'll create our synchronous API for uploading the content and the event trigger mechanism for our asynchronous message passing as well as the API for our asynchronous stream processing.Data Model:We\u2019ll handle our data model and authorization rules first. In Hasura, either through the CLI, raw SQL statements, or with the UI, the process is simple. See the screenshot below for our Upload table.Upload table schematic.We\u2019ve created the remaining models in a similar manner. Let\u2019s create the relationships between them which will enable our write layer from our upload API to create the required image and upload inserts in a single mutation.Upload table relations view.We\u2019ll now create our locked-down permissions so that only an approved user can access their own uploads and images.Upload table permissions view.And with those configurations in place, we\u2019ll test this in the integrated API explorer.Testing the permissions control for uploads.Everything is working as expected. Let\u2019s move on to our service handlers.Upload API: An exposed endpoint with our lambda provider (Pipedream) that passes a binary to our storage provider (Cloudinary), and uploads the object response with our associated user ID to Hasura, responding with the Hasura response of Upload ID plus the image URL.In Pipedream, our workflow looks like the following.Workflow for the upload API.An alternative to Postman or another API testing suite for testing, Pipedream has a built-in testing option, which allows us to iterate on our logic within the platform. Test mechanism for the upload API.Our service is working!Asynchronous API: Another Pipedream lambda that will run in response to insert events on our user uploads and orchestrate with our object recognition AI service (Imagga) and write the returned tags back into Hasura.Again, here's our workflow from Pipedream, and we'll use the same built-in testing mechanism.Auto tagging workflow.Event Handler: Whenever a user inserts an Upload, we\u2019ll trigger an event to our asynchronous event handler. We configure this under the actions tab of Hasura.Eventing configuration.Final Checks:With our model designed, or events configured, and our handlers in place. Let\u2019s review our access controls to ensure that our user can access their images only, but that anyone has access to the tags.Permission check overview.With our access controls in place, let\u2019s run a few independent tests. First, we\u2019ll check our custom endpoint that uploads our asset. We'll leverage Postman's ability to upload binaries in their utility.Upload API demo.Now, we\u2019ll check our \"events processed\" tab in Hasura to see if our events triggered as we expected. This level of observability detail is helpful when designing reactive systems where background tasks and actors may be working behind opaque surfaces.Processed events.From the results, we can see the expected behavior. \u00a0The last thing to test is our subscription results that are ultimately reacting to the change in the data streams.\n\n\n    Sorry, your browser doesn't support embedded videos.\n\nIt appears that everything is working as we expected! Our independent actors in this distributed system are sending, receiving, and responding to our feature interactions. Our internal logic as defined in our handlers is performing the correct content transforms and our event driven system is delivering the intended results. \u00a0Time for an architectural review.Reviewing the featureIn the end, our feature leverages functional reactive programming concepts of distributed systems, working with a data stream, and monitoring an observable sequence which we could then react to. This programming model allows our users to upload a photo, receive the image back within short order, and then opaquely receive auto-tagged results from our annotation service.With this functional pattern, the user could just as easily upload the photo, and then abort the browser session and the content would be created without any further input and/or confirmation.The user experience benefits from this pattern, and our architecture is resilient thanks to the actor model as it handles queued transactions instead of a flood of long-running processes and mounting back pressure.Did we meet our architectural goals and non-functional requirements?Evaluating the process needs to look at some key metrics that cover the ease of setting up the feature, scaling the feature, and maintaining the feature. This spans both our technical (tangible) requirements, but also those requirements that are difficult to quantify but still have a large impact on the overall success of adopting a specific pattern.From a technical perspective, is this feature scale-ready?Deployed on the cloud with resilient architecture patterns in place, both our proxy handlers and Hasura will handle all of our API delivery and eventing tasks with high uptime guarantees. The eventing system guarantees \u201cat-least-once\u201d delivery and the APIs themselves are highly available, so our stack is also reliable. Thanks to the built-in queue management of the Hasura platform, we don\u2019t need to worry about overrunning our proxies and long-running processes.What about data portability? Is there any vendor lock-in? As an open-source platform, Hasura is exceptionally open as a vendor. While Hasura will annotate your data (in a separate database) to enable the auto-generated GraphQL API, this metadata is yours to take when you would want.But what about the developer experience as a whole?How easy was it to configure, how easy will it be to maintain? \u00a0Utilizing this approach of connecting multiple service providers together as independent actors allowed us to design, develop, and test each unit in isolation. Secondly, once the code was working as desired, our task was done. We don\u2019t need to deploy to a production server, scale up our instances, or handle any other operations. Both the Pipedream platform and the Hasura Cloud platform allowed us to \u201ccode-and-forget\u201d - once the services were functional, the pieces were service-ready.As a category, and speaking specifically of Hasura, this concept of auto-handled scaling, automated configurations, and free logging, is known as \u201cNoOps\u201d. Though very specifically, NoOps is not what we want. We want LowOps. We don\u2019t want our critical functionality to be hidden behind a layer of promises from a service provider roughly hinted at as \u201cmagic\u201d or \u201csecret sauce\u201d - we need to know what\u2019s happening. What we don\u2019t want is to have to write it ourselves, maintain it, and think about the implications of scale.With Hasura, we identified our data model, pointed our event triggers to their appropriate handlers, and took advantage of an auto-generated API complete with subscriptions. This is robust, transparent, and ready to scale. We can effectively check off our operations goal on running Hasura itself, by using Hasura Cloud.How much effort have we saved with Hasura?The effort saved with Hasura is as follows, for the purpose of comparison, we\u2019ll assume that we already have our tables created in a Postgres database:\n\n\nRead APIs\nWithout Hasura\nWith Hasura\n\n\n\n\nBuilding queries & subscriptions APIs\nBuilding & scaling an API server that supports HTTP/Websocket connections.   Writing performant resolvers that will fetch authorized data, or changes to data for subscriptions.\nWith Hasura, we add our database to the data sources tab. We indicate which tables we want to track. Done!\n\n\nAuthorization rules to ensure users can access only their images\nBuilding authentication middleware to handle/validate incoming tokens. Building a performant authorization engine or writing authorization code per resolver that checks the permissions of a query against the database.\nEnable authentication by adding JWT configuration. Add authorization rules per model. Hasura will automatically apply them in any GraphQL query or subscription that fetches data from that model.\n\n\n\nWrite APIs\nWithout Hasura\nWith Hasura\n\n\n\nConfigure an eventing system\nCapture the right set of database changes into a queue. Read from the queue or set up a system for the queue to deliver events over HTTP to your handler.Enforce atomic capture and delivery reliability.\nConfigure which operations on which tables should generate events.Configure a webhook to the events.Hasura takes care of capture and delivery.\n\n\nAs we can see, Hasura helps us implement complicated features with clarity and simple primitives. We've added event driven, reactive functionality on top of our distributed system. Abstracting the toolset that Hasura provides lets us implement the best patter for any given feature we would want to create.Thanks for reading!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura 2.0.0-beta.1 Description: We continue our progress towards 2.0 stability with today\u2019s release of 2.0.0-beta1. \n\nLet us explain what a beta means to the humans at Hasura.\n Author: Tyler Hannan"}, {"role": "assistant", "content": "We continue our progress towards 2.0 stability with today\u2019s release of 2.0.0-beta1.Hasura 2.0.0-beta.1 docker quickstartHasura 2.0.0-beta.1 changelogGetting Started with Hasura CloudWhat is a \u2018beta\u2019?Let us explain what a beta means to the humans at Hasura.At its simplest, when a release achieves beta it means:The code is feature frozen and no more features will be added (unless absolutely necessary)All breaking changes are incorporated, documented in prior release notes (or addressed), and there will not be more surprises to comeThe code is in a state to undergo final performance & security reviewThis does not, by any means, mean that the value of our community is limited to alpha stages. In fact, now is the perfect time to test the beta releases and give us your feedback as we move towards the 2.0 stable release.A Summary of ReleasesAs mentioned in recent release posts, we have historically announced our releases via Discord and updates to the GitHub release page. As adoption continues to increase, we believe this is insufficient for the future. To that end, we will begin to blog for each release so that you can track releases via blog, twitter (@HasuraHQ), Discord, and the Github Release page. In these posts, we will highlight a few specific elements of each release, but the exhaustive list of features and improvements is always in the changelog.OverviewSQL Server FeaturesThis release includes numerous fixes and capabilities for SQL Server. These include:Fixing a regression with SQL Server execution (#6976)Adding create table functionality for SQL Server to the Hasura ConsoleAdding drop table capability for SQL Server to the Hasura ConsoleAnd moreIn addition, you can now rename data sources in the Console. These are, of course, just the highlights. All other fixes are included in the changelog.ConclusionPlease use the docker target hasura/graphql-engine:v2.0.0-beta.1, try it out, let us know what works and what needs improvement. You can find us on Twitter (@HasuraHQ) and in Github. For ad-hoc conversation, feel free to join us on the Hasura Community Discord.And, if you want to get started now with the latest stable release, check out the deployment simplicity of Hasura Cloud. If you want to begin your upgrade journey with 2.0, and want to learn a bit more about how Hasura is being used in the enterprise, do feel free to contact us to set up a call with our solutions team.Previous PostsFor previous release announcements in the 2.x series, please review:Announcing Hasura 2.0.0-alpha.11Announcing Hasura 2.0.0-alpha.10Announcing Hasura 2.0.0-alpha.9Announcing Hasura 2.0.0-alpha.8"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Hasura Community Call Roundup - May 2021 Description: The Hasura Community Call is a monthly event hosted on the last Thursdays of the month at 9 AM PT. In this post, we summarise the content from the last event. Author: Vishwa Mehta"}, {"role": "assistant", "content": "This blog post is a part of our newest series of monthly roundups for the Hasura Community Call in which we summarise all the cool demos and updates from the previous event.\ud83c\udf99 What is the Hasura Community Call?If this is the first time you're coming across this event, below is all you need to know about it\u2014The Hasura Community Call is a monthly event hosted by yours truly (Vish \ud83d\udc9c) on the last Thursdays of the month at 9AM PT. This is a place where our engineering and product teams hang out with our user community to showcase upcoming features, give product updates and take deep-dives into the roadmap, present interesting RFCs, and everything awesome; you name it!Additionally, we invite a community member to present a brief demo of the work they've been doing using Hasura and talk about their experience with it. The community presenters put the 'Community' in our 'Calls'... No kidding!So without further ado, let's jump right into the last month's spread!\ud83d\udde3 Tanmai's Product UpdateIn this customary 10 min product update, our CEO & Co-founder, Tanmai Gopal takes us over the roadmap and gives us an exciting view into the upcoming features as well as cool stuff that the team is currently working on.In the month of May, Tanmai talked about two super exciting specifications that we're starting to actively work on:Inherited Roles: Currently, we only allow for deriving 'table select' permissions for an inherited role and not for mutations/functions/remote schema permissions, etc. Hence, we want to bring three primary improvements in inherited roles to make them more useful\u2014- Derive non-table select permissions for simpler cases where there is no scope for conflicting definitions.- Allow explicit permissions definitions or inherited roles and unify 'roles' & 'inherited roles'.- Try & handle special cases where there are no presets defined for the parent roles.Schema Sharing: Have an idea about what you want to build in Hasura? How amazing would it be to be able to load a sample data model from an existing repository of schemas? We're working on bringing the ability to either create a new or load an existing sample schema into your database with the metadata associated to it which would make bootstrapping common use-cases from the community a lot easier. Tanmai gave a really cool sneak-peek of what you can be expecting.Tanmai also briefly talked about our recent Hasura 2.0-alpha.11 upgrade that includes read-only Postgres support and the SSO launch on our Cloud & Enterprise offerings this week (we'd love your feedback on it!)\ud83d\udd01 Vamshi talks One-to-One relationships & data sources!Vamshi demonstrated what's new with one-to-one relationships & the features updates in our latest OLAP data sources\u2014BigQuery & Citus.One-to-One Relationships: With the recent upgrade of Hasura GraphQL Engine 2.0-alpha.11, you can now create one-to-one object relationships between two tables. Vamshi shows a typical use-case of splitting a larger table into smaller ones where a one-to-one relationships would be useful. Prior to the upgrade, this wasn't possible. This new API improvement has certain benefits like better response shapes, ability to use complex ordering clauses, and easier filtering. This also allows for relational inserts in the case of mutations. \u00a0Multiple Data sources: At Hasura, we\u2019re working on empowering our users with universal data access to build modern apps and integrations by supporting all your databases. In this segment, we bring you exciting demos of graphql-engine\u2019s newly added support for two popular OLAP data sourcesBigQuery: Google Big Query is one from our suite of data sources that you can leverage to create reusable GraphQL and REST APIs within minutes. Vamshi demonstrated the features that are currently supported on BigQuery.Citus: Citus transforms Postgres into a distributed database. It is one of the new generation of databases that aim to target both OLAP and OLTP use cases. Vamshi talked about the exciting work the team has been doing in bringing the official beta to you.\ud83d\udd0f The what, why, & how of Client SSL Certificates by SolomonPostgres lets you use SSL certificates for client authentication when you're connecting to your database. We currently have support for Client SSL certificates only for our Open Source and On-Premise offerings as a user has full control over the file system in those environments and can easily create their own certificates to add them to the file system. Solomon explained what Client SSL certificates are, how they work, and the challenges of adding it to a Cloud service like Hasura Cloud as different users use a shared environment and do not have access to the file system.Our Cloud API team is working on a basic implementation that brings this feature to Hasura Cloud as well. The requirements are:Securely operate in a shared environmentUser has no visibility into the file systemSimple integration into the existing metadata API and Console UXAbout the implementation:Stores certificate data into a secure vault\"Lazily\" writes certificates on the disk at runtimeLeverage the existing env var systemBasically, when the project is launched, the implementation looks up the source metadata, looks up the env variables and writes them to the disk, constructs a connection string and all of this happens internally in the GraphQL Engine.Check out the recording for a really cool demo of this!\ud83d\udd10 Tiru's Security 3x updateTiru showcased some of the upcoming security features in Hasura Cloud such as Node Limit, GraphQL Request Timeout & Disabling GraphQL Introspection for Roles.Node Limit: This feature lets you restrict your API's runtime characteristics. This is useful when you want to customise the limits on your public facing APIs or restrict the operations that can be run to minimise the load. We already offer a similar feature called depth limit that helps limit the nesting in your GraphQL query. Similarly, the node limit feature lets you limit your API for a certain number of nodes. You can define the node limit both per role and globally for all roles.Disabling Introspection for Roles: This one is pretty self-explanatory. You can now disable the schema introspection for certain roles to expose less information in your GraphQL API to those users.Operation Timeout: This is yet another powerful security feature that allows you to configure an expiration time on your GraphQL operations on a per role or global level so that you can avoid certain operations running for longer times.\ud83d\udc9c Community Demo by Gus FuneOur community champion, Gustavo Fune, joined us in May to talk about how they implemented authentication using Supertokens in Hasura at Off Script.Here are a few things we loved about Gus's demo:Implementing Auth with Supertokens has rotating JWT keys. To avoid having to update their keys on their end, they implemented authentication using Webhooks on Hasura.To configure the Webhook on Hasura, they initially tested out lambdas and API gateways. However, they decided to build their own server to solve for latency in their Webhook configuration. This turned out to be faster than some of the other JWT providers out there.They open sourced their implementation that follows the Webhook pattern on their dedicated server. Check it out!The hasura-supertokens implementation has out-of-the-box support for role based access control with Hasura. Hasura checks with hasura-supertokens for a session handle to make sure the user id exists and changes in Hasura are reflected in real-time without refreshing the keys.You can return custom session variables for your production instances with the hasura-supertokens implementation. They're working on a tutorial that teaches how to do that, so keep your eyes peeled for it!\ud83d\udca5 The next community call: HasuraCon\u201921 \ud83d\udca5Hasura\u2019s annual user conference is back! HasuraCon'21, our 'Biggest Community Call of The Year', is happening on 23-24 June, 2021. Agenda is live, check it out and register to join us for 2 days of exciting talks and in-depth workshops.If you've built something with (or for) Hasura, we'd love for you to showcase this in an upcoming community call. Let us know through this form. Lastly, we'd appreciate your feedback so that we can make our community calls better.See you at HasuraCon'21!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building Brave Care using Hasura: A case study of a YC backed healthcare startup Description: Brave Care, a YC backed healthcare startup that provides primary, urgent & remote care for kids, recently raised $10M. Asa Miller, CTO & Co-founder, speaks about how Hasura helped them build & scale rapidly. Author: Rajoshi"}, {"role": "assistant", "content": "Brave Care is a YC backed healthcare startup that provides primary, urgent & remote care for kids. Brave Care runs in person & mobile clinics with a team of front line medical workers. Brave Care recently announced a 10M funding round with which they are planning to expand to building more clinics across several states in the United States. When COVID hit in 2020, the Brave Care team rapidly launched telehealth visits and got licensed in 14 states. They became the first company to release an online COVID symptom checker and helped thousands of kids get their COVID tests. We recently spoke to Asa Miller, CTO and co-founder at Brave Care to talk about how they're using Hasura to build and scale rapidly. We have gotten a lot done as a small team. While looking at other companies of our size, we have a much smaller engineering team, and that\u2019s largely because of Hasura and Hasura Cloud - Asa MillerApplications & Tech Stack at Brave CareBrave Care has built software for facilitating online appointment bookings, visitor management, records storage and management as well as mobile apps with real time chat functionality to support their patients and their caregivers.Brave Care uses React Native, TypeScript, Postgres and Hasura as their tech stack across all of their applications. All of their business logic is hosted as serverless functions on AWS Lambda. Adopting Hasura at Brave CareBrave Care had already decided to use GraphQL, and even built a few custom GraphQL services. Their applications were initially built using a NoSQL database. As they were building out more functionality, they needed more out of their backend.\"Our lead engineer spent a weekend trying out Hasura on a personal project. He really liked it and came back to the team to share what Hasura did and why he thought it could be a good fit for us. This was early in the Hasura development process so we were a little nervous about that, but after talking with Hasura's team we felt like it had a strong future. We were able to stand up a quick test and build some of our backend with it.\"Hasura is open source at its core, so the team knows there is no lock-in, and not being locked into Hasura also gave the team the confidence to move ahead with Hasura.Brave Care started incrementally adopting Hasura by bringing their custom built GraphQL servers into Hasura with remote schemas. They later switched over to use Hasura Actions to delegate custom business logic to their custom GraphQL servers. Prior to Hasura Cloud, Brave Care was hosting Hasura themselves on a Kubernetes cluster. They\u2019ve now moved their stack over to Hasura Cloud so as to not worry about operating & scaling Hasura.Brave Care's Architecture Diagram with HasuraBenefits that Brave Care has observed from using HasuraDeveloper Productivity & Time to MarketHasura lets the team focus on the parts of the application that differentiates Brave Care from their competitors. The redundant code that needs to be written time and again for building secure APIs is reduced drastically saving the team a lot of time.Brave Care uses React Native and TypeScript heavily. With Hasura, all their developers are able to work across the stack and write TypeScript end-to-end, on the front end as well as on the back end.Hasura removed the need to write CRUD logic. At least 50% of our application backend is just CRUD operations and it\u2019s good to not have to write individual endpoints for each operation.2. Role Based Access Control:Hasura comes with extremely granular role based access control which removes the need to write permission logic. This allows Brave Care to not have access control logic in their code, and not have to worry about the security aspects of accessing data. They get to set up different authorization and permission rules, and assign it to the relevant roles from right within the Hasura Console.3. Flexibility of Custom LogicHasura doesn\u2019t restrict how you write your custom business logic. This allows Brave Care the flexibility of writing custom logic in any framework or language as a custom server or a lambda. Another common pattern at Brave Care is to use Hasura event triggers in a reactive programming model to react to data changes, and fire other logic as required.Brave Care uses event triggers to handle things like sending email messages, recording events into their analytics, and communicating with hardware in the clinics.4. No OpsIn a typical backend stack, there are multiple services to install, configure and manage, including an app server, an ORM and an API gateway. Hasura reduces the ops requirement drastically by collapsing these three layers into one Hasura layer, and Hasura Cloud goes further and eliminates the ops requirement completely by fully managing this Hasura layer. SummaryThe adoption of GraphQL in general, and Hasura Cloud in specific, has been of great benefit for a company like Brave Care. Could it be the same for you? Get started in 30 seconds on Hasura and let us know."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Adding GraphQL Using Hasura To An Existing FinTech App to Speed Up Development by Pamela Gotti of Credimi @HasuraCon'20 Description: Pamela Gotti spoke last year at HasuraCon'20 on how Credimi shifted from REST APIs to GraphQL APIs with Hasura and sped up their development. Author: Hasura"}, {"role": "assistant", "content": "Pamela Gotti spoke last year at HasuraCon'20 on how Credimi shifted from REST APIs to GraphQL APIs with Hasura and sped up their development.As HasuraCon'21 is almost here, we thought it would be the perfect time to take a look back on all the amazing talks by our amazing speakers from 2020!You can sign up for HasuraCon'21 here!Here's Pamela's talk in video form:Adding GraphQL Using Hasura To An Existing FinTech App to Speed Up Development by Pamela Gotti of Credimi @HasuraCon'20And below is a text transcription of the talk for those who prefer reading! (The text is slightly paraphrased and edited here and there for clarity's sake.)TRANSCRIPTION:Hello, everybody, I'm Pamela. I'm a senior software engineer in Credimi - that's a startup based in Milan - and then CTO of She Tech Italy - that's a community always based in Milan, whose mission is to empower women in tech. Today, I'm here to talk to you about how we have used Hasura in Credimi, for helping us to develop new features. What is CredimiAnd just gonna take a couple of minutes to tell you what Credimi is. We are a startup born in 2016, in Milan, and we are in FinTech. Our main businesses are invoice financing and medium-long term loans. And we have loaned so far 930 million Euros and that made us the leader digital lender in continental Europe. I'm going to tell you today how we have used Hasura to improve the time for us to deploy feature in production, as well as how we have used the GraphQL to improve the developer experience by allowing our developers to focus in delivering on business value. A Time for a TaleBut before I go into detail, I'm going to tell you a story. So this is a PM, a PM in Credimi that arrives one day, he has this brilliant idea that's going to make our customers very happy. And of course, the developers are really excited to jump on the project to start working immediately. And they are ready to eventually release the feature to production. So release date, everybody's happy, our customers are happy, everybody is celebrating. But the day after the PM comes back, he has yet another idea to make our customer even happier if it's possible. But we have to, as developers, we have to get rid of half the code we have written, we have to remove half of the feature. And we have to deploy yet another change that we were not able to anticipate. The developers know that they're going to delete a lot of line of code and that they're going to write some boilerplate over and over again. So they're not quite happy this time. What the developers needSo basically, when you're working at a startup, of course, you have to deliver your features as soon as possible to deliver nice user experiences to your customers. And from a developer point of view, that means that you need some versatile tools that allow you to build on what you already have, without necessarily go back, get rid of what you've done, and then building something up that maybe we need to be brought back again. And that also means that the developer needs to focus on delivering value, they just don't want to spend time reading and writing code all over the places. To argument this, I'm gonna tell you a little bit about how Credimi used to work.How Credimi Used to WorkThis is Credimi 1.0 we're talking about two years and a half ago, we used to have a bunch of microservices written in Scala mostly, we had an Angular JS application client-facing that was interacting with the first layer of backend written in Nodejs. And the complexity of all the microservices was hidden by this Kubernetes Ingress that was taking all the requests from the node and was redirecting them to the right microservice. So I am talking about Kubernetes here, all our microservices are deployed as a Docker image in a separate pod, and its microservice has its own database. And the communication back then was mainly relying on REST APIs. What does this mean? This means we have several problems. Problems with Credimi 1.0The first one being boilerplate. Whenever we needed to add a new API, we needed to write a lot of code from down to the database up to the REST layer. And then also, we had to remember for example, to add the API at the Nginx level, that's the Ingress I was talking about. Otherwise, the API would not be reachable. That's the point that had a lot of boilerplate to write. There was no schema shared between the backend and the front end. So even a small typo was giving us problems when developing the new feature, it was kind of tricky to understand where the communication was breaking. Authorization, why am I talking about authorization because I told you guys that we were doing invoice financing, and after a couple of years, the invoice resource - remember that we are doing, if you're doing a REST back then the inverse resources are huge - we have almost 60 fields on it. And every time maybe the front end would query the API, the front end would get the full resource back. That means that even if the front end needed 2 or 3 fields, it was still getting the full resource. And if any of those 60 fields was not necessary to the client, or maybe it was not secure to be shown to the client, we had to rearrange the API so that the users, our final user, could see only the data that they needed to see. And at the same time, this was resulting in a heavy infrastructure. Because at the end of the day to be this huge invoice, we were querying a lot of tables, we were left joining, aggregating, and some of our APIs were taking a lot to execute, we are talking about minutes. And this is, of course, it's not good for the customers. Credimi was EvolvingIn the meanwhile, Credimi was evolving, we got new people in the team, we got new skills, we got new ideas, and we started improving our infrastructure, adding new microservices, and also adopting new languages. We started with Python, we also added Rust. And at the same certain point, also Kafka came into the stack.We started using Kafka as, of course, the aggregator of events, and we started switching from a REST API communication model towards a more event sourcing communication model. And we saw an opportunity here. The OpportunitySo when we started adopting Kafka, it was clear that our write side was behaving really great. But our read side was kind of suffering from using REST APIs and from the fact that all our data were scattered around all the microservices. So we started thinking about taking all the events we were putting on Kafka, and to materialize them into a database. In this way, we could really start using CQRS, so we could also potentially have a different read side for each user type that was querying our APIs. And at the same time, we also wanted to modernize the stack. We heard a lot about GraphQL, and how it potentially could solve our issues with, as I told by the boilerplate authorization and heavy queries. Credimi 2.1 - Exposing the Read SideSo the first step was to build the read side database, you can see Postgres database in there is where we actually, we currently materialize all the events that we put on Kafka. And at this point, we started to try to understand how to expose that data to our front end. And, of course, we could have written the server ourselves, but we prefer to rely on somebody that already had experience with GraphQL.So we were looking for a tool that was easy to integrate with our architecture. So something that was Dockerized, and that could just take the image and to plug it into our Kubernetes cluster. We wanted something that was easy to work with, I don't want my new resource in the team to be stuck waiting for somebody else to fix the server or to expose a new field. I wanted, everybody can just with a couple of configurations would have what they need. And then we also found in time that it's nice to work with a community that is able to share what they are doing with you and to start a collaboration. Using Hasura - Credimi 3.0So eventually, we decided to go with Hasura. And this is more or less how Credimi works at the moment.We still have some REST API here and there. But more or less all the microservices even if there are no ROS in there are communicating with Kafka, they are publishing events. Those events are then materialized on our website that is exposed through Hasura to our front end. How Development happens at Credimi NowAnd I'm going to tell you a little bit about how the developer experience looks like now.So the very first step that developers do, they meet up and they start to talk about the kinds of resources that they need to develop the new feature. So they try to understand if the resources they need are already there, if they need additional resources, additional fields, if they need additional relationships between the tables, and if they need to expose any new fields with some particular layer of authorization. Basically, they're building the graph of the resources. And this point is really important, because thanks to this, the developer can also get a grasp of the domain they're working into, and they can bring better value to the feature they are developing.So the first step, instead kinda like, usually, in Credimi the, at this point, the front end developers, I put a cup of coffee there, because literally, they could wait for half an hour, so then the environment is all set up. Or they also could mock the answer, then the response that the server would give back to them and start to work immediately, but just pretend for now that they just went to grab a cup of coffee. And the backend developers starts to bring up a new environment where they can both work together. We have our CI/CD tool that can set up a new namespace on our Kubernetes cluster that is totally isolated from the other developing environments that we have up at the moment. So the namespace is going to have all the microservices that are needed to develop the feature, a dedicated database that is going to act like the read side database we have in production, and the Hasura instance that is going to expose that read side.A Look at Credimi\u2019s Repo with HasuraTo do that, I'm going to show you the repository that we actually use for bringing up Hasura in a new namespace. This is of course, just an extract of our repository. But you can see on the left-hand side, at the top level of the tree, there's this deployment of YAML file that tells our CI/CD pipeline, what it needs to do. The first thing our pipeline does, of course, is referred to is to run tests. You can see that in the tree, there are both Hasura folder and the migrations folder. The migrations folder, of course, contains all the migrations that needs to go on the rich side database. And the reason why we have them in the same repository is because we, are assure in this way that Hasura never breaks and that is always aligned with the version of Postgres we expect. So the first thing with the CI/CD pipelines does we say it is to run the test. How does he do it? He brings up a Docker composition, one that has a Postgres database where all the migrations are applied, and another container is going to run Hasura and then the tests are going to be run.Note that when we started using Hasura, Hasura still didn't have integration tests, integrating the tool. So we wrote our own tests in Python. So we do have both tests for the migrations and tests for the authorization fields, that test in the Hasura folder, actually check that depending on the type of token we use for querying the server, we get back the fields we are supposed to get. So this is kind of like testing the authorization layer of Hasura. And the metadata dot JSON file, it will help us to bring up the Hasura exactly as it is in production. It is a data that defines all the relationships and the authorization at field level that builds up the graph. On the right hand side you can see the Docker image that actually runs Hasura. It is literally the, that is the file, we get the image from Hasura, then we copy over the metadata file, JSON, and that's it, we have Hasura exactly as it supposed to be, plugged into our read side database.After the tests have run, our CI/CD pipeline runs the migrations, it brings up Hasura, and at that point, we do have a namespace in Kubernetes, that is an exact replica of what we have in production. Then at this point to go on with the development process. If there's a need of any additional migration, the backend developer writes the migration, runs the new migration.And then if anything needs to be added on the GraphQL schema, the backend developer modifies the metadata, that JSON file that I have showed you before. Or she can go on the Hasura UI and she can check that just clicking around making the tables visible, the columns visible to a specific user, then she exports the metadata file and then at that point, that file's version and this is also helpful in phases of code review. Whenever a pull request is open, whoever reviews that code can clearly see what has changed in terms of permissions on the metadata that are exposed by Hasura.At this point, we have a namespace that is totally set up. Again, it's ready for the development. The front end developer comes back, uses Apollo to introspect the graph on Hasura. And at this point, he can literally start writing code, whereas the back end developer can focus on some more fun stuff to do. Note that at this point, if the PM comes back with yet another idea, and he decides that he doesn't want anymore to show data x, y, z to the customer, but he wants instead of to show a, b, and c, the front end developer is totally able to modify the metadata JSON file itself without waiting for the backend developer to modify the server. LearningsSo this is basically the conclusion of the talk. What I tried to explain is that following this process, our developers have a better domain comprehension. And they can have really a great comprehension of the big picture. So they can really focus on delivering value. And they stopped writing most of the boilerplate they used to write. We are able to deliver features faster and why is that because thanks to the first step, the domain comprehension, we have a resource graph that is well defined and well understood from the developer. If the PM asks for modification, then it's trivial to go on the Hasura UI or to modify directly the graph. So the new data are exposed, performance are improved. Remember, at the beginning, I was explaining that we had this huge resources that were creating a lot of tables in a not efficient way. Now, if the front end only has 3 of those 60 fields that we used to expose them Hasura holds the queries, only the tables that are involved, and APIs that used to take minutes now take milliseconds.And last but not least, the authorization, I don't need to care anymore that the user don't need to see certain fields of the invoice resource, I can just expose them all. And then at the authorization layer, I'm going to expose, to the, our customers only the fields they need to see and I am able to discriminate on the kind of customer using the token that is used to create a server.Thanks!And that's it for the process we use in Credimi if you have any questions feel free to reach out (@pamela_gotti, pamela.gotti@credimi.com), and thank you for tuning in."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Architecture & Authorization For A Complex Multi-Tenant SaaS Platform With Hasura | Prefect | by Zachary Hughes @HasuraCon'20 Description: Zachary Hughes spoke last year at HasuraCon'20 on how Prefect's complex multi-tenant architecture and authorization is built with Hasura. Author: Hasura"}, {"role": "assistant", "content": "Zachary Hughes spoke last year at HasuraCon'20 on how Prefect's complex multi-tenant architecture and authorization is built with Hasura.As HasuraCon'21 is almost here, we thought it would be the perfect time to take a look back on all the amazing talks by our amazing speakers from 2020!You can sign up for HasuraCon'21 here!Here's Zachary's talk in video form:Architecture & Authorization For A Complex Multi-Tenant SaaS Platform With Hasura | Prefect by Zachary Hughes @HasuraCon'20And below is a text transcription of the talk for those who prefer reading! (The text is slightly paraphrased and edited here and there for clarity's sake.)TRANSCRIPTION:Hey folks, thanks for coming. Today we're going to talk a little bit about architecture and authorization modeling for a complex multi-tenant SaaS platform. And why using Hasura to achieve this is a Hasura-fire way.A bit about meSo for those of you who didn't hit \u201cleave meeting\u201d as soon as you heard that awful pun, let's get some introductions out of the way. My name is Zachary Hughes. I'm based in Washington DC, and I'm a cloud engineer at a company called Prefect. Here's a little bit of contact information about myself. And in case you want to reach me.The Agenda for todayWhat are we going to be talking about today? We're going to start by talking a little bit about Prefect. I know it's HasuraCon and not PrefectCon, but context is helpful to understanding what Prefect is trying to do helps explain why we do what we do with Hasura. Once we've gotten that out of the way, we'll talk a little bit about the architecture and how Hasura fits in with Prefect's overall design.Following that, we'll talk a little bit about using Hasura, both internally and externally. So when people hit our API versus how we develop with it, and at once that foundation is set, we'll talk a little bit about Auth, that's gonna include roles, multi-tenancy, permissions, all that good stuff.About PrefectA little bit about Prefect, let's dive in.At a really high-level Prefect is just responsible for making sure that workflow is executed properly. For any of y'all in the audience who might be fans of Hitchhiker's Guide to the Galaxy, the name is 100% inspired by that wonderful roving researcher Ford Prefect. It's a little bit of an homage.Prefect CorePrefect at the moment has two main components: Prefect Core, which is workflow management, that takes your code and transforms it into a robust distributed pipeline that we call flow, and Prefect Cloud, which is the layer of abstraction and orchestration on top of that.This is an API as well as a UI that provides additional orchestration, observability, and auth and IAM data pipeline. Basically, all that stuff that you don't really have to worry about. We call that stuff negative engineering.Prefect CloudDiving a little bit more deeply in Prefect Cloud, as we mentioned, it's a UI and an API that lets us manage scheduling your flows, which are those pipelines we discussed, helps you monitor and maintain the health of flows, and persist information about flow execution history.This includes logs and flow states and flow states are basically just indicators of the history of the flow: when it started running, whether it failed or succeeded, any sort of information like that. And the great thing is, a significant portion of that is covered by Hasura.Prefect's ArchitectureGiven that, let's talk a little bit about the architecture and how sort of fits into it.Here's a high-level diagram of Prefect's architecture, it's a little bit pared down, but I don't want to bore you with the details. As you can see most of our compute is located in a GKE cluster. So let's start with the beginning.We have an inbound request with the JWT in Header. That seems a little bit specific, but it's going to be important for our auth. Once the request hits upon the server, the Apollo server then talks to our auth service, verifies the token, it populates a role in a new JWT.Once that new JWT has reached the Apollo server, it can take one of two paths. If the inbound request was in a query, you send it directly to Hasura. Or if it's a mutation, it takes a little bit of a detour. This detour takes it to Python business logic. It's bundled with an ORM. We'll get into the details of that a little bit later.But the important part to consider right now is that once everything is said and done, this middleware posts any request to Hasura, and then everything ends up in Cloud SQL on the back end.JWT ContentsNow digging a bit more into JWTs that we discussed earlier, it's important to consider that there are two pieces or three pieces of content that are really important here.We have a user ID, which indicates a specific user associated with this token, we have a tenant ID, which indicates the team this token belongs to. And we have a role, which indicates the user's role in Hasura.Example, before & after auth serviceSo interestingly enough, when we first get our token, it doesn't have a role, we dynamically populate that. So if you notice, in this example, we have a tenant ID and a user ID, which we would expect, but we don't have a role.However, once the Apollo server gets that auth that it does have a role. Seems like sort of a detail, but it's going to be important as far as how we manage dynamic roles.Using HasuraLet's move on to how we actually use Hasura.Consuming the Prefect API is pretty easy, and really anytime we consume the Prefect API, we consume the Hasura API. So as we discussed earlier, queries were sent directly to Hasura after the JWT is auth'd. Mutations take that detour that we discussed which goes to the Python business logic we've defined. And then any interaction between that business logic and Hasura is done via the ORM.As far as humans go, everything is consolidated in a column, so the user has a seamless experience. Everyone just hits API.prefect.io and they're good to go. And then is half mean-half serious: and the, you know, I have tried each of these three approaches, they're each as painful as they sound. And we're getting a little bit more into the detail. But it's been really delightful letting Hasura handle all of this stuff.The ORMSpeaking of the ORM, let's give it a little bit of attention, because it is super nice, and it helps improve the productivity of our internal dev experience. The ORM is just a lightweight layer in front of Hasura. It's implemented in Python, the actual serialization logic there's identic, and it lets us interact with Hasura in a convenient manner.Rather than having to open up the session, post everything, every time you want to talk to Hasura, we just use our ORM. Another nice thing about this is that the ORM operates within a token context that's populated by the JWT we've been discussing. This sounds like another implementation detail, but it's actually really nice because by using the ORM in conjunction with the token context, it means that every time we send something to Hasura using the ORM, we have pre-populated tenant IDs and user IDs for the roles and the header. This lets us sort of delegate our permissions work to Hasura without having to handle anything else on the front end.Moving over, here are just some samples of what this actual ORM looks like. Our query would look fairly simple and lightweight: models.Flow.where we define our WHERE clause. Let's say something like name is equal to fool, and then we just get it. Similarly, if you wanted to insert something, we'd say models.Flow, define our actual flow, name through here are the tasks inside the flow, yada, yada. And then we'd go ahead and insert it.AuthSo with that foundation layer, let's talk a little bit about auth.Our auth model has three main components. We have users, who represent individual consumers and cloud - sounds a little bit silly to define that. But we also have a user role. We have memberships, which link a user's role to a team. And then we have teams, which are effectively tenants, many groups of people working with the same flows, have access to the same resources, like flows, projects, all that good stuff.Users and tenants both have unique IDs, we use UUIDs. And these IDs are used to enforce role-based permissions. They both make liberal use of Hasura relationships. This is partially in order to just help improve the querying experience. It's also partially to help improve data integrity. And it's worth noting that users and tenants are many to many of us model, users can belong to one or more team tenant and switch it as well. And tenants can have one or more user.ExamplesSo with that out of the way, I know it's a lot of text, let's take a little bit of a look at an example. Let's check out our flow.tenant ID. Our flow table has a certain ID column, which maps to the ID column or a tenant. This helps us enforce our row permissions, which are here where we check to make sure that our tenant ID is equal to the X-Hasura-tenant ID. That x-Hasura-tenant ID is part of what's passed through by our ORM or populated directly if it was passed through by a query.RolesSo roles, these are important. Users have access to four primary roles.We have a login role, which is sort of invisible, allows users to view information necessary to find and select their membership. Once they've found and selected their membership, we have three remaining roles.We have a read-only user, just sort of what it says on the tin. It allows the user to view everything in a given tenant, but not update or insert anything. It's sort of an observer/auditor-type role.We have users who are sort of day-to-day folks using the system. This gives the user everything they need to do in order to operate in that tenant. That means they can view things, they can kick off and create flows, create projects, update states, whatever you'd like.And then we have kind of admins who give the user the ability to actively manage everything in that tenant. This includes things like billing. It also includes managing things like who actually belongs to a tenant. If you have a bad actor, then admins have the ability to remove them.We also have a fifth role called runner, which is less crucial to how users actually interact with the system, but it's important for our orchestration, we're just gonna leave that off for the time being.Roles ExampleAs an example, let's take a look at this. You can see right off the bat that our permissions are actually fairly granular, and they're not an all standard. Certain things have permissions for everything. Certain things have no permissions.This is our membership table. So let's see who can do what. A user has permission to delete a membership that matches their user ID. This makes a fair amount of sense, right, because a user should be able to remove themselves from a tenant if they would like. But they shouldn't be able to actually remove someone else.On the other hand, a tenant admin who is responsible for managing the tenant as a whole has permission to remove any membership that matches their tenant ID, which gives them a bit more power. This is a really nice, clean illustration of how you can actually give tenant and user permissions, and use roles in conjunction with them to help that intersection be a little bit more manageable.Switching TenantsSo with all this in mind, switching tenants is actually super easy. And a key part of what we do as far as multi-tenancy goes. With this pattern in place that we've discussed, switching tenants is just as easy as issuing a new JWT. You issue a new JWT, that populates the new context, which defines the user ID, the tenant ID, and the role. And then once that's all said and done, you can move on to another tenant.No need to write custom permissions & auth with HasuraSo something I want to call out here is using roles and role-based permissions lets us delegate permissions in Hasura which sounds sort of obvious, but it's something I know I take for granted since I've been working with Hasura for long enough. I've written custom permissions before as well as custom auth, and it is so painful. I would officially like to never write one again. I am sure there are folks out there who are great at it. And I'm sure there are folks out there who might even enjoy it. It's not really where I'd like to focus my attention.Views can help usBut there is one gotcha. What if you want to allow folks with the user role to view other users in their tenant. It's a little bit tricky. In this case, the pattern we've approached with is views. So we can define a view and map memberships and roles and permissions do them, the same way we would any other table. So by defining views alongside your tables, you can define an entirely separate set of permissions to fit your needs. In this case, we want users to be able to see email, first, last name of other users, but no more. And by using views and defining a separate set of permissions there, it allows us to take the union of what they can do.Parting ThoughtsSo I'd like to leave with a couple of parting thoughts. The big thing is between its GraphQL capabilities and flexible permission schema, Hasura roughly halves the amount of time it would take us to add and expose a data model. I cannot say just how nice it is to be able to implement a feature, create our new model, define how users will query for it, and have that be the easiest part of our system. We basically define our permissions, we make sure that our tenant guards are in place, and we're good to go.Thanks!And that about wraps it up. I really appreciate your attendance. I'll be sticking around to answer any questions and if you want to contact me, do so at zachary@prefect.io\u200c\u200c\u200c\u200c"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura 2.0.0-alpha.11 Description: We are super excited to announce the final alpha in the 2.0 series with 2.0.0-alpha.11.\n\nIncluding multiple bug fixes, SQL Server Explain support, and more... Author: Tyler Hannan"}, {"role": "assistant", "content": "We are super excited to announce the final alpha in the 2.0 series with 2.0 alpha 11.While \u2018alpha\u2019 in name, it is not alpha in quality. However, we do use this signifier to denote that it is a work in progress and invite feedback about any problems you may experience. Hasura 2.0.0-alpha.11 docker quickstartHasura 2.0.0-alpha.11 changelogGetting Started with Hasura CloudA Summary of ReleasesAs mentioned in recent release posts, we have historically announced our releases via Discord and updates to the GitHub release page. As adoption continues to increase, we believe this is insufficient for the future. To that end, we will begin to blog for each release (even if it is only small) so that you can track releases via blog, twitter (@HasuraHQ), Discord, and the Github Release page. In these posts, we will highlight a few specific elements of each release, but the exhaustive list of features and improvements is always in the changelog.OverviewBreaking ChangeIn this release, the name of the computed field argument has changed from <function_name>_args to <computed_field_name>_<table_name>_args as the function name is internal detail for a computed field. This change also enables adding a root-level tracked function as a computed field which previously would have thrown input type conflict error.Allow tables with the same name in different sourcesIn previous releases there was a bug that prevented tables with the same name but from multiple sources. If you have the same table names from different contexts, you can now customise one of them to have a different name from the Modify page in Hasura consoleSQL Server Explain APIYou\u2019ll also now be able to use the explain feature for SQL Server queries and subscriptions.And more...Those are, of course, just the highlights. All other fixes, and there are many, are included in the changelog.ConclusionPlease use the docker target `hasura/graphql-engine:v2.0.0-alpha.11`, try it out, let us know what works and what needs improvement. You can find us on Twitter (@HasuraHQ) and in Github. For ad-hoc conversation, feel free to join us on the Hasura Community Discord.And, if you want to get started now with the latest stable release, check out the deployment simplicity of Hasura Cloud.Previous PostsFor previous release announcements in the 2.x series, please review:Announcing Hasura 2.0.0-alpha.8Announcing Hasura 2.0.0-alpha.9Announcing Hasura 2.0.0-alpha.10"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: YCombinator Startup Athlane Uses Hasura Cloud to Increase Their Developer Velocity 200% Description: Athlane, an alumnus of YCombinator 2019, provides esports athletes and gamers a marketing, analytics, and e-commerce platform.  \nUtilizing Hasura Cloud and GraphQL,they were able to quickly scale and enable their team of five developers to quickly build the leading platform in the space.   Author: Steven Yi"}, {"role": "assistant", "content": "Athlane, an alumnus of YCombinator 2019, provides esports athletes and gamers a marketing, analytics, and e-commerce platform to connect content creators to engage and monetize from brands, sponsors and streaming communities. \u00a0Utilizing Hasura Cloud and GraphQL, this fast-moving startup was able to quickly scale to cater to the global esports market and enable their team of five developers to quickly build the leading platform in the space. \u00a0\u201cHasura increased our velocity as a development team by 2x\u201dAfter migrating away from Firebase, Hasura provides the GraphQL-based data access layer used by all its services, and secures and authorizes all the API requests. \u00a0Hasura also synchronizes their overall serverless infrastructure by utilizing Hasura Event Actions to initiate webhooks to other services. \u00a0Additionally, Athlane built their interpersonal messaging feature utilizing Hasura\u2019s full support for GraphQL subscriptions.Additional benefits included:Doubling the delivery velocity of their development teamProviding an easy integration with their codebase and expertise in Node.js and VueEliminating the time necessary to write and maintain inflexible REST-based data access APIsHasura Cloud auto-scales to meet user demand, auto-upgrades to the latest releases, and eliminates administration and system maintenance\u201cHasura is a core part of all our web applications\u201dAthlane\u2019s ChallengeAthlane realized they needed to make some architectural changes to scale to an increasing number of users, and increase the efficiency of their small development team. \u00a0 They migrated their data store from Firebase to Postgres because they needed the capabilities of a relational database. \u00a0The next challenge Athlane faced was how to build their data access layer. \u00a0They quickly realized that developing a REST-based architecture would consume a large amount of developer time and resources that could rather be spent working on the entire application stack and quickly creating new user features. \u00a0Athlane\u2019s backend systems were authored in Node.js and their website frontend in Vue. \u00a0They needed to find a solution that was compatible with those frameworks and additional constraints and technical requirements:Small Development Team: Athlane\u2019s 6 engineers needed to stay lean and flexible - understanding all layers of their system - database, backend, integrations, and the frontend web UIFlexible Web APIs: One of the things they wanted to avoid is the inflexibility of utilizing and maintaining REST APIs, where multiple variations of similar request types would require its own REST endpoint because of differing end-user requirements.Serverless Architecture: Athlane\u2019s small development team made a commitment to use a serverless architecture to reduce the administration and operational requirements as much as possibleSecurity & Authorization: They needed to make sure that the data was secure and that users could only view data they were authorized to seeWhy Athlane Chose Hasura CloudAfter considering multiple options, including writing their data access API layer themselves, Athlane chose Hasura Cloud for its comprehensive GraphQL capabilities, ease of use, security, and because it didn\u2019t require maintenance as a managed service.\u201cHasura\u2019s GraphQL capabilities unlocked the ability for everyone on the team to be a full-stack developer\u201dEase of Use and EfficiencyUtilizing Hasura and its GraphQL engine made it easier for every engineer to understand the flow of data from the database to their frontend, making it possible for every team member to operate as a full stack developer and add features quickly. \u00a0Connecting their Vue frontend to Hasura was simple with existing open-source GraphQL libraries.Hasura\u2019s intuitive tools and documentation made it easy to set up and connect to their Postgres database, instantly auto-generating GraphQL schemas and resolvers based on the tables and views in their database. \u00a0This saved them development and maintenance time not having to author any more of their own data access APIs themselves and not having to modify the code every time they made a database change.In-App Messaging Powered By GraphQL SubscriptionsOne of the main features of Athlane\u2019s website is the ability for users to message each other. \u00a0Athlane implemented this utilizing Hasura\u2019s full support for GraphQL subscriptions, giving Athlane a solution to provide real-time notifications and messages between users.Managed Service by Hasura CloudHasura Cloud provides a managed service, providing auto-scaling to meet spikes in traffic, and auto-upgrades to the latest versions. \u00a0This no-ops approach fit into Athlane\u2019s desire to use a serverless architecture as much as possible.Data SecurityAnother benefit Hasura provided is its built-in security and authorization capabilities, which can limit what data is returned down to the individual column and row level. \u00a0This capability saved on development time implementing it themselves and ensured that customers could only view their own data.Event Triggers & WebhooksAthlane also utilized Hasura Event Triggers and webhooks to provide real-time integrations to multiple AWS Lambda functions they\u2019re also using. \u00a0Thank you to Athlane and Ryan Jiang - CTO for sharing their Hasura story!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Creating a GraphQL API for Notion with Hasura Actions Description: Notion launched their REST API recently letting you connect your pages and databases to create powerful workflows. In this post, we will leverage the Notion API to create a GraphQL wrapper for one API endpoint. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Notion launched their REST API recently letting you connect your pages and databases with any tool to create powerful workflows. In this post, we will leverage the Notion API to create a GraphQL wrapper for their pages API endpoint. This will use Hasura Actions to define GraphQL types and we will be writing an Action webhook handler that will make the call to Notion's API.Convert Notion REST API to GraphQL API using Hasura ActionsLet's get started by setting up things on Notion's side for the API integration.Create a workspace in Notion You need to be an admin of a workspace to be able to use the APIs. If you are an admin in an existing workspace, you can skip this step. Let's get started by creating a new personal workspace by following the docs on Creating a personal workspace.Create a new integration in NotionThe next step is to create an integration. Head to My integrations and create a new integration with relevant name and workspace.Create a new integration on NotionAfter submitting the above form, note down the Internal Integration secret. This will be used during the API integration. The integration type will be internal. There's also the option of making it external so that the API is public and can be used by anyone. For this example, we will stick to internal since the Hasura Action will be used only for one private workspace.Sharing a page / databaseAccording to Notion docs, Integrations don't have access to any pages (or databases) in the workspace at first. A user must share specific pages with an integration in order for those pages to be accessed using the APIThis means, the API can be used only for those pages that already exist and has the right permissions (sharing) added to be used by the API.Share a page with IntegrationNote that, you need to create a database as a full page to be able to use the API. You can do this by typing in /database and selecting Table - Full Page.Head to Share on the top menu and click on Copy link. Paste this somewhere to identify the ID of the newly created database.For example, it will look something along the lines of https://www.notion.so/0664caf380ec482d92b74b01730480f9?v=8b0f9cba5e72499380fcb251369f5b4bwhere the database ID is the one before the query parameter. Note down the database ID because we will reference that in the API request for making changes.Deploy Hasura to get a GraphQL APIClick on the following button to deploy GraphQL engine on Hasura Cloud.\n2. Open the Hasura console by clicking on the button \"Launch console\".3. Head to the Data tab and click on Create Heroku Database to create a new Postgres database. Note that this is an optional step. We will not use the database in this example since we are writing an Action to create a GraphQL wrapper over Notion.Create Heroku Database4. Now let's create an Action. Head to Actions tab from the top menu and click on Create to create a new action.5. We will add a custom mutation to add a new item to the DB. The mutation type would look something like this:type Mutation {\n  addNotionItem (\n    databaseId: String!\n    properties: jsonb!\n    children: jsonb\n  ): ItemOutput\n}and the output type for ItemOutput would look something like this:type ItemOutput {\n  id : String!\n  object : String!\n  created_time : timestamptz!\n  last_edited_time : timestamptz!\n}Note: You can modify the above type defintions to include more fields as required or return a more generic JSON so that everything from Notion's response can be returned back as output.Leave the other inputs as it is and we will come back to modifying the handler URL later.Action TypesNotion API to add item to DBNotion's API reference gives the various endpoints and the request body that can be sent to make changes.To add an item to the DB, we can make use of the following APIAPI Endpoint - https://api.notion.com/v1/pagesHeaders - Authorization: Bearer <integration_token>Request Body{\n    \"parent\": { \"database_id\": <database_id> },\n    \"properties\": {\n      \"Name\": {\n        \"title\": [\n          {\n            \"text\": {\n              \"content\": \"iPhone 12\"\n            }\n          }\n        ]\n      }\n    }\n  }The above request body assumes you have the default Name field in your table.Writing and Deploying Action CodeOnce the Action is created with the above types, head to the Codegen tab and click on Try on glitch. This will create an app on Glitch platform with the Node.js boilerplate. We need to add an env variable in the .env file. Add the env NOTION_TOKEN and paste the Integration token that we obtained at the beginning of this post.Here's the code for the new endpoint that you can paste in the Glitch app source code under src/server.js.app.post('/addNotionItem', async (req, res) => {\n\n  // get request input\n  const { databaseId, properties, children } = req.body.input;\n\n  // run some business logic\n  const NOTION_TOKEN = process.env.NOTION_TOKEN;\n  const NOTION_PAGES_ENDPOINT = \"https://api.notion.com/v1/pages\";\n  const headers = {\n    \"Authorization\": `Bearer ${NOTION_TOKEN}`,\n    \"Content-Type\": \"application/json\"\n  };\n  \n  const reqBody = {\n    \"parent\": { \"database_id\": databaseId },\n    \"properties\": properties\n  }\n  \n  if(children) {\n    reqBody[\"children\"] = children;\n  }\n  \n  const options = {\n    method: 'POST',\n    headers: headers,\n    body: JSON.stringify(reqBody)\n  }\n  \n  const fetchResponse = await fetch(NOTION_PAGES_ENDPOINT, options);\n  const responseJson = await fetchResponse.json();\n  \n  console.log(responseJson);\n\n  /*\n  // In case of errors:\n  return res.status(400).json({\n    message: \"error happened\"\n  })\n  */\n\n  // success\n  return res.json({\n    id: responseJson.id,\n    object: responseJson.object,\n    created_time: responseJson.created_time,\n    last_edited_time: responseJson.last_edited_time\n  })\n\n});\nHasura Action Handler CodeOnce this Action code is updated on the Glitch app, copy the Glitch app URL and add the /addNotionItem to that and modify the Action URL to point to the glitch endpoint.An example glitch endpoint would look something like this https://lovely-bustling-environment.glitch.me/addNotionItemAction Handler URLTest the GraphQL APIHead to Hasura Console API Explorer and make the following GraphQL mutation:mutation addNotionItem ($databaseId: String!, $properties: jsonb!) {\n  addNotionItem(databaseId: $databaseId, properties: $properties) {\n    id\n    object\n    created_time\n    last_edited_time\n  }\n}With the following variables:{\n  \"databaseId\": \"<database_id>\",\n  \"properties\": {\n    \"Name\": {\n        \"title\": [\n          {\n            \"text\": {\n              \"content\": \"iPhone 12\"\n            }\n          }\n        ]\n      }\n  }\n}Replace the <database_id> with your own database ID.GraphiQLYou should get a response which looks something like:{\n  \"data\": {\n    \"addNotionItem\": {\n      \"id\": \"4bf24dc3-8a98-4247-856c-ba15500082cc\",\n      \"object\": \"page\",\n      \"created_time\": \"2021-05-21T12:57:46.751Z\",\n      \"last_edited_time\": \"2021-05-21T12:57:46.751Z\"\n    }\n  }\n}GraphQL Response for Notion MutationAdding PermissionsYou can also define permissions for this Action so that you can restrict or allow access only to certain roles.Head to the Permissions tab of the created Action and allow access for specific roles.Permission for ActionsWe have written a GraphQL wrapper for Notion's Create a page API. Now of course you can repeat this for any of the Notion's REST API by defining custom types appropriately and writing the webhook handler."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Graphqurl v1.0: curl for GraphQL Description: Graphqurl is a curl like CLI for GraphQL. Its features include:\n* CLI for making GraphQL queries.\n* Run a custom GraphiQL locally against any endpoint.\n* Can be used as a library with Node.js or from the browser.\n* Supports subscriptions over websocket.\n Author: Rishichandra Wawhal"}, {"role": "assistant", "content": "\nGraphqurl is a curl like CLI for GraphQL. Its features include:CLI for making GraphQL queries. It also provisions queries with autocomplete.Run a custom GraphiQL, where you can specify request's headers, locally against any endpoint.Can be used as a library with Node.js or from the browser.Supports subscriptions over websocket.Can export GraphQL schema(s).What's new in v1.0?Reduced bundle sizeGraphqurl, until v0.3.3, used Apollo client under the hood for making GraphQL queries and subscriptions. This caused the bundle size to explode considerably (142kB).With v1.0, Graphqurl uses light-weight isomorphic HTTP clients which reduced the bundle size by over 80kB, i.e by a whopping 56%. It is now an \u00a0ideal tool for use cases where users wish to make GraphQL queries, mutations and subscriptions without any extra overhead of caching and networking. This is the bundle composition of graphqurl v0.3.3 as compared to graphqurl v1.0.1:Note that the size of the CLI binary is not considered in the above bundle size distribution. That said, the CLI binary in v1.0.1 has also been optimised and is about 300kB lighter than that of v0.3.3.Improved GraphiQLYou can run a custom Graphiql in graphqurl. In v1.0 we have extended it to include popular community tools like:Graphiql ExplorerGraphql Code ExporterYou can also run graphiql on host and port of your choice by using the --graphiqlHost and --graphiqlPort flags on the CLI.Improved scripting API (breaking)The parsing of GraphQL queries before execution, has been removed because it is unnecessary for a scripting tool like graphqurl, as most users build GraphQL queries through auto-completion tools like GraphiQL, which highly reduce the possibility of syntax errors. This means, the syntax errors in your GraphQL queries, if any, shall not be caught before the query actually hits the GraphQL server.If you've been using graphqurl in node scripts and in the browser, the API has changed considerably. The usage of graphqurl as a node library v1.0 onwards involves creating a client followed by executing GraphQL operations, whereas until v0.3.3, you could import a query function and execute it.Please find the updated API documentation in the README and a sample script in the graphqurl repository.The complete changelog can be found here for complete list of changes in the latest release. What next?Moving forward, we have the following things on the roadmap: TypeScript supportAllow refreshing websocket connection if auth session expiresAllow subscribing to a GraphQL query through pollingPlease let us know what else you'd like to see on the roadmap. We are open to contributions, you can read our contributing guide to help you get started. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: How Pulley Cut Development Time by 50% with Hasura Description: Hasura helped Pulley cut development time by 50% by auto-generating and securing GraphQL for over 50 database tables Author: Steven Yi"}, {"role": "assistant", "content": "Hasura helped Pulley cut development time by 50% by auto-generating and securing GraphQL for over 50 database tablesNote: Pulley is providing a generous discount to Hasura Cloud and enterprise customers - see the end of this article for more details.Pulley, an alumnus of YCombinator 2020, helps ambitious founders manage their cap table and employee equity, helping startups manage stock options and perform pro forma modeling to optimize their equity. \u00a0In its first year of existence, Pulley achieved rapid success, gaining 1,000 customers by enabling founders to manage the intimidating process of managing their cap table within an hour.\u201cBy using Hasura we cut the development time in half and built our product in 3 months.\u201dPulley decided that Hasura\u2019s GraphQL engine would accelerate their development by auto-generating the GraphQL schemas and resolvers to their Postgres database. They also leveraged other Hasura features, including:Data migrations when database changes occurredSecurity and permissions model to keep sensitive data safeFast performance, resulting in a excellent user experiencePulley's architecture includes React, a Go backend, and Hasura's GraphQL engine connected to a Postgres database.Pulley\u2019s ChallengePulley\u2019s technical architecture was composed of a Postgres database, backend written in Go, and web front end using React. \u00a0As they iterated quickly in the beginning they needed to find the balance between rapidly making changes, adding features, while also making sure the solution kept very sensitive financial data secure. \u00a0Other challenges included:Creating a universal data access layer for over 50 database tablesMaking frequent database changes without having to rewrite significant amounts of data access, backend, and frontend codePulley helps founders managed their cap table and stock options to optimize their equity.Why Pulley Chose Hasura\u201cWe\u2019ve been impressed with Hasura\u2019s speed. \u00a0It\u2019s a core part of why our overall app is fast\u201dEarly in the development process, the Pulley team decided GraphQL would offer the benefits of easily transporting data between the different layers of their application - from the Postgres database, to their Go backend, and React frontend. \u00a0They adopted Hasura after discovering it met and exceeded their expectations:Auto-Generated GraphQL Schemas and ResolversHasura provided \u201cGraphQL out of the box\u201d with its ability to auto-generate GraphQL schemas and resolvers of their Postgres database, saving Pulley months of development time - especially in their early stages when changes to features and the underlying database happened frequently. \u00a0Pulley utilizes Hasura for nearly all queries and updates of over 50 database tables.Data MigrationsHasura\u2019s migrations feature was also essential during the early phases of Pulley\u2019s product development, tracking Postgres database schema changes in metadata, allowing them to keep their database consistent between development and production.Real-Time Updates via GraphQL SubscriptionsSubscriptions simplified their frontend development, utilizing React components to to dynamically update their web frontend as new data arrived. \u00a0This eased the burden of having to write their own notification framework to provide real time updates.\u201cHasura\u2019s built-in role-based authorization system made it easy to secure our data\u201dSecurity, Authorization & AuditingTo protect its customer\u2019s sensitive financial and personal data, Pulley relied on Hasura\u2019s granular authorization features, which can be applied down to the row and column level. \u00a0To ensure the security of their system, Pulley regularly performs penetration testing to make sure that their entire system, including Hasura, are hardened from external attack.Pulley also relies on Hasura\u2019s built-in auditing capabilities to track changes in user permissions, providing them a history log of changes they can refer back to.Management ConsoleHasura\u2019s console also increased the developer\u2019s productivity by making it easy to perform GraphQL schema generation tasks, perform GraphQL queries, and perform traces to identify any performance bottlenecks. Thank you to Pulley and Mark Erdmann - Software Engineer for sharing their story with us!Special offer! Pulley is offering a 25% discount ($750 value) for the first year to Hasura Cloud and Enterprise customers. \u00a0Please contact us on this form, mention this offer and include your Hasura login email! "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: What's new in Hasura Cloud: May 2021 Description: Hasura Cloud gives you a fully managed, production ready GraphQL API as a service to help you build modern apps faster. You can get started in 30 seconds. Recently, along with the usual stability improvements, we introduced new regions, and tooling for better observability. Author: Jesse Martin"}, {"role": "assistant", "content": "Hasura Cloud gives you a fully managed, production ready GraphQL API as a service to help you build modern apps faster. You can get started in 30 seconds. Of course, we aren't staying still but frequently releasing new versions. Recently, along with the usual stability improvements, we introduced new regions, and tooling for better observability. Namely, these features are:Regions in the US west, east, and Asia PacificAn updated monitoring dashboardLet\u2019s have a look at each of these in more detail.Hasura Cloud now in Oregon, Virginia, and TokyoThis release brings three new regions for Hasura:US West - Oregon (us-west-2)US East - N. Virginia (us-east-1)Asia Pacific - Tokyo (ap-northeast-1)In our continued commitment to serving more customers in more places, we are happy to offer these three new regions to bring a faster Hasura Cloud experience, wherever you are.All the important metrics in a single viewWe\u2019ve refreshed the monitoring tab\u2019s general appearance as well as introduced a much more informative landing screen. View all the most vital statistics in a glance, with quick access to multi-db performance measurements.Monitoring in Hasura CloudConclusionOf course, as you experience the simplicity of Hasura Cloud you can find us on Twitter (@HasuraHQ) and, for ad-hoc conversation, feel free to join us on the Hasura Community Discord. If you want an overview of the process of using Hasura Cloud, our Getting Started with Hasura Cloud docs are the place to start. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building Applications with CloudFlare Workers and Hasura GraphQL Engine Description: Today, we explore using Cloudflare Workers to implement JWT auth and role-based content access to a relational database. Author: Gavin Ray"}, {"role": "assistant", "content": "\nCloudflare Workers and Hasura\n\nIntroduction\n\nCheck out the Finished Product\n\n\nInitial CF Worker Scaffold\nInitial Hasura Setup\nImplementing JWT Auth and Role-based Content Access via CF Workers + Hasura Actions\n\nSetting up Authorization in Hasura\nSetting up Authentication with Cloudflare Workers\nDeploying to Cloudflare Workers\n\n\nWriting a frontend, testing it all out\n\n\n\nIntroduction\nModern applications (and the migrations of legacy applications) come in all shapes, sizes, and -- importantly -- deployment patterns. And, yet, the database tends toward a traditional monolithic infrastructure deployment. When the user experience, and code that powers it, have to scale a new challenge emerges.\nState management.\nIf the application needs to connect to a relational database, if the application needs to handle connection pooling, if the application requires state management (like data triggers and scheduled triggers)...a new deployment paradigm is required.\nWhile we have these sorts of conversations regularly with our Hasura users...today we explore using Cloudflare Workers to implement JWT auth and role-based content access to a relational database. Follow along, try it out, give us feedback.\nYou'll learn how to:\n\nWrite your own JWT Auth service for authenticating users to Hasura\nIntegrate it into Hasura's GraphQL API using Hasura Actions\nDeploy the auth service as a serverless function with Cloudlare Workers\nUse Hasura's permissions system to control access to records in your database\nWrite a small React frontend which allows users to sign up + log in, and see private data from Hasura\n\nBy the end of it, we hope that you'll walk away with:\n\nA better understanding of the application of Cloudflare workers\nThe fundamental Hasura knowledge to build and be productive\nA sense of accomplishment and motivation that will inspire you to take these skills, and build something awesome out of them\n\nCheck out the Finished Product\nYou can view the code and live frontend we'll be building, hooked up to Hasura and the CF Worker, here in your browser:\n\nhttps://codesandbox.io/s/trusting-leakey-d4u4k?file=/src/App.tsx\n\nThe source code for these projects and the content for this blogpost are also available on our Github, at:\n\nhttps://github.com/hasura/cloudflare-workers-blogpost\n\nInitial CF Worker Scaffold\n\nRegister at https://dash.cloudflare.com/sign-up/workers\n\n\nFrom the Workers dashboard, follow the link to the documentation on setting up the Wrangler CLI\n\n\nComplete the steps listed, finally generating a new TypeScript worker with the command:\n\n$ wrangler generate my-worker https://github.com/cloudflare/worker-typescript-template\n\nInside of the my-worker directory that was created, run npm install, and then run wrangler dev to start the local development service\nIf you visit http://127.0.0.1:8787, you should see the response from the starter code like below:\n\nNote that you can use wrangler preview to run your Worker against a test Cloudflare HTTPS URL to more accurately simulate a working environment:\n$ wrangler preview\nOpened a link in your default browser: https://cloudflareworkers.com/?hide_editor#35cf785b73bcdc52e3a1aaf581867ecd:https://example.com/\nYour Worker responded with: request method: GET\n\nInitial Hasura Setup\n\nCreate an account at Hasura Cloud, and generate a new free-tier project\n\n\n\nAfter the project is created, it will bring you to your project settings page. Click \"Launch Dashboard\" at the top right to open the Hasura web console\n\n\n\nFrom the Hasura web console, you will be greeted with a getting-started menu. At the top of this menu, select \"Connect your first Database\". (If this menu is not present, or you have dismissed it, you may also click the \"DATA\" tab at the top of the page)\n\n\n\nClick \"Connect Database\"\n\n\n\nSwitch tabs on the page you're navigated to, from the default \"Connect existing Database\" to \"Create Heroku Database (Free)\". Then press the \"Create Database\" button to automatically create and connect a Hobby-tier Heroku DB to your Hasura Cloud instance (requires a Heroku account).\n\n\nImplementing JWT Auth and Role-based Content Access via CF Workers + Hasura Actions\nNow we'll look at how to use Cloudflare Workers to authenticate users and gate/control access to content in our database.\nThere are two things that we need to do accomplish this:\n\n\n\nTell Hasura what role unauthenticated/anonymous clients should be given, for use in permissions\n\n\nImplement some form of authentication (Hasura is unopinionated about how this is done -- we just have to return a JWT token with the right claims shape)\n\n\n\nSetting up Authorization in Hasura\nTo accomplish the first part, we simply need to change an ENV variable on our Cloud instance.\nLet's set HASURA_GRAPHQL_UNAUTHORIZED_ROLE to anonymous from the Cloud dashboard settings page:\nNow, all requests made by users without a valid authentication token will be set to the role anonymous inside of Hasura.\nFor the second part, all we need is a Cloudflare Worker endpoint that returns a signed JSON Web Token in the shape that Hasura requires.\n\nSee: https://hasura.io/docs/latest/graphql/core/auth/authentication/jwt.html#the-spec for more info\n\nTo enable JWT authentication, we have to tell Hasura about what the right signing key for our JWT is, so that it can check the validity of JWT's sent to it in Authorization: headers during requests.\nTo do this, we have to add an environment variable from the same settings page we're on now. Click the + New Env Var button, and fill it out like below:\n{\n  \"type\": \"HS256\",\n  \"key\": \"this-is-a-generic-HS256-secret-key-and-you-should-really-change-it\"\n}\n\nNow our Hasura Cloud application is configured with the ability to authenticate users, and defaults all requests to anonymous unless properly auth'ed!\nLastly, we need to create a database table, and set up some permissions so that only authorized users can see it.\nFirst, we will create our user table. Let's do this one with the \"SQL Editor\":\nCREATE TABLE user (\n    id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n    email text NOT NULL UNIQUE,\n    password text NOT NULL\n);\n\nThen, we'll create a table private_table_of_awesome_stuff in Hasura, and set permissions so that user can select the rows, but not anonymous:\nNow to verify this, we need to insert a few test rows into private_table_of_awesome_stuff, and try to query it both as role anonymous and as role user:\nAt this point, we've successfully configured our Hasura instance with authorization and confirmed this is so!\nThe final step is to write & deploy our Cloudflare Worker handler that performs authentication, and then create and wire-up a small frontend to test everything is as it should be.\nSetting up Authentication with Cloudflare Workers\nIntegrating Authentication with Hasura is straightforward. Hasura doesn't have opinions on HOW you authenticate clients, only the response object your Auth endpoint sends back.\nOr, in less words: Hasura is flexible.\n\nNote: In the case of \"Webhook\" mode, this will be a JSON object. In the \"JWT\" mode we'll be demonstrating here, this is also a JSON object -- just with a specific object key and then signed using jwt.sign()\n\nWe should consult the spec for authentication in the JWT mode:\nhttps://hasura.io/docs/latest/graphql/core/auth/authentication/jwt.html#the-spec\nThe important items have been hightlighted below:\nKnowing this, then a basic example of a JWT auth server implementation running as a Worker, with two endpoints:\n\n/signup for registering users\n/login for authenticating users\n\nLooks like the below:\nimport * as jwt from \"jsonwebtoken\"\n\n// You would use an ENV var for this\nconst HASURA_ENDPOINT = \"https://<my-hasura-app>.hasura.app/v1/graphql\"\n// You can set up \"Backend Only\" mutations, or use a secret header or a service account for this\n// Do not do this in a real application please\nconst HASURA_ADMIN_SECRET = \"please-dont-actually-do-this\"\n\nconst CORS_HEADERS = {\n  \"Access-Control-Allow-Origin\": \"*\",\n  \"Access-Control-Allow-Methods\": \"GET,HEAD,POST,OPTIONS\",\n  \"Access-Control-Max-Age\": \"86400\",\n}\n\ninterface User {\n  id: number\n  email: string\n  password: string\n}\n\n////////////////////////////////////////////////////////\n// AUTH STUFF\n////////////////////////////////////////////////////////\n\nfunction generateHasuraJWT(user: User) {\n  // Really poor pseudo-example of AuthZ logic\n  const isAdmin = user.email == \"admin@site.com\"\n\n  const claims = {} as any\n  claims[\"https://hasura.io/jwt/claims\"] = {\n    \"x-hasura-allowed-roles\": isAdmin ? [\"admin\", \"user\"] : [\"user\"],\n    \"x-hasura-default-role\": isAdmin ? \"admin\" : \"user\",\n    \"x-hasura-user-id\": String(user.id),\n  }\n\n  // Don't do this, read the key from an environment var via \"process.env\"\n  const secret =\n    \"this-is-a-generic-HS256-secret-key-and-you-should-really-change-it\"\n  return jwt.sign(claims, secret, { algorithm: \"HS256\" })\n}\n\n////////////////////////////////////////////////////////\n// ROUTE HANDLER STUFF\n////////////////////////////////////////////////////////\n\nfunction makeHasuraError(code: string, message: string) {\n  return new Response(JSON.stringify({ message, code }), {\n    status: 400,\n    headers: CORS_HEADERS,\n  })\n}\n\nasync function handleSignup(req: Request) {\n  const payload = await req.json()\n  const params = payload.input.args\n\n  // Here you would store the password hashed, you would hash-compare when logging a user in\n  // params.password = await bcrypt.hash(params.password)\n  const gqlRequest = await fetch(HASURA_ENDPOINT, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      \"X-Hasura-Admin-Secret\": HASURA_ADMIN_SECRET,\n    },\n    body: JSON.stringify({\n      query: `\n      mutation Signup($email: String!, $password: String!) {\n        insert_user_one(object: {\n          email: $email,\n          password: $password\n        }) {\n          id\n          email\n          password\n        }\n      }\n      `,\n      variables: {\n        email: params.email,\n        password: params.password,\n      },\n    }),\n  })\n  const gqlResponse = await gqlRequest.json()\n\n  const user = gqlResponse.data.insert_user_one\n  if (!user)\n    return makeHasuraError(\n      \"auth/error-inserting-user\",\n      \"Failed to create new user\"\n    )\n\n  const jwtToken = generateHasuraJWT(user as User)\n  return new Response(JSON.stringify({ token: jwtToken }), {\n    status: 200,\n    headers: CORS_HEADERS,\n  })\n}\n\nasync function handleLogin(req: Request) {\n  const payload = await req.json()\n  const params = payload.input.args\n\n  const gqlRequest = await fetch(HASURA_ENDPOINT, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      \"X-Hasura-Admin-Secret\": HASURA_ADMIN_SECRET,\n    },\n    body: JSON.stringify({\n      query: `\n      query FindUserByEmail($email: String!) {\n        user(where: { email: { _eq: $email } }) {\n          id\n          email\n          password\n        }\n      }\n      `,\n      variables: {\n        email: params.email,\n      },\n    }),\n  })\n  const gqlResponse = await gqlRequest.json()\n\n  const user = gqlResponse.data.user[0]\n  // if (!user) <handle case of no user created and return an error here>\n  // check that user.password (hashed) successfully compares against plaintext password\n  if (params.password != user.password)\n    return makeHasuraError(\"auth/invalid-credentials\", \"Wrong credentials\")\n\n  const jwtToken = generateHasuraJWT(user as User)\n  return new Response(JSON.stringify({ token: jwtToken }), {\n    status: 200,\n    headers: CORS_HEADERS,\n  })\n}\n\n////////////////////////////////////////////////////////\n// MAIN\n////////////////////////////////////////////////////////\n\nexport async function handleRequest(request: Request): Promise<Response> {\n  const url = new URL(request.url)\n  console.log(\"url.pathname=\", url.pathname)\n\n  switch (url.pathname) {\n    case \"/signup\":\n      return handleSignup(request)\n    case \"/login\":\n      return handleLogin(request)\n    default:\n      return new Response(`request method: ${request.method}`, {\n        status: 200,\n        headers: CORS_HEADERS,\n      })\n  }\n}\n\nAs a test that this is working, we can send requests to the /signup and /login endpoints.\n\nYou can use curl/wget to do this, or Postman/Insomnia etc., but here we'll just use a browser devtools and fetch() to make the request:\n\n\nEnsure that your local Worker is still running via wrangler dev\nRun the following in your browser devtools\n\n\nvar req = await fetch(\"http://127.0.0.1:8787/signup\", {\n  method: \"POST\",\n  headers: {\n    contentType: \"application/json\",\n  },\n  body: JSON.stringify({\n    input: {\n      args: {\n        email: \"newuser@site.com\",\n        password: \"mypassword\",\n      },\n    },\n  }),\n})\nvar res = await req.json()\nconsole.log(res)\n\n\n\nvar req = await fetch(\"http://127.0.0.1:8787/login\", {\n  method: \"POST\",\n  headers: {\n    contentType: \"application/json\",\n  },\n  body: JSON.stringify({\n    input: {\n      args: {\n        email: \"newuser@site.com\",\n        password: \"mypassword\",\n      },\n    },\n  }),\n})\nvar res = await req.json()\nconsole.log(res)\n\n\n\nvar req = await fetch(\"http://127.0.0.1:8787/login\", {\n  method: \"POST\",\n  headers: {\n    contentType: \"application/json\",\n  },\n  body: JSON.stringify({\n    input: {\n      args: {\n        email: \"newuser@site.com\",\n        password: \"WRONGPASSWORD\",\n      },\n    },\n  }),\n})\nvar res = await req.json()\nconsole.log(res)\n\n\n\n\n\nYou should get:\nDeploying to Cloudflare Workers\nTo deploy your working API and turn it into a live, production endpoint, we just need to use the Wrangler CLI to run the publish command.\nThat looks like this (run from inside of the directory containing your Worker code and TOML config):\n$ wrangler publish\n\n\nNote: You'll need to make sure that you have authenticated using the Wrangler CLI which will configure your wrangler.toml with necessary values\nTo do this, you can run wrangler login, and then wrangler whoami to verify your credentials are correct\n\nWriting a frontend, testing it all out\nBelow is an overly-simplified example of frontend code that consumes this CF Worker auth API and uses the JWT bearer token to authenticate the client to Hasura, to fetch private data.\nimport React, { useEffect, useRef, useState } from \"react\"\nimport \"./App.css\"\n\nconst HASURA_ENDPOINT = \"https://<your-cloud-app>.hasura.app/v1/graphql\"\n\nfunction App() {\n  const form = useRef(null)\n  const [jwt, setJWT] = useState(\"\")\n  const [isLoggingIn, setIsLoggingIn] = useState(false)\n  const [isSigningUp, setIsSigningUp] = useState(false)\n  const [privateStuff, setPrivateStuff] = useState<any>([])\n\n  useEffect(() => {\n    if (!jwt) return\n    fetchPrivateStuff().then(setPrivateStuff)\n  }, [jwt])\n\n  async function signup(email: string, password: string) {\n    const req = await fetch(HASURA_ENDPOINT, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n      },\n      body: JSON.stringify({\n        query: `\n          mutation Signup($email: String!, $password: String!) {\n            signup(args: { email: $email, password: $password }) {\n              token\n            }\n          }\n        `,\n        variables: {\n          email,\n          password,\n        },\n      }),\n    })\n    const res = await req.json()\n    const token = res?.data?.signup?.token\n    if (!token) alert(\"Signup failed\")\n    setJWT(token)\n  }\n\n  async function login(email: string, password: string) {\n    const req = await fetch(HASURA_ENDPOINT, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n      },\n      body: JSON.stringify({\n        query: `\n          mutation Login($email: String!, $password: String!) {\n            login(args: { email: $email, password: $password }) {\n              token\n            }\n          }\n        `,\n        variables: {\n          email,\n          password,\n        },\n      }),\n    })\n    const res = await req.json()\n    const token = res?.data?.login?.token\n    if (!token) alert(\"Login failed\")\n    setJWT(token)\n  }\n\n  async function fetchPrivateStuff() {\n    const req = await fetch(HASURA_ENDPOINT, {\n      method: \"POST\",\n      headers: {\n        Authorization: `Bearer ${jwt}`,\n        \"Content-Type\": \"application/json\",\n      },\n      body: JSON.stringify({\n        query: `\n        query AllPrivateStuff {\n          private_table_of_awesome_stuff {\n            id\n            something\n          }\n        }\n        `,\n      }),\n    })\n    const res = await req.json()\n    const data = res?.data?.private_table_of_awesome_stuff\n    if (!data) alert(\"Couldn't retrieve any records\")\n    return data\n  }\n\n  if (!jwt) {\n    if (isSigningUp || isLoggingIn)\n      return (\n        <form\n          ref={form}\n          onSubmit={(e) => {\n            e.preventDefault()\n            const data = new FormData(form.current!)\n            const email = data.get(\"email\") as string\n            const password = data.get(\"password\") as string\n            if (isLoggingIn) return login(email, password)\n            if (isSigningUp) return signup(email, password)\n          }}\n        >\n          <input name=\"email\" type=\"email\" placeholder=\"email\" />\n          <input name=\"password\" type=\"password\" placeholder=\"password\" />\n          <button type=\"submit\">Submit</button>\n        </form>\n      )\n    else\n      return (\n        <div>\n          <p>Please sign up or log in</p>\n          <button onClick={() => setIsSigningUp(true)}>\n            Click here to sign up\n          </button>\n          <button onClick={() => setIsLoggingIn(true)}>\n            Click here to log in\n          </button>\n        </div>\n      )\n  }\n\n  return (\n    <div>\n      <p>Here is a list of private stuff only authenticated users can see:</p>\n      <ul>\n        {privateStuff?.map((it: any) => (\n          <li>\n            {it.id}: {it.something}\n          </li>\n        ))}\n      </ul>\n    </div>\n  )\n}\n\nexport default App\n\nIf we run this, and then sign up/log in as a user, we should see:\nAnd we can use the Hasura web console (by entering this same value for the Authorization header, and then clicking the inspect/decode icon that shows on the far-right) to look at the claims of our token:\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How TypeScript template literal types helped us with multiple database support Description: Last year November's release of TypeScript was one of the most exciting releases. Literal types took over the TypeScript community by opening a whole new set of possibilities. Author: Aleksandra"}, {"role": "assistant", "content": "Last year November's release of TypeScript was one of the most exciting releases. Literal types took over the TypeScript community by opening a whole new set of possibilities. The community created many amazing projects: router parameter parsing, JSON parser, GraphQL typed AST, SQL query validation, CSS parsing, games, a database implemented purely with TypeScript types, and many more. At Hasura, we recently had an interesting use case as well. But firstly \u2014 what are the literal template types?A few words on template literal typesWe already had Template literals in JavaScript that provide a convenient way to have values inside of strings. For example:const variant = `button_${size}_${color}`\nIn TypeScript, the type of the above value would be a string, which is fair, but can we have more? Imagine that we know exactly those values because there are only two possible sizes and three possible colours. We could do this:type Size = \"small\" | \"big\"\ntype Color = \"green\" | \"red\" | \"white\"\n\ntype Variant = \n  | \"button_small_green\" \n  | \"button_small_red\" \n  | \"button_small_white\" \n  | \"button_big_green\"\n  | \"button_big_red\"\n  | \"button_big_white\"\nBetter, but... maintaining the Variant type seems like a nightmare, doesn't it? Every time we extend either Color or Size, we need to add new literals to our Variant union. That's when TypeScript 4.1 comes to the rescue. We can use template syntax not only for values, but also for types! Thanks to template literal types we can simplify the above code:type Size = \"small\" | \"big\"\ntype Color = \"green\" | \"red\" | \"white\"\n\ntype Variant = `button_${Size}_${Color}`Isn\u2019t it great? Now, let's see why we needed template types and what's our use case.Multiple database support in the Hasura ConsoleIn the Hasura 2.0 release, we introduced support for multiple databases. You can not only connect many databases to your Hasura, but also you can connect different databases! Postgres, BigQuery, MS SQL Server, and soon more. They now can all be managed via Hasura Console. However, each of them supports a different set of features. For example, some DDL functionalities are disabled for Big Query. MS SQL Server handles fetching data differently. A few features are not yet supported for a particular database. There are a lot of cases to handle! We could do this:if (currentDatabase === \"postgres\") {\n   // render a feature\nelse {\n  // don't\n}orconst supportedDatabases = [\"bigquery\", \"postgres\"]\n\nconst Component = ({ currentDatabase }) => {\n  if (!supportedDatabases.includes(currentDatabase) {\n     return null\n  }\n  // ...\n}But I think we can agree that it won't scale well. Especially if we need to handle those cases in dozens of components. And when we add a new database, we'd need to go over it again.We needed some abstraction for that. Basically we needed something like this:if (isFeatureSupported('tables.browse.enabled')) {\n  return (\n    <BrowseData {...props} />\n  );\n}andconst supportedDrivers = getSupportedDrivers('actions.relationships');One source of truth to declare enabled features for each database that can be extended in any way and multiple levels deep.A function that, for a given feature, will tell us if it's supported based on the currently active database.A function that, for a given feature, will tell us all the supported databases (needed when filtering out unsupported databases).Our source of truth can be extended in any way. It can be multiple levels deep. It can be nested as well.Here's an example of a database\u2019s features configuration:export const supportedFeatures = {\n  driver: {\n    name: 'mssql',\n  },\n  tables: {\n    create: {\n      enabled: false,\n    },\n    browse: {\n      enabled: true,\n      customPagination: true,\n      aggregation: false,\n    },\n    ...\n  },\n  events: {\n    triggers: {\n      enabled: true,\n      create: true,\n      edit: false,\n    },\n  },\n  ...\n};While the isFeatureSupported and getSupportedDrivers functions are fairly straightforward to implement, it's the developer experience that we needed to be enhanced. We didn\u2019t want to type tables.browse.enabled by hand. We didn\u2019t want to check the supported features object every time to see the shape. We wanted to have it autocompleted!After all, they don't make those memes without reason.Taking advantage of template literal typesWe used template literal types to have a type-safe string dot notation. The following types Path and PathValue allowed us to have a function get that accepts an object and a path as string dot notation and returns a values under the path. type Path<T, Key extends keyof T = keyof T> =\n  (Key extends string\n  ? T[Key] extends Record<string, unknown>\n    ? | `${Key}.${Path<T[Key], Exclude<keyof T[Key], keyof Array<unknown>>> & string}`\n      | `${Key}.${Exclude<keyof T[Key], keyof Array<unknown>> & string}`\n      | Key\n    : never\n  : never)\n\ntype PathValue<T, P extends Path<T>> =\n  P extends `${infer Key}.${infer Rest}`\n  ? Key extends keyof T\n    ? Rest extends Path<T[Key]>\n      ? PathValue<T[Key], Rest>\n      : never\n    : never\n  : P extends keyof T\n    ? T[P]\n    : never;\n\ndeclare function get<T, P extends Path<T>>(obj: T, path: P): PathValue<T, P>;I won\u2019t go into the details of the above code. There are already plenty of amazing articles explaining how template types work. I link to a few of them at the bottom of this blogpost. I encourage you to check them out!You can also play with the above code here: \u00a0https://tsplay.dev/mL9dbWWith the Path and PathValue types and get function we were able to implement our database utilities as:export const isFeatureSupported = (feature: Path<typeof supportedFeatures>) => {\n  return get(dataSource.supportedFeatures, feature);\n};and export const getSupportedDatabases = (\n  feature: Path<SupportedFeaturesType>\n) => {\n  return databasesConfig\n    .filter(config => get(config, feature))    \n    .map(d => d.driver.name);\n};As the result we we have: a) type-safe dot string notation, b) autocomplete \ud83c\udf89SummaryAbout a year ago, we decided to adopt TypeScript, which definitely empowered us to write code with confidence. It not only allows us to write safer code but also hugely improves our developer experience. Template literal types were a big help when implementing multiple databases support, and we're excited to explore more use cases for them!Do you have any use cases for template literal types that you'd like to share? Let us know in a comment!Further reading:Learn more about TypeScript template literal types:TypeScript 4.1 announcement postI need to learn about TypeScript Template Literal TypesTypeScript: Template Literal TypesTypeScript Release Notes\u200b\u200bIf you are interested in learning more about how we addressed some of the other technical complexities of multiple database support, check out the post Building a GraphQL to SQL compiler on Postgres, MS SQL, and MySQL."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top GraphQL Errors and How to Fix them Description: Error handling in GraphQL is different on both server and client side tooling. You no longer are looking at HTTP status codes to debug what went wrong for the request made. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Top GraphQL Errors and How to Fix themError handling in GraphQL is different on both server and client side tooling. You no longer are looking at HTTP status codes to debug what went wrong for the request made. This makes one more involved in debugging errors, but thankfully there's a structure to the errors in GraphQL and we will look into it to see how to debug and fix them.Let's take a typical GraphQL query:query {\n  author {\n    id\n    name\n  }\n}The response of the above query can have the following objects: dataerrors or both.{\n  \"data\": [{ id: 1, name: \"Praveen\"}],\n  \"errors\": []\n}Top Level Errors Object in GraphQL ResponsesWhen the response contains the errors object along with the data object, it could mean a partially correct response for the request. Digging into the errors object will give a better idea into what part of the query went wrong.Identifying error types will help quickly fix them. Clients can encounter these types of errors while communicating with the GraphQL server.Server Problems / Network errorsAll the errors above come under the category of GraphQL errors with a response status code of 200. When there is a network error while trying to contact a GraphQL server, due to either the server being down or timeouts etc, then the response will be a status code of 4xx or 5xx.If the server responds anything other than 200, the response is not successful due to either being a:Bad request (400)Unauthorized (401)Gateway errors (5xx)These are non-ideal scenarios where the original response couldn't be delivered via 200 (despite there is a possibility of a request containing an error). This non-standard response needs to be checked with the server for underlying issue like network errors etc.Client side problems - Validation rules of GraphQL QueryGraphQL is strongly typed and it performs validation of all queries before executing them. It can reject a query if it is invalid based on a set of rules.The validation rules could again be split intomalformed querysyntax or schema logicvariables and fragments not defined properly.For example:{\n  \"errors\": [\n    {\n      \"extensions\": {\n        \"path\": \"$.variableValues\",\n        \"code\": \"validation-failed\"\n      },\n      \"message\": \"unexpected variables in variableValues: products\"\n    }\n  ]\n}unexpected variables in variableValues: idThis is a typical error that you get when a client makes a request with a variable that has not been defined. A common nightmare when using GUI tools like GraphiQL where the variables UI has remnants of variable usage from previous queries.Read the source code for validation rules in graphql-js server to understand various query validation that goes through before executing the query.Do note that validation errors never return any data to the client, the execution errors have the ability to return partial data sets. The operation could have been largely fulfilled but due to slow performance of a specific field or specific part of the query, it could result in partial data set.Top-level errorsThe default errors JSON includes message, locations and path. According to the working draft of the spec, it is also possible to send custom data in the extensions key.Field level errors{\n \u201cerrors\u201d: [{\n    \u201cextensions\u201d: {\n    \u201cpath\u201d: \u201c$.selectionSet.dogs.selectionSet.name\u201d,\n    \u201ccode\u201d: \u201cvalidation-failed\u201d\n   },\n   \u201cmessage\u201d: \u201cfield \\\u201dname\\\u201d not found in type: \u2018dogs\u2019\u201d\n }]\n}In the above response, you can see the extensions key inside errors object that contains more metadata on what went wrong. Technically this error came out of using a field which doesn't exist (either due to Auth or schema not really having the field).GraphQL Subscription ErrorsAnother common set of errors typically encountered while connecting with GraphQL servers comes when connecting to websocket connections for realtime subscriptions:websocket connection could not be initalised. This needs a check with the URL of the websocket connection; note that websocket URLs start with ws or wss and not https.invalid http upgrade (this could happen when http(s) is used for a websocket connection and the right connection split is not configured for ws)connectionParams not passed for authorization headers. If you are using a JWT token, it needs to be passed via connectionParams using the GraphQL Client.ResourcesHandling GraphQL Errors with React"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Shifting to Hasura: Lessons from a Rails developer by Avinash Moondra @HasuraCon'20 Description: Avinash Moondra spoke last year at HasuraCon'20 on how to shift from Rails to Hasura and what the difference and similarities are. Author: Hasura"}, {"role": "assistant", "content": "Avinash Moondra spoke last year at HasuraCon'20 on how to shift from Rails to Hasura and what the difference and similarities are.As HasuraCon'21 is almost here, we thought it would be the perfect time to take a look back on all the amazing talks by our amazing speakers from 2020!You can sign up for HasuraCon'21 here!Here's Avi's talk in video form:Shifting to Hasura: Lessons from a Rails developer by Avinash Moondra @HasuraCon'20And below is a text transcription of the talk for those who prefer reading! (The text is slightly paraphrased and edited here and there for clarity's sake.)TRANSCRIPTION:Hey, everyone, this is Avi. A bit about meI will be speaking on the topic of shifting to Hasura - lessons from a Rails developer. And I'll just get right into it because 10 minutes is a short time. So just some context on me. I'm a software engineer that's worked mostly in web startups so far. Rails vs Hasura (but not really!)Having used both Rails and Hasura, in fast-moving production environments, my goal here today is to help y'all shift your mindset from Rails to Hasura, without getting into which is better. So we'll highlight some major architectural differences and map some shared concepts, and hopefully, at the end of it, you'll have less surprises and more confidence as you get started with Hasura. Alright, let's get into it. Shifting from Rails to Hasura: Shift #1 - Monolithic MVC vs Event-Driven SOASo I'm gonna go over a couple of different what I call shifts.And the first one is, I would say the most important going from the Rails monolithic MVC architecture to a serious event-driven service-oriented architecture. So Rails covers, and seemingly always will, more web development surface area than Hasura. And not just because it has a 13 years head start. It sort of prides itself in the monolith, you know, everything from routing a request to fetching the data for that request and displaying that data in a user-friendly way, serving up the page, and everything else.Hasura, on the other hand, is a backend as a service, and it will cover sort of the common and exhaustingly sort of boring ORM and API server requirements, but you know, bring your own friends\u2019 event handlers and authentication table. It will come prepackaged with you in a similar value integrated systems sort of way with a programmable permissions framework, subscriptions, and then most recently scheduled triggers alongside triggers. Now, I kind of anticipate Hasura will quickly eat more of these common back-end concerns and abstractions. Anything from inbound/outbound email, database choice, and the management of the database for replicas and partitions is only two years old. And while Rails has sort of this, you know, equivalent abstractions to say, as subscriptions or triggers like action cable and active job, be it with, you know, Sidekick, or Rescue or whatever, that I would argue that these capabilities they integrate well with Rails, but are not native to its architecture. And this is a result of a sort of fundamental difference in the architecture of the data flow between Rails and Hasura. I think that's worth taking a closer look at.So let's first take a look at Rails in a sort of model view controller fashion. A controller receives RESTful requests from the outside world, the router interacts with the model to update the application state and prepares the appropriate view to the user. Side effects are sometimes handled inline. At other times, they're pushed to job queues. Another thing to note is the web workers, the job workers, and the pub/sub workers all run from the same shared codebase.Now we can, sort of, compare that to Hasura, whoops, compare that to Hasura, which you can see the request and the response cycle and the pub/sub share the same, sort of, specifically GraphQL API interface and infrastructure, and all the side effects are persisted as events to be processed asynchronously. And to process those events, webhooks, and other custom, sort of, business logic that you have, you're gonna need some services outside of Hasura to do that, which kind of lends itself to this sort of service-oriented architecture. And pub/sub and inventing, in general, is brought to the forefront to make an event-driven architecture. And a sort of useful way of looking at this difference is from a functional programming perspective, kind of.This is a screenshot from one of my favorite talks by Martin Kleppmann, and, kind of, what it covers is basically a story, that story that you can tell that the browser page, what the browser paints is just a function of what the template renders, which is just a function of the JSON data from your API, which is just a function of the relational data from your database. So if the entire application is just a function of our data, then we can run that function every time a state change has been made. And so that sort of mindset actually, is very incorporated into the core of Hasura. Actually, they describe the pattern as a three-factor app, which kind of has some analogies to Redux. And so you can check out the three-factor app for some more details on that. But I would also emphasize that GraphQL here is, not the difference. It's, you know, Firebase would be the restful equivalent, more so that, Rails centers around the sort of concepts of model view controller, MVC, REST on a request and response like, out and back. And Hasura kind of centers more around an event-driven architecture, GraphQL, and subscribe and notify more like a loop. And so that I would say is like the biggest difference and fighting the framework in either case, is sort of going against the Rails way or the Hasura way, you'll kind of find yourself having a less happy experience with either framework if you sort of don't incorporate these opinions or these kinds of fairly opinionated pieces of software. Shifting from Rails to Hasura: Shift #2 - REST vs GraphQLSo let's now jump to a second shift here, which is sort of REST vs GraphQL, and obviously, you'll be using GraphQL, when working with Hasura, and fighting that is pretty futile. And it's not just any GraphQL API, I'd call it sort of this perfectly defined API for you know, both, you know, it's auto-generation, we didn't write most of the code, but also for its expressiveness and compatibility with sub specifications like Relay. It also forces a sort of built-in command query separation, especially when workflow in your domain isn't fitting into CRUD, you can kind of safely set aside your sharpened Ruby knives when you're moving from Rails to Hasura.And so, on the left here, of course, is like your classic Rails, sort of, paths to doing different sort of actions. And on the right, is the query fields that you get just out of the box from Hasura. And there before diving deeper into those, it's important to note that, like, you can implement GraphQL in Rails, of course, really, anything is possible in Rails, and yeah, that's possible with the GraphQL Ruby gem plus API only controllers. But it's just not at the forefront of the Rails way, like REST and sort of CRUD is. And this is just a screenshot pulled from, you know, their Getting Started guides, of course. And instead of discussing further into how I would map you know, the left side here and the right side - I think there's plenty of coverage on how to shift your mindset from REST to GraphQL or REST-based CRUD to GraphQL - I wanted to, sort of, talk about two general points that relate to how applications talk to the database in the web world.And one of those, of course, is authentication, it's like a super important part of how clients make requests to servers. And in Rails, you have action dispatch session, or this sort of loved, hated, devise gem when working with Rails, but in Hasura, authentication is totally handled outside and they have just built-in ways to plug into it. And those standard ways are webhooks and JWTs or JSON Web Tokens. This position is, kind of, generally in line with the trend of the vendorisation of, sort of, authentication. And there's plenty of tutorials on Auth0, Auth Guardian, AWS Cognito, kind of any integration that you're looking for.The other bit is I wanted to talk about is in Rails, if some flow isn't feeling RESTful, or just you know, you want to, you need to get around the convention, it's really easy, you just, you just write more code. And it's pretty straightforward. But Hasura, we didn't write Hasura so how can we customize it? There are a couple of options, but custom actions is your simplest escape hatch when pushing up against the limits of CRUD GraphQL API, or you need to depend on, sort of, some libraries or, or some custom conditional logic. Some simple examples here would be through generating or returning a JWT, from an unauthenticated Hasura query field, right. So this is setting up your authentication endpoint. Or using the result of one complex mutation to make another. And it's not always perfect. Sometimes you have to work a little harder when you don't have a server that you wrote yourself. And an example here is getting what's the current time on the server and, and this is not as straightforward. You have to create a view here on in your Postgres database, and just get the now timestamp and return that as a view and expose that through the API, but it's all possible. And for the most part, CRUD will get you so far along the way, especially in the GraphQL context and the expressive API that they've given you. Shifting from Rails to Hasura: Shift #3 - ORM vs ?The third shift that I wanted to talk about was true of the ORM, which is an Active Record, is one of the most important, sort of, parts of Rails - it's the M in the MVC, and regardless of whether you're rolling your own front end or using GraphQL, you\u2019ll definitely be generating migrations, building and querying models or configuring validations and callbacks. So where is the ORM in Hasura?And the catch is there isn't one, at least in the same shape, size, or form. In Hasura we kind of experienced the ORM and the API layer as one. And from a CRUD perspective, Hasura GraphQL API provided out of the box is equally, if not more expressive abstraction over SQL and we can, kind of, look at some examples here. On the left side, here\u2019s some pretty standard Rails Active Record code. And on the right sort of the mapping that you can get, sort of, find, find by select filters, joins, nested, where clauses that are doing much more complex joins that you can, sort of, struggle to do with an Active Record, create or update or find your update, or find or create, update all pretty out of the box given to you from Hasura.And so that covers CRUD, what about all the other good parts of Active Record? I think there's some listed here, which, I won't get into all of them. But migrations, for example, largely inspired, I think, by Rails on the Hasura side, is supported out of the box. And instead of using a CLI and a DSL, that's like borderline ergonomic, instead, you get a user-friendly UI, and you can kind of generate all the up and down migrations as you need. The other bit is around JScripting, let's pick that as an example, just from the list here. And it's probably what I'll miss the most from Rails, and Ruby, and while GraphiQL like the interface where you can kind of type queries and test out and edit one-off queries, and from the Hasura UI, you know, the ability to like just jump into a Rails console, it's something I always have open, while developing in Rails and to kind of like, you know, get around this, I think there's two, kind of, at least one saving grace is that the backfill scripts or seed scripts in Hasura all communicate over HTTP API, so at least there's only one system that you had to learn versus learning, you know, the API or end ORM, at the same time. And the other saving grace, as a complement to that, is Hasura is super fast. So you wouldn't have to really worry about performance here. In general, I think, regarding the ORM and the approach that Rails and, sort of, Hasura takes, I wanted to sort of highlight this as a key, I think, like, tendency difference between the two technologies: abstraction of the database.I think Rails, you will see, tends to hide it from the user and protect you from it. And Hasura tends to say, no dive in, and kind of exalts to sort of embrace the database. Actually, a lot of the features about how that powers Hasura is very database-native features.Shifting from Rails to Hasura: Shift #4 - PillarsThe last shift is hardly a shift at all. And it's talking about the pillars at the core of these technologies. And I think it, it's hardly a shift at all because when I, when researching this topic, I went back to reread, sort of, this controversial Rails doctrine that outlines the most important pillars, that led to the rise of Rails, and it's kind of guides, it's continued development on the left here, the nine, sort of, principles.And I was surprised to find Hasura shares, like, many of these foundational pillars, you know. Everyone is, Hasura is built to make it super easy to program then feel pretty good while programming in it, and sort of, increase the productivity of the developer. I think the, probably the, two major differences here between Hasura and Rails, is \u201cno one paradigm\u201d and \u201cprovide sharp knives\u201d. I think, \u201cno one paradigm\u201d, think because Hasura pretty strongly is tied to GraphQL and the event-driven microservices approach. It's also a back end as a service versus a standing sort of web development framework. And I think the other one is, that I think, differs a little bit as providing sharp knives. And I think Hasura, you know, instead pushes this codeless, pre-cut approach with auto-generation - and your custom actions here is a counterexample - but it focuses on the common and the plumbing, the same basic toolbox leaving you to the rest, either with a built-in escape hatch, or just complete freedom. In SummaryAnd so with that, I kinda wanted to just give a summary here of different shifts that you can make, and kind of help you, sort of, move from this monolithic MVC REST, CRUD based, sort of model to a more event-driven, service-oriented architecture that, sort of, revolves around GraphQL. So thanks for tuning in! And feel free to get in touch with any links here:https://www.linkedin.com/in/avimoondra/https://github.com/avimoondraavimoondra@gmail.comAnd I hope that's helpful as you make the shift to Hasura. Thanks!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Everywhere: Use Hasura + GraphQL Mesh to create the ultimate data graph - Uri Goldshtein @HasuraCon'20 Description: Uri Goldshtein spoke last year at HasuraCon'20 on how one can use Hasura's remote joins feature and GraphQL Mesh to create data graphs.\nAs HasuraCon'21 is almost here, we thought it would be the perfect time to take a look back on all the amazing talks by our amazing speakers from 2020! Author: Hasura"}, {"role": "assistant", "content": "Uri Goldshtein spoke last year at HasuraCon'20 on how one can use Hasura's remote joins feature and GraphQL Mesh to create data graphs.As HasuraCon'21 is almost here, we thought it would be the perfect time to take a look back on all the amazing talks by our amazing speakers from 2020!You can sign up for HasuraCon'21 here!Here's Uri's talk in video form:GraphQL Everywhere: Use Hasura + GraphQL Mesh to create the ultimate data graph - Uri GoldshteinAnd below is a text transcription of the talk for those who prefer reading! (The text is slightly paraphrased and edited here and there for clarity's sake.)TRANSCRIPTION:Hi, everyone. My name is Uri. And I'm here to talk with you about Hasura and GraphQL Mesh, and all the new capabilities that you can now do with Hasura's new remote joins and with GraphQL Mesh. A bit about me, The Guild, and what we doI'm the founder of a group called The Guild. We do a lot of open source work in the community, mostly around GraphQL. Some of our libraries you may have heard of are:GraphQL Code Generator - where you can generate types for your back end and your front endGraphQL Inspector - which basically prevents breaking when changes are made, and every time you go into production, you can make sure your API is validGraphQL CLIGraphQL Modules - you probably heard about GraphQL modules if you want to work in separate teams, and each team has its own responsibilities on the graph.And also recently, we took over a library, the GraphQL tools library, which is a very famous library that a lot of the people in the community are dependent on. It wasn't so well maintained. We took it over, rebuilt it, added tons of features into it, and also revealed schema stitching. So now schema stitching is way better than it was before, it's no longer deprecated. If you\u2019ve seen it before, really check it out again. We have a lot of libraries that we\u2019ve built for the community. We wanted to start a new group that can basically create sustainable, open-source that you can count on for many years to come. And you can use those libraries individually or you can just pull them together into one coherent platform. How Hasura fits into Enterprise EnvironmentsToday, I'm going to talk about how Hasura fits into the whole picture. So Hasura, as we all know here, and we don't need to explain is an amazing, amazing tool. For me, the best thing about Hasura is that you can build super-fast apps, and you can build them super quickly. But you can also do that in any environment. It's an open-source solution that runs on existing databases. So what we\u2019ve seen is that all of a sudden, these types of apps are being built inside enterprise companies, which are the companies that we mostly work with - we want to work with large companies that have a lot of existing code and a lot of existing infrastructure. Seeing applications like the applications that Hasura helps you create in those companies is amazing to us. Now, what we see is that once you start with Hasura, obviously you're gonna experience success pretty quickly inside those organizations. And then when you have success inside those organizations, very quickly, people want to get to be a part of it. And very quickly, you get more and more features, and you need to start integrating with other existing systems. And that might be hard because those existing systems might not be as flexible as Hasura is. Hasura's Remote JoinsAnd, one of the most powerful and exciting things about Hasura, and why we are so excited about it, is remote joins. Remote joins are just is an amazing ability because we can take this very fast-paced application that we built with Hasura and start integrating other sources into that system. I think that's an extremely powerful concept. But the question is, how do we go about doing the transition?So if we look at Hasura as our thing that communicates with our current app and our client, we now need to start integrating it with other servers, and maybe third-party APIs and things like that. Now, maybe we can start converting them. The thing is, with remote joins is that Hasura expects a GraphQL API to be integrated into. So in order to integrate with other sources, we might want to maybe, start converting all the sources that we want to merge into GraphQL. Or maybe you know, if that's not possible, we want to build small gateways in front of those services. Now, that sounds good, but in reality, a lot of those services, won't migrate to GraphQL. In reality, they have enough work as it is, they're not going to convert to and learn a new protocol. Maybe those are legacy services that nobody's touching anymore. So, doing this is in large organizations is sometimes not practical. But we saw the success of Hasura and GraphQL inside those organizations, and we wanted it to continue. So we thought, well, maybe we can do something interesting. Since those services are working, they do have schemas, at least some of them have schemas. Let's say they have a GRPC or open API or swagger schema, or maybe a soap schema. But some of them may use REST without using rest without any schema, there's just information there on the logs that we can extract and understand what's going on. So what we thought is, maybe you can use all that information, maybe a schema, our data, or whatever we have, and convert this information into a GraphQL schema. And maybe even take older schemas and convert them into one GraphQL source and then we can integrate it with Hasura with remote joins. So that's GraphQL Mesh. GraphQL MeshGraphQL Mesh basically takes any source and you can give it any information - it can be open API, GRPC, maybe even queue systems, or if you have an event system, or RabbitMQ, or Kafka or whatever, it can integrate with those as well. And then it generates an executable GraphQL. And the sources can be any source - open API and swagger, GRPC - we have handlers for all those technologies. When it comes to old data, we actually took the whole Microsoft graph, which is an old database, and converted the whole graph into GraphQL. They had their own project, trying to do it manually, and couldn't succeed. And we did it automatically. Now, once GraphQL Mesh gets that information and converts it, it can be run as a gateway, or it can be run as an SDK on your existing services. So there's full flexibility of how you can run it and where you want to run it and how you want to use that graph. For example, one person in the community took - basically, if you google GraphQL Mesh Docker, he basically built the library where you can just input it, this swag, the YAML config file for Mesh, and then you just get the gateway running that exposes GraphQL. And again, we built Mesh to be completely flexible. So, every step of the way is customizable, and you can do transformations on any step. That's why it\u2019s extremely flexible and can support so many use cases. Now, not only did we understand, like what can happen, but actually, the community understood as well. And we started seeing people in the community actually taking GraphQL Mesh, and then converting all the existing APIs that head in their organization, and then merging it with their existing Hasura applications. So that's very, very powerful. GraphQL Mesh DemoAnd that's what I want to show you right now. So I created a video, you know, forgive me that I'm not live-coding, but I'm going to explain it a bit. What you see here is an existing Hasura database that has basically series API - series information. We can query a city and its name. Now let's go ahead and create our Mesh application. We're just creating it here. And just yarn in it an application. I'm doing it from scratch here, there is nothing in that library. Now what we're going to do is we're going to install Mesh. And you'll see that everything I'm doing here basically, you could also have done with just a Docker now, what we want to integrate with is breweries and breweries API. So I wanted an example where we have an API that is out of our control. And it doesn't have a schema. You can watch other talks of mine, and there are also examples on the repo where we show how to just generate everything for Mesh from schemas. But in many, many cases, in a real enterprise, they don't have schemas or the schemas are completely outdated or things like that. So what I'm gonna do here is, I'm gonna just take an example response and then there's an example Input and Output basically of that API and GraphQL Mesh will take that information, generate a JSON schema out of it, and then generate a GraphQL API off of it. So let's see how it goes. I'm just going to paste this here. And I'm also going to paste an example, the example input. And again, you can do that just by looking at your logs or bring in whatever you want here. Now let's create our actual Mesh YAML file. I can tell Mesh, what my sources are, and I can name them. In this case, we'll just call it \u201copen brewery\u201d or something like that. And then let's look at the config. So we have many different handlers. In that case, we brought in the JSON schema handler, we told it where the base URL is, and then for the operations, we actually just told it. And in order to get the schema, we basically just told it where to search. And again, you can just bring in your own logging information, or whatever, and generate that yourself from existing services. And now, just by doing that, let's start Mesh. Let's create the script, let's run yarn start, Mesh serve, and what you're going to see now is that I\u2019ve basically got a fully working GraphQL API. Now, I didn't write any code in here, you see, I have all the docs as if it was a GraphQL service. But it's not. It's a regular REST service that didn't have a schema either. So now I can have the autocomplete and all the amazing experience even on the input, they have autocomplete on the parameters. And I can query for whatever I want I get a fully typesafe interface. You can see that it works. But again, this is from an API that I have no control of. Now, once that\u2019s done, we can actually join that API into Hasura. So let's just go to remote joins, and paste the URL of my new Mesh instance. So now we can query not only the city through the API as we had before, but we can also query the breweries. But what we really want is we want to get the breweries inside a city. So let's use the powerful remote joins of Hasura for it. Now in \u201ccity\u201d, we can integrate and declare a relationship with the brewery. And we're going to take it from the remote schema. And we're going to, you know, use it by the input of a city like we saw before. And now, if we go back, what we can see is that now we can actually query the breweries in a city. And again, we are using here our own Hasura data, together with an API that doesn't have any schema. And we have no control of it and it\u2019s not GraphQL at all. So that's a very short example. This example, by the way, exists on our graph on our GitHub repository, you can find it there together with other examples. And just go ahead and try it out. Now, let's go back to the presentation. So now you saw how powerful this combination is with Hasura remote joints and GraphQL Mesh. Now, the thing is, we can take it a step further: we can start actually implementing, and exposing those modules, those Mesh modules into any API, like on our website. As I said, we have the Microsoft API, we have Stripe, we have all kinds of APIs that are not GraphQL at all, but now you can just take them as GraphQL. And you can create your own integrations and include those integrations into Hasura. And we can start creating a lot of existing integrations and with any API you can think of, and you can share those integrations inside your company, or, you know, with the whole world and basically create a huge graph of all the sources we can think of. Thanks!So, I hope that was interesting for you. You can follow us, you can go to the website, GraphQLMesh.com. You can follow us on our Twitter. And you can join our community. I think you know, our community and the Hasura community, we're always been very close and we are very close. We love everything Hasura does and we want to come up with more things that we can help everyone at Hasura. So thanks for having me and I hope to hear from you online. Thank you!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: How Startup Pipe Went From Prototype to Production In 11 Days With Hasura Description: In just one year Pipe has experienced amazing success accomplishing this with just 11 engineers. Helping fuel this efficiency was Pipe\u2019s decision to make Hasura an essential part of their technical architecture, which has provided tangible benefits to their dev processes and their bottom line. Author: Steven Yi"}, {"role": "assistant", "content": "How Startup Pipe Went From Prototype to Production In 11 Days With HasuraThe on-demand recording of this webinar is available here.Pipe is a rapidly growing fintech startup that\u2019s transforming how companies fund their growth by unlocking their biggest asset - revenue. Through Pipe\u2019s platform, companies can trade their monthly or quarterly contracts for their annual value upfront, instead of taking on highly dilutive equity and restrictive debt.11 Days:The time it took to go from prototype to production with HasuraIn just one year Pipe has experienced amazing success, gaining over 3,500 customers and $1 billion in tradable annual recurring revenue - accomplishing this with just 11 engineers. \u00a0$600k USD Saved Annually:Using Hasura, Pipe needed 50% fewer developers than they anticipatedHelping fuel this efficiency was Pipe\u2019s decision early on to make Hasura an essential part of their technical architecture, which has provided tangible benefits to their dev processes and their bottom line:\u201cHasura is a fantastic way to create a data fetching layer to our database. \u00a0It\u2019s ultra-stable and often better at planning queries than ones we were writing ourselves.\u201dOther advantages include:90% reduction in time for new feature developmentSimplified GraphQL data fetching architecture makes it easy to add new featuresAuthorization and permissions model secures sensitive financial dataIncremental migration to GraphQL rather than monolithic rewriteBreaking down barriers between frontend and backend engineeringConsistency in production deployment and configuration utilizing Hasura metadataPipe's technical architectureLet\u2019s dive deeper to learn more!Pipe\u2019s ChallengePipe makes it easy for companies with recurring revenue streams and subscription-based business models to convert that into capital to scale and quickly scale and grow their businesses.Pipe helps businesses convert subscriptions and recurring contracts into up-front capitalPipe\u2019s development team needed to thread the needle balancing the immediate requirements of the business while also building for the future. \u00a0Their technical architecture started with a Postgres database, Go as their backend language, REST APIs, and React for their web front-end running on Google Cloud. \u00a0As they rapidly grew, they needed to consider additional concerns:Fast Time to Market & Feature Innovation: Pipe\u2019s team needed to quickly create or change features based to gain new customers and respond to user feedbackFlexible Architecture: One of Pipe\u2019s architectural goals was to keep the architecture as agile as possible, where changes to the underlying database and frontend UX could happen rapidly. \u00a0Small Development Team: Pipe\u2019s 11 engineers needed to stay agile and flexible - understanding all layers of their system - database, backend, integrations, and the frontend web UIFlexible Web APIs: One of their early challenges was inflexibility of utilizing REST APIs, where they encountered multiple variations of similar request types, but each requiring its own REST endpoint because of differing requirements.Performance: Pipe\u2019s website needed to run quickly to provide a great user experience, and scale to meet the traffic of their rapidly growing customer base.Security & Authorization: Pipe\u2019s system stores sensitive financial and customer data of their customers and they needed to make sure that the data was secure and that users could only view data they were authorized to seeWhy Pipe Chose HasuraAs Pipe started investigating solutions to these challenges, they discovered Hasura and went into production with it within 11 days.\u201cWe needed 50% fewer frontend developers than we thought we needed. \u00a0Hasura and GraphQL reduced the toil to build and iterate on the frontend.\u201dPipe put Hasura into production within 11 days of discovering it, helping them develop new features rapidly and breaking down barriers between backend and frontend development.Ease of UseHasura\u2019s intuitive tools and documentation made it easy to set up and connect to their Postgres database, instantly auto-generating GraphQL schemas and resolvers based on the tables and views in their database. \u00a0This saved them development and maintenance time not having to author any more of their data access APIs themselves and modifying the code every time they made a database change.GraphQL-based Data AccessMoving to GraphQL from REST also enabled them to unify and simplify their data access APIs. \u00a0The flexibility allows the same endpoint to return only the requested fields and child (tree) data structures - making the queries more efficient and eliminating the need to create a different API for each unique request.As Pipe made changes in their database, they didn\u2019t have to rewrite their APIs - they just had to adjust requests made from their React frontend to access the new data.Security and AuthorizationAnother benefit Hasura provided is its built-in security and authorization capabilities, which can limit what data is returned down to the individual column and row level. \u00a0This capability saved on development time implementing it themselves and ensured that customers could only view their own data.Development Team & ProcessesUtilizing Hasura made it easier for every engineer to understand the flow of data from the database to their frontend, making it possible for backend or frontend developers to make changes and add features quickly.Operationally, their deployment processes are simplified by utilizing Hasura\u2019s metadata capabilities to make sure configuration changes made in development and staging are consistently deployed to production.\u201cHasura helps us remove the strict boundaries that traditionally separate front-end and back-end engineers, and makes it easier for any developer to make changes.\u201dWhat\u2019s Next for PipePipe is currently using Hasura to query data to present in their React web frontend. \u00a0In the future they plan on exploring additional Hasura features such as mutations - utilizing their existing Hasura GraphQL APIs to perform data inserts and updates -- and remote schemas to fetch data from 3rd party services.They're also looking closely at GraphQL subscriptions to add real-time updates and notifications to the user experience. As their site traffic increases they plan to use Hasura\u2019s database read-replica support to balance queries and subscriptions across database replicas.Finally, webhooks - initiated by Hasura Event Triggers when database changes occur - may also provide a layer of real-time integration to their other microservices.Thank you to our friends at Pipe, and Peter Downs - Director of Engineering for sharing their story with us!Read more case studies: Pulley, an alumnus of YCombinator 2020, cut development time by 50% by auto-generating and securing GraphQL for over 50 database tables with Hasura."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How Hasura Makes Learning Code Easier - Ali Spittel @HasuraCon'20 Description: Ali Spittel explains how using Hasura makes learning and teaching code super easy! Author: Hasura"}, {"role": "assistant", "content": "Ali Spittel spoke last year at HasuraCon'20 on how Hasura helps with teaching code and how it helps students learn faster! As HasuraCon'21 is almost here, we thought it would be the perfect time to take a look back on all the amazing talks by our amazing speakers from 2020!You can sign up for HasuraCon'21 here!Here's Ali's talk in video form:How Hasura Makes Learning Code Easier - Ali Spittel\u00a0And below is a text transcription of the talk for those who prefer reading! (The text is slightly paraphrased and edited here and there for clarity's sake.)TRANSCRIPTION:Hey, I'm Ali, and I'm going to be talking about how Hasura makes learning to code a little bit easier! A bit about meI teach people how to code both as my job and as my hobby. I am a lead instructor at General Assembly where I teach people to code as my full-time job all day, every day, teaching people from \"hello world\" all the way up until they are professional developers. I've also been really privileged to teach a lot of people through online platforms such as videos, podcasting, blogging, and all that. So, I definitely have some experience in getting new developers to learn to code and it\u2019s something that I get to do every day!How I learned about HasuraI tried Hasura for the first time in 2019, and my mind was blown! I thought it was the coolest thing ever! Rajoshi came on my podcast, the Ladybug Podcast, that I host with two other women, and I was super, super amazed by the technology. I have also gotten to teach with it a couple of times. I have a workshop that's up and a blog post on how to use Hasura. It's been so cool to see how easily some new developers can get started with it, and how much more accessible it makes coding for them. It would have made my life a lot easier too back in the day!GraphQL used to be hardI started using GraphQL pretty much exactly when it came out. And it was really, really, really tough at that point. I was working on our Django back end, and getting GraphQL set up for it was a really, really, really painful process. Remember, there was pretty much no documentation for how to make a GraphQL query, let alone how to hook it into a Django app, and then how to hook it into a Vue frontend after the fact. It was really, really tough. And I, at that point, just thought that GraphQL was really hard and that it wasn't technology that would work for me. But Hasura made it so much easierSo, I came into Hasura biased against GraphQL for that reason but was completely taken aback by how accessible it was to create endpoints with Hasura instead of having to set up GraphQL from zero by yourself. With Hasura, it just takes a couple of clicks to get a full back end up and running. There are full guides for hooking Hasura into a bunch of different technologies. So you don't need to use it in isolation. I've always been really, really impressed by that. There's even a button to try out Hasura on Heroku in 30 seconds and have a whole entire deployed application ready for you to use with just a couple of clicks. Pretty incredible! Everyone\u2019s a full-stack dev with Hasura!Hasura also allows everybody to be a full-stack developer. It can be really challenging to see all the different technologies that you have to learn in order to get something up and running. Even for just a GraphQL application, just think about having to write that whole entire back end from scratch, the whole entire front end from scratch, connecting them from zero, connecting to the database from zero, doing all the queries, and more. That's pretty intimidating! And the world of front-end development has really expanded over the years from just HTML and CSS and a little bit of JavaScript, to fully JavaScript and then thinking about hooking into GraphQL, where you can make queries directly. All that\u2019s a lot to put on front-end developers. Hasura enables anybody to make that process a little bit easier. Instead of having to write all of the code yourself, a lot of that is provided for you so that everybody can write a full-stack application even if you are just a relatively new front-end developer. Pretty cool! You may have seen this diagram of what it means to be a full stack developer where you know one part of the stack really well and then the other part of the stack, maybe not so well. So in this case, we need a really strong front-end developer who's really great at that but then has to do a little bit more backend work. And maybe that doesn't go so well. Hasura makes it so that that developer doesn't have to worry too much about all that back-end code. It makes that process a lot more simple to build an API and a GraphQL endpoint without having to worry too much about writing all the code yourself. TeachingSo I want to talk a little bit about teaching. My first rule is to teach the pool, not the ocean. It can be tempting to give your input on absolutely everything, teach every detail that you know about something. And that doesn't normally help your learners because giving it to them simply at first, teaching them one way to do things simplifies a new topic for them, and makes it so that it's not super intimidating for them. Teaching with HasuraWith Hasura, instead of having to teach your students every single piece of writing a back end and doing that from absolute zero, instead, you can teach them the pieces that they really need to get something working and something that they can be proud of demonstrating. So Hasura allows us to teach maybe just the front-end piece of programming instead of working too hard on the back-end side. Another big piece of this is to instill confidence and celebrate people's wins! Especially when learners are starting off, there's a lot of imposter syndrome, and a lot of feelings of this is never going to be something that they can do. That's something that I really dealt with when I was learning to program. But something like Hasura allows students to have quick wins, where they can build something that displays in the browser or connects to that back end or displays their data without having to do a massive amount of work. And that's going to make them more confident, it's going to make them feel like they can build applications, and they're going to be able to celebrate that when they see what they've built in the browser. Building Games with HasuraAnother really awesome thing is gamifying learning. Making it so that the students are building games, building things that are interactive, things that are fun. That's going to make it more entertaining for them. In addition, writing game logic is usually a lot more difficult than basic application logic that we might use day-to-day. So it's a really great challenge for that.I have had a ton of fun building games with Hasura. I built this real-time drawing application (https://quizzical-mclean-87c84c.netlify.app/) where you can click on any of the pixels and the colors and draw things in right now. Some people have made it maybe not so attractive! :)You can have it with multiple users. So I can have two tabs open of this game. And you can see that it updates in real-time. So what I'm drawing on this tab, it's also drawing on this tab. And it's the same thing across computers. Implementing this in Hasura is not an overly difficult process. It's just changing your GraphQL query to a subscription. Which is pretty awesome! You have this fully real real-time application that you can use. The Gradual Release of Responsibility (with Hasura)Another thing I like to talk about is the Gradual Release of Responsibility. This is when at first, you give a lot of structured instructions, things are really handed to students and it's a little bit more handheld. But then as you move on, that responsibility is released to students, and it's more on them to research things and teach themselves and extend the learnings and interpret instructions in a little bit more of a loose manner. Hasura allows for this as well where it can start really simple and just have a couple of clicks to build out that back end. But then you can go more in-depth and build something more complex or hook the parser up to a back end that you built yourself or build it through the command line instead of the Heroku one-click button. So, Hasura definitely enables the gradual release of responsibility as well. Building things yourself is the best way to learn something, and Hasura allows you to build full-stack web applications without having to know a million different tools and technologies and also be able to deploy it without a huge amount of effort. So you can have these portfolio projects that are released to the public that other people can see without having to go super, super wild with learning everything. So you can build things, show them off, and all that, using Hasura. Well, I hope that was a good introduction to a few teaching tips and how Hasura can help with learning these new things. If you want to learn more about teaching or ask me any questions about any of this, please do! I also have some Hasura tutorials on my profile. Keep in touch, I'm @ASpittel everywhere! Thanks!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura 2.0.0-alpha.10 Description: The release train continues towards the station with the release of 2.0 alpha 10. \n\nIncluding aggregation fields for SQL Server and BigQuery support. Author: Tyler Hannan"}, {"role": "assistant", "content": "The release train continues towards the station (note: the station is named \u2018Stable\u2019) with the release of 2.0 alpha 10.While \u2018alpha\u2019 in name, it is not alpha in quality. However, we do use this signifier to denote that it is a work in progress and invite feedback about any problems you may experience. Hasura 2.0.0-alpha.10 docker quickstartHasura 2.0.0-alpha.10 changelogGetting Started with Hasura CloudA Summary of ReleasesAs mentioned in recent release posts, we have historically announced our releases via Discord and updates to the GitHub release page. As adoption continues to increase, we believe this is insufficient for the future. To that end, we will begin to blog for each release (even if it is only small) so that you can track releases via blog, twitter (@HasuraHQ), Discord, and the Github Release page. In these posts, we will highlight a few specific elements of each release, but the exhaustive list of features and improvements is always in the changelog.Overviewserver: aggregation fields are now supported on mssqlWe are delighted to bring aggregate fields to MS SQL Server.If you aren\u2019t familiar, the ability to fetch aggregations on columns (along with nodes) is the outcome of using a Hasura aggregation query. The name of the aggregate field is <field-name> + _aggregate. Common functions include count, sum, avg, max, min, etc. For more information, and a simple example, check out the Aggregation queries section of the documentation or review the API reference for all detail.BigQuery supportWhile, strictly, released in alpha 9 we neglected to highlight the arrival of Google BigQuery support in Hasura. All features are not, yet, available but we will continue to iterate. Postgres, SQL Server, and now Big Query all served via Hasura!(If you are interested in some of the technical approaches to solving the multi-backend problem, check out the post: Building a GraphQL to SQL Compiler on Postgres, MS SQL and MySQL)And moreThose are, of course, just the highlights. All other fixes are included in the changelog.ConclusionPlease use the docker target `hasura/graphql-engine:v2.0.0-alpha.10`, try it out, let us know what works and what needs improvement. You can find us on Twitter (@HasuraHQ) and in Github. For ad-hoc conversation, feel free to join us on the Hasura Community Discord.And, if you want to get started now with the latest stable release, check out the deployment simplicity of Hasura Cloud.Previous PostsFor previous release announcements in the 2.x series, please review:Announcing Hasura 2.0.0-alpha.8Announcing Hasura 2.0.0-alpha.9"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a GraphQL to SQL Compiler on Postgres, MS SQL and MySQL Description: The release of Hasura 2.0 included one of the most exciting features to date - multiple database backends, and in particular, support for our first new backend, Microsoft SQL Server. \n\nOn the face of it, this probably seems like a relatively straightforward change. Author: Phil Freeman"}, {"role": "assistant", "content": "Building a GraphQL to SQL Compiler on Postgres, MS SQL, and MySQLThe release of Hasura 2.0 included one of the most exciting features to date - multiple database backends, and in particular, support for our first new backend, Microsoft SQL Server. Our roadmap already includes support for several new backends -- Google BigQuery (in alpha already), MySQL, Citus (coming soon), and several other databases over time. Soon you will be able to use all of the Hasura features you know and love with the choice of your favorite database!On the face of it, this probably seems like a relatively straightforward change. After all, Hasura is ostensibly a GraphQL-to-SQL \u201ccompiler\u201d, and once we have generated the SQL, we should be able to run it against our choice of database. However, as anyone who has used more than one relational database implementation before will tell you, it is rarely the case that databases support features of SQL in the same way -- or even with the same syntax. Certainly, many features of Hasura make clever use of some lesser-known features of Postgres in order to deliver high performance, and it has not been straightforward to naively translate those to other backend databases.In addition, Hasura supports a great number of features on top of a simple query language. The ultimate goal is to be able to bring any database and get access to all of Hasura features: row-based permissions, remote schemas with remote joins, custom actions with action relationships, event triggers, caching, distributed tracing, high-performance subscriptions, to name just a few. We are not there yet, and each of these generic features brings its own set of challenges when it comes to the problem of generalizing from Postgres-only to any backend.In this post, I\u2019m going to take a look at some of the features we have started to support across multiple backends, some of the ones we\u2019re currently working on, and some longer term plans for additional integrations.An overview of our backend architectureAs I talked about briefly in my last post (Hasura 2.0 Engineering Overview), abstracting over multiple backends is made possible in the Hasura code using several Haskell type classes. Each of these type classes abstracts some set of operations that are used when working with different aspects of the pipeline: metadata parsing, schema generation, data transport, and execution. As you might imagine, these abstractions have been introduced by looking at the previous Postgres implementations and, ultimately, extracting signatures for common operations that we expect to implement across multiple backends.As we have worked on adding more generic support for existing features across various backends, this set of type class abstractions has expanded. Of course, care is needed to abstract the right things at the right level. We don\u2019t want to reimplement too many lines of code per backend because our level of abstraction was too coarse, but at the same time, we don\u2019t want an explosion of abstractions because the level was too granular.No doubt the hierarchy of abstractions we need will change over time as refinements are made. \u00a0The ideal final state is a compact set of abstractions, each with its own clear semantics and -- ideally -- a reference implementation as well.Queries with joins - not so simple!Even before we consider advanced features, we already run into trouble when we consider querying functionality.For example, Postgres allows us to take a query involving several tables, object and array relationships, and push all of those data fetches down as one single query. How do we do that? Consider a simple example in SQL, fetching all order information for a single customer ID, along with product information from a product table:SELECT \n  name, street, city, state, zip \nFROM customers \nWHERE id = $1;\n\nSELECT \n  p.id, p.name, p.price, o.tax_rate, o.quantity\nFROM orders o\nJOIN products p ON p.id = o.product_id\nWHERE o.customer_id = $1;\nHere, we make two queries: The first to fetch the customer data and a second to fetch order and product data for that customer. You can imagine that this might be generated by a GraphQL query.query { \n  customers(where: { id: { _eq: $cust_id } }) {\n    name street city state zip\n    orders {\n      tax_rate quantity\n      Product {\n        id name price\n      }\n    }\n  }\n}It wouldn\u2019t make sense to use a join to fetch the customer data for each order/customer row, because the same data would be duplicated across each row. Also, we\u2019d have to do quite a bit of manual work in order to collect all of the data back into a single JSON result after we were done.Instead, in Postgres, we can do something better, by using JSON aggregations and subselects to make Postgres do the minimal amount of work, and even give us the JSON we want directly!SELECT \n  json_build_object(\n    'name', name, \n    'street', street, \n    'city', city, \n    'state', state, \n    'zip', zip,\n    (SELECT json_agg(\n       json_build_object(\n         'id', p.id, \n         'name', p.name, \n         'price', p.price, \n         'tax_rate', o.tax_rate, \n         'quantity', o.quantity\n       ))\n     FROM orders o\n     JOIN products p ON p.id = o.product_id\n     WHERE o.customer_id = customers.id\n    ) as orders\nFROM customers \nWHERE id = $1;What\u2019s happening here? At each level, we are using the json_build_object function to construct a JSON object inside of Postgres itself. Instead of returning multiple columns in our result set, we return a single column of type JSON. More importantly, for every array relationship (for example, fetching the orders, here), we use a subselect and the json_agg aggregation function to aggregate a JSON array of subresults. The JSON aggregation is done at the database, avoiding unnecessary serialisation overhead, and returning a compressed form of the data without duplication.Clever!Except there is a problem. Neither BigQuery nor MySQL support JSON aggregations in the specific way that Hasrua requires. Specifically, as the documentation confirms, MySQL does not provide a way to aggregate a JSON array with a well-defined element order.So, we cannot implement queries for array relationships uniformly across backends, and we reach our first necessary abstraction: Hasura must abstract over the way code generation proceeds for array relationships. However, because there are sufficiently many other small differences in code generation between backends, we choose to make the abstraction much coarser. Ultimately, we abstract over the whole query generation, from incoming query AST to generated SQL.In practice, this means we duplicate some code across backend implementations. But the alternative would be an extremely fine-grained set of abstractions over many details of the target SQL implementation. Instead, to address the possible code duplication problem, we implement similar backends using a set of shared functions. For example, in the case of MySQL, we have to fall back on an approach where we issue multiple queries and perform aggregation in Hasura itself. But this code is generic and can be reused across different backends where it is needed.As a guideline for backend implementers, we use subselects and JSON aggregation wherever it is available, and fall back on the multiple-queries implementation if we have to.SubscriptionsAnother well-used, well-loved feature in Hasura is support for high-performance, highly-concurrent subscriptions against Postgres tables. In an older blog post, Tanmai has outlined the architecture which allows us to scale Hasura to millions of concurrent subscriptions.One trick that we require is to batch similar queries for execution. That is, we want to identify queries which only differ in their variables, and send those as a single batch to Postgres.At the level of code generation, we use some advanced features of Postgres to make sure this can happen. Specifically, we use the unnest function and lateral joins in order to generate a single Postgres query for multiple sets of variables. Assuming the same database schema as the customer/order example above, we can imagine that we\u2019d like to subscribe to a live list of all recent orders for a customer. In that case, we would generate code to batch queries for several customer IDs, and generate code like this:SELECT results\nFROM unnest($1) query_variables (customer_id)\nLEFT JOIN LATERAL (\n  SELECT ...\n  FROM orders WHERE customer_id = query_variables.customer_id\n  LIMIT 10\n) results ON trueThe lateral join here is going to execute the inner query for each row in the result of the call to unnest, that is, once for each set of variables we pass in. This is great for code reuse, because we can simply take the SQL we would have used to run the query normally, and wrap it in a query of this form.However, as you may have guessed, these features are not always available across all backends in the way that we need, so we cannot implement this batching behavior generically. Specifically, in MS SQL Server, we don\u2019t have access to parameterized queries using our existing ODBC bindings, so we must use a different approach.However, the logic for managing cohorts of similar queries for batched execution is common across all backends supporting subscriptions, so we can choose the level of abstraction accordingly: we don\u2019t abstract over the process of batching itself, but just the process of executing a batch of queries for a given set of variables.Action RelationshipsA third example of a Postgres-specific feature comes from action relationships, a feature of Hasura which allows users to perform joins between relational database tables and data from external APIs.When running a query with an action relationship, we first fetch the data from the external API (the \u201caction\u201d) and then extract portions of the JSON response which will indicate the primary keys we need to fetch from Postgres. This is a little bit like the inverse of the problem from the first example: instead of taking a record set from Postgres and turning it into a single blob of JSON to return to the user, we need to take a single blob of JSON produced by an API and turn it into a Postgres record set, so that we can then join that record set with a table in Postgres.To do this, we use one of the two functions jsonb_to_recordset or jsonb_to_record, depending on the type of relationship we are implementing.If you\u2019ve noticed the pattern so far, you\u2019ll know what\u2019s coming next: we can\u2019t expect every backend to provide analogues of these functions, so we cannot provide a generic implementation of action relationships across all backends.So, when we come to generalize action relationships, we will need to decide on the correct abstraction to add to our collection. We could choose to add a new abstraction for running a query in the context of JSON fetched from an action. However, maybe we don\u2019t need to do this, because we already have a very similar abstraction, namely the one we just saw. Subscriptions need the backend to support batching of queries with multiple sets of variables, which is obviously similar in a lot of ways to the problem here. So hopefully, we will be able to design a single batching abstraction that backends can support, powering both subscriptions and action relationships.Language Features, GenerallyWe\u2019ve seen three specific examples of problems which arise when we try to naively generalize existing features to multiple backends, but of course they are all examples of a much more general problem: in each case, the target language (all variants of SQL) did not support some feature we needed, so we were required to work around that, using only the language features which were available to us.In general, the problem we face is to identify all the different sorts of languages we may want to support in the long term, and to distill those down into a small set of language features which are required by each Hasura feature.The lack of a language feature in a target language is not necessarily a problem - in the best case, we will be able to add a new abstraction and work around the lack of that feature by using some other approach, maybe sacrificing some performance in order to do so. In the worst case, if we cannot work around the lack of that feature, we will simply need to turn off the corresponding Hasura feature for that backend. This is unavoidable in some cases.For example, providing row-level permissions for a new backend requires support for code generation of predicates, which seems like a low barrier to entry. However, in the case of remote schemas, we can\u2019t even guarantee that, because an arbitrary GraphQL schema certainly doesn\u2019t need to provide support for where clauses and predicates. This is acceptable, because we can switch our level of abstraction - we don\u2019t really need to be able to generate code for predicates; all that is required is to modify a query for execution to incorporate permission rules. \u00a0In the case of remote schemas we can do that by passing any relevant session variables to the GraphQL resolver as a function argument, for example.However, with the recent addition of inherited roles, users can now issue queries in the context of multiple roles. There is not an obvious way to generalize the new abstraction to support inherited roles for remote schemas, and the answer may have to be that we must disable that feature for a subset of backends.In general, there is a tension between these language features that we demand from new backends, and new general-purpose features that we want to implement for all backends. More language features implemented by a given backend mean that we can claim support for more general purpose features, but correspondingly, as we add more general-purpose features, we must demand more from our backend implementations, and in some cases, that might mean we have to exclude features from certain backends.There is another interplay between language features and static analyses of queries. For example, with the recent support for query response caching in Hasura, we might want to look at a query and decide if it is likely to perform well in the cache or not. As we add more language features, this problem will become harder and harder, and we may need to pare down the language features we allow in order to be able to support certain analyses.ConclusionAdding support for product features can be challenging. Genericising support for product features across multiple back-ends, doubly so.As we develop these features, we\u2019ll identify a hierarchy of language features that are relied upon. At the bottom of this hierarchy will be something like an arbitrary remote schema, where we can assume almost nothing, and at the top will be backends like Postgres which support a vast collection of language features. New features will have to be developed generically, requiring only abstract language features, and not specific backends. Over time, we\u2019ll fill in the gaps in the matrix of language features and backends, getting us closer to the ideal of supporting every Hasura feature against every type of backend.As a related aside, if you find any of this work and description exciting, we are hiring for a variety of engineering roles."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Automatic image tagging in Hasura Cloud with Cloudinary and Autocode Description: In this short tutorial, we will build an image storage integration with Hasura that auto-tags our images for us. Our user journey involves uploading a photo, waiting for the photo to get analyzed, then be presented with the suggested categories, and save them to the right location. Author: Jesse Martin"}, {"role": "assistant", "content": "In this short tutorial, we will build an image storage integration with Hasura that auto-tags our images for us. We need to account for the following touch-points in our stack. We have a client that will act as the user interface. We have persistent storage for the images and for the user interaction data. Additionally, we need an external service that will provide the object recognition analysis of our image and suggest the additional, relevant tags in the image.Our user journey involves uploading a photo, waiting for the photo to get analyzed, then be presented with the suggested categories, and save them to the right location.Our data journey involves uploading a binary to our CDN, persist a reference to that along with our user and user data in our user database, update the image with some suggested categories, present that data to the user, then return with their confirmations back into the database, either keeping the machine suggested categories that were not confirmed or discarding \u2013 depending on the pattern we\u2019ve chosen for.Let\u2019s reference the following chart to identify the parts in overview.First, let\u2019s look at a naive data diagram. How will our data travel through the application in a traditional architecture?Well, that diagram is a substantial departure from the simplicity of our user-flow. Let\u2019s look at a domain-driven data diagram, an example of what a purist flow could be.Something doesn\u2019t align. Let\u2019s see how we can improve on this design.Building the feature in HasuraFrom the above diagram, we can see that our architecture is dramatically simplified. Let\u2019s look at those distinct parts in some more detail with a small working example.The heavy lifter of our architecture is a feature in Hasura called \u201cactions\u201d - these allow you to define custom resolvers that effectively expose a webhook as part of your API.We\u2019ve chosen three services for our stack:Our Image analytics, storage, and auto tagging is provided by Cloudinary (using the Imagga analytics addon).Our resolver, or webhook, that acts as our proxy, will be managed in a platform called Autocode for convenience and educational purposes.And lastly, our user-data persistence layer, service orchestration happens in Hasura.CloudinaryConfiguring Cloudinary involves little more than registering for an account, choosing the analytics service add-on that we\u2019d like to use, and locating our API settings. Cloudinary itself offers about as powerful of image transformations as anyone in the game, so if you\u2019re looking for a good storage CDN, it\u2019s worth a visit.Modeling our data using Hasura ConsoleYou can configure everything through a CLI and migration scripts, but for our purposes, we\u2019ll use the UI in Hasura Cloud. Our data model looks like this:To support the idea of individual images having individual tags with their own confidence rating, we needed the join table. The join table also allows us to create a many-to-many cardinality in our data. This structure allows us to query our data with the following structure:Custom business logic with AutocodeOur proxy layer is where we\u2019ll do our business logic. For simplicity, I\u2019ve abstracted my codebase into three main services. One is simply a promise wrapper around the Cloudinary SDK. The second is a simple transformation utility to help alter the shape of our data for sending back to Hasura. The third service is our glue layer where we stitch the two services together.Cloudinary wrapperTag TransformationUpload the image and transform the responseThis first half of the code is where we declare dependencies, authenticate with the SDK, upload the asset, and transform the response.This last half of the code is where we write that response back into Hasura. Our mutation is worth looking at in detail.Our data model is split across three different tables with varying degrees of dependency between them. Ordinarily, this would require a number of round trips. Utilizing the underlying GraphQL execution engine, we are able to write our mutation in a descriptive way, where we insert data intro three different tables, and benefit from their relationship to each other.Shipping the APIWith our data connected to each other, and our custom resolver in place, we are able to expose a single, elegant API to build our app with. Because our response resolver returns a known type, we can enforce a relationship with our other tables, and be able to resolve the data that our proxy later wrote into our database for us.From MVP to monitoring and performance tuningOur API is now a lean, shippable product that can be consumed on web-aware clients. But resilient apps are more than shipping functioning code. Let\u2019s have a look at our monitoring tab to see how our API is performing.As we can see from the errors monitoring, very few errors are surfacing, and we can drill into the individual causes.Let us check to see what the response time is for our upload mutation, which is performing AI content analysis, storing data, and performing some general IO.If we inspect a test-case, our operation time is slow.Looking into the tracing, we see that the underlying engine, because there\u2019s no existing data to reference at the time we create our relationships, is generating individual inserts for each tag we have. As a developer, it\u2019s up to us to determine which optimizations are more important. We can split our mutations into few overall inserts, or keep the elegance if our users could be expected to wait. Let\u2019s see what a small refactor would accommodate. In this case, we\u2019ll split our mutation into two pieces. First we\u2019ll write our image and our tags to the database, and then we\u2019ll write our relationships.If we inspect our mutation after that, we can see things improved nicely! Our two mutations combined are now just over a 1 second.Our initial mutation without the relationship inserts is less than half a second, that\u2019s a substantial performance improvement! That\u2019s a 20x improvement in our mutation time.Beyond that, with Hasura, we also offer the ability to convert actions to asynchronous events, which will return an id we can subscribe to for when the long-running process finishes. And since Hasura provides subscriptions out of the box, we can take immediate advantage of them.Now our action is performing much better thanks to the monitoring tab analysis, and our API is ready for shipping to our developers.Additional Reading: Creating a GraphQL API for Notion with Hasura Actions"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Postgres Types Mapped to JavaScript via GraphQL with Hasura Description: Today I've collected together a number of video shorts, extra details, and information about primary keys, primary key data types, and data types in general. In this post I've included links to mapping from Postgres types to JavaScript types via GraphQL and Hasura. Author: Adron Hall"}, {"role": "assistant", "content": "Hello all, Adron here, your host on Hasura Coding. Today I've collected together a number of video shorts, extra details, and information about primary keys, primary key data types, and data types in general. In this post I've included links to mapping from Postgres types to JavaScript types via GraphQL and Hasura.Primary KeysPrimary Key: A primary key in a relational database uniquely identifies the record among many records stored in a table.Primary keys can be made up of one or more columns, of existing data, or made up data that serves only the purpose to identify the record itself. The following are the kinds of primary keys a table can have.Super Key - A super key is a single or multiple keys that make up a group of keys which identifies a row within a table.Candidate Key - A candidate key are attributes that uniquely identify rows in a table. A candidate key is a super key with none of the attributes repeated, thus a natural key within the existing stored data.Alternate Key - An alternate key is a column or group of columns that uniquely identify every row in the table. It is a key that isn't particularly selected as a primary key, but can be used as one if the primary key weren't available for some reason.Compound Key - A compound key has two or more columns included as the primary key of a table to uniquely identify each row of data.Composite Key - An artificial key (think of a generated autonumber, short ID, CUID, or UUID) that uniquely idenfies each record. Also referred to as a surrogate key, these keys are used when a natural key doesn't exist in the schema of the existing table.Surrogate Key - See Composite Key.I put together two videos related to specific primary keys: UUIDs & creating Short URL Safe UUIDs. Check those out on Youtube.The short ID function I add in this short, is embedded below and provided in this gist for easy and quick access!CREATE OR REPLACE FUNCTION gen_unique_short_id() returns text\n    language plpgsql\nas $$\nDECLARE\n    id text;\nBEGIN\n    id := encode(gen_random_bytes(6), 'base64');\n    id := replace(id, '/', '_');\n    id := replace(id, '+', '_');\n    RETURN id;\nEND;\n$$;\n\nalter function gen_unique_short_id() owner to postgres;\n\nThe above definitions and details I discuss in \"A Hasura Bit - What is the UUID column type good for and what exactly is a UUID?\" leave some additional details that should be added. UUIDs are great as surrogate keys for many situations, but there are some situations where they can cause significant problems.For example. Some of the characteristics of UUIDs can become significant problems in higher scale scenarios or in certain hardware situations. Being the UUID is a largely random value and numbered value, the database engine attempts to order these during writes, which if there are too many writes that then require reordering immediately come in, it can cause disk thrashing. Something that in the end can become a significant performance issue! At the same time, 128-bit can be excessively large with large volumes of data just for the key that may not be needed.These problems, as with any trade off, just emphasize the fact that for each and every scenario the pluses and minuses need to be measured carefully for the intended purpose and outcome for the mission at hand.Other considerations with both the UUID and short ID above include the issue with writes incurring a read/write and possible disk thrashing if the insert volume gets too high too fast. Another issue for the short IDs is that there is a slightly higher possibility of duplicates being generated. However, in the sense of a high possibility it's still absurdly low and statistically almost nonexistent. However, it doesn't hurt to have a check and write a new key if a duplicate does occur to prevent that error. But the investment in that level vs. just dealing with the error might not be worth it, every project might have a different tolerance for a possible insert error of that sort. Maybe a retry is good enough?Character Data & Numeric Data TypesIn the next two shorts I tackle a few details about character data and numeric data in GraphQL, how it maps back against Postgres types and reference the key grid that maps Postgres types to their respective API types one would get in Hasura served GraphQL JSON result objects here, and below the videos for quick reference in this article.\n\n\nName\nAliases\nDescription\nHasura Type\n\n\n\n\nbigint\nint8\nsigned eight-byte integer\nString\n\n\nbigserial\nserial8\nautoincrementing eight-byte integer\nString\n\n\nbit [ (n) ]\n\nfixed-length bit string\nImplicit\n\n\nbit varying [ (n) ]\nvarbit [ (n) ]\nvariable-length bit string\nImplicit\n\n\nboolean\nbool\nlogical Boolean (true/false)\nBool\n\n\nbox\n\nrectangular box on a plane\nImplicit\n\n\nbytea\n\nbinary data (\u201cbyte array\u201d)\nImplicit\n\n\ncharacter [ (n) ]\nchar [ (n) ]\nfixed-length character string\nChar\n\n\ncharacter varying [ (n) ]\nvarchar [ (n) ]\nvariable-length character string\nString\n\n\ncidr\n\nIPv4 or IPv6 network address\nImplicit\n\n\ncircle\n\ncircle on a plane\nImplicit\n\n\ndate\n\ncalendar date (year | month | day)\nDate\n\n\ndouble precision\nfloat8\ndouble precision floating-point number (8 bytes)\nFloat\n\n\ninet\n\nIPv4 or IPv6 host address\nImplicit\n\n\ninteger\nint | int4\nsigned four-byte integer\nInt\n\n\ninterval [ fields ] [ (p) ]\n\ntime span\nImplicit\n\n\njson\n\ntextual JSON data\nJSON\n\n\njsonb\n\nbinary JSON data | decomposed\nJSONB\n\n\nline\n\ninfinite line on a plane\nImplicit\n\n\nlseg\n\nline segment on a plane\nImplicit\n\n\nltree\n\nlabels of data stored in a hierarchical tree-like structure\nImplicit\n\n\ngeometry\n\nPostGIS Geometry type\nGeometry\n\n\ngeography\n\nPostGIS Geography type\nGeography\n\n\nmacaddr\n\nMAC (Media Access Control) address\nImplicit\n\n\nmacaddr8\n\nMAC (Media Access Control) address (EUI-64 format)\nImplicit\n\n\nmoney\n\ncurrency amount\nImplicit\n\n\nnumeric [ (p | s) ]\ndecimal [ (p | s) ]\nexact numeric of selectable precision\nNumeric\n\n\npath\n\ngeometric path on a plane\nImplicit\n\n\npg_lsn\n\nPostgreSQL Log Sequence Number\nImplicit\n\n\npoint\n\ngeometric point on a plane\nImplicit\n\n\npolygon\n\nclosed geometric path on a plane\nImplicit\n\n\nreal\nfloat4\nsingle precision floating-point number (4 bytes)\nFloat\n\n\nsmallint\nint2\nsigned two-byte integer\nInt\n\n\nsmallserial\nserial2\nautoincrementing two-byte integer\nInt\n\n\nserial\nserial4\nautoincrementing four-byte integer\nInt\n\n\ntext\n\nvariable-length character string\nString\n\n\ntime [ (p) ] (without time zone)\n\ntime of day (no time zone)\nImplicit\n\n\ntime [ (p) ] (with time zone)\ntimetz\ntime of day | including time zone\nTimetz\n\n\ntimestamp [ (p) ] (without time zone)\n\ndate and time (no time zone)\nImplicit\n\n\ntimestamp [ (p) ] (with time zone)\ntimestamptz\ndate and time | including time zone\nTimestamptz\n\n\ntsquery\n\ntext search query\nImplicit\n\n\ntsvector\n\ntext search document\nImplicit\n\n\ntxid_snapshot\n\nuser-level transaction ID snapshot\nImplicit\n\n\nuuid\n\nuniversally unique identifier\nImplicit\n\n\nxml\n\nXML data\nImplicit\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Testing Hasura GraphQL APIs with Karate Description: So you've created your new GraphQL API with Hasura - what's next? \n\nIt'd be a great idea to write some unit tests against your expected queries to make sure everything keeps working in your application as you make changes to your GraphQL API. Author: Martin Mark"}, {"role": "assistant", "content": "So you've created your new GraphQL API with Hasura - what's next?\nIt'd be a great idea to write some unit tests against your expected queries to make sure everything keeps working in your application as you make changes to your GraphQL API.\nThere are a few differences between how to test your GraphQL endpoint vs a set of REST endpoints.\nThe first is what you're querying against.\nREST endpoints will most often times be broken up against a set of endpoints which will return the responses for their equivalent resources.\nIn GraphQL, you'll be looking at hitting one endpoint (in Hasura's case http://yourapplication.com/v1/graphql) - using a query which will request your selected resources.\nThis is super-powerful since being able to select resources (especially joined / relationship-based) on the fly increases the speed of development, though also increases the need for unit tests in development and production.\nThe second change is the response itself.\nIn querying REST endpoints a response will usually yield 200 (ok) response to signify that a request was successful. A lot of the time REST-based tooling will rely on this response specifically.\nIn contrast, in GraphQL it is possible to have a partial success returning some information, while also return an error other parts. These would be returned as a response of 200 with an errors object in the GraphQL JSON response.\nWhile this can be good for granular error logging - it does bring up a few corner cases in testing GraphQL APIs if they are relying too heavily on the REST-like 200 (ok) response paradigm.\nKarate\nIn this quick tutorial we're going to be taking a look at using Karate (https://github.com/intuit/karate) which is an open-source test automation suite by Intuit to test our Hasura GraphQL API.\nKarate has a ton of features from browser automation for UI testing, to parallel testing, to OCR - we're going to be taking a look at some of the more basic API request / response features, as well as the ability to create some nice Cucumber HTML test reporting.\nGetting Started\nTo get started, you download the repo from this location:\nhttps://github.com/martin-hasura/testing-hasura-api-karate\nRunning this command from the directory will bring up your GraphQL API with a sample schema and some dummy data:\ndocker-compose up hasura db\n\nTesting with Karate\nWe're using the implementation of Docker-izing Karate from Quentin Barlas here: https://github.com/qbarlas/karate-dsl - which is a little more lightweight than the official Karate containers which contain scaffolding for doing browser-based testing in Chrome.\nYou'll be able to find the main test file we'll be using at: ./karate/tests/testCases.feature (tip: if you're using VSCode there's actually a Karate extension which will give you some nice code formatting).\nYour First Karate Feature\nOnce we're in the file you'll notice a number of test cases with commented with descriptions of what each area does in Karate syntax.\nFeature: Test Hasura Blog Engine GraphQL API\n\nBackground:\n* url 'http://host.docker.internal:8080/v1/graphql'\n\nThis will define the name of the Feature we're testing along with the endpoint we'll be testing. For this example we'll just be creating one Feature and will be running five tests.\nQueries and Variables\nGiven def query = read('getPosts.gql')\n\nKarate has the ability to read GraphQL requests from files (these are files are found relative to your .feature file you're running).\nAnd request { query: '#(query)' }\n\n// or\n\nAnd request { query: '#(query)', variables: '#(variables)' }\n\nYou can then pass your query object with your request - as well as create variable object(s) to use in your GraphQL requests.\nQueries can also be written inline as well, and can use '```' for linebreaks and increased legibility (an example is in the testCases.feature).\nHeaders\nIn the examples we'll be passing your headers like the following:\nAnd header x-hasura-admin-secret = 'admin_secret_for_testing'\nAnd header x-hasura-role = 'author'\nAnd header x-hasura-user-id = 1\n\nThis will allow us to test things like emulating our x-hasura-role as well as x-hasura-user-id to check things like our permissions system setup (for example, we have a test case for Users Cannot View Draft Posts).\nYou can read more about customizing headers for more complicated use cases, such as dynamic headers, here: https://github.com/intuit/karate#managing-headers-ssl-timeouts-and-http-proxy and here: https://github.com/intuit/karate#configure-headers.\nAssertions\nWe'll also want to actually test for something. Karate has a deep set of assertion functions for checking your responses for the data you're requesting: https://github.com/intuit/karate#payload-assertions\n* status 200\n\nWe can start by checking that our response status has returned as 200 (ok).\nNow, remember earlier on we mentioned that just because a GraphQL response returns 200 doesn't necessarily mean that everything is ok.\n* match response == \"#object\"\n* match response.errors == '#notpresent'\n\nWhat we're doing here is checking that 1) our response is returned as an object, and 2) our response doesn't have any error object included.\nWe can then get into testing the actual data which is returned as part of the response, such as:\n* def postList = response.data.cms_post\n* match postList !contains { status : \"draft\" }\n\nThe post list doesn't contain any draft posts.\n* def postCount = (response.data.cms_post.length)\n* assert postCount >= 1\n\nThe post list is greater than 1.\n* def return_obj = response.data.delete_cms_post.returning[0]\n* match return_obj contains { title: \"karate_test\" }\n\nThe return object contains an exact match (in this case to a deleted post).\nRunning Your Tests\nFinally, we'll run the tests in a Docker container.\nFirst, make sure your API is up and running from the earlier docker-compose up - you can then run your test suite with:\ndocker-compose up karate\n\nThis will output your test results in the stdout and create a new  test result HTML report in the ./karate/results directory of your project.\nHappy testing!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Monitoring in Hasura Description: Hasura helps you monitor what matters so that you can optimize your apps, products, and services with precision. Monitoring can also be scoped to specific contributor roles Author: Jesse Martin"}, {"role": "assistant", "content": "Hasura helps you monitor what matters so that you can optimize your apps, products, and services with precision. With our monitoring tooling sitting in the same infrastructure as the metal running your apps, there's no truer source of metrics than what you can find in your console. Monitoring can also be scoped to specific contributor roles so that an architect or business operations account would only be able to access the monitoring tab and not the underlying API settings or data.In this post, we'll do a breakdown the different monitoring sections of the Hasura console.OverviewOverview, as the name describes, shows you the general averages of your API. Requests per minute, success rate, average execution time, active subscriptions and number of open websockets (the primary protocol used for subscriptions at the time of writing.) Lastly, it graphs the results so you can can quickly spot spikes and valleys for a more complete picture of your data.Overview TabErrorsErrors are a consolidated view of the specific errors in your project. They are additionally grouped by most common errors in the window at the bottom of the screen.Errors TabUsageUsage shows you the entire application usage metrics from the frequency of particular operations, the response size, execution time, and more.Usage TabOperationsDrilling down on more details under errors or usage queries, will lead you to the top-level overview of operations. Operations show you every request coming into Hasura as well as the ability to inspect all parts of the request. Further, you can enable logging query variables and display when null/empty values were returned from the database.Operations TabOperation inspect viewWebsocketsWebsockets show you the active, concurrent, websocket connections to Hasura. You can see if any clients are running unusually long, their health metrics, and how well they are performing.Websockets TabSubscription workersSubscriptions are the other half of the websocket analysis as these two are typically bundled together. Subscription workers show you what is being queried, what the responses are, the payload size, and again how many workers are currently running.Subscriptions TabYou can view the monitoring tab walk through in this attached video.Monitoring in Hasura Cloud"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building Hasura - CI/CD and the story of a monorepo Description: In this blog post, I will share the story of the Hasura monorepo, and some internal tooling, that facilitates day-to-day engineering work. Author: Vishnu Bharathi"}, {"role": "assistant", "content": "Internal tooling at HasuraI will open up with the magic word: monorepo(Yes! We have one at Hasura)In this blog post, I will share the story of the Hasura monorepo, and some internal tooling, that facilitates day-to-day engineering work.The monorepoThe Hasura monorepo was born 6 months ago. The reason for its existence is simple: To keep the Hasura GraphQL Engine open source and the commercial codebases in sync.Previously there used to be two GitHub repositories:hasura/graphql-engine (the open source version of graphql-engine)hasura/graphql-engine-pro (the commercial version of graphql-engine)Both the OSS and pro version have three main components:serverconsolecliThe common pattern among them is that the pro components are using the respective OSS components as libraries. Example: the pro server component would have an entry in its cabal file that points to the OSS server as a library like thissource-repository-package\n  type: git\n  location: https://github.com/hasura/graphql-engine.git\n  tag: 10f41e755921667030fc575e210b13e3242badcd\n  subdir: server\n\nThe tag is the commit SHA present in the hasura/graphql-engine repository. This was fine but, as we scaled the team and the frequency of our commits, it began to manifest some problems.\nBirth of the monorepoWhenever an OSS contribution was merged to OSS, it risked breaking the pro code (if it had breaking changes). The merges to the OSS repository were independent from the pro repo and this delayed identifying that something breaking was merged to OSS. It would only manifest at the time when someone tried to upgrade the OSS library in the pro code base. Suddenly someone had to work on fixing the pro code base to sync in the breaking changes from the OSS repository.Whenever an engineer was working on both the OSS and the pro component at the same time, they would need to push their changes to their own fork everytime they made a change to OSS and use the new ref in the pro code base for development.It was time to improve the workflowSome of the results that we wanted to achieve were.Avoid the need for pushing OSS server changes every time (to a git remote) while trying to write code for both OSS and pro simultaneously.If a pull request in the OSS repository contained breaking changes, get feedback about it and fix the pro code base appropriately before merging it. That way OSS never breaks the pro code base.This, ultimately, led to the birth of the monorepo. During this process, the constraint we had was to preserve the histories coming in from multiple repositories. Git definitely made it easy for us.# on the main branch of empty monorepo\n\ngit reset oss/main --hard\ngit merge --allow-unrelated-histories pro/main\n\nHaving both the code bases in different directories of the monorepo removed the need for pushing OSS server changes every time to test them. Because now we could use the local file path of OSS components in the pro components. (1 out of 2 results achieved)Always greenIn-order for the OSS codebase to never break the pro codebase we needed the guarantee that nothing was merged into the OSS repository without first testing the pro codebase with the OSS changes.This means that we needed a way to check if a pull request in the OSS repository was not breaking the pro codebase. The way to check that would be to basically import the OSS repository PR into the monorepo and let the CI run. If it passes, we have the bare minimum guarantee that it won\u2019t break the pro codebase.So we ended up plotting how the developer workflow for this would look and arrived at something like this:Hasura developer workflowShadowing\u201cShadow pull request\u201d (in the above shown image) is the part where we take the code of an OSS pull request, transform it to fit the monorepo, create an equivalent PR in the monorepo, and then review and check by running CI. If a Hasura team member would like to add to the changes of the OSS contribution (such as fixing the pro code for a failing test), they are free to push those commits on the shadowed PR in the monorepo. The shadowed pull request contains a squashed commit of the entire pull request preserving the author and co-author information.Once the review is approved, the shadowed PR in the monorepo gets added to a merge queue and is merged to the `main` branch of the monorepo. The next thing would be to shadow back the merged commits to the OSS repository. The constraint here would be to shadow back only the commits that touch the OSS files.To achieve this shadowing in the monorepo, we needed a tool that could transform source code commit by commit for a git repository. Luckily, Google open sourced such a tool called copybara and it seemed to be doing most of what we wanted to do.CopybaraWhile it saved us a lot of time (Thanks for that!), it definitely missed some things that we needed. For example, making sure to give attribution to multiple authors if a pull request had commits from different authors. Of-course we tackled that by forking and adding the missing parts that we needed. In retrospect, I feel that copybara is too much coupled with Google's workflows resulting in giving a tough time for users outside Google to understand and use it to full potential. There is still stuff that we would very much like to see but couldn\u2019t push for in the upstream or forked version. So, if anyone is interested in a cool side project idea, here is one: \u201ca copybara alternative\u201d - a tool that reads a git repository commit by commit and transforms the contents of the commit to fit in another git repository. If you are taking a stab at it, come talk to us on our discord server - very much happy to brainstorm and contribute - of-course use it at Hasura!GitHub ActionShadowing both ways make use of GitHub actions - it is the place where we run copybara. We chose it for a reason! It helps in what I call the \u201cAutomation for the past, present and future\u201d. We even gave a talk about it a few weeks back at GitHub Satellite.Workflow dispatch - Automation for the past, present, and futureIn case of \u00a0shadowing commits from the monorepo to the OSS repo, this needs to happen only when one or more commits get pushed to our `main` or release branches (like `alpha`)So we trigger a simple GitHub workflow that runs on the `push` event and happily calls copybara to do the magic.name: shadow push\n\non:\n  push:\n    branches:\n      - main\n      - alpha\n\nIn case of shadowing the pull request from OSS repo to the monorepo, this needs to happen for the following casesFor all newly created pull requests that will be created in the OSS repository.For any pull request that is already present in the OSS repository.Using the `on: pull_request` event in GitHub Action will only help us run the automation for newly created PRs. So, we went on with implementing this as a workflow dispatch.Github workflow dispatchWorkflow dispatch is a kind of workflow in Github Action, that we can manually trigger via the GitHub UI (or the API) by giving in a set of inputs. In our case, the workflow dispatch just takes the pull request number from the OSS repository as input and imports it.Using Workflow dispatch gave us the ability to quickly share this shadowing automation with early adopters within the team and get feedback and check correctness of our scripts in real-world use cases. Once we were confident, we took the call and automated the workflow dispatch whenever there is a new pull request or whenever a pull request is updated.At last, our setup looks like this.Workflow architectureConclusionWe hope you enjoyed the taste of our monorepo and the internal tooling we\u2019ve developed at Hasura. We hope to bring more such posts (with hidden side project ideas) and are interested in hearing from you about tools and workflows that could help us.We are actively building these kinds of tools and automation to increase the overall engineering productivity of the team. The ultimate north star that guides in building these tools is the immense feedback that we actively get from the engineers trying out these tools at work. (Thanks to them!)Additional Reading: Creating a GraphQL API for Notion with Hasura Actions"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura 2.0.0-alpha.9 Description: Never one to wait when product is ready, we are excited to announce the release of Hasura 2.0.0-alpha.9 fresh on the heels of alpha.8 Author: Tyler Hannan"}, {"role": "assistant", "content": "Announcing 2.0.0-alpha.9UPDATE: 2.0.0-alpha.10 is now available.Never one to wait when product is ready, we are excited to announce the release of Hasura 2.0.0-alpha.9 fresh on the heels of alpha.8This continues our journey towards 2.0 being fully feature stable. And, while \u2018alpha\u2019 in name, it is not alpha in quality. However, we do use this signifier to denote that it is a work in progress and invite feedback about any problems you may experience. Hasura 2.0.0-alpha.9 docker quickstartHasura 2.0.0-alpha.9 changelogGetting Started with Hasura CloudA Summary of ReleasesAs mentioned in the most recent release post, we have historically announced our releases via Discord and updates to the GitHub release page. As adoption continues to increase, we believe this is insufficient for the future. To that end, we will begin to blog for each release (even if it is only small) so that you can track releases via blog, twitter (@HasuraHQ), Discord, and the Github Release page. In these posts, we will highlight a few specific elements of each release, but the exhaustive list of features and improvements is always in the changelog.OverviewIMPORTANT: There was an issue with 2.0.0-alpha.8 when upgrading from 2.0.0-alpha.7. A workaround is enumerated in issue #6802.Support comparing columns across related tables in permission's boolean expressionsWe now support comparing columns across related tables. For example:\nConsider two tables, items(id, name, stock) and shopping_cart(id, item_id, quantity) and these two tables are related via the item_id column. Now, while defining insert permission on the shopping_cart table, there can be a check to insert an item into the shopping cart only when there is enough stock present in the items table.\nComparing ColumnsConclusionPlease use the docker target hasura/graphql-engine:v2.0.0-alpha.9, try it out, let us know what works and what needs improvement. You can find us on Twitter (@HasuraHQ) and in Github. For ad-hoc conversation, feel free to join us on the Hasura Community Discord.\nAnd, if you want to get started now with the latest stable release, check out the deployment simplicity of Hasura Cloud.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Query hierarchical data structures on Hasura with Postgres ltree Description: Introducing `ltree` operator support in Hasura\n\nThis post introduces support in Hasura 2.0 for queries on hierarchical data structures, with the help of Postgres `ltree` operators. It includes several examples, so we recommend that you create a new project in Hasura Cloud to try them out. Author: Abby Sassel"}, {"role": "assistant", "content": "Query hierarchical data structures on Hasura with Postgres ltreeIntroducing ltree operator support in Hasura\nThis post introduces support in Hasura 2.0 for queries on hierarchical data structures, with the help of Postgres ltree operators. It includes several examples, so we recommend that you create a new project in Hasura Cloud to try them out.\nWhat's ltree?\nltree is a Postgres extension for representing and querying data stored in a hierarchical tree-like structure. This post assumes some knowledge of the ltree extension in Postgres, which you can read about here.\nltree enables powerful search functionality that can be used to model, query and validate  hierarchical and arbitrarily nested data structures. Here are just a few other use-cases, including suggestions from current Hasura users:\n\nconfiguring permission rules in Hasura for a Customer Relationship Management tool.\nmodelling employee reporting relationships, or a marketplace with sub/categories.\nvalidating filesystem paths or DNS records.\n\nExploring ltree\nThis post illustrates some of the newly supported ltree comparison operators in Hasura. We'll be working with the same ltree example data structure mentioned in the Postgres docs, which looks like this:\nexample ltree data structureProject setup\n\nSet up a new project in Hasura Cloud 2.0 or higher with a Postgres database.\nEnable the extension and populate the Postgres database with a value for each label path. You can do this via the Data > SQL tab, as follows:\n\nCREATE EXTENSION IF NOT EXISTS ltree;\n\nCREATE TABLE test (path ltree);\n\nINSERT INTO\n    test\nVALUES\n    ('Top'),\n    ('Top.Science'),\n    ('Top.Science.Astronomy'),\n    ('Top.Science.Astronomy.Astrophysics'),\n    ('Top.Science.Astronomy.Cosmology'),\n    ('Top.Hobbies'),\n    ('Top.Hobbies.Amateurs_Astronomy'),\n    ('Top.Collections'),\n    ('Top.Collections.Pictures'),\n    ('Top.Collections.Pictures.Astronomy'),\n    ('Top.Collections.Pictures.Astronomy.Stars'),\n    ('Top.Collections.Pictures.Astronomy.Galaxies'),\n    ('Top.Collections.Pictures.Astronomy.Astronauts');\n\n-- Optionally, create indexes to speed up certain operations\nCREATE INDEX path_gist_idx ON test USING GIST (path);\nCREATE INDEX path_idx ON test USING BTREE (path);\n\n\n\nVisit the Explorer section of the API tab in your Hasura Cloud project. You should see ltree operators such as _ancestor and _descendant under the path filter options.\n\nHasura Cloud Project Explorerltree operators\nThe standard comparison operators such as _eq, _gt and _is_null were available on ltree data before Hasura v2.0, so this post will only illustrate the newer ltree comparison operators, including:\n_ancestor\nquery {\n  test(where: { path: { _ancestor: \"Top.Collections.Pictures.Astronomy\" } }) {\n    path\n  }\n}\n\n{\n  \"data\": {\n    \"test\": [\n      {\n        \"path\": \"Top\"\n      },\n      {\n        \"path\": \"Top.Collections\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy\"\n      }\n    ]\n  }\n}\n\nFor all ltree queries, the given label path must be valid, otherwise an error message is returned:\nquery {\n  test(where: { path: { _ancestor: \"Top.Collections.Pictures.\" } }) {\n    path\n  }\n}\n\n{\n  \"errors\": [\n    {\n      \"extensions\": {\n        \"path\": \"$.selectionSet.test.args.where.path._ancestor\",\n        \"code\": \"parse-failed\"\n      },\n      \"message\": \"Expecting label path: a sequence of zero or more labels separated by dots, for example L1.L2.L3\"\n    }\n  ]\n}\n\n_descendant\nquery {\n  test(where: { path: { _descendant: \"Top.Collections.Pictures.Astronomy\" } }) {\n    path\n  }\n}\n\n{\n  \"data\": {\n    \"test\": [\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy.Stars\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy.Galaxies\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy.Astronauts\"\n      }\n    ]\n  }\n}\n\n_matches\n\nMatch any label path containing the node Astronomy\n# Match any label path containing the node `Astronomy`\nquery {\n  test(where: { path: { _matches: \"*.Astronomy.*\" } }) {\n    path\n  }\n}\n\n{\n  \"data\": {\n    \"test\": [\n      {\n        \"path\": \"Top.Science.Astronomy\"\n      },\n      {\n        \"path\": \"Top.Science.Astronomy.Astrophysics\"\n      },\n      {\n        \"path\": \"Top.Science.Astronomy.Cosmology\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy.Stars\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy.Galaxies\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy.Astronauts\"\n      }\n    ]\n  }\n}\n\n_matches_fulltext\nIn addition to ltree comparison, Hasura now supports queries on lquery, a String type suitable for pattern matching on ltree label paths.\nMatch any label path containing a node containing the substring \"Astro\"\nquery {\n  test(where: { path: { _matches_fulltext: \"Astro*%\" } }) {\n    path\n  }\n}\n\n{\n  \"data\": {\n    \"test\": [\n      {\n        \"path\": \"Top.Science.Astronomy\"\n      },\n      {\n        \"path\": \"Top.Science.Astronomy.Astrophysics\"\n      },\n      {\n        \"path\": \"Top.Science.Astronomy.Cosmology\"\n      },\n      {\n        \"path\": \"Top.Hobbies.Amateurs_Astronomy\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy.Stars\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy.Galaxies\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy.Astronauts\"\n      }\n    ]\n  }\n}\n\nMatch any label path containing a node containing the substring \"Astro\", excluding any descendants of the (case-insensitive) \"Pictures\" node\nquery {\n  test(where: { path: { _matches_fulltext: \"Astro*% & !pictures@\" } }) {\n    path\n  }\n}\n\n{\n  \"data\": {\n    \"test\": [\n      {\n        \"path\": \"Top.Science.Astronomy\"\n      },\n      {\n        \"path\": \"Top.Science.Astronomy.Astrophysics\"\n      },\n      {\n        \"path\": \"Top.Science.Astronomy.Cosmology\"\n      },\n      {\n        \"path\": \"Top.Hobbies.Amateurs_Astronomy\"\n      }\n    ]\n  }\n}\n\n_any variants\nancestor, descendant and matches operators all have the *_any variant available, too.\nquery {\n  test(\n    where: {\n      path: {\n        _ancestor_any: [\n          \"Top.Collections.Pictures.Astronomy\"\n          \"Top.Science.Astronomy\"\n        ]\n      }\n    }\n  ) {\n    path\n  }\n}\n\n\n{\n  \"data\": {\n    \"test\": [\n      {\n        \"path\": \"Top\"\n      },\n      {\n        \"path\": \"Top.Science\"\n      },\n      {\n        \"path\": \"Top.Science.Astronomy\"\n      },\n      {\n        \"path\": \"Top.Collections\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures\"\n      },\n      {\n        \"path\": \"Top.Collections.Pictures.Astronomy\"\n      }\n    ]\n  }\n}\n\nFurther resources\nIn this post we looked at the newly supported ltree operators, along with some motivating use-cases and examples. A recording of the same demo from Hasura's community call is embedded below.\nPlease add a comment to the original ltree support GitHub issue if you'd like to share your own use case, or request support for other ltree operators.\nSee the API reference in Hasura docs for the up-to-date list of supported ltree operators.\nIf you'd like to extend support for ltree or other operators, here's a CodeFlow walkthrough of the relevant parts of the codebase. We look forward to your contribution!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura 2.0.0-alpha.8 Description: We are excited to announce the release of Hasura 2.0.0-alpha.8! This continues our journey towards 2.0 being fully feature stable. And, while \u2018alpha\u2019 in name, it is not alpha in quality. Author: Tyler Hannan"}, {"role": "assistant", "content": "Announcing Hasura 2.0.0-alpha.8We are excited to announce the release of Hasura 2.0.0-alpha.8!This continues our journey towards 2.0 being fully feature stable. And, while \u2018alpha\u2019 in name, it is not alpha in quality. However, we do use this signifier to denote that it is a work in progress and invite feedback about any problems you may experience. Hasura 2.0.0-alpha.8 docker quickstartHasura 2.0.0-alpha.8 changelogGetting Started with Hasura CloudA Summary of ReleasesMore will be coming, in the near future, about the Hasura release process holistically. We have historically announced our releases via Discord and updates to the GitHub release page. As adoption continues to increase, we believe this is insufficient for the future. To that end, we will begin to blog for each release (even if it is only small) so that you can track releases via blog, twitter (@HasuraHQ), Discord, and the Github Release page. In these posts, we will highlight a few specific elements of each release, but the exhaustive list of features and improvements is always in the changelog. Given this change, this blog will cover not only \u00a0but also some of the capability in alpha.7.OverviewSupport for 3D PostGIS OperatorsWe now support the use of the functions ST_3DDWithin and ST_3DIntersects in boolean expressions.\nNote that ST_3DIntersects requires PostGIS be built with SFCGAL support which may depend on the PostGIS distribution used.\nSupport for null values in boolean expressionsIn v2, we introduced a breaking change, that aimed at fixing a long-standing issue: a null value in a boolean expression would always evaluate to True for all rows. For example, the following queries were all equivalent:\ndelete_users(where: {_id: {_eq: null}})  # field is null, which is as if it were omitted\ndelete_users(where: {_id: {}})           # object is empty, evaluates to True for all rows\ndelete_users(where: {})                  # object is empty, evaluates to True for all rows\ndelete_users()                           # delete all users\n\nThis behaviour was unintuitive, and could be an unpleasant surprise for users that expected the first query to mean \"delete all users for whom the id column is null\". Therefore in v2, we changed the implementation of boolean operators to reject null values, as we deemed it safer:\ndelete_users(where: {_id: {_eq: null}})  # error: argument of _eq cannot be null\n\nHowever, this change broke the workflows of some of our users who were relying on this property of boolean operators. This was used, for instance, to conditionally enable a test:\nquery($isVerified: Boolean) {\n  users(where: {_isVerified: {_eq: $isVerified}}) {\n    name\n  }\n}\n\nThis release provides a way to revert the engine to its previous behaviour: if the HASURA_GRAPHQL_V1_BOOLEAN_NULL_COLLAPSE environment variable is set to \"true\", null values in boolean expression will behave like they did in v1 for the following operators: _is_null, _eq, _neq, _in, _nin, _gt, _lt, _gte, _lte.\nTransactions for Postgres mutations - alpha.7With v2 came the introduction of heterogeneous execution: in one query or mutation, you can target different sources: it is possible, for instance, in one mutation, to both insert a row in a table in a table on Postgres and another row in another table on MSSQL:\nmutation {\n  // goes to Postgres\n  insert_author_one(object: {name: \"Simon Peyton Jones\"}) {\n    name\n  }\n\n  // goes to MSSQL\n  insert_publication_one(object: {name: \"Template meta-programming for Haskell\"}) {\n    name\n  }\n}\n\nHowever, heterogeneous execution has a cost: we can no longer run mutations as a transaction, given that each part may target a different database. This is a regression compared to v1.\nWhile we want to fix this by offering, in the future, an explicit API that allows our users to choose when a series of mutations are executed as a transaction, for now we are introducing the following optimisation: when all the fields in a mutation target the same Postgres source, we will run them as a transaction like we would have in v1, to prevent any breaking changes or regressions for Hasura v1 users upgrading to v2.\nConclusionPlease use the docker target \"hasura/graphql-engine:v2.0.0-alpha.8\", try it out, let us know what works and what needs improvement. You can find us on Twitter (@HasuraHQ) and in Github. For ad-hoc conversation, feel free to join us on the Hasura Community Discord.And, if you want to get started now with the latest stable release, check out the deployment simplicity of Hasura Cloud."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Logging input variables and response body analysis in Hasura Cloud Description: In Hasura cloud, you can turn on the ability (in the monitoring tab) to see what input variables a customer passed in, ie. {topic: \"Historical novels on the transatlantic pottery trade\"} and what was returned, null. Author: Jesse Martin"}, {"role": "assistant", "content": "Knowing what someone was asking for, and what they found, are the two critical parts of understanding what your customers want, and thus, your ability to optimize your offerings.In Hasura cloud, you can turn on the ability (in the monitoring tab) to see what input variables a customer passed in, ie. {topic: \"Historical novels on the transatlantic pottery trade\"} and what was returned, null.GraphQL LoggingBecause of it's high degree of flexibility, GraphQL logging can often be highly implementation specific. By capturing query variables, the user defined input of your queries/mutations, you have a robust toolset to understand the full picture of how your users are using your application, product, or service.In this video, we'll show you where to find the the logging settings, how to view them, and understand why you would want to perform response body analysis.Logging input variables and responses in Hasura CloudNOTE: We will be updating the UI in Hasura Cloud to reflect the ability to 'Enable Response Body Analysis' and 'Capture Query Variables' in the near future."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Remote Schema Joins Description: Remote schema joins provides the most simple approach to federated data. Provide a GraphQL endpoint, identify the connection parameters, and give the new field a name and you have two disparate data sources from different systems resolving under a single API. Author: Jesse Martin"}, {"role": "assistant", "content": "Remote schema joins provides the most simple approach to federated data. Provide a GraphQL endpoint, identify the connection parameters, and give the new field a name and you have two disparate data sources from different systems resolving under a single API.Federating your Data: Different Types of DataNot all data serves the same purpose. User activity data (\"Application Data\") has a strict set of access requirements, time-to-live optimisations, and more. Editorial content, something that might live in a CMS, has an entirely different set of needs - more flexibility, infrequent writes, long-lived cache, etc. Often times the editing interfaces for these two different types of data vary wildly and so it is best to choose the optimal application for each, but not need to sacrifice developer velocity by managing data merging in a proxy layer between your back-end and your front-end.Federating your Data: Bring your Old Stack with YouAnother common case for federating your data is when you have existing architecture in place that you can wrap with a thin GraphQL API but want to leverage newer database technologies or design patterns for your next initiative. In this case, you can still join your data across multiple sources, leveraging the existing infrastructure and embracing the new.In this next video in our \"Learn Hasura Deep Dive\", we'll walk through the basics of federating your data with Hasura Remote Joins.Remote schema joins"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Finding and fixing errors in Hasura Cloud Description: With Hasura Cloud's in-depth error handling you can trace the protocol, the operation name, the input variables, and more, and filter by all of those vectors, to quickly identify which error was experienced and identify the root cause. Author: Jesse Martin"}, {"role": "assistant", "content": "Serverless architecture is renowned for the difficulty to track and trace errors. Whether it's a customer or a developer reporting unexpected behaviour, the cause of the error is often opaque behind the error handling (ie. \"Please contact support\"). As Hasura Cloud sits at the intersection of the entire application stack, it becomes a natural place to surface what originally caused the problem in the first place.Errors, catch them allIn Hasura Cloud, we have in-depth error handling, available under the monitoring tab. You can trace the protocol, the operation name, the input variables and more, and filter by all of those vectors, to quickly identify which error was experienced and identify the root cause.In this deep-dive video we'll cover how to use the error monitoring tab in Hasura Cloud to improve customer-service response times and application hardening.Finding and fixing errorsWe also have an article on the top GraphQL errors and how to fix them."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: HasuraCon\u201921: Announcing the Hasura User Conference Description: We are delighted to announce HasuraCon\u201921! A conference about you, your success, and...of course\u2026full of deep-dive technical content. We are bringing together users from all over the world for a 2 day celebration of all things Hasura! Author: Tyler Hannan"}, {"role": "assistant", "content": "As we say on our Community page\u2026\n\nCommunity is core to the identity of Hasura - Our team is bigger than just our employees. Whether you are joining an event, getting the newsletter, writing code, answering questions, or otherwise, you are welcome here.\n\nAnd now, we have the opportunity to make good on that promise.\nWe are delighted to announce HasuraCon\u201921! A conference about you, your success, and...of course\u2026full of deep-dive technical content. We are bringing together users from all over the world for a 2 day celebration of all things Hasura!\nRegister now, for free, and plan on joining us on the 23rd and 24th of June.\nAny conference often comes with a set of questions that need answered. Below are some of the most common. Do feel free to reach out to us if we can answer anything else.\nWho is speaking?\nWe have already announced 4 speakers. This includes Hasura\u2019s own Tanmai Gopal (CEO / Co-founder) and Davy Stevenson (VP of Engineering). In addition, we are delighted to welcome Guillermo Rauch (CEO / Founder, Vercel) and Erik Reyna (Engineering Lead, Washington Post) to the event.\nAlso, the CFP is now open and can be accessed from the HasuraCon site.\nSome of the topics we are interested in hearing about, include the below. However, we\u2019d love to hear any talk ideas that you may have!\n\nUse-cases & product showcases of applications using Hasura in production\nModernisation stories: How you used Hasura to modernise a legacy tech stack or how you used it to build new apps using an existing database\nPotential impact of Hasura on the industry your product is in\nReference architecture of your Hasura apps.\nTooling that you've created that works well with Hasura.\nProduct integrations with Hasura (devrels, this one is for you :)).\nCommunity oriented talks around using Hasura or being a part of the Hasura community\n\nWho should attend?\nYou.\nNo, but really, you are welcome.\nThe conference will have content that ranges from live, interactive, in-depth workshops to stories of people building products with Hasura to the Hasura team sharing our roadmap to deep-dives into the tech that make Hasura...well...Hasura.\nAre you an indie-dev building something in your spare time?\nThis conference is for you.\nAre you an architect at a multinational enterprise?\nThis conference is for you.\nAre you interested in GraphQL or have a personal attachment to a community that interacts with GraphQL?\nThis conference is for you.\nAll are welcome. All are invited. All types of humans make up the Hasura community.\nHow can I participate?\nPut simply:\n\nRegister Now\nSubmit to the CFP\nContact us about sponsorships\n\nWhy am I seeing all these tickets with #HasuraCon and my favourite tech on twitter as hashtags?\nIn a broadly applicable software offering, like Hasura, there exist smaller communities.\nDo you have an affinity for TypeScript? Show it off on your ticket.\nIs Haskell the language in which you transform your monads? Make sure your friends know.\nAre you a full stack engineer building in a variety of .js frameworks? Pick your favourite and announce it to the twitterverse.\nDo these front-ends frameworks just prove a distraction from your love of a specific database? Ensure folk know about your passion for Postgres.\nAnd of course this \u2018simple\u2019 ticketing feature has some rather interesting code behind it. Perhaps we will cover that in the future.\nAny final words?\nA conference is defined by its speakers and its participants. Please join us in making this an amazing event.\nOur commitment is a diverse and welcoming HasuraCon for everyone, always. We are committed to an event that highlights speakers and participants from underrepresented groups in technology.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL and Government Technology Description: Not surprisingly, the question of GraphQL in government -- or highly regulated -- spaces is a topic that is broached somewhat regularly. Of course, no post about regulated industries (finance, banking, healthcare, public-sector, etc) is complete without a consideration of security and compliance. Author: Tyler Hannan"}, {"role": "assistant", "content": "Not surprisingly, the question of GraphQL in government -- or highly regulated -- spaces is a topic that is broached somewhat regularly. The questions range from:\n\nIs GraphQL appropriate for legacy tech?\nCan GraphQL be used in my modernisation project?\nDoes GraphQL work in a highly-regulated environment filled with a mix of modern and legacy systems?\nCan GraphQL be deployed securely?\n\nThe answer to all is, of course, yes.\nHowever, there are several insights that can be derived from hearing someone describe how they have solved for these specific challenges.\nAdding bleed-edge tech to archaic government\nRecently, Hasura hosted GraphQL Asia. The content was...as always...incredibly compelling. However one talk, in particular, was focused on GraphQL adoption in government (embedded at the end of this post).\nBharat Kashyap, software engineer at Samagra Development Associates, gave a talk described as:\n\nAt the Central Tech Team at Samagra - a governance consulting team working in four different states in India - we've been trying to create a culture of using open-source, scalable and fast (bleeding-edge) tech for government use cases. What we've done so far - enable tracking and monitoring of student and teacher attendance in all public schools of Haryana; building the technology architecture behind Mission Prerna, Uttar Pradesh's mission to transform basic education and enable tech systems that allowed Himachal Pradesh to be declared the country's best state in terms of learning continuity in 2020, according to the Annual State of Education Report. How do we do it - and how does GraphQL fit into the picture?\n\nWhat is GovTech?\nBharat shared the 3 phases of evolution of government technology (particularly in context of the Indian government system) as:\n\nComputerisation\nDigitisation\nPublic Digital Infrastructure\n\nThe latter is an emergent property and less well-defined than the others but Bharat provides detail of his viewpoint.\nInterestingly, and having personally worked in several highly-regulated industries including government projects, the maturation cycle follows quite closely with our experience in a variety of countries. The difference being how, and where, on the journey a country (or industry) is in its journey.\nWhat is an example use case?\nIt differs wildly! In fact, you will see that the use cases in smaller, or private, industry matches quite closely with initiatives in the government.\nIn this talk, the example of e-Samwad was given. This is referred to as \u201cdigital public goods for school education\u201d and includes a mobile application for a parent-student interface, a unified student database, and a learning progress monitoring system.\nInterestingly, this same capability has been taken -- sometimes renamed -- and used to solve similar challenges in neighbouring states.\nLearnings & Benefits\nThis section began with a fantastic turn of phrase, \u201cDeploying GraphQL in the greatest enterprise of all...the government.\u201d In it, several challenges that are faced by government tech providers were enumerated. This included migration from legacy systems with a focus on the wide-variety of products that are already deployed in the public sector. It is interesting to note that Hasura\u2019s support for SQL Server was directly referenced here as important and necessary.\nInterestingly, the second challenge was particularly interesting and simply was called \u2018Community\u2019. To quote:\n\nGraphQL exists because of its active community. But, sometimes the features that we would want to prioritise for governments are not the ones that are being prioritised by the community. That is, of course, a common challenge for open-source systems.\n\nHaving lived through this tension more than once, I am not surprised it is of concern. However, the community at Hasura is incredibly welcoming of -- and open to the needs of -- all categories and types of users. I suspect we will see the same trend more broadly in the GraphQL ecosystem.\nThe benefits can be as important as the challenges. In this case, GraphQL was described as \u201cfast\u201d. Fast to build, to develop, to deploy, to iterate. Also, GraphQL is configurable in that you aren\u2019t dependent on a particular stack or library. And, finally, it is scalable...both in its ability to handle traffic as well as to handle use-cases across multiple government entities or states. In fact, the solutions referenced are deployed in a region with a population of 200M!\nSecurity & Compliance\nOf course, no post about regulated industries (finance, banking, healthcare, public-sector, etc) is complete without a consideration of security and compliance. While this did not feature heavily in the talk (as lightning talks can be restrictive) it is an important topic. This is, in fact, why Hasura Cloud attesting to SOC2 type 1 and HIPAA certifications was such an important item on our roadmap. In fact, recently, we have reworked the entirety of our Hasura Security page to cover the topic of security and compliance more broadly.\nWe all know that compliance is simply a milestone and achievement (or if you are slightly more cynical, a \u2018checklist\u2019). However, adopting the required security posture for applications in highly-regulated environments requires not only compliance but planning your system for confidentiality, integrity, and availability. Our rigorous compliance audits are only a small component of the Hasura offering.\nNext Steps\nIf you are interested in learning more about GraphQL, we\u2019ve pulled together everything from a high-level overview to a comparison of GraphQL vs Rest to more information about the GraphQL Community.\nOf course, the best way to learn is to \u2018try\u2019. Hasura Cloud is compliant, secure, highly-available and has a fully-realised free tier that lets you explore our offering in context of your desires. It takes about 30 seconds to get started.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: 10 months, 11 engineers, 3000 customers & $1B in tradable ARR - How Pipe is using Hasura Description: Pipe have found that Hasura plugs right into their database, removes the need for custom data fetching routes, and reduces the need for front-end developers by 50%. Peter Downs speaks to Steven Yi about it all! Author: Tyler Hannan"}, {"role": "assistant", "content": "How Pipe.com is using HasuraAs a technologist, who talks about the work that others do for a living, it is particularly delightful when a webinar (a conversation with a user) (a conversation with a user) is both compelling and technically intriguing. Hasura's recent interaction with Pipe was both.\nAbout Pipe\nPipe describes themselves as:\n\nPipe transforms recurring revenue into upfront capital for growth without debt or dilution.\n\nFor a year old company, this fintech startup now with over 3000 companies signed up, and  $1B in tradable ARR is fundamentally transforming the way that companies fund their growth. Even more compelling, they are doing so with a globally distributed team. In fact, the webinar (you really should watch it) Peter Downs, Director of Engineering at Pipe spends some time discussing the different ways to structure a company and employee skills in a fully distributed context.\nOf particular import is the note that they are moving away from a traditional Frontend/Backend split of ability and migrating towards 'full stack' focus for their engineering team. To quote, \"We are moving to a world where everyone is able to do a bit of everything.\"\nThis is, in part, where Hasura brings a unique benefit.\nTechnical & Architecture Highlights\nThe architecture of the system, itself, was shared and discussed in some detail. The image from the slides is shared below.\nPipe.com Architecture Diagramme\nWhat we found is Hasura is a fantastic way to synchronise data access to our database.\n\nWhile sharing the architecture, after demoing the Pipe experience, the team shared some of the key benefits of adopting Hasura. This included 50% fewer front-end developers required to build their application than initially planned as \"Hasura plugs right in to our database\" and removes the need for custom data fetching routes. Pipe\u2019s engineering team today consists of 11 engineers!\nIn addition, the query performance was discussed in some detail. Hasura is \"not so bad at planning queries\" and -- somewhat to their surprise -- it was performant out of the box without extensive query customisation or planning. To quote, it \"solves the n+1 problem pretty well\" and \u201cWe\u2019ve seen absolutely no performance problems. It\u2019s been fantastic.\u201d If there are performance problems, it has largely been on the database side and the \u201canalyze\u201d function on the  Hasura console has enabled them to diagnose and troubleshoot the problem and make it go away.\nOf course, speed and simplicity are meaningless without control...particularly when dealing with sensitive customer financial data. So, the permissions model of Hasura allows Pipe to model its business logic with simplicity.\nOf particular interest, to me, was the process of using, testing, and adopting Hasura. In an exploration of Pipe's internal systems it was 11 days from discovery to production. This, by necessity, was not a fundamental re-architecture of the application in its entirety. But followed an incremental approach to deployment allowing for a seamless rollout while determining what API requests were/are/will be appropriate to be proxied through Hasura.\nInterested in Learning More?\nThis, of course, is only a small sampling of what was discussed over the hour of the call.\nWe, as an industry, learn through the examples of others. The team at Pipe open, and transparently, shared about their business, their structure, their thought-process, the benefits of Hasura that they've seen, as well as the roadmap that they see for the future.\nRegister to check out the webinar at and, as always, feel free to try out Hasura Cloud if the information that you hear is compelling.\nYou can also check out our Case Study on How Startup Pipe Went From Prototype to Production In 11 Days With Hasura.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: A guide to integrate Datadog to your Hasura Cloud project Description: Continuing in our series of adding APM integrations with Hasura Cloud, we offer you this quick guide in adding Datadog in a few simple steps. Author: Jesse Martin"}, {"role": "assistant", "content": "We announced our integration with Datadog back in November. With this integration, you can observe all your GraphQL queries and mutations made against your Hasura Cloud project's API, and well as query success, failure, errors, and any exceptions that may occur.ICYMI, we also recently announced our integration with New Relic. In this post, we have a quick video guide on adding Datadog to your Hasura Cloud project in a few simple steps.Why add Datadog to HasuraMonitoring and tracking is a critical component to building robust tech-stacks, and having a native integration for Datadog that can consume your logs and metrics simplifies your APM infrastructure and logging infrastructure in general.What data gets sent to DatadogAt the time of writing, we send logging events across http and ws with the following 5 metrics:Average number of requestsAverage request execution timeSuccess rate of requestsActive subscriptionsNumber of websockets open.You can read more in our docs page.We will be adding additional metrics in the future. If you have a specific measurement need, let us know!How to get StartedYou can reference the documentation above, or view the following guide that shows how to quickly add Datadog to your Hasura projects and view the exported logs.Integrating Datadog with Hasura Cloud"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Hasura Monitoring with New Relic Description: Introducing Hasura monitoring with New Relic as the latest APM integration. Read more about getting started with New Relic today. Author: Jesse Martin"}, {"role": "assistant", "content": "We are excited to announce the latest in our APM integrations with the release of New Relic. Available for Hasura Cloud's Standard Plan and with Hasura Enterprise for on-prem or private cloud deployments, this feature expands our support for logging and metrics to even more teams.Integrating New Relic with Hasura Cloud is a matter of copy-pasting your access tokens into the Hasura project configuration. Taken directly from the New Relic website, New Relic lets you \"instrument, analyze, troubleshoot, and optimize your entire software stack.\"Why add New Relic to HasuraObservability is a critical component to building resilient tech-stacks, and having a native integration for tools that can ingest your logs and metrics simplifies your APM infrastructure and logging infrastructure in general.What data gets sent to New RelicYou can read much more about the integration in our in-depth documentation, but in brief, we send logging events across http and ws with the following 5 metrics:Average number of requestsAverage request execution timeSuccess rate of requestsActive subscriptionsNumber of websockets open.Additional metrics will be added in the future. If you have a specific measurement need, let us know!How to get StartedYou can reference the documentation above, or view the following guide that shows how to quickly add New Relic to your Hasura projects and view the exported logs.Integrating New Relic with Hasura Cloud"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Cascading permissions with inherited roles in Hasura Description: Instead of replicating functionality across multiple role definitions, define each of those abilities in a descriptive way that can then be combined across multiple users. Author: Jesse Martin"}, {"role": "assistant", "content": "Inherited roles in Hasura have been one of the most requested features. Many times roles are a combination of roles that already exist in your system. A standard user may have unlimited read access, an editor the ability to update, an admin the ability to create and delete, and a super-user the ability to do all of those. Instead of replicating functionality across multiple role definitions, define each of those abilities in a descriptive way that can then be combined across multiple users.Inheriting Members and Member-Inheritors with Authorization GroupsLet\u2019s explain this feature in more depth. Imagine a base role called Role A. Role A has the ability to edit items of X class. Role B has the ability to edit items of Y class. Role C has the ability to edit items of Z class.If we define B and C as member-inheritors of A, then B can now edit X,Y and C can edit X,Z respectively. We could add a third role D that inherits from A,B, and C and has the ability to edit X, Y, and Z class items.Multiple Roles in PracticeWhat may be confusing to read in enterprise speak, is very accessible in practice. With the Hasura console, CLI, and API all supporting role inheritance definitions \u2013 defining well architected roles is just a few clicks or commands away.To see how to enable role inheritance in Hasura Cloud, watch this feature\u2019s deep-dive video below.Learn how to create inherited roles in Hasura"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Joint Webinar: How Pipe Uses Hasura To Help Companies Grow On Their Terms Description: Learn how fast-growing Pipe is using Hasura in this joint webinar on Wednesday April 7, 2021 at 11:00 AM PST / 2:00 PM EST. Author: Steven Yi"}, {"role": "assistant", "content": "Learn how fast-growing Pipe is using Hasura in this joint webinar on Wednesday April 7, 2021 at 11:00 AM PST / 2:00 PM EST.Register for the webinarPipe is a fintech startup that\u2019s transforming how companies fund their growth by unlocking their biggest asset - revenue. Through Pipe\u2019s platform, companies can trade their monthly or quarterly contracts for their annual value upfront, instead of taking on highly dilutive equity and restrictive debt.They're also using Hasura to balance delivery velocity, scalability, and to manage the stringent security requirements for financial services.Come learn more about Pipe, and how they\u2019ve built a scalable and modern architecture, and how they\u2019re using Hasura to easily create GraphQL APIs to ship features quickly.Bring your questions - we\u2019ll also have live Q&A!Learn About Pipe. Discover how Pipe is unlocking a new asset class, the challenges they faced, and why they chose GraphQL for their API.Architecture. Learn how Pipe architected their solution and the benefits they gained by utilizing Hasura.What\u2019s Next. See where Pipe is headed next and their technical vision for achieving it.Register for the webinarPresentersPeter Downs, Director of Engineering at PipePeter is the Director of Engineering at Pipe, joining as one of the company\u2019s first engineering hires. Prior to Pipe, Peter worked at Stripe and numerous tech startups including, Coinbase, B12 and more. Peter holds a degree in Computer Science from MIT.Steven Yi, Head of Product Marketing at HasuraSteve leads product marketing at Hasura. \u00a0Steve has held leadership roles at Microsoft, Red Hat, and multiple startups - ranging from software architecture & development, developer tools, cloud, mobile development, and AI."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Working with Dates, Time, Timezones in GraphQL and PostgreSQL Description: In this post, we will look at the various data/time types available through Postgres and exposed via GraphQL by Hasura. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Dates and timezones are one of the most commonly used data types in building modern apps. GraphQL comes with default scalar types like Int, Float, String, Boolean and ID. But dates and times have to be defined as custom scalars like Date or timestamp etc. In this post, we will look at the various data/time types available through Postgres and exposed via GraphQL by Hasura.All the types are Implicitly supported and hence the values for them should be given as a String and Postgres takes care of the rest.Datedate is a GraphQL custom scalar. Allowed values are yyyy-mm-dd. For example: in a mutation, you would use it in the following format:mutation {\n  insert_users(objects: [{ date: \"1996-03-15\" }]) {\n    returning {\n     id\n     name\n    }\n  }\n}Time with time zonetimetz is the GraphQL custom scalar for denoting time of day only, with time zone. Allowed values should be of ISO8601 format (e.g. 17:30:15Z, 17:30:15+05:30, 17:30:15.234890+05:30). For example: in a mutation, you would use it in the following format:objects: [{ time: \"17:30:15+05:30\" }]Timestamp with time zoneTo use both date and time, with time zone, there is a GraphQL custom scalar of type timestamptz. Allowed values should be of ISO8601 format (e.g. 2016-07-20T17:30:15Z, 2016-07-20T17:30:15+05:30, 2016-07-20T17:30:15.234890+05:30). For example:objects: [{ created_at: \"2016-07-20T17:30:15+05:30\" }]where created_at is the timestamptz column.Reusing Postgres types in Custom CodeHasura lets you write custom code through webhooks in the form of Actions, Remote Schemas and Events. In case of Actions, you can define the custom GraphQL types for the definition.ActionsLet's define an action for fetching user data.type Query {\n  fetchUser (\n    id: Int!\n  ): UserOutput\n}The UserOutput will return fields of different date/timestamp variations.type UserOutput {\n  id : Int\n  name : String\n  created_at : timestamptz\n  dob : date\n  time_of_activity : timetz\n}Here, created_at is of type timestamptz, dob is of type date and time_of_activity is of type timetz, all reused from Postgres.Action with Custom GraphQL TypesRemote SchemasWhile creating a remote schema, the date/time values will come from a custom defined GraphQL scalar. You can make use of the community maintained GraphQL scalars.To start off, install the module:npm install --save graphql-scalarsand assuming that you are using apollo-server to write your custom GraphQL server, the following code will give you a rough idea.import { ApolloServer } from 'apollo-server';\nimport { makeExecutableSchema } from '@graphql-tools/schema';\nimport { DateTimeResolver, DateTimeTypeDefinition } from \"graphql-scalars\"\n\nconst server = new ApolloServer({\n  schema: makeExecutableSchema({\n    typeDefs: [\n      ...DateTimeTypeDefinition\n    ],\n    resolvers: {\n      ...DateTimeResolver\n    },\n  }),\n});\n\nserver.listen().then(({ url }) => {\n  console.log(`\ud83d\ude80 Server ready at ${url}`);\n});We are importing a specific scalar, DateTime and using them inside the typeDefs and resolvers. The custom scalars supported by this library around dates / times are:DateTimeDateLocalDateLocalEndTimeLocalTimeTimeTimestampIn your schema, you can return an actual scalar type for the a created_at field on the User type.type User { \n  id: Int! \n  text: String! \n  created_at: Date! \n}Using in a computed fieldIf you are storing date and time values in separate columns and you want to merge them to return a single column, you can create a Postgres function like:CREATE FUNCTION date_and_time(row, users)\nRETURNS TEXT AS $$\n  SELECT row.date || ':' || row.time\n$$ LANGUAGE sql STABLE;Postgres function to merge date and timeCreating Postgres views for modifying typesYou can make use of views to modify raw strings to actual date/timestamptz values, provided the string conforms to the data type specification of PostgreSQL. For example:CREATE OR REPLACE VIEW \"view_with_timestamp\" AS \n SELECT users.id,\n    users.created_string :: timestamptz\n   FROM users;This view contains type cast of string column (created_string) to a timestamptz column. Any string column can be type cast to another data type if the value conforms.Using server timestamp inside a mutationConsider having a timestamptz column and typically you would be passing a value from the client. Since it's an implicit type, you will need to send a string value. But the string value can also be a Postgres expression like now(). This will set the value to the server timestamp. For example:mutation insert_user {\n   insert_user_one(object: { id: 1, name: \"Praveen\", created_at: \"now()\"\n   }) {\n     id\n   }\n}Querying with rangesSuppose you have a date range to query, you can do so in Hasura using the $gte and $lte operators.query {\n  users(where: {last_login_date: {_gte: \"2021-01-20\", _lte: \"2021-02-20\"}}) {\n    id\n    name\n  }\n}GraphQL SubscriptionWhen you want to subscribe to \"events\" or \"live results\" from the backend, typically it will be based on timestamps. For example, if you wanted to listen to a realtime feed updates from the client, you can make a subscription request, ordered by timestamp and sorted by latest desc.subscription notifyNewData {\n    feed (limit: 1, order_by: {created_at: desc }) {\n      id\n      name\n      created_at\n    }\n  }Here, the created_at is a timestamptz column sorted by desc and limited by 1 row. This pattern is useful to subscribe to new data for a feed and further making a GraphQL query based on the newly available data.If you are wondering which data type to use for what use case, check out the video from Adron going over the types in Postgres that's best suited for timestamps.Is there a good data type to use for time stamps in Hasura?Do you use timezones? Let us know in the comments on what data types do you most commonly use for dates/timestamps."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: OWASP, SAMM and Hasura Description: This document will outline Hasura\u2019s security-first approach to running our own services, the security tooling we provide that allows our customers to pursue OWASP compliance and our commitment to achieving high marks according to the SAMM maturity model. Author: Jesse Martin"}, {"role": "assistant", "content": "The goal of the Open Web Application Security Project, or OWASP, is to help ensure that the exploding web-first market can develop with a security-first mindset. For nearly 20 years, OWASP has evolved into an extensive set of best practices, community resources, open tooling, and more - all in pursuit of this goal. Today, OWASP is most famous for the \u201cTop Ten\u201d framework on structuring secure applications. As the industry expands into a micro-service driven approach, it\u2019s important for organisations to validate all of their dependencies according to the OWASP framework.When you abstract these micro-services into hosted products - \u201cSaaS\u201d, the one-time-compliant status with OWASP is not enough. Modern companies need to be assured that the business in question will maintain a security-first mindset, which is where SAMM, the Software Assurance Maturity Model, comes into play.Hasura provides developers the data and API platform for developers to make better apps, faster. We provide industry-standard security for everything we make. Once you start using Hasura Cloud with VPC, or self-hosted Hasura Enterprise, we provide in-depth training and tutorials to ensure that you are able to maintain the same rigorous security standards we maintain ourselves. Being an open-source software company dependent on other open source components and standards, there is a high degree of community scrutiny that ensures any new issues have multiple layers of checks where they can be caught and appropriately handled. There\u2019s no black-box of magic running any of our services.This document will outline Hasura\u2019s security-first approach to running our own services, the security tooling we provide that allows our customers to pursue OWASP compliance and our commitment to achieving high marks according to the SAMM maturity model.OWASP FeaturesInjectionHasura doesn\u2019t allow for literal string substitution and all queries are escaped during the query planning and compilation pipeline that processes and executes API requests. There\u2019s no opportunity for untrusted code to execute in the Hasura application.Broken AuthenticationHasura supports two authentication strategies: either through JWT or through a webhook call to a trusted service. In both situations, the authentication is handled behind the Hasura service and not open to manipulation from the client.Sensitive Data ExposureHasura has industry-leading access control capability. We can lock down access for both roles and records. With our granular access checks and access controls for remote schemas, you can maintain the observability of access controls in a single platform.Additionally, the paid Hasura offerings enforce HTTPS by default (our open source offering is up to the implementation strategy of the development team.) Available for early access is our audit log utility which provides in-depth information about all actions occurring within the platform and across data access - whether reads or writes.XML External EntitiesWhile not one of the more common data-types we see passed through our service, all XML reads and writes are escaped.Broken Access ControlBecause we have fine-grained access controls, we support extensive tooling to ensure that those access controls are tightly monitored. There\u2019s observability on individual tables as well as a general overview. With full support for migrations, you can generate access controls programmatically and enable \u201csafety check\u201d queries and mutations as part of a CI/CD process.Security MisconfigurationOur configuration files are validated as part of our type-checking process, which reduces the overall surface area for programmer error. The audit log will also allow for quick isolation of where changes occurred to identify and proactively deal with any risks.Cross-Site Scripting XSSWe don\u2019t allow session-based access which mitigates a large array of attack vectors. That said, it\u2019s still possible to have mistakes in your services. We provide code-gen templates and a wide array of technical resources to help you embrace best practices in your infrastructure.Insecure DeserializationWe do not run untrusted code. The only user-supplied JSON that gets executed is our declarative configuration metadata (that is supplied by the team that owns and manages Hasura), which is strictly parsed by our type-checking system. All other deserialisation conforms to industry standards for delivering JSON from a server.Using Components with Known VulnerabilitiesWe self-host all of our libraries in the critical path and perform regular audits to ensure no vulnerabilities are present in our hosted applications and downloadable binaries.Insufficient Logging & MonitoringAll of our paid services come with extensive logging options and the tooling itself allows for trivial custom logging solutions through database triggers and logging events. Hasura\u2019s paid offering provides deep insight into what was queried, what the performance metrics were as well as offering fine-grained control for rate-limiting and more.SAMMSAMM, like any good maturity model, is a barometer for a company\u2019s security aptitude and wherewithal. Broken down into five categories, SAMM provides a helpful framework to compare a company\u2019s \u201csecurity readiness\u201d to ensure that current OWASP standards are products of intention and not a coincidence. While our security process was not designed according to the SAMM model intentionally, it aligns as they are both derived from industry best practices and therefore acts as a good evaluation of Hasura\u2019s commitment to security.GovernanceAs a certified SOC2 Type 1 and HIPAA compliant service provider (Hasura Cloud), we have undergone extensive security planning. Additionally, as part of our ongoing compliance, we are required to train our employees to comply with the standards of SOC2 & HIPAA in order to maintain these certifications.DesignWe have a threat assessment team with regular audits on our critical path to ensure that no vulnerabilities are introduced. We also run regular reviews of our architecture with the express purpose of looking for, finding, and eliminating new risks and threats.ImplementationWe have a completely managed build system and deploy targets for all our services. Our software goes through an extensive public beta program allowing for additional threat assessment. We also follow blue-green deploys to ensure that we can roll back to a previous release with no down-time.VerificationWe continue to hire and maintain major industry contributors to review the security of our application. The afore-mentioned beta program ensures that there are many eyes and many deployments testing a release before it ever goes into production.OperationsWe have versioned environments and a multi-tier incident response team with rotating schedules clearly defined as per compliance with our other certifications.Hasura sits between your data and your application. It\u2019s a responsibility we take seriously and any compromise in this regard would be an existential threat to our business. We have built a tremendous amount of trust with our community over the last 100 million plus downloads of our product. It\u2019s something we prize above all else and will continue to be essential to the Hasura DNA."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Azure Active Directory Integration with Hasura JWT Claims Mapping Description: The power of customisation is that Hasura, and our authn/authz capabilities, can support a wide variety of providers and implementations. Take, for example, the process of integrating Azure Active Directory with Hasura.  Author: Allison Kunz"}, {"role": "assistant", "content": "Azure Active Directory IntegrationThe power of customisation is that Hasura, and our authn/authz capabilities, can support a wide variety of providers and implementations. The opportunity that results is that flexibility in implementation results in a wide variety of adoption patterns, services considered, etc.\nTake, for example, the process of integrating Azure Active Directory with Hasura. In particular the process of implementing JWT claims mapping.\nIn short, some auth providers do not let users add custom claims in JWT. In such cases, the server can take a JWT configuration option called claims_map to specify a mapping of Hasura session variables to values in existing claims via JSONPath or literal values. The claims_map is a JSON object where keys are session\nvariables and values can be a JSON path (with a default value option, when the key specified by the JSON path doesn't exist) or a literal value. This capability is described in further detail in the documentation.\nWhile working with users on the implementation, we've pulled together a sample process that illustrates how this can be achieved and built a simple presentation deck here\nNOTE: See the end of this post for notes on usage with Azure AD (for non-B2C tenants)\nPrerequisites\n\nAzure account with subscription\nAzure functions core tools installed (VSCode with Azure Functions extension recommended)\nHeroku account\n\nSet up Hasura Cloud\n\nCreate an account at Hasura Cloud, and generate a new free-tier project\n\n\n\nAfter the project is created, it will bring you to your project settings page. Click \"Launch Dashboard\" at the top right to open the Hasura web console\n\n\n\nFrom the Hasura web console, you will be greeted with a getting-started menu. At the top of this menu, select \"Connect your first Database\". (If this menu is not present, or you have dismissed it, you may also click the \"DATA\" tab at the top of the page)\n\n\n\nClick \"Connect Database\"\n\n\n\nSwitch tabs on the page you're navigated to, from the default \"Connect existing Database\" to \"Create Heroku Database (Free)\". Then press the \"Create Database\" button to automatically create and connect a Hobby-tier Heroku DB to your Hasura Cloud instance (requires a Heroku account).\n\n\n\nAdd a \"users\" table to your new Hasura project with fields id, azure_id, and role.\n\n\n\n\nMake a few fake users and check that your users query works as expected\n\n\nAdd \"user\" permissions, limiting access to non-admin queries to only those authenticated users who have session variable x-hasura-user-id that matches the user's id\n\n\n\nSet up Azure Functions\n\nCreate a directory for your new Azure functions project and initialize it. You'll be taken through a couple of setup questions: in this demo, we've selected \"node\" and \"JavaScript\"\n\n$ mkdir <functions-directory-name>\n$ cd <functions-directory-name>\n$ func init .\n\n\nCreate your first function! Again, you'll be asked a couple of questions: here we selected \"HTTP trigger\" and named our function \"getHasuraInfo\". Check to see that it all works locally, but then you can stop the server.\n\n$ func new\n$ npm install\n$ func start\n\n\n\nFor the moment, we'll keep things very simple. Just hard-code your custom claims into the response body; we'll add the business logic later. Check again that it still works locally.\n\n\nDeploy! (This article is helpful for setting up VSCode and deploying via the VSCode Azure Functions extension.) Get the function endpoint with code included\n\n\nSet up Azure AD B2C\n\nSet up an Azure AD B2C tenant according to the docs\n\n\n\nAt the 'Grant admin consent for (your tenant name)' step, you can grant by clicking the \". . .\" button next to \"Add a permission\"\n\n\nWhen setting up your app, add https://jwt.ms as a Web Redirect URI. This is a useful tool for checking Azure JWT's\n\n\n\nSet up the B2C tenant for custom claims according to the docs\n\n\nThis demo skips the Facebook social login\nThis step is just setup of the custom claims flow; we will add the actual custom fields in the next step\n\nTest that you can generate a valid JWT with this new flow\n\nNavigate back to Identity Experience Framework dashboard\nScroll down to see the list of custom policies, select \"B2C_1A_signup_signin\"\nSelect https://jwt.ms as the reply url\nRun! Sign up and see that a new user is created and a valid JWT provided\n\n\nModify your Azure AD B2C setup to include your custom claims\n\nFollow the instructions from this tutorial to modify your custom policies with custom claims\n\n\nSee the demo custom policy files in this repo\nUse the static-response version of your Azure function for now\n\n\n\nTest with \"B2C_1A_signup_signin\" again to verify that you're getting a valid JWT with the expected (static, fake) claims\n\n\nUpdate and re-deploy your Azure function with the actual business logic to provide the desired claims response. See 'getHasuraInfo.js' in this repo for an example that checks for an existing Hasura user with that Azure ID, creates a new one if Azure ID not found, and returns the role + user ID to be provided in the final claim.\n\n\n\nSet up environment variables locally in local.settings.json under \"values\". Add the environment variables for prod via the Azure Portal: go to your Functions App, navigate to 'Configuration' (under 'Settings'), and add your environment variables as new application settings. Don't forget to save!\nCheck again that everything still starts locally as a sanity check before re-deploying the function\n\n\nTest again with \"B2C_1A_signup_signin\" again to verify that you're getting a valid JWT with the expected claims and that a user is created in Hasura\n\n\n\nUpdate your Hasura instance to use Azure tokens\n\nAdd the HASURA_GRAPHQL_JWT_SECRET environment variable to your Cloud app from the settings page in the Cloud dashboard:\n\n\n\nSet the following as the value for HASURA_GRAPHQL_JWT_SECRET, to configure the JWK url and use the \"Claims Mapping\" feature (see more here) to map Azure AD's claims to the values Hasura needs for it's JWT tokens:\n\n{\n  \"jwk_url\": \"https://<mytenant>.b2clogin.com/<mytenant>.onmicrosoft.com/discovery/v2.0/keys?p=b2c_1a_signup_signin\",\n  \"claims_map\": {\n    \"x-hasura-allowed-roles\": [\"user\", \"admin\"],\n    \"x-hasura-default-role\": { \"path\": \"$.extension_hasura_role\" },\n    \"x-hasura-user-id\": { \"path\": \"$.extension_hasura_id\" }\n  }\n}\n\n\n\nLog in with \"B2C_1A_signup_signin\" again and check that the JWT now includes your custom claims. Copy the token\n\n\nUse the token via an Authorization header in the graphiql playground in the Hasura console. Check that you can query for users and get back only the one whose user ID matches the one in the token!\n\n\n\nUsing with Azure AD (not B2C)\nWith Azure AD, you'll probably have a tenant with set users and groups (no arbitrary sign-ups). With this context, it'll probably make the most sense to\n\nExport the existing users from your Azure AD tenant\nBulk import the user data into a users table in Hasura with the Azure AD objectId (idToken claim: oid) as the user id. Roles could be created from group id's or separately.\n\nHere is an example of JWT token claims (with group id's included):\n\nAnd here's an example of the JWT secret with the claims_map added:\n{\n  \"jwk_url\": \"https://login.windows.net/common/discovery/keys\",\n  \"claims_map\": {\n    \"x-hasura-allowed-roles\": { \"path\": \"$.groups\" },\n    \"x-hasura-default-role\": { \"path\": \"$.groups[0]\" },\n    \"x-hasura-user-id\": { \"path\": \"$.oid\" }\n  }\n}\n\nThis maps the token's oid (Azure AD user id) and groups as Hasura user id's and roles, respectively. With this, you could do interesting permissions configurations such as per-group (now 'role') access controls.\nHow to automatically update new/updated user info to your Postgres database, and how to architect key Hasura session variables like roles, group or team ID's, etc. is up to you!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Adding multiple Postgres databases to Hasura Description: Another new feature exclusive to the Hasura 2.0 release is support for multiple Postgres database types. In just a few clicks you can add an an extra database, existing or new, track the tables, enable relations, and unlock the power of GraphQL. Author: Jesse Martin"}, {"role": "assistant", "content": "Another new feature exclusive to the Hasura 2.0 release is support for multiple Postgres database types. In just a few clicks you can add an an extra database, existing or new, track the tables, enable relations, and unlock the power of GraphQL.Supporting multi-tenant, federated data graphs with HasuraThe reason multiple database support is so helpful is that it enables two powerful goals in modern app development, namely, multi-tenant architecture and federated graphs, also called unified graphs.Multi-tenant architectureMulti-tenant architecture, or multi-tenancy, is the ability to have isolated data structures maintained in a unified ops platform. Different application types often require different data structures, or new initiatives often inherit legacy database structures that are expensive or difficult to migrate, and maintaining out-of-date practices would lead to slower development of the new features/initiatives. To maintain these disparate systems leads to managing login/authentication in multiple systems, log management in different platforms, and, quite often, billing complexity spread across a wide array of cloud providers.Unified dev-opsWith Hasura you can add your existing databases so that you have observability, tracking, maintenance, and migrations in a single console. You no longer need to pay the high cost of context switching or negotiate the dance of browsers sessions. With a singular backend, you also have a common method for accessing data which leads to an increase in developer efficiency.Data model optimization with polygot workloadsModern applications are a hodgepodge of different data structures. Some data needs to be wide-table, some data needs to be highly descriptive, others need to be ultra lean. Some data needs to be optimized for tight access controls, others for high-read. These data models are often custom tailored for transactions, time-series eventual consistency, or for multiple indexes for multi-dimensional analytics. Switching between single-purpose providers for each of these services can lead to significant cognitive, financial, staffing, and security overhead. Supporting and unifying all these services with a single back-end again yields substantial operational improvements.Creating federated graphsThe last large benefit we see arise as a bi-product of this feature is support for federated (unified) graphs out of the box. As applications increase in sophistication and embrace wildly varying workloads, being able to communicate across data sets without sacrificing on performance or security allows teams to surface insights that otherwise lay behind layers of complicated infrastructure or sitting at the bottom of a data lake.Multiple Postgres support will substantially change the way teams build modern applications and we can't wait to have you try it out. Learn more in the video below.Learn how to add multiple Postgres to Hasura.Here's an excellent article by Aleksandra on how our team used Typescript template literal types to build multiple database support."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura 2.0 Engineering Overview Description: This release contains some large refactorings to the core of the Hasura product, as well as some large new feature additions. I\u2019m going to talk about the software engineering tasks we tackled, challenges we encountered and how we met them. Author: Phil Freeman"}, {"role": "assistant", "content": "Refactoring the Core and Generalising the FutureAt the end of February, we had the pleasure of announcing the first alpha release of Hasura GraphQL Engine version 2.0 - the culmination of over six months of engineering effort across the company. This release contains some large refactorings to the core of the Hasura product, as well as some large new feature additions. I\u2019m going to talk about the software engineering tasks we tackled, challenges we encountered and how we met them.I\u2019d like to say a big congratulations and thank you to all of my colleagues who were involved in the release - it\u2019s a great achievement, and it wouldn\u2019t have been possible without everyone\u2019s dedicated work.Summary of changesThe 2.0 release includes several milestone changes in the core of the product:Switching to the new \u201cParse Don\u2019t Validate\u201d approachSeparating the storage of Hasura\u2019s metadata from the Postgres databaseSupport for multiple Postgres backendsSupport for new kinds of relational database backend, starting with SQL ServerSupport for inherited rolesSupport for REST endpoints in addition to GraphQL (which Lyndon has already covered in his blog post here)Despite this long list of features, 2.0 is still backwards compatible with previous releases (after an automated metadata upgrade), so you can update your Hasura deployments today. We have started the process of rolling out 2.0 upgrades on Hasura Cloud already, and cloud users can expect the upgrade at some point in the coming weeks.We\u2019ll take a look at each of the larger changes in turn, and the issues we faced while implementing them.Parse, Don\u2019t Validate!The journey from version 1.3.3 to version 2.0 began in August last year, when we started the process of refactoring the core of our application logic to reflect the \u201cparse, don\u2019t validate\u201d (or PDV) principle. The idea is outlined in Alexis\u2019 excellent blog post, but I\u2019ll give a simplified overview:When receiving unvalidated inputs, attempt to convert (i.e. parse) them into values whose types refine the types of the inputs, ruling out invalid states where possible, using the information learned during validation.Don\u2019t simply check for invalid states and pass along the inputs regardless (i.e. don\u2019t just validate)An example illustrates this nicely, and I\u2019ll paraphrase the one from the blog post. If we receive a list of data as an input, and we wish to validate the property that the list is not empty, then we should not simply check for an empty list, and pass along the input unchanged. If the input list is in fact not empty, then we can give its value a stronger type - the type of non-empty lists. We\u2019ve turned our validator into a parser which parses possibly-empty lists into non-empty lists, or fails with a validation error.Simple examples like these are found everywhere in real-world code. If I encounter a date represented as a string, I can parse it into my programming language\u2019s representation of an actual date at the point of validation. We see these simple examples all the time when (for example) parsing JSON structures into typed representations. But what does this have to do with the Hasura 2.0 release?Server MetadataIn Hasura, we configure our running GraphQL servers with a data structure which we refer to as metadata. The Hasura metadata can be exported and manipulated as a JSON/YAML document, or manipulated indirectly by the running server when we perform operations such as tracking tables. It contains information about all of the data sources that we can talk to, any tables and columns references in relational data sources, relationships between those, permissions, functions, computed fields, and so on - everything needed to take your data and turn it into a functioning GraphQL service.In older versions of the product, we would essentially validate the input metadata and store it directly in the application\u2019s schema cache. The schema cache is our own in-memory copy of the metadata, optimized for the lookups we need to perform while servicing GraphQL operations.As you can probably guess, in version 2.0, we\u2019ve switched to parsing the input metadata as much as possible, refining it in various ways to capture all of the things we know about it after validation. For example:Unchecked table definitions in the input metadata get elaborated in the schema cache by adding relevant column information from the data source itself.Unchecked relationships between tables are parsed into objects graphs on the Haskell heap itself.Unchecked remote schema definitions in the input metadata get elaborated in the schema cache by adding schema information discovered by introspection. Any remote relationships in the metadata can also be cross-referenced with the result of introspection.Unchecked REST endpoint definitions in the input metadata get turned into a tree of validated endpoint definitions (ruling out any possible overlapping definitions), each accompanied by a parsed and validated GraphQL AST which will be used to serve any HTTPS requests on those endpoints.Dealing with memory issuesThe parse-don\u2019t-validate approach allowed us to rule out a lot of error cases in our data, because after we have done our initial checks on the input data, we can use the Haskell type system to represent the condition on the data that we have checked. However, the approach came with a cost - after implementing and merging the PDV changes, we found that the application\u2019s memory usage had increased on certain workloads. We weren\u2019t able to detect any regression during development because the new system was functionally equivalent to the original (not counting memory usage), and we didn\u2019t have adequate performance benchmarks in our CI system (lesson now learned - we will be integrating a regression testing benchmark suite into our build system soon).This regression was a real concern because we had already merged the functionality to the main branch, and we had merged several other changes on top of the PDV changes before we noticed the regression. For a while, we maintained two parallel branches while we worked to identify the source of the memory issues, but this forced us to backport many fixes to the release branches, slowing down the development process.Fortunately, we were able to identify the issue, collaborating with the excellent GHC / Haskell folks at Well-Typed: the brand new ghc-debug tool is able to let us inspect the Haskell heap at runtime, and look for unexpected patterns of behavior. In this way, we can identify conditions such asReferences which are escaping their intended scopeLazily-evaluated values which are not being forced in a timely manner (i.e. space leaks)Values which are being forced unnecessarilyValues on the heap which are being duplicated instead of sharedIn our case, we were able to notice two of the conditions above: a reference to a large value was leaking causing that value to be retained on the heap unnecessarily, and also we were not taking advantage of sharing on the heap. Using ghc-debug, Well-Typed helped us to identify and fix these two issues.Now, the baseline memory consumption had gone up compared to the v1.3.3 release, and looking at the profile, we ruled out the case that there was an obvious memory leak, and instead decided that this was just a price of the new abstractions we are using. As it turned out, this was not the end of the story, but we were happy enough with the application\u2019s memory patterns for a broad category of use cases.Well-Typed has written about ghc-debug in the context of Hasura.Supporting Multiple SourcesThe second large change included in the 2.0 release is the support for multiple database sources, and for different kinds of sources. Specifically, we now support zero or many Postgres databases, and zero or many SQL Server databases, and we\u2019re planning support for several other popular relational databases. In the longer term, we\u2019ll be looking at supporting other non-relational sources as well.From the user perspective, this dramatically simplifies the setup cost for a new Hasura Cloud deployment. There is no longer a need to have a database ready, or to create a database in order to get started. Simply create a fresh Hasura Cloud instance and open it - if you want to test out Postgres connections, you can now do so entirely from within the console, but you could also get started by connecting various actions and remote schemas instead, with no database sources whatsoever.Multiple PostgresOn the engineering side, the first step was to support multiple Postgres sources, and this is not as simple as it might sound. We had to deal with things like namespace collisions across sources, connection pools whose lifetimes were no longer tied to the lifetime of the running server, several fundamental changes to the user interface, and changes to basic APIs which had assumed the existence of a single source database.In order to support the case of zero Postgres databases, we also had to decouple the storage of Hasura metadata from the storage in the source database itself. Again, this was no small task, but we are now able to store our metadata in a completely separate database from your own data (via a new configuration option). In the case of Hasura Cloud deployments, we will host all of the metadata on our own servers, so it is no longer a user concern at all. In addition, we\u2019ve taken the opportunity to improve several things about metadata storage, including adding optimistic locking to the data involved (avoiding race conditions during multiple updates in the UI), and simplifying the approach to synchronized schema updates across Hasura nodes in a high-availability setting.Different databasesThe second engineering step was to add support for multiple types of data source, including support for SQL Server. Again, this was a large refactoring effort, but thankfully we have been able to refactor with confidence due to Haskell\u2019s strong type system and our extensive test suite.Almost every aspect of the operation pipeline from parsing to type-checking to execution has been generalized in 2.0. It is not simply a case of generating the same SQL and sending it to a different type of server (wouldn\u2019t that be nice!). Instead, we need to consider several issues:It may not be possible to push down N+1 queries and other sorts of operations to the database: in Postgres, we did this using jsonb aggregations, but we need to take a different approach with SQL Server and other sourcesSome Postgres data types are simply not available in other databases: json(b), ltree, geometry and geography, and so onDifferent databases may have subtly different behavior when it comes to things like table and column naming (case sensitivity, for example)We may have to turn off features entirely for different data sources, temporarily at least.In order to solve these problems, we introduced several new type classes (similar to interfaces in OO languages, if you are not familiar) into our code, which defined and abstracted over the various backend operations we need to support:The Backend type class, which is the root of the new hierarchy of these abstractions, defining the data types for various leaf nodes and relationships in our AST. For example, the type of connection information is defined here, but also types such as the type of table names for a data source (you may think this should always be a string, but it may be a case-insensitive string, or even an integer if we consider supporting Redis).The BackendSchema type class, which is responsible for defining any GraphQL root fields exposed by a data sourceThe BackendExecute type class, which is responsible for constructing execution plans, and BackendTransport which is responsible for executing them.The idea is this: if you want to implement a new backend for Hasura, simply implement these new type classes. We will be trying to improve the development story in this area, making it as painless as possible to contribute new backends in this way.Aside: the basic structure of the syntax trees used for Postgres and MSSQL are very similar, with differences only at the leaves of the tree, for example, to turn off features which do not exist or are not supported in SQL Server. We have implemented the ideas from the Trees that Grow paper (also used in the GHC compiler to progressively turn off AST features during the various phases of the compiler pipeline) in order to get maximal code reuse from the commonalities in these syntax trees.Designing and implementing these type classes has proven to be straightforward enough, but we have gone back and forward deciding how they should be incorporated into the broader product design. The basic problem is that we have implemented various backend types such as Postgres and MSSQL (on which the type class implementations for the classes above will be pinned), but at the outermost layer of the application, we only want to present an API which talks abstractly about a collection of some database sources, not any particular implementations in general. That is, in our source list, we might want Postgres and SQL Server implementations side by side in the same list - and mixing values of different types in a single list is not something that Haskell\u2019s type system is particularly happy about.Fortunately, Haskell allows us to unify the various backend implementations in a single list using a feature called existential types, and this is what we have decided to use to solve the problem. To the extent that type classes are like interfaces in OO languages, this approach is similar to using bounded quantification: we erase enough type information to store values with different underlying implementations in the same list, but keep their specific implementations available in the form of their type class instances. Just like bounded quantification, the downside is that we can no longer take advantage of features specific to (let\u2019s say) the Postgres implementation, but only access the sources via their type class instances.Memory issues, againAs we were nearing the end of the development cycle for 2.0, we ran into another unfortunate memory issue. We had already eliminated the memory leaks introduced by the PDV refactoring, but something strange was now showing up with mutation-heavy workloads.During workloads involving long bursts of mutations (inserts, updates, etc.), we were seeing expected increases in memory consumption. However, after the burst completed, we expected the garbage collector to do its job to return the memory consumption back to the baseline amount (i.e. the amount needed to store the schema cache in memory), but this was not happening. Instead, the final memory consumption was quite a lot higher than expected. Obviously, this is not good for either customer or cloud deployments.Some initial investigation and graph plotting revealed something going on in the runtime system: the memory usage as reported by the operating system was consistently higher than the number reported by the GHC runtime, even after the burst of mutations ended. It seemed as though memory was not being returned promptly to the operating system by the runtime system.Again, with the help of the GHC experts at Well-Typed, we were able to satisfactorily resolve this. To some extent, it is expected that GHC will try to hold onto allocated memory in case it needs it in future, but this is obviously not good when we expect bursty workloads. Therefore, in a future version of GHC, there will be additional flags available to control the rate of return of memory to the OS.In addition, it turned out that there was a bug in the GHC runtime system, which has now been addressed: pinned memory was being allocated in a suboptimal way by the runtime.Now we see the memory consumption return to the expected baseline after bursts of mutations, and a generally healthy memory profile during similar workloads.Conclusion2.0 was a large milestone release, including many large features and refactorings, and we saw all of the trade-offs of using a language like Haskell for our development. On the one hand, we can achieve a very high rate of development on complex features, and we are able to refactor confidently, thanks to the strong guarantees of the Haskell type system. This is the benefit of Haskell that you are most likely to hear when you read blog posts like this one. However, using Haskell certainly comes with a large cost, and it\u2019s necessary for companies using languages like Haskell to really invest in the ecosystem and community, both in terms of time and money. We\u2019ve been working with Well-Typed to support the development of tools we need in order to make Haskell easier to work with for the development of an application this size. At the same time, we\u2019re able to make these tools available to the broader Haskell community, increasing the quality of Haskell tooling for everyone. As we use Haskell to build the next generations of Hasura features, we will continue to do our best to contribute to these improvements to Haskell in an open way.On the feature side, we\u2019ve now set ourselves up for the development of many new features: we\u2019ve tried to commoditize the development of new database backends, and we are also well set up to support next-level features such as generalized joins and generalized permissions. This will allow us to stitch together all of your different data sources into a single, unified graph with all of the additional Hasura features that we\u2019ve featured so far."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Adding REST endpoints to Hasura Description: Hasura's latest release supports REST endpoints and you get all the benefits of GraphQL and the ability to maintain and deliver REST endpoints where needed. Author: Jesse Martin"}, {"role": "assistant", "content": "Of the many new features added to Hasura 2.0 \u2013 adding REST endpoints to stored queries and mutations has to be a standout. You can hardly finish typing \"GraphQL\" into a search engine before the \"GraphQL vs REST\", \"Is GraphQL going to replace REST\" punditry floods the suggested query autocomplete.The fact is, was, and was bound to always be, REST is not going to go anywhere. There's a number of compelling use-cases where REST is still a preferred method for fetching data over the wire. REST vs GraphQL? REST and GraphQL with HasuraThe question should always be \"when\" to use which API specification. If you are building for embedded systems, it's by far easier to modify the resolver than it is to modify the query. If you are building for the Jamstack, ensuring protected execution of your content is critical. From the beginning of GraphQL's adoption in the industry, we've seen three common points of friction for adoption. Flagging vendor tooling for the existing production/deploy environments, security concerns about handing out an open API with infinite query depth, legacy systems that will require perpetual REST endpoint and so adopting GraphQL becomes an additional burden, not consolidating the pain points.These days, and specifically with Hasura, and even more specifically with our latest release supporting REST endpoints, you get all the benefits of GraphQL and the ability to maintain and deliver REST endpoints where needed.Fortunately, with REST support in Hasura, you can have both. To create a REST endpoint from a query or mutation, you first create a named query/mutation in the GraphiQL API explorer, then press the REST button in the GraphiQL interface. From there, provide a helpful name to identify your endpoint later, provide the suffix path for where there endpoint should be called, and choose which verb (GET, POST, PATCH, PUT, DELETE) your endpoint should respond to.Hasura supports traditional body parsing for dynamic variables as well as dynamic URL segments. Dynamic URL segments are denoted with : as a prefix in the path. Calling /shapes/round to a URL defined as /shapes/:shape would yield in a body of {\"shape\": \"round\"} being passed to the query or mutation serving as the resolver.Whether the project you are building is a better target for REST or you have a legacy project you are planning on migrating slowly to GraphQL, REST endpoints in Hasura allow you to support the best of both worlds with sacrificing the developer velocity you've come to expect from Hasura and Hasura Cloud.Learn more in this deep dive where we cover REST endpoints and a quick introduction to access controls to go with it.Learn how to create a REST endpoint in Hasura Cloud with this deep-dive guide."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Migrating from Hasura v1.3 to v2.0 Description: In this post, we will look at how to migrate from Hasura v1.3 to the latest 2.0 version in different scenarios. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Hasura 2.0 was announced last month with significant changes inside the GraphQL Engine. But we have made sure that user facing APIs didn't go through a breaking change for this release. There are fundamental changes like managing multiple data sources and additional support for generating REST APIs.In this post, we will look at how to migrate from Hasura v1.3 to the latest 2.0 version in different scenarios.Migrating from Hasura v1.3 to v2.0What's new in v2?Before we get started, let's look at what's new in v2. With support for multiple databases, you are no more constrained in connecting to just one Postgres db. You can now connect to multiple Postgres DBs or even different databases like SQL Server (and other databases in the future). So broadly,Multiple databases can be connected to Hasura for generating APIs.The metadata is stored in Postgres and can be configured using HASURA_GRAPHQL_METADATA_URL pointing to a Postgres DB. The difference in v2 is that, this can now be a separate Postgres DB, different from your application db where you create and manage your models and data.In Hasura Cloud, the metadata database is provisioned and automatically taken care.Updating: Backwards Compatibility ModeHasura GraphQL Engine in versions <= v1.3.x uses the ENV variable HASURA_GRAPHQL_DATABASE_URL for connecting to the Postgres database. In Hasura 2.0, you can continue to use this env and update your self hosted instance. In this mode, the same Postgres DB will be used as a source for the metadata database as well.For self-hosted OSS Hasura GraphQL Engine, you can update the docker image to point to the latest version and everything will continue to work.The latest docker image at the time of writing this:hasura/graphql-engine:v2.0.0-alpha.4Use this image in your docker-compose.yaml or any other setup where Hasura is running currently.In case you are using Hasura Cloud, do note that the updates will be rolled out automatically to all existing instances. But if you wish to do it yourself before the rollout happens, you can read the section below on Updating on Hasura Cloud.Migrations and MetadataHasura 2.0 fundamentally changes a few things and requires a change in the way metadata and migrations are managed using the CLI. A new config version v3 is introduced for managing migrations and metadata.Pre-requisite: Update Hasura CLIThe latest version of Hasura CLI (at the time of writing this post) is 2.0.0-alpha.4. If you are using an older version of CLI, something like 1.3.3 then you will have to update the CLI as the first step.hasura versionhasura cli version commandRun the following command to update the cli to the latest version.hasura update-cli --version v2.0.0-alpha.4Upgrade Hasura Project to v3 ConfigIn the project directory of Hasura, run the following command:hasura scripts update-project-v3This would update the existing v2 config project structure to v3 config. The metadata directory looks something like this:metadata\n\u251c\u2500\u2500 actions.graphql\n\u251c\u2500\u2500 actions.yaml\n\u251c\u2500\u2500 allow_list.yaml\n\u251c\u2500\u2500 cron_triggers.yaml\n\u251c\u2500\u2500 databases\n\u2502   \u251c\u2500\u2500 databases.yaml\n\u2502   \u2514\u2500\u2500 default\n\u2502       \u251c\u2500\u2500 functions\n\u2502       \u2502   \u251c\u2500\u2500 functions.yaml\n\u2502       \u2502   \u2514\u2500\u2500 public_search_artists.yaml\n\u2502       \u2514\u2500\u2500 tables\n\u2502           \u251c\u2500\u2500 public_albums.yaml\n\u2502           \u2514\u2500\u2500 tables.yaml\n\u251c\u2500\u2500 query_collections.yaml\n\u251c\u2500\u2500 remote_schemas.yaml\n\u251c\u2500\u2500 rest_endpoints.yaml\n\u2514\u2500\u2500 version.yamlMetadata changes in config v3There is a new directory called databases which contains metadata about the different dbs connected to the project and all the tables put into their individual yaml files.The migrations directory gets an update as well. Each database gets a child directory inside migrations and the relevant up.sql and down.sql files will be inside these.migrations\n\u2514\u2500\u2500 default\n  \u2514\u2500\u2500 1613987232674_init\n      \u2514\u2500\u2500 up.sqlMigrations changes in config v3Applying Migrations and MetadataIn order to apply the metadata that's exported from an existing project, you can continue to use the same command that was used earlier:hasura metadata applyOn the other hand, the migration apply needs a database flag.hasura migrate apply --database defaultUpgrading on Hasura CloudAll new projects created on Hasura Cloud will be using v2.0 (currently in alpha).Existing projects running on v1.3.x will be updated automatically to v2.0 in the coming days. But in case it is required earlier than that, you can manually create a new cluster and point your old postgres db to the new instance of Hasura.Once a new project is created, and the Hasura project is updated to config v3, you can point the old database connection string and everything should continue to work. Connect existing databaseThings to Note:During the migration, it is important to first update the Hasura server to the latest version before touching anything related to migrations and metadata. The config v3 will only work with the latest Hasura v2.0 server and anything else will throw an error.If you are changing the name of the database source (either via console or via metadata), you also need to ensure that it is changed in the migrations directory as well to make sure the the CLI commands continue to work for applying the migrations."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura 2.0 Video Series Description: Today we are beginning a new series of videos covering Hasura 2.0, the new features, some updated features, and long standing features that were often overlooked. Author: Jesse Martin"}, {"role": "assistant", "content": "Hasura 2.0 Video SeriesToday we are beginning a new series of videos covering Hasura 2.0, the new features, some updated features, and long standing features that were often overlooked. Over the coming weeks, we'll be releasing a series of videos with two distinct focuses. Each feature will be highlighted in a 1 minute snapshot for quickly learning and sharing the new features, as well as a complementary longer-form version with more narrative and details about the individual features. To begin our series, we've created a couple of quick snapshots and a long form take on creating a new project, setting up some tables and adding relations. This first video may be review for anyone familiar with the platform, but stay tuned for a fast-paced release cycle covering the more advanced features in the videos to come. Let us know what you think and do follow us on all the platforms to stay up to date with all the new releases!Watch the OverviewGetting started with creating projects, tables, content, and relations in Hasura Cloud."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using Postgres for Cron Description: How to use Postgres and your application server to build a flexible and robust cron system ourselves with a few lines of code! Author: Tirumarai Selvan"}, {"role": "assistant", "content": "Using Postgres for cronWhy even ?Any medium-sized application will have tasks that need to be performed at certain times of day or even ad-hoc times. This, typically, introduces a different infrastructure component to the application, e.g. in Rails you might use Sidekiq or with Python you might use Celery and of course there is the system level cron. Although these are typically seen as job queues, they also have a time-based scheduling component. But there are other ways to achieve time-based task scheduling as well ! Of course, we could use the aforementioned system cron but the utility of it is really limited by virtue of it being at OS level. And a crontab can\u2019t even express a single instance in time, they are always recurring (although there are extensions which can express this). So, is the only option to introduce these new components into your application? Or can we do something more judicious? Yes, we can use our database and application server to build a flexible and robust cron system ourselves with a few lines of code!Postgres is great for eventingFirst, let\u2019s talk about Postgres. Postgres is great and it\u2019s actually sufficient for typical eventing use-cases. This is not just because we implemented a scalable event-delivery mechanism over Postgres, \u00a0but also widely validated in the community. Here are few resources which talk Postgres for job queue like cases: https://layerci.com/blog/postgres-is-the-answer/ ,https://www.2ndquadrant.com/en/blog/what-is-select-skip-locked-for-in-postgresql-9-5/Apart from the transactional guarantees which traditional RDBMSs provide, Postgres also comes with nifty features like SELECT FOR UPDATE SKIP LOCKED which makes it very apt for building eventing systems. In fact, quite recently a new product called MessageDB was launched and it aims to solve the entire gamut of event and messaging capabilities with Postgres: https://blog.eventide-project.org/articles/announcing-message-db/ .Now that we agree that Postgres is great for eventing (if you are not, the rest of the content might still convince you...so read on), we will see how we can make our cron service over Postgres.How cron can be modeled using eventsLet\u2019s imagine that an instance of time is modeled as a row with a timestamp column. This means a cron schedule like * * * * * (run something every minute) can be modeled as a table with rows that has a timestamp for every minute. So that\u2019s an infinite table but it\u2019s easy to cap the timestamp to some max value (say one day ahead or one year ahead, you can choose). Now, let\u2019s see how we can make use of this table.In our application code, suppose we run a thread which does the following simple activity every minute:Fetches all rows that have timestamp <= current_timeDelivers these event for processing and marks the row as \u201cdone\u201dWhat did we just achieve? Well, a cron system of course!We can even run multiple threads and using Postgres tricks like SELECT FOR UPDATE SKIP LOCKED parallely process them for horizontal scale. Obviously, we also need to keep generating new events for future: just have another thread that inserts rows with timestamps till next x hours (or any configurable value).Advantages of event-based cronNow, why would you ever do this when you have Sidekiq or Celery or system cron? Let\u2019s see some of the added benefits of having event-based cron on Postgres:Ability to see future events and pause/cancel them if you want.Ability to create one-off events with a set timestamp (not possible in standard cron).Scale as needed by simply running more delivery threads.Store additional metadata inside a row for adding context, marking event as `dead`, etcAll these benefits are worth it!ConclusionPostgres is great for eventing and cron can be nicely modeled with events! It follows that Postgres is great as cron. We built Hasura Scheduled Triggers following this model and you can read more about the implementation in the docs or try it out on Hasura Cloud."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top 7 GraphQL IDEs you should know about in 2021 Description: To cater to the growing number of developers using GraphQL, there is now a wide range of GraphQL IDEs that one can choose from. In this blog post, we\u2019ve put together a list of the top 7 IDEs that you should know about in 2021 (in no particular order). Author: Vishwa Mehta"}, {"role": "assistant", "content": "In 2015, Facebook launched GraphQL\u2014A query language for APIs, after internally developing it back in 2012. Ever since its inception, GraphQL has grown immensely with a buzzing community and vibrant ecosystem of tools, libraries, IDEs, & more around it. With companies like Twitter, Airbnb, GitHub and myriad others moving to GraphQL, the adoption trend has gone up in the past 6 years.To cater to the growing number of developers using GraphQL, there is now a wide range of GraphQL IDEs that one can choose from. In this blog post, I\u2019ve put together a list of the top 7 IDEs that you should know about in 2021 (in no particular order). Feel free to comment to add your suggestions and tell us about your preferred IDE!1.GraphiQLGraphiQL is a popular lightweight and interactive in-browser GraphQL IDE that is built with React by the GraphQL Foundation. It supports the official GraphQL spec out-of-the-box and includes strong features like a visual query builder, API documentation explorer, syntax & error highlighting, and automatic code completion.Since this blog post is dedicated to GraphQL IDEs, there is one candidate that deserves an honourable mention\u2014graphiql-online.com. This version of GraphiQL is a fork of the original version with a simple header management UI that lets you configure environment variables that are required in various stages of development. For instance, the production build uses values directly present in index.html serving this app.2. InsomniaInsomnia is an open-source API design platform. Although it isn\u2019t specifically a GraphQL IDE, the API client supports GraphQL extensively. It is a great tool for collaboration across teams with its multiple workspaces and import/export features. Other interesting features of the Insomnia editor include automatic query completion and real-time error checking.3. PostmanPostman is another API development platform that is quite popular among developers. Much like Insomnia, Postman isn't specifically built for GraphQL, rather better suited for REST APIs. However, the support for GraphQL has become a bit more sophisticated and robust. Its auto-generated API documentation is one of its strongest features. It also lets you connect to mock servers for simulating endpoints which allows developers to test different responses without running an actual backend.4. GraphQL PlaygroundGraphQL Playground is a GraphQL IDE built on top of GraphiQL, developed by Prisma. Apart from the existing features of GraphiQL, it includes improvements and enhancements to the original GraphiQL experience, like enhanced documentation and schema exploration, and better query debugging.5. GraphQL EditorGraphQL Editor is an open-source GraphQL IDE that lets you preview your frontend as a live JAMstack engine by allowing you to connect to a mock backend and comes with a great schema exploration & visualization experience.6. Apollo StudioThe Apollo Studio Explorer is a cloud-based GraphQL IDE that is specifically tailored to suit GraphQL developers. It lets you save your query history to the cloud automatically.7. Altair ClientAltair is an open-source GraphQL Client that lets you debug GraphQL queries and implementations, available for multiple platforms including browsers like Chrome, Edge, & Firefox and OS like MacOS, Linux, & Windows. The IDE comes with an advance search for schema documentation and auto-refresh for the docs. Other strong features include multiple windows, extensive plugin system, import/export for your query collections, ability to switch between development environments (local, staging, production), etc.Special Mention \ud83c\udf81Graphqurl\u2014A curl like CLI for GraphQL, made with \u2764\ufe0f by Hasura. It's features include:CLI for making GraphQL queries. It also provisions queries with autocomplete.Run a custom GraphiQL, where you can specify requests headers, locally against any endpointUse as a library with Node.js or from the browserSupports subscriptionsExport GraphQL schemaThat's a wrap on our lineup of top GraphQL IDEs you should know about! Now that you know your options, try them out with Hasura Cloud. Hasura gives you instant realtime GraphQL APIs over multiple databases. Connect the GraphQL endpoint to your choice of IDE and get going with creating, running, and managing your GraphQL operations in an interactive way. \ud83d\ude80"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Best Practices for JAMStack Projects in Production at Scale Description: In this post, we will look at the best practices for using JAMStack in production and at scale. The architecture leverages static first to perform fast and dynamic on the client to cater to any complex interactions or data requirements. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Best Practices: JAMStackMore and more developers are embracing JAMStack for building and shipping web \u00a0apps that perform fast. The frontend tooling around building websites have become complex and powerful over the years. Static sites are not new. In fact the early web had pure static sites where developers could just drop html files on to a server via FTP. Server side rendering dominated the next wave of websites as data requirements became more dynamic. Once the server became the bottleneck, single page apps came into the foray and has been predominatly used for any web apps that doesn't require search indexing.In comes JAMStack (JavaScript, APIs, Markup) architecture that leverages the best of both worlds. Static first to perform fast and dynamic on the client to cater to any complex interactions or data requirements.Hasura fits into JAMStack by being the data source via its unified GraphQL API endpoint.It's easy to say build static sites, deploy and be done with it. But with the frontend tooling that's available, there are certain nuances to take care of. In this post, we will look at the best practices for using JAMStack in production and at scale. Most of the fundamental best practices are listed on the official JAMStack site and this post is inspired by that, albeit a detailed breakdown of that.CDNThe first and the most popular - The entire project lives on a CDN. The idea behind this is to distribute serving of the web app to be as close as possible to the user's region (edge). The HTML files are pre-compiled and available already. There is no server dependency. This vastly improves performance and user experience. But there are other side benefits you get by using a CDN for your project.Automatic Gzip/Brotli compressionHTTP/2SSLSetting up redirects, restricting accessCaching of static assetsPurging of CacheCaching of static assets sounds prudent. But as we keep deploying changes to our web apps often, it is important to configure cache settings correctly to avoid stale or buggy webpages from being served to the user. CDNs typically maintain a cache that can be purged via their UI/API. But then there is browser caching. With JAMStack apps, browsersShould never cache HTML resources - The cache-control header should be cache-control: public, max-age=0, must-revalidate1Should cache JS and CSS resources. The cache-control header should be cache-control: public, max-age=31536000, immutable. This is because the JS and CSS resources with modern frontend tooling are hashed with unique names on every build and hence you need not worry about stale files in subsequent deployments. Of course this doesn't apply to you if you are NOT using webpack or other build tools that does this automatically for you.Should always cache static assets like images. Again the cache-control header should be cache-control: public, max-age=31536000, immutable.For example, this Hasura Blog is built using Gatsby.js and leverages some of the cache settings for optimal performance and frequent deployments. It is served by Cloudflare, leveraging the CDN benefits.You might also want to cache your API layer for the dynamically fetched portions of data on the client side. Hasura Cloud's caching layer for GraphQL APIs significantly improves performance for frequently accessed queries.Git - Version ControlThe second, The entire project lives on git / version control. This way any body can clone and develop the site locally on their machine without complex dependencies. Typically, for web apps, this means an npm install should suffice. But what if your apps require custom logic to function for some interactive elements on the page? You will probably have some Javascript functions (serverless) that can also run in parallel with your application with dependencies installed.Automated Builds and DeploysSince HTML files are pre-compiled, there would be a build step to (re)-compile again whenever there is a change. This change could be cosmetic UI changes or even content on the page sourced from a data source (like Hasura) being updated. The build step can be automated through git push triggers on a branch Webhook API callsCMS triggersdepending on how your project is structured. For example, with Hasura, you can trigger a netlify build whenever there is a mutation made in Hasura. This way, you can use API calls to automate your build / deploy step.Atomic DeploysAt scale, when there are a large number of files any changes should reflect at the same time and not inconsistently file by file. Back in the day, these would be issues when files are \"uploaded\" in batches. With modern deployment tooling, changes go live only when the entire deployment is ready to go live. This way you ensure consistency for what users see on the page, irrespective of where they are from. Performance and User ExperienceThis particular one is probably applicable for most web apps. But specifically for JAMStack because they can make the most out of it through the tooling. Google has guidelines around Core Web Vitals which will start impacting search rankings from May 2021. Some of the metrics being tracked areFirst Contentful PaintFCP measures the point at which text or graphics are first rendered to the screen. Take a look at this post for reference. Modern tooling can be leveraged to ensure critical CSS and fonts are loaded quickly on priority for the above the fold content.Time to first byteSince static-sites are precompiled, it doesn't need to \"wait\" for the server to compile on the fly and respond back to the browser. A CDN can typically return the HTML from it's cache and serve it through the edge. This will vastly improve the time to first byte.Add loading=\u201clazy\u201d attribute to all images below the first fold.Add <link rel=\u201cpreload\u201d> to your font assetsUse tools like purgecss to remove unused stylesPreconnect to critical origins.Self host your fonts. Note: Google fonts typically have 2 roundtrips to load and render your font.Headless CMSDon't be afraid to use Headless CMS solutions for the backend. This is different from the WYSIWYG editors that do SSR. The way Headless CMS works is that, any changes you make via the CMS can be accessed via APIs. The build tooling can then integrate with your CMS APIs to recompile your HTML files. Or your client can also make dynamic data fetching through the APIs for the not-so important content.An example at Hasura: Our Blog uses the Ghost CMS for content creation. We like it for the ease of use, self-hosting capabilities and team management. Having said, using the website served by Ghost is not recommended. It is server-side rendered and hence slow and unoptimized for blogs. So we use a simple pipeline where our blog posts get created on Ghost using their CMS UI.Having said all of these, JAMStack fits in as a good architecture for specific use cases only. Single Page Apps (SPAs) have a clear use case to exist and those apps most likely do not need most of these recommendations to work better.Additional ResourcesApproaches to add dynamic content to statically generated sites (JAMStack)Dynamic JAMStack with Gatsby and Hasura GraphQL"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Data Model Diagramming from Theory to Practice Description: In this article, we'll look at the parts of a data model, ways to get started with modeling, and helpful terms to know to help you parse the data models of others. Author: Jesse Martin"}, {"role": "assistant", "content": "Data model diagramming (or entity relationship diagramming \u2013 ERD) \u00a0is the discipline of structuring the parts of your project in an easy to follow diagram that exposes the cardinality (one-to-many, many-to-one, etc), attributes, and functionality of your data domain. In this article, we'll look at the parts of a data model, ways to get started with modelling, and helpful terms to know to help you parse the data models of others. Whether you are creating conceptual data models, logical data models, or physical data models, the content below should help you get started quickly.Why Model your DataYou've undoubtedly come across data models before if you are in the business of configuring databases. Data model diagramming \u00a0itself is a type of balance act, finding a way to add enough fidelity to explain what needs to happen while not getting too caught up in the modelling that you never actually build. The fidelity needed for any given model will depend on the team and context of the project, which we'll look at later.But for the pitfalls and effort involved, modelling our data solves a very critical function \u2013 it helps our \"ideas\" fail fast. Even with the best of tooling, configuring a database takes time, and some architectural patterns are simply unsalvageable if you find out you structured the project wrong. Creating a model helps us think through the logical dependencies of our project.Modelling, really important for GraphQLWith GraphQL, our graph is often a direct exposure of our table architecture. While not required, and you can certainly abstract your exposed API away from the underlying database structure, the possibility of this tight coupling of the graph to data is part of what makes GraphQL so great to reason about. However, it also means that our model needs to be even sharper than before so that we can ensure the best developer experience possible when exposing our schema to them. The names we choose and the way we define the relationship requires extra thought because we aren't just exposing a REST endpoint delivering an opaque payload, we are exposing the structure of our business domain.Modelling, even more important for HasuraAn important note to add regarding Hasura is that with our new Multi-Database support, you can bring even more of your content into the console. Time series data, wide-table data and more can now be extended with GraphQL Engine. While this provides a lot of power to build with, it means modelling which pieces of data should live where and how they'll all connect together are critical questions to answer before building. If you are bringing an existing database, you can still create a conceptual model of what your GraphQL API should look like and abstract out a naming convention using saved views enabled by the Hasura GraphQL Engine.Given how much of the architectural layer Hasura is able to handle, you'll often find yourself layering the three classic models types (the conceptual, architectural/logical and the physical) in parallel or in a sort of hybrid approach.How to ModelThe short answer of how to model your data is, \"any way you want to.\" Really, the industry is full of over-thinkers who try to solve the meaning of life with flow-charts. The rise of multi-user-mode online whiteboards has finally tipped the needle to the digital medium for my own tooling preferences. The most common place to start a model used to be scratching out a crude bubble chart on a piece of paper or squeaking one out on the conference room whiteboard \u2013 and many businesses met tight deadlines riding on little more than dry-erase fumes.Today, and especially in light of the drastic rise in working from home, digital white-boarding with it's imperfect inputs, is a great choice. The ability to immediately persist the whiteboard in a format where anyone from around the world can collaborate, in real-time, with any degree of annotative tools at their disposal, is simply a force that can't be beat. A particular favorite of this author is Miro. It's hard to understate how far you can get with some simple primitives. If you are working with smaller groups or satellite teams - something like the Google Jamboard might be worth looking at. If you already think in terms of tables and spreadsheets, you might want to have a look at dbdiagram.io which boasts an expressive environment that bridges text driven and UI driven diagramming.Picking the right toolFor whatever tool you choose, you should look to solve the following feature set.The ability to annotate the diagram.Diagrams, absent of the original creator(s) can still be hard to understand. A tool that supports sticky notes, arrows, and another annotation primitives will help ask and answer questions about the chart.The ability to fluidly change diagram type.Not every diagram type is suited for every type of data model. Being able to change structure mid-flow is important.Multi-user edit mode.The ability for multiple people to edit a diagram at one time is such a big advantage in a tool. It's everything that makes office white-boarding great without the chemical-party room notes.Export.Be sure that you can export your chart in a variety of formats to support embedding in videos, presentations, documents, and more.Parts of a ModelLoosely broken down, your model will be composed of entities that connect to each other. Depending on what the entity is and what the diagram is, we can describe these parts in different ways.EntitiesThe term entity is relatively consistent throughout most diagrams. This can be a thing (something that is modelled in a table), a service, or a step in a process \u2013 not exhaustive. Depending on those purposes of an entities existence, the additional data we can ascribe to an entity will vary. When defining a data model around content types / tables, we can add attributes which often translate to column values in our db. If the entity is a service, we might add capabilities or functions as additional values. If we are modelling a class diagram, we might even mix the two. Conceptual models are often entity-name only without additional data added.ConnectionsConnections, those lines between the entities, bring much more semantic value to the diagram. In a graph, these are the edges. These could be a state transition in a state diagram, a relationship in a standard data model, a function execution in a service digram or more. More often than not you'll see them referred to as relationships, where the relationship itself will be annotated to indicate additional value.Understanding One-to-Many, Many-to-One, and Many-to-ManyA common annotation to a relationship is to identify the cardinality. Cardinality refers to the plurality of one of the ends of the relation. A cookie can have zero or more chocolate chips. A chocolate chip can be in exactly one cookie. A chocolate chip cookie, by definition, must have one or more (not zero or more) - these are different forms of cardinality. We often express these relationships more simply as one-to-many, many-to-many, and many-to-one. Though, technically speaking, there are more \"edge cases\" possible by defining lower and upper bounds on quantity.These variances in cardinality are significant enough to our architecture for a DBA to optimize our table structures that the industry has developed an entire series of grammars around annotating the edges. To illustrate, here's an example from a common data model diagramming notation called Information EngineeringIf we applied the above notation to our chocolate chip diagram, we'd end up with something indicating chocolate chip to cookie as one only and cookie to chocolate chip as one or more.Picking a Model Grammar/NotationThe four most common notation types are Information Engineering (IE), Barker, IDEF1x, and the unified Modelling Language (UML). While breaking adding this level of detail to our models can feel like over-engineering a simple problem (and it may be!) the benefit is that we can communicate a commonly understood representation of the models without the need for sticky notes and marginalia to explain what our thick, thin, or colored lines are supposed to represent.Barker is one of the most common ones, due in part to it's heavy adoption by the Oracle ecosystem, but in recent years, UML has been making a lot of headway thanks to its attempt to unify the other standards. UML has a particularly nice notational style for logical operators that keep the size of the model from exploding off the paper.Optimize your ModelThe process of breaking our content models apart into discrete entities (removing the amount of shared or repeating attributes) is called \"normalizing\" our data model. And it's an important part in understanding the moving pieces. That being said, and a point that is particularly true for GraphQL implementations, is that every level of normalization introduces a fixed cost for database efficiency. Every join between two or more tables takes additional computation time. And so the challenge for the DBA becomes determining if the attributes should be fit into one table (also known as a wide-table database like Google Big Query or Amazon RDS), or breaking out the tables into more entities for a more descriptive data-set and architecture.In Practice - Start Simple, add FidelityIn real-world application, the road to the eloquent data-model that considers all edges cases, provides a perfect hand off from architect to developer and leads to international awards and fame is a bit of a myth. The reality is always somewhere in the middle and compromises will need to be accounted for.To accept this reality, it's important to remember that the model's goal is to provide a common understanding to all involved and to \"fail fast\" with our ideas and planning before going down a potentially costly, errant path. Starting with a conceptual model, slowly adding the parts of annotation that are needed to indicate attributes, functionalities, and cardinality until the whole team understands the moving parts will help strike the balance for just-enough detail for timely deliverables.Learn more, watch this video"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building REST from GraphQL - a story of engineering in Haskell Description: A development case-study and project planning story in which Haskell\u2019s motto plays a central role in the development of a new Hasura feature. Author: Lyndon Maydwell"}, {"role": "assistant", "content": "Lightning strikes, you are in a state of flow, an elated feeling of \u201cthis is the way things are meant to be\u201d, but at the same time a nagging anxiety - \u201clast time this happened it was only temporary\u201d.Recently our team delivered a new Hasura feature and the experience was one of those moments where you ask yourself - Why can\u2019t it always be like this? Rather than slowly forget how this happened I thought I\u2019d try to capture the mechanics and aesthetics of what made this feature so pleasant to develop. This is a development case-study and also a project planning story in which Haskell\u2019s motto plays a central role.The FeatureProvide a mechanism to allow the definition of REST interfaces to GraphQL queriesThis feature was first discussed towards the end of 2020, with a general desire to provide a safe and efficient mechanism for exposing GraphQL capabilities to environments where REST had better prospects for integration. The caching properties of most REST clients makes this feature even more appealing with very good caching behavior \u201cfor free\u201d thanks to our work on the @cached directive.It\u2019s important to note that what was established at the beginning was not particular implementation details or considerations, but what was important to achieve from a user perspective. The particular interactions and underlying details would come later.RoadmapBefore embarking on any development work, a very-high-level roadmap was established. This served to limit the scope of development before feedback and new considerations could be established. This clearly separated the levels of refinement that were required as development moved forward and also provided very visible gates between the steps of the project. The importance of this was that decisions needed for more sophisticated capabilities could be set aside in order to speed up progress towards the next gate.IterationFor every stage of project delivery, the design and implementation are iterated on fluidly, but with clear boundaries on what could change. Discussions that fell outside the scope of the current work were noted, but didn\u2019t interrupt the current tasks. Rather than creating heavy-weight documentation around these side-discussions, a nested TODO checklist was kept in the top-level issue. We favoured checking back in with stakeholders on the raised points as opposed to trying to fully nail down requirements at the current point in time.POCPossibly the most important phase of the whole project was the POC phase. This demonstrated several things:\nThe core interactions would be possible\n\nConfiguration via Metadata\nExposing a referenced query via REST\n\n\nThe approach would not be too onerous\n\nEstimation of time required to flesh out later stages\nInsight into complexity of system modifications\n\n\n\nThe following were purposefully ignored during this phase of development:PerformanceCorrectnessFeature completenessInterfacesStable interfaces and implementationsAlthough there were some high-level requirements and specifications developed going into the POC stage, the bulk of the specification is fleshed out at this point. Demonstration with stakeholders is the most important activity if the ideas show technical viability.Many considerations are deliberately ignored and the prevailing attitude could be summed up as \u201cexpect to rewrite almost the entire implementation of the POC\u201d. This should be liberating, as there is no need to obsess over any particular details.Version 1The launch-target. There should be a clear gap between the MVP and what\u2019s remaining for version one of the feature offering. This gap is composed of additional sub-features, consideration of lesser error-paths, and refinement of interfaces. Further performance improvements and implementation refactoring could be done here, but that would also be ok to postpone if it looks to require significant work and wouldn\u2019t cause major issues to attempt later.Possibly the hardest part of this phase of the roadmap is getting final sign-off for merge into the main branch. The work done to narrow down the specification during this and previous phases will allow pushback on feature extension as it can be shown that these could be undertaken as part of \u201cVersion 2\u201d.Version 2The placeholder. At prior phases, version 2 serves as an intangible future release that can be used to store \u201coverflow\u201d feature requests that are completely valid, but could also be deferred without problematic consequences.FutureOther very speculative enhancements can be scheduled beyond \u201cversion 2\u201d in a nebulous \u201cfuture\u201d category. In practice, once version 1 is released, any planned work can now be recast into a new feature, and the process begins again from the point of roadmap construction.A Poor Worker Blames their ToolsThis is a favourite refrain of proud programmers everywhere. Proclaiming that anemic tooling should be overcome by discipline, ingenuity and abstraction. Now, go and visit a master craftsperson - Their tools will be obsessively selected. The pithy quote has done a great job at falsely equivocating the need to be able to make the best of a bad situation, and the premise that all tools are created equal.When all you have is a hammer, everything looks like a nail.What is the purpose of a tool? To help! Haskell helps very effectively with the workflow I\u2019ve described in the approach to our roadmap. If you have very different approaches to development then maybe Haskell isn\u2019t the best tool for the job. That being said, I feel like the combination of the two is a very optimal configuration.The crux of what made Haskell so effective in this situation is that it provides \u201cFearless Refactoring\u201d better than any other ecosystem that I\u2019ve encountered. Refactoring or even rewriting is extremely important in the workflow I\u2019ve described. Not only is it likely, it\u2019s even recommended between various phases of the roadmap. The key properties of Haskell that make this so effective are strong static types, purity, and expressivity. Laziness led to purity and laziness was the key research area that necessitated the creation of Haskell. Research goals was where the motto \u201cAvoid success at all costs\u201d came from.One counter-intuitive implication of this is that planning for refactoring during development is more important than structural correctness. At least initially. Where this manifested most prominently was the construction of our route-matching data-structures and logic.We went from a very naive approach that did some fancy string matching, to a highly specialised approach that captured the essence of the problem and offered optimised performance and correctness. The initial naive implementation actually benefited us greatly in that work could proceed on related areas of the code while some deep thinking went towards figuring out what the correct abstraction was to represent the problem space.This is interesting from a Haskell perspective since it seems to go against the advice of \u201cparse, don\u2019t validate\u201d, however they are just different journeys towards the same destination. An important consideration in a team setting when you want to eliminate as many critical paths as possible during delivery. There is often an intuition that you must get the abstraction right early on, but instead I propose that you must allow for evolution towards a good abstraction.Haskell is a language that makes this process very natural. Type-driven-development works as well, or even better, during refactoring as it does during green-field development. What this feels like concretely is moving the needle from validation to parsing. This feels like a \u201ctightening up\u201d of correctness as the types become better aligned with the problem.What I\u2019d like to impart is that while this might be the intuitive end goal, there\u2019s a real quantifiable flexibility that is imparted from utilizing less well-fitted types while the design and implementation are still in flux. If you blindly charge towards the \u201cbest possible types\u201d without initial flexibility for experimentation you can end up in a \u201ccoffin corner\u201d scenario trapped into an incorrect, or deprecated implementation due to the rigidness of its adherence to its conceived specification. The extra controversial implication is that you should allow some flexibility for your estimations of future changes even post V1.Move Fast and (don\u2019t) Break ThingsHaskell can help break false-tradeoffs:\nMoving Fast and Breaking Things\n\nVs. Going Slow and being Safe\n\n\nRemaining Flexible and Having Boring Types\n\nVs. (Over?) committing to a specification and having Expressive Types\n\n\nEnabling Refactoring and Having Many Tests\n\nVs. Strongly adhering to an original spec and staying lightweight\n\n\nAllowing debugging issues at runtime and being dynamically typed\n\nVs. Having to reason statically about your data and having static types\n\n\n\nThe mechanisms by which Haskell succeeds so well at \u201chaving its cake and eating it\u201d stem from its motto - \u201cAvoid Success at All Costs\u201d.Is this \u201cBoring Haskell\u201d?Am I recommending that you use what\u2019s been called \u201cBoring Haskell\u201d? Maybe partially! I\u2019ve always taken a somewhat centrist position in the debate around how you should leverage the sophistication of Haskell - promoting a right-tool-for-the-job attitude towards solution approach. Antoine said it well when they suggested that we shouldn\u2019t be afraid to bring Haskell\u2019s full power to bear on our most critical code, but that we should provide clean interfaces to work with these central objects so that you only need to understand the abstractions, and can hopefully not require a full understanding of the details and deep domain skills in order to contribute to the wider codebase, ecosystem, and product features.My perspective is that your Haskell code should get more \u201cboring\u201d the further you go from the inner cogs of the solution. This should allow leveraging very expressive language for describing and constraining your foundations, ensuring that they are correct and yet composing them easily, however, to interact with the outer interfaces you should be able to follow simple patterns and get concrete feedback from the types and errors when you\u2019re navigating the development process.EngineeringWhat is the point of an engine? The aim is to allow commoditization of fuel. In that regard, when undertaking engineering, we can consider it from a product perspective and allow our customers to power their processes, but from a continuous-improvement perspective the engine we build facilitates feature development and improvement through commoditized contribution. This sounds almost pejorative, but this is the same proposition that Hasura makes with its 3-factor architecture. Make it easier to focus on what actually matters rather than repetitive distractions and reinventing the wheel.In the case of Hasura as a product: Less creation and maintenance of API endpoints + More development of domain models and business logic. For Hasura as a codebase: Less restructuring the entire codebase to provide new features to end-users + More refinement of core models and faster development of integrations, backends, operations capabilities, etc. I hope that we can continue to value this approach to software-development and make our codebase even more effective in this regard.Are we there yet?While I believe that you can leverage Haskell to follow this pattern very effectively Right-Now, I think there\u2019s still many technical improvements to be made to the language. Here are my top-two:Compile Speed - This is by far the #1 point of friction for me, and I feel many others. Incremental improvements in performance are being made, but we are still fundamentally in a SLOW paradigm, and one of the worst examples of a slow compiler. I won\u2019t pretend that fixing this is easy, but if it can be fixed it will enable so many fundamentally different workflows. HLS is a godsend, but this is a work-around, not a solution.Record Types - The current record implementation fundamentally prohibits ergonomic flexibility when changing or extending your design. There should be a row-polymorphic capability adopted natively with real syntax like in Elm or PureScript. Many work arounds exist, but none of them feel intuitive or blessed.https://www.microsoft.com/en-us/research/publication/wearing-hair-shirt-retrospective-haskell-2003/Ultimately, considering the ergonomics of a language, codebase, and organisation from a flexibility and evolutionary perspective brings a fresh way to interpret your software architecture. From a purely economic perspective this should be valued very highly as it will directly affect time-to-market for new products, features, and services.Avoiding Success at all CostsSimon Peyton Jones wrote \u201cWearing the hair shirt\u201d as a 15 year retrospective on Haskell. In some ways it was a similar exercise to examining what made our REST feature successful. What choices did they make, what processes did they follow that lead to such a unique and effective language? There are a lot of technical details, but also a philosophical one - \u201cprolonged embarrassment\u201d, prolonged discomfort, and resisting \u201ceasy\u201d solutions to problems actually allowed jumping out of a local-optimum and finding a more fundamentally composable approach. Obstacles turned out to not be roadblocks, but hurdles that could be overcome.https://www.microsoft.com/en-us/research/publication/wearing-hair-shirt-retrospective-haskell-2003/This is one implication of \u201cAvoid Success at all Costs\u201d. Remaining flexible on details, but steadfast on bigger goals may take longer, but can yield truly new and better conclusions. The hairshirt represents ideological discipline, \u201cat all costs\u201d represents a greedy approach to short term compromised implementations and success represents the vision.The VisionI think that we all strive for a productive and expressive experience when developing software. It\u2019s easy to end up in a local maximum when incrementally improving tooling and not consider grander goals. You see this in discussions around GraphQL where some argue that it couldn\u2019t possibly offer any substantive improvement in experience over REST. When you\u2019ve experienced real pain-points and seen how new interfaces address them you don\u2019t see issues such as resolver-boilerplate or n+1 as insurmountable obstacles, but hurdles to be overcome. Tools such as Hasura do the hard work to allow the change to more expressive paradigms by offering solutions to the frictions that are cited as roadblocks to adoption.So too it is with the Haskell ecosystem and tools. Type boilerplate issues are cited as reasons why the user fundamentally can\u2019t have safety at the same time as expressivity. Data-copying proves that code using immutable data-structures can\u2019t be efficient. Refactoring can only be painless if you write exhaustive unit-tests. Haskell aims to not just enable an environment for existing FP ideas, but to fundamentally break these false dichotomies. Let\u2019s take this further.I want creativity to feel expressive, with code flowing painlessly. Fast feedback cycles. \u00a0Strong types with guiding warnings and error messages. Terse, intuitive and elegant syntax / interfaces and composable and extensible objects. I want to be able to refine my solutions, evolving and improving them. I want to be able to signal an intention to change behaviour by undertaking a partial refactoring that then generates a cascade of new recommendations guiding towards the complete implementation. I want to \u00a0have code act not just as a cog, but also a form of communication - the implementation communicating ideas - Highly readable, not just easy to write, but easy to understand with layered modules allowing a bird\u2019s-eye view of the concepts and approach at play.Where do the gaps exist? What can we do to close them?We will get there by Avoiding (Success at all Costs).This article was written by Lyndon Maydwell."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura GraphQL Engine 2.0! Description: On behalf of the entire team at Hasura, I'm delighted to announce the release of Hasura GraphQL Engine 2.0 (also just called Hasura 2.0). \ud83c\udf89 Author: Tanmai Gopal"}, {"role": "assistant", "content": "Announcing Hasura 2.0On behalf of the entire team at Hasura, I'm delighted to announce the release of Hasura GraphQL Engine 2.0 (also just called Hasura 2.0). \ud83c\udf89Hasura 2.0 is our most significant release over the last few years, but is also entirely backwards compatible with Hasura 1.x. Over the last 85 releases of Hasura over 2 years, we've endeavoured to never break user facing APIs and this major release is another checkpoint in that ensuring that users get new features without having to refactor their work. In fact, Hasura 2.0 == Hasura 1.4, but it was such a big release that we decided to call our next 1.4 release, 2.0.TL;DR: How do I get started?If you'd like to skip the spiel and just head on straight to trying it out, head to the docs to try 2.0 with Docker or try it for free on Hasura Cloud - our managed service offering of the Hasura GraphQL Engine. Watch this webinar to get an overview of all the new capabilities in our v2 release.So, what is Hasura 2.0?Hasura 2.0 captures fundamental changes inside the GraphQL engine that allows Hasura to serve the needs of a much larger class of mission critical applications. There are 5 main areas of work and feature additions you'll see in Hasura 2.0:Connect multiple databases simultaneously and database generalizationSupport for REST, in addition to GraphQL, for the user facing APIAuthorization engine enhancementsHA & Distributed Ops - Reliability & Scalability (aka - prevent GraphQL is the single point of failure related problems)Metadata API tooling1. Connecting multiple databases simultaneously and database generalizationWe first built Hasura with exclusive support for Postgres. This was a deliberate choice so that we could focus our engineering and product efforts and given the rise of Postgres and the Postgres ecosystem exploding this choice paid off well.With Hasura 2.0 we've set the foundation to unlock 2 key capabilities:Connect to multiple independent databases simultaneouslyDatabase generalization - accelerate Hasura support for additional database engines1.1 Connecting to multiple databases simultaneouslyAs we've worked with our users, especially enterprise & mission critical workloads, we've noticed increasingly that users often bring multiple new or existing databases, even if its just Postgres. These are 2 of the top use-cases we've seen:Multi-tenancy: Different tenants have their data grouped into different Postgres databases. Often this is to enable scaling or to enforce stricter data isolation.Different workloads: A Postgres database optimized for search might be very provisioned and tuned very differently from a database optimised for timeseries (using say, Timescale) which in turn would be very different from a Postgres database tuned for high transaction volumes. Data is consumed from these different sources together.Hasura 2.0 allows bringing in multiple databases simultaneously to a single instance of Hasura GraphQL engine (or a scaled up cluster of Hasura instances).Instead of having to supply a single env var, you can now add and remove database sources on the fly on a running Hasura instance!1.2 Database generalizationPostgres was always just the beginning at Hasura. We've now refactored our query compilation pipeline so that it can be generalized to any database easily. Existing or legacy mission critical data is often not on Postgres and the cost of ETL-ing that data to Postgres is infeasible and defeats the purpose of being able to query the source of truth directly - especially for fast-moving operational/realtime data. And ofcourse, new data workloads also might go into other databases that aren't just Postgres!With Hasura 2.0 we're launching with support for SQL Server and our first OLAP database - BigQuery.We'll be rolling out SQL Server support on Hasura Cloud in phases over the coming day. You can also head to the docs to try Hasura with Docker and your own SQL Server database.We've started off with generalizing to relational databases with the goal of making sure a new relational database engine can be added to Hasura within 2 weeks.Hasura's philosophy of adding a new database is to keep the final GraphQL API similar, but without trying to normalize all database engines to the same common denominator interface. Hasura aims to bring out the best of a particular database engine while ensuring all the Hasura goodness of GraphQL, authorization and eventing Just Work.We've started to put together a contribution guide of how you can extend a few typeclasses to add support for your favourite SQL system and also work closely with the maintainers to keep improve native support for that database and its goodness (its specific types, functions, operators, connection handling etc.).Over the course of the year, we'll be bringing other types of databases systems into Hasura: document and key-value stores are big on our minds! Let us know what databases you'd like to see support for via our Github so that others can weigh in as well and help us prioritize.You can also read excellent article on how we built a GraphQL to SQL Compiler on Postgres, MS SQL, and MySQL.1.3 Remote JoinsHasura supports \"joining data\" over heterogeneous sources so that API consumers see a semantically unified Graph of their API models. Currently, Hasura supports joining in certain specific directions. Coming soon, in order of priority, are the following remote join capabilities:Remote Schemas to databases: Join the output of any GraphQL API datasource with a data modelRemote schemas to remote schemas: Join types across different GraphQL servicesDatabase to database: Join across different databases2. Support REST endpoints, not just GraphQLGraphQL APIs have emerged as the best modern API for humans to explore, consume and integrate.While GraphQL's goal at inception was to provide a specification for automating API composition on UIs and creating a type-safe client-server contract over JSON, its success goes far beyond that. Arguably, and much to the frustration of true GraphQL stans, GraphQL fragments and Relay style GraphQL are not as popular as they ought to be. This is because GraphQL solved a much bigger problem than just automating the tedious portions of API composition and integration on the UI.GraphQL represents two big shifts in the way we think about APIs:Self-serve consumption: API consumers can browse, explore, test and integrate APIs with a much tighter feedback loop without requiring the typical kind of hand written documentation & tooling.Query what you want: API consumers can craft an API query to get precisely the data they want without requiring humans to build custom endpoints for their favourite shape of data. Imagine a database that served data only in a fixed shape - ridiculous right! And now that's what we want, but from our APIs too. Performance, caching, security, etcetera should Just Work as much as possible.However, in practice, we've seen three big deterrents to using a GraphQL API in production - even where it's \"technically\" feasible:The lack of GraphQL vendor tooling around GraphQL in production - ops, caching, monitoring, error and QoS reporting, authorization, security are all entirely new with GraphQL. In mission critical scenarios, especially in enterprise environments, the cost of GraphQL adoption might simply be too high. Consumers who are not building frontend applications (say on a public API, or for developers building other services) might not want to or be able to use GraphQL clients in their code. As humans, they still love GraphQL for the tooling, but actual usage might not be possibleREST APIs are not going away. This means that teams might have to maintain multiple API engines forever to make sure they don't alienate some API consumers. The cost of maintaining a GraphQL API and simultaneously a REST API is ridiculous. (REST can also be seen as a placeholder here for teams that use gRPC, say).Hasura 2.0 allows users to create idiomatic REST endpoints based on GraphQL templates.Create a REST endpoint from a GraphQL query templateMaking a REST API call to HasuraThis allows Hasura users to get all the benefits of GraphQL and REST - when the situation demands it. Hasura uses GraphQL as an intermediate representation and converts a RESTful request with its parameters to a parametrized GraphQL query and then executes it, with minimal overhead - query plans are cached after all!Check out the REST to GraphQL interop specification on Github.This might seem similar to the idea of \"persisted queries\" but the key difference here is supporting idiomatic REST. This implies supporting error codes, caching headers, REST style verbs and parameterization (path params, URL parameters, JSON body etc.) and even OpenAPI/Swagger documentation for created endpoints.3. Authorization engineHasura contains a sophisticated authorization engine - very similar to RLS style authorization in the database world.This allows Hasura to extend its authorization policy abilities across data-sources without being restricted to the authorization features supported by particular databases only.Hasura 2.0 features authorization on remote GraphQL services (your own GraphQL services or external GraphQL APIs you don't own): role-based schemas and preset-arguments.Over time we plan to extend this authorization engine to create a generic and unified RLS style authorization layer across all data sources. With support for authorization on remote GraphQL services we've only just begun!As data explodes, the need for declarative and \"human observable\" authorization becomes even more important. Hasura 2.0 also features a policy inheritance system that allows composing fine-grained RLS or ABAC style policies on the fly.4. High Availability & Distributed OpsA GraphQL layer fronting multiple heterogeneous data sources is an ideal candidate for being a single point of failure. Hasura 1.x was already seamless to run in a scaled up fashion as a cluster of replicas and they would sync with each other automatically.With Hasura 2.0 we wanted to ensure that we can start bringing in features that make running a Highly Available and \"distributed\" Hasura cluster along with its interaction with a set of upstream data sources as seamless and easy as possible. This is no longer about just making Hasura scale up and work, but also to make its interaction with upstream sources fault-tolerant & scalable to prevent the single-point-of-failure problem.Hasura 2.0 features a \"maintenance mode\" that allows for major Hasura version upgrades of itself and connected sources without any downtime to the GraphQL API and event delivery systems.These are the features coming to Hasura soon:Automated failover, circuit-breaking and retry logic customized to the upstream datasource's characteristics. Eg: The postgres primary replica is down!Upstream source monitoring: Often when API performance slows down, it is hard to isolate whether the source of the problem is the API layer, or the upstream datasource. Eg: API requests are queuing up because the connection pool is saturated.5. Metadata APIsICYMI, Hasura is a GraphQL server that doesn't require a build step! All of Hasura's configuration is entirely dynamic and API driven. This \"metadata\" API - which can be locked down in production depending on the use case - makes \"change management\" extremely convenient. However, these metadata APIs are also extremely powerful because they allow Hasura users to \"fetch\" all of the configuration of a Hasura system on the fly.This unlocks unprecedented metadata capabilities:Creating a data-dictionary of what the underlying data models, their documentation, frequently used columns, recommended query patterns. We've already put together a starter kit for building your own data dictionary on top of Hasura here.Observing security and authorization configuration and rules on the fly. Observing which authorization clauses are actually used by particular requests.Creating community and org-specific base metadata configurations and sharing them to encourage common standards and best practices.While some of the features above available aren't in this release of Hasura 2.0, expect a lot of metadata tooling to come your way!Here's the one we're most excited about are working on right now:Instant git push (or github webhook) to deploy \u00a0database migrations and Hasura metadata changes to your staging and production Hasura instances.SummaryHasura 2.0 is here and along with the features that are a part of this release, it sets the technical foundation for engineers and contributors to innovate and ship rapidly for the coming months and years. Join me at the Hasura 2.0 launch webinar to go over these features in action, learn more and ask questions!And of course, in case you missed it, don't forget to register for the awesome (and free!) virtual GraphQL conference: https://graphql.asia/Here's the announcement post for Hasura 2.0.0-beta.1."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top 3 GraphQL Resources for Vue.js Developers in 2021 Description: GraphQL is seeing greater adoption in the developer space because of features that make it suitable for building modern applications. We have curated the top 3 GraphQL resources for Vue.js developers as Vue.js takes a similar trend in popularity as GraphQL in the frontend space. Author: Vishwa Mehta"}, {"role": "assistant", "content": "The year 2021 completes the 6th anniversary of GraphQL ever since its inception in 2015 and as the spec grows, the community grows bigger. GraphQL is seeing greater adoption in the developer space because of features that make it suitable for building modern applications like real-time capabilities, introspection, auto-generated API documentation, etc. So we have curated the top 3 GraphQL resources for Vue.js developers as Vue.js takes a similar trend in popularity as GraphQL in the frontend space.This blog post will focus on the following top-level categories:GraphQL clientsAuthenticationData fetching1. GraphQL clientsGraphQL requests can be made using a simple fetch API call. However, specialised GraphQL clients are useful for automatically handling caching, improve query parsing and readability, and build reusable modules. Let\u2019s take a look at some of these client libraries that have native support for your Vue projects:Apollo client: Apollo Client is a comprehensive and fully-featured client for managing GraphQL on the frontend. It has many benefits like caching, fetching, state management, garbage collection, etc.We have created a learn course to talk about how to integrate Apollo Client with Vue.js.urql: urql is a lightweight featured client with automatic caching and a slightly different approach to caching (everything is automagical as opposed to the manual caching approach). urql provides the Vue API, made available via Hooks that can be plugged into your Vue.js components.Check out the API documentation here.SWRV: With caching being an important piece of the modern app development process, a new wave of caching mechanisms hits the frontend ecosystem called SWR (stale-while-revalidate). SWR basically means whenever your app fetches data, the local cache (stale) will be served, then a fetch request (revalidate) will be made. SWR has a Vue.js implementation called SWRV meant to be used with the Vue Composition API.Fluent GraphQL clients: GraphQL queries are typically written as strings and parsed by these specialised clients better. Imagine how much better the experience could be if you can write your GraphQL query as objects instead. This is possible with the help of fluent GraphQL Clients.Villus: Villus is a lightweight client for Vue.js with a bundle size <4kb and is written in Typescript and supports typed queries. It is also cache ready.2. AuthenticationJWT-based authentication in Vue.jsThere are various ways to approach user authentication for your Vue.js applications. However, token-based authentication is traditionally used as a more straightforward way of creating an interface for users to create an account that they can log in and out of. Both actions of signing up and logging in will provide the user with a token.You can make use of local storage for token management for authentication in your app. Vue comes with Vuex\u2014a centralised state management store that helps you manage your auth tokens for your user data, and Vue-router\u2014a tool that adds support for dynamic routing and handles authentication by authenticating routes in your Vue.js application.Check out this course by Vue Mastery to learn how to combine JWT with Vuex and Vue-router to handle authentication in your Vue.js projects.Additionally, there are various 3rd party solutions that you can use to manage user authentication in Vue.js:Auth0: Auth0 is an easy to implement, adaptable authentication and authorization platform. You can read more about how to configure Auth0 with Vue.js from our fullstack tutorial.Firebase: Firebase gives you a flexible UI, comprehensive security, and rapid implementation platform for user authentication. Learn more about using Firebase for authentication in Vue.js applications in this free course by Vue School.3. Data FetchingThere are various modes for data fetching that can be implemented in your Vue.js apps. Server-side rendering can be implemented to render pages on every request as opposed to statically generated sites that render pages on build time.Server-side Rendered: Nuxt.js is a great tool to implement SSR in Vue.js. It is an intuitive and powerful open-source framework for building Vue applications that supports server-side rendering of data and abstracts away boilerplate code and gives out-of-the-box functionality for .Statically generated: In this approach for data fetching, the website\u2019s pages are generated at build time and HTML files are pre-generated with that data. Typical use-cases for statically generated sites are blogs, product pages in e-commerce websites, and content heavy sites that don\u2019t change often.Nuxt.js: Besides SSR, Nuxt.js also supports statically generated websites. You don't require a server but still have SEO as it pre-renders all pages with HTML files pre-generated with the data.Gridsome: Gridsome is a JAMStack framework for building statically generated Vue.js websites and applications.Let us know about your pick from the top GraphQL resources for Vue.js developers!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introspections and Ideas on Caching GraphQL Description: We're starting a series to research and provide overviews of the many ways in which different caching technologies and approaches can be used for GraphQL.  Author: Adron Hall"}, {"role": "assistant", "content": "We're starting an endeavour to research and provide overviews of the many ways in which different caching technologies and approaches can be used for GraphQL. This is the first of numerous posts that we will be writing on this topic here at Hasura. As an idea of what approaches we are starting out with, here is a list of some that we'll be taking a more in depth look at in the subsequent articles.Web Cache - Browser & mixed local and shared caching approaches.Database Caching - How queries cache at the database tier, how we can most effectively use those in our subsequent application tiers such as graphql, and beyond.Client Cache - How can, or do a range of client technologies provide caching? For example;iPhone or Android App Cache.Android or iPad Tablet Caching.Operating System PC/Systems Level Cache.Application Client CacheServer CacheCDN (Content Delivery Network)Reverse ProxiesWeb AcceleratorsKey/Value StoreHTTP / Network Level CacheCache Servers - MemCache, ElastiCache, Redis, etc.These are just a few that we'll be covering in the coming weeks. But before we get into any of these caching options let's discuss what exactly a cache is and the purpose of using a cache in our systems we build.Definition: What is cache?At a fundamental level, the term cache means to hide things in a place to conceal and preserve the items in the place they're hidden. The definition, as it reads from the Merriam-Webster Dictionary itselfCache1a: a hiding place especially for concealing and preserving provisions or implements b: a secure place of storage discovered a cache of weapons2: something hidden or stored in a cache The cache consisted of documents and private letters.3: a computer memory with very short access time used for storage of frequently or recently used instructions or data \u2014 called also cache memoryand then there is the definition for cache being used as a verb. To cache something in the cache.Why Cache?The purpose of a cache is to take the load off of something else, to make things available for another element of a process or system. This is important to note, even if it seems obvious at first. Sometimes a cache may be put into an architecture and not serve a functional purpose around redistribution of data or compute. In these scenarios it's important to keep in mind the intent of using a cache in the first place, and determine if it should even be in place.Application PerformanceOne of the most common reasons for implementing a cache is to improve, specifically, the performance of an application. Such as improving the performance of data displayed in the application, improving the speed of reads or writes in various interfaces, or other demands. At the root, it's always about performance and load, and the application is easily the first place to find demands for performance!DatabaseThe database can routinely become a point of focus for costs, performance, and other reasons. Putting in place a cache to take some of the load off of the database can provide huge benefits. The implementation for a cache for the database can take several forms; from eliminating a hotspot within the database itself, to caching an entire copy of data the database stores in a delivery network or by some other means.IOPsSometimes a reason to add a cache to a system architecture is to place the most performant IOPs (Input/Output Operations Per Second) available system by the highest IOPs demanded of the system. This can be done by placing the database on hardware that would provide extremely high IOPs such as SSD vs. spindle drives. In some scenarios maybe read IOPs are higher than write IOPs so a write-cache is put into place to store writes, and then write them as IOPs are available at a later time. The mix and match of caches in this space are numerous, and can be catered, as with other areas of caching to meet the needs of the system.Session ManagementSometimes the caching isn't purely about performance, sometimes the emphasis is on data integrity across disparate systems. In the case of session management, many back end systems may be stateless from a distributed nature, but the need to maintain session data for clients is still necessary. A session management cache can be put into place to do specifically this. It often takes the place of an in-memory database, such as Redis.API CachingA favorite of ours here at Hasura, is API caching. With this we can get some blazing fast query times and easily cache those to maintain an extremely high rate of response. For example, any data that isn't changing on a frequent basis is a prime candidate for this type of caching. It can be turned on with use of query response caching.SummaryThis article serves to provide a short overview of the types of cache, what purpose they serve, and at the root of design what a cache is. Now we can take the opportunity to step into specific cache types, services, products, and implementations to see what we can gain with each type. In our next post we'll take at the Hasura approach to caching, via Hasura cloud, in specific.As a note of promotion, Hasura Cloud provides for Query Response Caching when you choose to use the Standard tier. If you are interested in testing it out, starting at the Free Tier gives you the experience of a fully managed, production ready GraphQL API as a service to help you build modern apps faster. Get started in 30 seconds at https://cloud.hasura.io/"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Top 5 GraphQL Resources for Next.js Developers in 2021 Description: GraphQL is stepping in to the 6th year in 2021 and is seeing great adoption in the developer community. We have compiled a short list of 5 best resources (in no particular order) specifically catering to Next.js developers. Author: Praveen Durairaju"}, {"role": "assistant", "content": "GraphQL is stepping in to the 6th year in 2021 and is seeing great adoption in the developer community. We have compiled a short list of 5 best resources (in no particular order) specifically catering to Next.js developers, since it's one of the most popular frontend frameworks in the community. The list broadly focusses on the categories like GraphQL clients, data fetching, authentication, API routes in Next.js and Serverless.1. GraphQL ClientsGraphQL requests can be made using a simple fetch API call. But specialised GraphQL clients exist to serve use cases around caching, better querying and for building reusuable modules. Here are a few clients that we recommend trying out for Next.js.Apollo ClientApollo Client is a fully featured client for managing GraphQL on the frontend. We have created a learn course to talk about how to integrate Apollo Client with Next.js.urqlurql is a lightweight featured client with automatic caching and a slightly different approach to caching (everything is automagical and manual cache access isn't there). But otherwise the API is available via Hooks that can be plugged into your Next.js components.SWRSWR (stale-while-revalidate) is a react hooks library for data fetching, where the data is returned from the cache (stale) and then a request is made to revalidate. Self plug - I made a GraphQL client with SWR and have the boilerplate hosted on github - swr-graphqlRelayGraphQL APIs that support the relay spec can leverage the Relay client. It's built to be production ready and fundamentally revolves around local declarations of data requirements. Each component specifies what they want, more like how each GraphQL query specifies what response should be returned.Fluent GraphQL ClientsGraphQL queries are typically written as strings and parsed by these specialised clients better. But what if you can write your GraphQL query as objects? You can do this using a bunch of fluent GraphQL Clients.2. Data FetchingNext.js has different data fetching modes. We will see how GraphQL fits in these modes and what tooling can help around these use cases.Server side rendering (SSR)Next.js has a SSR mode where the pages are rendered on every request. This is possible via getServerSideProps where data is fetched on each request. In this method, we can make use of a simple fetch API or even specialised GraphQL clients. Here's an example of SSR integration with Next.js + Apollo Client.Static ExportStatic export mode is when data is fetched at build time and html files are pre-generated with that data. Typical use cases for these are static sites like blogs, product pages in e-commerce and content heavy sites that barely change often. In this case, the GraphQL integration can be done via getStaticProps where any GraphQL API call can be made inside using simple fetch.Dynamic (Client side rendering and SSR)Dynamic interactions on the frontend requires API integrations to be done on the client. With Next.js client side rendering, we can make use of Hooks to make GraphQL calls. The popular ones from Apollo and urql let's you make useQuery, useMutation and useSubscription hooks on the client side rendering.Videos on Data FetchingHere's a video walk through by Lee Robinson talking about Fetching Data with Next.jsBuilding a forum software from scratch using Hasura GraphQL and Next.js by Jamie Barton.3. AuthenticationNext.js recommends two Authentication patterns based on what strategy is being used for data fetching (like SSR, Static or Dynamic).Use static generation to server-render a loading state, followed by fetching user data client-side.Fetch user data server-side to eliminate a flash of unauthenticated content.Based on this, if you are going for an approach with your own database, there is an open source library called next-auth that takes care of storing user and session data in db. Read more about Next.js JWT Authentication with next-auth. There's an equivalent live stream that we did as a follow up. There's also a community created boilerplate app for next-auth and Hasura: next-hasura-boilerplate by Nirmalya Ghosh.Auth0Auth0 can support both authentication patterns listed above. You can read more about how to configure Auth0 with Next.js from our fullstack Next.js serverless tutorial. This configures an Apollo GraphQL Client with authorization token (accessToken) that can be sent from the headers.If you are using Firebase, you can use the static generation pattern. 4. API RoutesYou can build APIs with Next.js via API routes. You can technically build a GraphQL server using Next.js API routes.There are two ways to approach this. One is to create a GraphQL Server inside of Next.js and it becomes serverless by default. Voila! The other option is to create the typical serverless function, write your logic and map it to GraphQL types using Hasura Actions.Custom REST API to GraphQL using Hasura ActionsTurn your Node.js REST API to GraphQLWrite a GraphQL Server in Next.js and add as a Remote Schema5. ServerlessThe easiest way to deploy your Next.js app is on the Vercel platform. Your Next.js API routes become serverless by default and there by your GraphQL server side APIs too.herm.dev is a free online workshop on everything you need to build fullstack GraphQL apps with Hasura, Next.js, and Serverless by Christian Nwamba. It is modelled on the 3factor app architecture (read more about building 3factor apps with event-driven programming).Next.js API Routes are deployed as Serverless functions and enjoys the benefit of scale and costs.Also read more on how a Django app architecture was migrated to Next.js Serverless architecture, saving dollars in the process. Migrating Django to Serverless with Next.js by Alan Balja.Let us know in the comments on what your favorite Next.js + GraphQL resource for 2021 :)"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How to Setup Authentication with Django Graphene and Hasura GraphQL Description: In this walkthrough, we're going to go through creating a simple GraphQL authentication service using Django Graphene, meshing it into your Hasura service, and creating a few sample requests. Author: Martin Mark"}, {"role": "assistant", "content": "Hasura has a great feature of being able to merge in external GraphQL schemas, allowing us to do things like stitch together a mesh of services powered by GraphQL.In this walkthrough, we're going to go through creating a simple GraphQL authentication service using Django Graphene, meshing it into your Hasura service, and creating a few sample requests.Getting StartedSimilar to the Food Network, we can start this off by taking a look at what the final project looks like.If you pull down this project, and run:docker-compose up\non the directory, you'll receive a setup like the following:Project Container LayoutPro Tip: If you look in the docker-compose.yml you'll notice we're using the image hasura/graphql-engine:latest.cli-migrations-v2.This migrations image will automatically apply our Hasura metadata (connected remote schema, tracked tables, permissions) as well as our SQL migrations (to generate our schema) from the ./hasura folders which are mounted as volumes in our docker-compose file.Migrations are a super-powerful way to work with Hasura instances, and I highly recommend them as a way to work through your deployment work-flow (more info: https://hasura.io/docs/1.0/graphql/core/migrations/index.html)The Django ServiceIf we take a look at the ./django/dockerfile you'll see that we're running a standard Python 3 container with:pip install django-graphql-jwt django\ninitiated (django-graphql-jwt is the package we'll be using as a helper with this project and it comes with graphene-django and PyJWT as dependencies - your can read more about this package here: https://github.com/flavors/django-graphql-jwt).We've also pinned PyJWT to < 2 as there's a compatibility issue with the current django-graphql-jwt and the latest major version of PyJWT.If you look at the project structure in the ./django folder you'll notice it's the same as if we had run:django-admin startproject app \nand then:python manage.py startapp api\nfrom within our project.We're going to be putting our global settings in app and then our API specific functions in api.We've also automatically run make and run migrations in the ./django/entrypoint.sh on each container startup.Project SetupLet's take a look at what we've added to our ./django/app/settings.py file:./django/app/settings.py...\n# Lets us load our secret from our docker-compose env variables\nSECRET_KEY = os.environ['DJANGO_SECRET']\n...\n# Make sure our Django instance is accessible locally\nALLOWED_HOSTS = ['localhost', 'host.docker.internal']\n...\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n# We're going to load:\n# - Graphene\n# - Optional refresh_token to use a token / refresh scheme in our API\n# - Our api app (which contains our API functions)\n    'graphene_django',\n    'graphql_jwt.refresh_token.apps.RefreshTokenConfig',\n    'api',\n]\n...\n# We're going to use an external SQLite DB for this demo, but if you wanted to use the same Postgres as your app DB, you would use similar to the below and this DB would be accessible in Hasura:\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    },\n    # 'default': {\n    #     'ENGINE': 'django.db.backends.postgresql',\n    #     'NAME': 'postgres',\n    #     'USER': 'postgres',\n    #     'PASSWORD': 'password',\n    #     'HOST': 'blog-postgres',\n    #     'PORT': '5432',\n    # }\n}\n...\n# Defines what Graphene settings and JWT middleware\n# You'll notice we've defined the app.schema - we'll walk through a review of this below.\nGRAPHENE = {\n    'SCHEMA': 'app.schema.schema',\n    'MIDDLEWARE': [\n        'graphql_jwt.middleware.JSONWebTokenMiddleware',\n    ],\n}\n...\n# Defines JWT settings and auth backends\n# You'll notice we've defined app.utils.jwt_payload - we'll be reviewing it below as well.\nGRAPHQL_JWT = {\n    'JWT_PAYLOAD_HANDLER': 'app.utils.jwt_payload',\n    'JWT_AUTH_HEADER_PREFIX': 'Bearer',\n    'JWT_VERIFY_EXPIRATION': True,\n    'JWT_LONG_RUNNING_REFRESH_TOKEN': True,\n    'JWT_EXPIRATION_DELTA': timedelta(minutes=5),\n    'JWT_REFRESH_EXPIRATION_DELTA': timedelta(days=7),    \n    'JWT_SECRET_KEY': os.environ['DJANGO_SECRET'],\n    'JWT_ALGORITHM': 'HS256',\n}\nAUTHENTICATION_BACKENDS = [\n    'graphql_jwt.backends.JSONWebTokenBackend',\n    'django.contrib.auth.backends.ModelBackend',\n]\nIn terms of setting up our JWT - we've gone with a short expiration time (5 minutes for our access token) and a longer time for our refresh token (7 days).In general, this setup works well in that we'll only use our refresh token for updating the expiration time on our access token, and our access token (which gives us access to our application) will frequently be refreshed in case of compromise.You saw from the above that we've mentioned 2 other files, app.utils and app.schema - let's walk through those and look at what they're doing../django/app/utils.pyimport jwt\nimport api.models\nfrom datetime import datetime\nfrom graphql_jwt.settings import jwt_settings\n    \n## JWT payload for Hasura\ndef jwt_payload(user, context=None):\n    jwt_datetime = datetime.utcnow() + jwt_settings.JWT_EXPIRATION_DELTA\n    jwt_expires = int(jwt_datetime.timestamp())\n    payload = {}\n    payload['username'] = str(user.username) # For library compatibility\n    payload['sub'] = str(user.id)\n    payload['sub_name'] = user.username\n    payload['sub_email'] = user.email\n    payload['exp'] = jwt_expires\n    payload['https://hasura.io/jwt/claims'] = {}\n    payload['https://hasura.io/jwt/claims']['x-hasura-allowed-roles'] = [user.profile.role]\n    payload['https://hasura.io/jwt/claims']['x-hasura-default-role'] = user.profile.role\n    payload['https://hasura.io/jwt/claims']['x-hasura-user-id'] = str(user.id)\n    return payload\nWe've got a utility function here to encode our Hasura JWT payload based on the spec at: https://hasura.io/docs/1.0/graphql/core/auth/authentication/jwt.html#the-specYou'll notice we make reference to api.models and user.profile.role - that's a little different.Down below we're going to go over how to extend our user model - once you know how to do that you'll be able to extend your JWT payload further with any other x-hasura... claims your application requires../django/app/schema.pyimport graphene\nimport graphql_jwt\nimport api.schema\n\n## Mutation: \n# - token_auth - for Login\n# - refresh_token - for Token refresh\n# + schema from api.schema.Mutation\nclass Mutation(api.schema.Mutation, graphene.ObjectType):\n    token_auth = graphql_jwt.ObtainJSONWebToken.Field()\n    refresh_token = graphql_jwt.Refresh.Field()\n    verify_token = graphql_jwt.Verify.Field()    \n    pass\n\n## Query:\n# + schema from api.schema.Query\nclass Query(api.schema.Query, graphene.ObjectType):\n    pass\n\n# Create schema\nschema = graphene.Schema(query=Query, mutation=Mutation)\nThis sets up our root GraphQL schema file.You'll notice we're declaring token_auth, refresh_token, and verify_token nodes which will be our default methods of logging in, refreshing our token's expiration, and verifying our token.The will create mutations for those actions.We also mention api.schema which contains the logic for our other query and mutation nodes which we'll be reviewing below.Extending the User ModelBefore we get to setting up our GraphQL logic, a little housekeeping.We want to define the role of our user. It's a really common paradigm in Django and there are a lot of ways to tackle adding more fields to your Django user.The easiest way is to extend your user model with a one-to-one model.This creates a new table which has a relationship to Django's default user model../django/api/models.pyfrom django.db import models\nfrom django.db import models\nfrom django.contrib.auth.models import User\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\n\n# Create your models here.\nactive_roles=(\n    (\"user\", \"user\"),\n    (\"manager\", \"manager\")\n)\n\nclass profile(models.Model):\n    user = models.OneToOneField(User, on_delete=models.CASCADE)\n    role = models.CharField(max_length=120, choices=active_roles, default=\"user\")\n    \n@receiver(post_save, sender=User)\ndef create_user_profile(sender, instance, created, **kwargs):\n    if created:\n        profile.objects.create(user=instance)\n\n@receiver(post_save, sender=User)\ndef save_user_profile(sender, instance, **kwargs):\n    instance.profile.save()\nWhat we're doing here is basically saying, each user has an associated profile row. Roles can be either user or manager (based on active_roles)- defaulting to user.On creation of a new user - make sure they have a profile entry.Just to make it nice, we can also add this relationship to our admin section - this will make this profile model available in our Django Admin (http://localhost:8000/admin/), under our default user model:./django/api/admin.pyfrom django.contrib import admin\nfrom django.contrib.auth.admin import UserAdmin as BaseUserAdmin\nfrom django.contrib.auth.models import User\nfrom api.models import profile\n\n# Register your models here.\n# Inline + descriptor\nclass AdminProfileInline(admin.StackedInline):\n    model = profile\n    can_delete = False\n    verbose_name_plural = 'profiles'\n\n# Define new admin with inline\nclass ProfileUserAdmin(BaseUserAdmin):\n    inlines = (AdminProfileInline,)\n\n# Re-register UserAdmin\nadmin.site.unregister(User)\nadmin.site.register(User, ProfileUserAdmin)\nSetting Up Our Schema FunctionsWe've so far created our mutations for logging in, refreshing, and verifying our token using our root schema file.You'll remember that we referenced api.schema.Query and api.schema.Mutation inside that root schema file.Inside this schema file we'll be defining:A mutation for creating a new user,A query for retrieving my own user profileAnother query for retrieving all userswith authentication restrictions for role and if the user is authenticated../django/api/schema.pyfrom django.contrib.auth import get_user_model\nfrom graphene_django import DjangoObjectType\nfrom api.models import profile\nfrom graphql_jwt.shortcuts import create_refresh_token, get_token\nimport graphene\nimport graphql_jwt\n\n## Mutation: Create User\n# We want to return:\n# - The new `user` entry\n# - The new associated `profile` entry - from our extended model\n# - The access_token (so that we're automatically logged in)\n# - The refresh_token (so that we can refresh my access token)\n\n# Make models available to graphene.Field\nclass UserType(DjangoObjectType):\n    class Meta:\n        model = get_user_model()\n\nclass UserProfile(DjangoObjectType):\n    class Meta:\n        model = profile\n\n# CreateUser\nclass CreateUser(graphene.Mutation):\n    user = graphene.Field(UserType)\n    profile = graphene.Field(UserProfile)\n    token = graphene.String()\n    refresh_token = graphene.String()\n    \n    class Arguments:\n        username = graphene.String(required=True)\n        password = graphene.String(required=True)\n        email = graphene.String(required=True)\n\n    def mutate(self, info, username, password, email):\n        user = get_user_model()(\n            username=username,\n            email=email,\n        )\n        user.set_password(password)\n        user.save()\n\n        profile_obj = profile.objects.get(user=user.id)        \n        token = get_token(user)\n        refresh_token = create_refresh_token(user)\n        \n        return CreateUser(user=user, profile=profile_obj, token=token, refresh_token=refresh_token)\n    \n# Finalize creating mutation for schema\nclass Mutation(graphene.ObjectType):\n    create_user = CreateUser.Field()\n  \n## Query: Find users / my own profile  \n# Demonstrates auth block on seeing all user - only if I'm a manager\n# Demonstrates auth block on seeing myself - only if I'm logged in\nclass Query(graphene.ObjectType):\n    whoami = graphene.Field(UserType)\n    users = graphene.List(UserType)\n    \n    def resolve_whoami(self, info):\n        user = info.context.user\n        # Check to to ensure you're signed-in to see yourself\n        if user.is_anonymous:\n            raise Exception('Authentication Failure: Your must be signed in')\n        return user\n    \n    def resolve_users(self, info):\n        user = info.context.user\n        print(user)\n        # Check to ensure user is a 'manager' to see all users\n        if user.is_anonymous:\n            raise Exception('Authentication Failure: Your must be signed in')\n        if user.profile.role != 'manager':\n            raise Exception('Authentication Failure: Must be Manager')\n        return get_user_model().objects.all()\nTying It All TogetherBy visiting urls.py, we can ensure that our routes are setup to make our GraphQL API accessible to our Hasura instance../django/app/urls.pyfrom django.contrib import admin\nfrom django.urls import path\nfrom django.views.decorators.csrf import csrf_exempt\nfrom graphene_django.views import GraphQLView\n\nurlpatterns = [\n    path('admin/', admin.site.urls), # Exposes your django admin at http://localhost:8000/admin/\n    path(\"graphql\", csrf_exempt(GraphQLView.as_view(graphiql=False))),  # Exposes your django admin at http://localhost:8000/graphql\n]\nNow, if you're using the docker-compose starter you should already be setup with docker being connected to Hasura - but if not, you can make sure all your containers are running and visit: http://localhost:8080/console/remote-schemas/manage/schemasFrom here, you'll be able to mesh your Graphene GraphQL API to Hasura by entering http://host.docker.internal:8000/graphql as your GraphQL Server URL:Connecting Graphene and HasuraTesting Our RequestsHasura has a great GraphiQL request tester which can be found here to run through some sample requests and responses - http://localhost:8080/console/api-explorerCreating a UserLet's get started with creating a user - we can see some of the nested relationship structures which are possible within GraphQL.We've returned a token here - \u00a0this token will be available as our access token for the next 5 minutes.mutation createUser {\n  createUser(username: \"<your_username>\", password: \"<your_password>\", email: \"<your_email>\") {\n    refreshToken\n    token\n    user {\n      username\n    }\n    profile {\n      role\n    }\n  }\n}\nVerify a TokenWe're able to take any token created and test it using verifyToken to validate and receive the token's corresponding payload.mutation verifyToken {\n  verifyToken(token: \"<your_token>\") {\n    payload\n  }\n}\nRefreshing a TokenIf our token expires, we're able to refresh the token using refreshToken - this will result in a response with a token with an updated expiry time (+ 5 minutes in this case).mutation refreshToken {\n  refreshToken(refreshToken: \"<refresh_token>\") {\n    token\n  }\n}\nLogging InLogging in will also provide us with a token / refresh_token pair.mutation loginUser {\n  tokenAuth(username: \"<username>\", password: \"<password>\") {\n    token\n    refreshToken\n    payload\n  }\n}\nUsing Graphene Authentication: Who Am I?We've covered talking with our API to retrieve a token - but what do we do with it now?If we create a new Authorization header, with Bearer followed by our token, we'll be able to authenticate as a user (pictured below).Configuring Authorization HeadersWe can also select the Decode JWT icon to the right of the field to help us analyze what's being decoded from our entered token.Running the following query will run our whoami queryquery whoami {\n  whoami {\n    email\n  }\n}\nUsing Graphene Authentication: Manager RoleWe've tested to make sure our role works, but next let's check to make sure our Manager role will work to show us all users.We can create a Django Superuser using the following command to give us access to the Django Admin at http://localhost:8000/admin/ :docker exec -it blog-django bash -c 'python manage.py createsuperuser'\nFrom that admin panel if we update our user to the manager role and then re-login, we'll be able to see a view of all users by running:query userList {\n  users {\n    email\n  }\n}\nWhere do we go from here?To start we should start off by changing any secrets that are found in our docker-compose.yml file - you can create a new secret with the following:docker exec blog-django bash -c 'python -c \"import secrets; print(secrets.token_urlsafe())\"'\nBut what else can we do once we have an external Django auth service?We can create more remote schema nodes similar to users and whoami above to to extend our API's logic.We can link up our Django service to work with Actions and Events.We can use our JWT implementation to authenticate with our Hasura service and configure row-level permissions using our x-hasura-... JWT claims which were set in the JWT encoder.If you'd like to talk to us about this article or just connect with the team, you should join our community! Check out the Python backend course. In this course, you will learn how to integrate Python in a GraphQL backend server stack with Hasura."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building GraphQL APIs with Deno and Hasura Description: In this post, we will look at building a GraphQL API backend with Deno and create a unified GraphQL layer with Hasura and Deno to solve custom business logic requirements of the app. Author: Praveen Durairaju"}, {"role": "assistant", "content": "In this post, we will look at building a GraphQL API backend with Deno and create a unified GraphQL layer with Hasura and Deno to solve custom business logic requirements of an app.What's Deno? A Quick PrimerDeno is a runtime for JavaScript (and TypeScript) that is secure by default, ships only a single executable that uses V8 engine (and hence performant). What more? It is written in Rust. The Deno runtime runs in a sandbox and hence doesn't have access to the file system (I/O), networking, envs etc unless explicitly specified. With Deno, you can import modules by URLs, alleviating the registry issues that are typically present in the Node.js ecosystem.New to GraphQL? Check out our Introduction to GraphQL tutorial and how Hasura fits in as a GraphQL API backend.Choosing the Deno module for GraphQLAlright, to setup a GraphQL server with Deno, we have a couple of modules to choose from.oak, oak-graphqlOak is a web server framework for Deno and feels very relatable for users coming from a Node.js/Express background. oak-graphql is a middleware for oak adding GraphQL support.obsidianObsidian is a fullstack GraphQL framework for Deno that comes with both a server and client module (works with SSR react apps) and specialised in normalized caching.In this example, we will make use of oak framework and the GraphQL middleware oak-graphql to create an endpoint. Create a new file called server.ts and add the following imports:import { Application, Router, RouterContext } from \"https://deno.land/x/oak@v6.2.0/mod.ts\";\n\nimport { applyGraphQL, gql } from \"https://deno.land/x/oak_graphql/mod.ts\";\n\nconst app = new Application();In the above code, we are importing modules from oak and oak_graphql and creating a new instance of the Application that will be used later.Defining GraphQL Schema and TypesNext up, we go ahead and define GraphQL schema with type definitions. I'm just going to assume a simple User schema that has 3 fields id, first_name and last_name. Add this code below to server.ts.const types = gql`\ntype User {\n  id: Int \n  first_name: String\n  last_name: String\n}\n\ntype UserOutput {\n  id: Int \n}\n\ntype Query {\n  fetchUser(id: Int): User \n}\n\ntype Mutation {\n  insertUser(first_name: String!, last_name: String!): UserOutput!\n}\n`;We have declared a couple of operations; one Query (fetchUser) and one Mutation (insertUser).GraphQL Resolvers in DenoFor the operations above, we need to define the resolvers. We are sticking to returning sample data, but this is the place for writing the logic to perform data fetching or inserts to the database, perform any validations etc.const resolvers = {\n  Query: {\n    fetchUser: (parent: any, { id }: any, context: any, info: any) => {\n      // make database calls or http requests inside and return data\n      return {\n        id: 1,\n        first_name: \"Praveen\",\n        last_name: \"Durairaju\",\n      };\n    },\n  },\n  Mutation: {\n    insertUser: (parent: any, { first_name, last_name }: any, context: any, info: any) => {\n      console.log(\"input:\", first_name, last_name);\n      return {\n        id: 1,\n      };\n    },\n  },\n};Finally, glue this typedefs and resolvers to create a GraphQL server. The server is running on port 8090.const GraphQLService = await applyGraphQL<Router>({\n  Router,\n  typeDefs: types,\n  resolvers: resolvers,\n  context: (ctx: RouterContext) => {\n    console.log(ctx);\n    return { user: \"Praveen\" };\n  }\n})\n\napp.use(GraphQLService.routes(), GraphQLService.allowedMethods());\n\nconsole.log(\"Server start at http://localhost:8090\");\nawait app.listen({ port: 8090 });Running Deno ServerTo run the deno server, we can execute a command like the one below:deno run --allow-net server.tsNote that deno ships with a single executable and we are passing it some flags to apply some permissions. By default Deno is secure and hence --allow-net flag lets you run the server on a given port and host.Now head to http://localhost:8090/graphql. You should see GraphQL Playground loading at this endpoint for this GET request. The POST requests to this endpoint will hit the GraphQL server.Make the following GraphQL query to verify our implementation:query {\n  fetchUser(id: 1) {\n    id\n    first_name\n    last_name\n  }\n}And the following mutation:mutation {\n  insertUser(first_name: \"Praveen\", last_name: \"Durairaju\") {\n    id\n  }\n}That's it! The above queries should give our hardcoded response from the server.ContextThe ctx argument is where you will have the auth context for the incoming request. Typically this is where you will handle Authorization headers sent from the client.Packaging and Deployment of Deno with DockerDeno can be packaged as a docker image and can be run with docker-compose. Let's look at the Dockerfile steps:FROM hayd/alpine-deno:1.7.2\n\nEXPOSE 8090\n\nWORKDIR /app\n\nUSER deno\n\nCOPY . .\nRUN deno cache server.ts\n\nCMD [\"run\", \"--allow-net\", \"server.ts\"]We can ofcourse change the PORT exposed. Throughout this example, we have used the PORT 8090, but can be replaced with any port of choice. I run Hasura on my local machine at 8080 and hence my custom business logic servers run at 8090 usually.Create the following docker-compose.yaml file in the same directory:version: \"3.7\"\n\nservices:\n  deno:\n    image: \"deno-docker:0.0.1\"\n    build: .\n    ports:\n      - \"8090:8090\"\n    networks: \n      - deno\n\nnetworks:\n  deno: {}And finally let's create the image and run it!docker-compose up -dFor further changes, we can use the above docker-compose command to restart the server and serve the endpoint at http://localhost:8090.Local Dev with ngrokWe can run this container setup on cloud vendors in production. But for local development, I'm going to make use of ngrok to expose this to the outside world. This is needed because once we create the Hasura Cloud project and try to add a remote schema (covered later below), we need a publicly accessible endpoint for things to work. Once you setup ngrok, run ngrok http 8090 to expose the local port 8090 on a publicly accessible URL. You should get a URL similar to https://2f10a069dc2a.ngrok.io.Deploy Hasura to get a GraphQL APIClick on the following button to deploy GraphQL engine on Hasura Cloud including Postgres add-on or using an existing Postgres database:\n2. Open the Hasura console by clicking on the button \"Launch console\".3. Create table users.Head to the Data tab and create a new table called users with columns:id (Int)email (text)created_at (timestamp now()).Select the primary key as id.Once the table has been created, create a new record by heading to the Insert Row tab of the users table. Create a user with id : 1. (Remember we hardcoded a user with the same id in our deno server?)Adding Deno GraphQL Server as a Remote SchemaHead to the Remote Schemas tab of the Hasura Console to add the Deno server to Hasura's GraphQL schema.Remote Schema with DenoThe GraphQL server URL will be the /graphql endpoint of the deno server. Once we add this, we will get a unified API layer with Hasura's endpoint.Establishing remote relationshipsNow that we have users table in Postgres and the fetchUser query from Deno, we can merge data from these by establishing a remote relationship. Head to the Relationships tab and configure a new Remote Schema Relationship as shown below:Remote Relationship with DenoNow you can make the following query from Hasura's API Explorer to fetch the merged data:query {\n  users {\n    id\n    created_at\n    user_metadata {\n      first_name\n      last_name\n    }\n  }\n}Note that, data for id and created_at fields come from the database (Postgres) and user_metadata fields like first_name and last_name come from our Deno server.Deno Server as a Hasura Action handlerThe example that we tried out above is for a standalone GraphQL server written with Deno that can be added as a Remote Schema on Hasura. But if you are comfortable writing REST APIs with Deno with oak, then you can totally do that as well. Once you have a REST API endpoint, you can define an Action handler with the GraphQL schema and types and point to that endpoint as the webhook URL. We can look at this in detail in an another post!Link to Github - https://github.com/praveenweb/deno-graphqlAre you already using Deno for writing your REST APIs or GraphQL due to the sandbox environment? Let us know in the comments."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Next.js JWT Authentication with NextAuth and Integration with Hasura Description: In this tutorial, we will look at implementing a custom JWT solution with next-auth, served by Next.js and integrate the same with Hasura and make authenticated GraphQL API calls. Author: Praveen Durairaju"}, {"role": "assistant", "content": "If you are a Next.js developer and looking for an Authentication solution, look no further than next-auth, an open-source Authentication library for Next.js. It's built for serverless (can run anywhere too) and supports various services like Sign in with Google, Apple, Facebook, Github or a simple email/password combination among others. You can bring your own database for storing sessions in the db or simply use JWT as we are going to do in this tutorial.Hasura supports Authentication in the form of JWT / webhooks. The recommendation is to typically use JWT over webhooks for most use cases. With JWT, you get latency free requests since the session information is stored on the client and not on the server. Read more on the Best Practices for using JWT on frontend clients.Clone next-auth exampleIn this tutorial, we will look at implementing a custom JWT solution with next-auth, served by Next.js and integrate the same with Hasura and make authenticated GraphQL API calls.Let's start with the official example app and configure it.git clone https://github.com/nextauthjs/next-auth-example.git\ncd next-auth-example\nnpm inpm i next-auth pgCopy environment variables:cp .env.local.example .env.localWe will come back and modify the values here later. Let's look at the directory structure. Inside the pages directory under the api routes, you have the logic written for auth in the file [...nextauth.js]. This will let this component handle all the requests coming in to /api/auth/*. Some example include, signin, signout, callback etc.Deploy Hasura to get a GraphQL APIClick on the following button to deploy GraphQL engine on Hasura Cloud including Postgres add-on or using an existing Postgres database:\n2. Open the Hasura console by clicking on the button \"Launch console\".3. Create table users.Head to the Data tab and create a new table called users with columns:id (text)name (text)created_at (timestamp now()).JWT ConfigurationHasura supports authentication via webhook and JWT. For this next-auth example, we will look at creating a custom JWT server to sign and verify tokens.Generating SecretWe need to generate a secret that can be used to hash the tokens and configure them on Hasura. There are a couple of options available.HS256RS256 / RS512If you are going to use HS256 algorithm, there is only a secret to be generated that will be used on both the Next.js server and inside Hasura config. On the other hand, if you are going to use RS256, we need to generate a public/private key pair and the private key will be used to sign the token, and the public key will be used to verify the token on the Hasura's end.JWT with HS256Head to https://generate-secret.now.sh/32 to generate a random secret that can be used on both the Next.js server and Hasura config. I'm taking a value 69f8fd4d54342b7ee3b0fcdf6def434c for the secret.Let's configure this on the .env.local file that we copied earlier and substitute this value for the SECRET env.On the Hasura Cloud projects page, head to the ENV vars page to configure this secret. Note that you need to add an admin secret using HASURA_GRAPHQL_ADMIN_SECRET before configuring the HASURA_GRAPHQL_JWT_SECRET.The JWT config for this would look like:{ \"type\": \"HS256\", \"key\": \"69f8fd4d54342b7ee3b0fcdf6def434c\"}Add Hasura_GRAPHQL_JWT_SECRET env varDo replace the secret with your own generated code.JWT with RS256Follow the steps to generate a private/public key pair for using the RS256 algorithm.Generate Private Keyopenssl genrsa -out private.pem 2048Generate Public Keyopenssl rsa -in private.pem -pubout -out public.pemTransform public key into a single lineawk -v ORS='\\\\n' '1' public.pem | pbcopyNow paste this value in your clipboard to Hasura Cloud env in the format { \"type\": \"RS256\", \"key\": \"<insert-your-public-key-here>\"}Transform private key into a single lineawk -v ORS='\\\\n' '1' private.pem | pbcopyConfigure this secret into your .env.local file under the SECRET env. This will be used by the next-auth config to sign the token.Sign In with GitHubnext-auth supports various data providers to integrate with Sign In services, OAuth providers and email/password combinations. We will use the GitHub provider in this example. Follow the docs for enabling that.Head to the [...nextauth.js] file and remove the other providers. The configuration for providers will look like this:providers: [\n    Providers.GitHub({\n      clientId: process.env.GITHUB_ID,\n      clientSecret: process.env.GITHUB_SECRET,\n    }),\n],Encode JWTInside the jwt object, we need to enable the secret that would come from process.env.SECRET and expand on the logic for encode and decode methods.We will need the jsonwebtoken module for signing the token with the secret.So let's do that by npm install jsonwebtoken. Here's the Hasura Docs on Custom Claims. All the custom claims for Hasura stays in https://hasura.io/jwt/claims. The values we care are x-hasura-role and x-hasura-user-id because these session variables will be used in this example's permission system.encode: async ({ secret, token, maxAge }) => {\n      const jwtClaims = {\n        \"sub\": token.id.toString() ,\n        \"name\": token.name ,\n        \"email\": token.email,\n        \"iat\": Date.now() / 1000,\n        \"exp\": Math.floor(Date.now() / 1000) + (24*60*60),\n        \"https://hasura.io/jwt/claims\": {\n          \"x-hasura-allowed-roles\": [\"user\"],\n          \"x-hasura-default-role\": \"user\",\n          \"x-hasura-role\": \"user\",\n          \"x-hasura-user-id\": token.id,\n        }\n      };\n      const encodedToken = jwt.sign(jwtClaims, secret, { algorithm: 'HS256'});\n      return encodedToken;\n},Decode JWTdecode: async ({ secret, token, maxAge }) => {\n      const decodedToken = jwt.verify(token, secret, { algorithms: ['HS256']});\n      return decodedToken;\n },Adding User ID to TokenFrom NextAuth Docs:If you want to pass data such as an Access Token or User ID to the browser when using JSON Web Tokens, you can persist the data in the token when the jwt callback is called, then pass the data through to the browser in the session callback.We need to handle the callbacks from our Sign In service to be able to add user ID to the token. Inside the callbacks object, handle the logic for session:async session(session, token) { \n      const encodedToken = jwt.sign(token, process.env.SECRET, { algorithm: 'HS256'});\n      session.id = token.id;\n      session.token = encodedToken;\n      return Promise.resolve(session);\n    },and jwt callbackasync jwt(token, user, account, profile, isNewUser) { \n      const isUserSignedIn = user ? true : false;\n      // make a http call to our graphql api\n      // store this in postgres\n      \n      if(isUserSignedIn) {\n        token.id = user.id.toString();\n      }\n      return Promise.resolve(token);\n    }Note that we need to write the handler to make an http call to GraphQL API to insert the user and sync it with the users table in the database.Head to server.js in the pages directory to see how the session prop is exported to use session information with Server Side Rendering.export async function getServerSideProps(context) {\n  return {\n    props: {\n      session: await getSession(context)\n    }\n  }\n}This set up enables you to access the session information in _app.js where we use the NextAuth <Provider> HOC to render the components.After logging in, check the token generated on the network tab. There is an API call made to /api/auth/session that would return the token. This token can now be used to make authenticated GraphQL API calls with Hasura via the Authorization header.Link to Github - https://github.com/praveenweb/next-auth-hasura-exampleCheck out the YouTube video to follow along the whole integration from scratch.Do let me know in the comments if you want specific use cases to be covered!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: TimescaleDB 2.0 with Hasura Part 3 - High Availability Description: Performance and High availability are critical challenges in production. Although TimescaleDB solves performance problems, we still need to care about remaining parts. In this article, I suggest some high availability setup for single-node as well as explore multi-node replication solutions. Author: Toan Nguyen"}, {"role": "assistant", "content": "Our #GraphQLJanuary continues with blog posts, live streams, Discord Q&A, office hours, and more. For a schedule of upcoming events, join the community or register at https://hasura.io/graphql/graphql-january/.In this post, Hasura engineer Toan shares his experience with using Hasura alongside TimescaleDB 2.0 and exploring what works...and what doesn\u2019t. Learning from the experience of others exploring workarounds to achieve their goals is always instructive.This post is part of a 3-part series exploring TimescaleDB with Hasura.Performance and High availability are critical challenges in production. Although TimescaleDB solves performance problems, we still need to care about remaining parts. In this article, I suggest some high availability setup for single-node as well as explore multi-node replication solutions.Single-nodeTimescaleDB is an extension of Postgres, so you can use any replicated solution of Postgres. The common solution is streaming replication, combined with repmgr or Patroni for automatic failover.If you use Kubernetes, you can install official helm repository of TimescaleDB that uses Patroni with 3 nodes by default.TimescaleDB single-node replicationTimescale Cloud and Timescale Forge are also great options. You can deploy a highly-available cluster easily with several clicks, and no worry about system management.Hasura works well with TimescaleDB replication. However, it's better to use Hasura Cloud/Pro to take advantage of read replicas. Hasura Cloud can load balance queries and subscriptions across read replicas while sending all mutations and metadata API calls to the master.Multi-nodeNow we have a multi-node cluster. However does it support high availability? If one data node is down, does the TimescaleDB cluster still work?With this test case, I stop data node timescaledb-data1, then test several operations:INSERT works. The access node can know the status of data nodes , and distribute to alive nodes.However SELECT, UPDATE and DELETE operations fail.postgres= INSERT INTO conditions VALUES (NOW(), 'US', 13, 1.3);\nINSERT 0 1\npostgres= SELECT * FROM conditions ORDER BY time DESC LIMIT 1;\nERROR:  could not connect to \"timescaledb-data1\"\nDETAIL:  could not translate host name \"timescaledb-data1\" to address: Name does not resolve\nSo, it only appears to support high availability on INSERT. The query planner isn't excluding outage data nodes. Therefore, you have to ensure high availability for all data nodes.I came up with an idea. Can we temporarily detach outage data nodes? No, it can't, even with the force option.postgres= SELECT detach_data_node('timescaledb-data1', force => true);\nERROR:  insufficient number of data nodes\nDETAIL:  Distributed hypertable \"conditions\" would lose data if data node \"timescaledb-data1\" is detached.\nHINT:  Ensure all chunks on the data node are fully replicated before detaching it.\nNative ReplicationTimescaleDB 2.0 provides built-in, native replication support for multi-node. This feature is promising although it is still in development preview.We aren't required to use any additional setup to use native replication. Just need to set the replication_factor argument with integer value. This argument represents the number of data nodes that the same data is written to. The value must be in between 1 and total data nodes. The default value is 1.Let's see how many rows are inserted into 2 data nodes:\n\n\naccess\ntimescaledb-data1\ntimescaledb-data2\n\n\n\n\n1,000,000\n1,000,000\n1,000,000\n\n\n\nFrom the above table, the number of rows are equal. That means the data is copied into both data nodes. This ensures the consistency of data. The trade off is slower performance. Data nodes take more work to replicate data. The delay depends on the number of replication factors.\n\n\nOperation\nreplication_factor = 1 (ms)\nreplication_factor = 2 (ms)\n\n\n\n\nINSERT\n12,700.597\n16,083.890\n\n\nUPDATE\n184,707.671\n259,930.983\n\n\nSELECT\n2,011.194\n2,023.870\n\n\nDELETE\n159,165.419\n248,658.194\n\n\n\nSurprisingly the SELECT performance is comparable with non-replication mode. The query planner knows how to include only one replica of each chunk in the query plan. However, the SELECT query still throws error after stopping one data node.postgres= SELECT COUNT(*) FROM conditions;\nERROR:  could not connect to \"timescaledb-data1\"           \nDETAIL:  could not translate host name \"timescaledb-data1\" to address: Name does not resolve\nFortunately we can detach the data node now, with force.-- Still error if you don't force detachment\npostgres= SELECT detach_data_node('timescaledb-data1');\nERROR:  data node \"timescaledb-data1\" still holds data for distributed hypertable \"conditions\"\n\npostgres= SELECT detach_data_node('timescaledb-data1', force => true);\nWARNING:  distributed hypertable \"conditions\" is under-replicated\nDETAIL:  Some chunks no longer meet the replication target after detaching data node \"timescaledb-data1\".\nWARNING:  insufficient number of data nodes for distributed hypertable \"conditions\"\nDETAIL:  Reducing the number of available data nodes on distributed hypertable \"conditions\" prevents full replication of new chunks.\n\n-- the SELECT query works now\npostgres= select COUNT(*) FROM conditions;\n\n  count    \n---------\n 1000000\nUnfortunately, we can't attach an existing table from the data node. What will we do? Drop the table in the data node and attach again? It isn't a good choice.postgres= SELECT attach_data_node('timescaledb-data1', 'conditions', if_not_attached => true);\nERROR:  [timescaledb-data1]: relation \"conditions\" already exists\nUp to now, the best choice for a high availability solution with TimescaleDB multi-node feature is streaming replication. For ideal design, every node has 1 replica with failover support.Multi-node replication infrastructureYou also can sign up Timescale Forge to deploy Multi-node. At least with enterprise support, Timescale team can help you solve many replication problems.CaveatsThe common limitation of single-node replication is data latency between master and replicas, especially when the master node receives a large amount of write requests. Although we can set synchronous replication mode, it isn't recommended. Write performance will be degraded radically. However it is acceptable if you don't require real-time metrics.The limitation is solved on multi-node and distributed hypertable. The access node doesn't store data. It's just a load balancer, and connects to master data nodes only. So query results are consistent on whatever replica access node. However, the latency still exists if you use a non-distributed hypertable as well as a vanilla table on the access node.Because of asynchronous replication, schema sync is also a problem. Sometimes we create a table on master, but it doesn't exist on replicas. In that case, there isn't another option than waiting and even reloading metadata on GraphQL Engine.ConclusionTimescaleDB single-node replication isn't different from vanilla Postgres. TimescaleDB team also provides high availability templates for common deployment systems (Kubernetes) as cloud solutions, depending on our use cases. It is also clear that native replication features are still in active development and the replication solution is cool and something to look forward to.You can give Hasura Cloud a try to enable Read Replicas's power. There will be fun things to play with and read replicas with upcoming versions. Let's keep in touch with Hasura blog and Events for new plans and cool demos about Hasura!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using Hasura as BaaS for Nuxt.js Description: In this post, we will look at using Hasura as BaaS for Nuxt.js and integrate GraphQL APIs with Apollo Client. Author: Harsha"}, {"role": "assistant", "content": "This article was written by Migsar Navarro as part of the Hasura Technical Writer Program. If you would like to \u00a0publish an article about Hasura or GraphQL on our blog, apply here.Hasura can be a really cool tool to get an immediate GraphQL API for an existing database, one thing I hear less often is that it is a great tool for early prototyping that allows you to create a nice Back-end as a Service(BaaS) for a site in minutes locally. Finally, you can be rest assured that it will be easy to migrate to a robust infrastructure like Hasura Cloud with minimal changes.This post assumes you already have:Docker and docker-compose installed and runningNode.js and Nuxt.js command-line toolWe will use Fastify as our server but you can use Express, Nest or any serverless functions\u2019 provider. It will run inside Docker, same as Hasura, so you don\u2019t need to install any additional software.The ideas here are intended to give you a quick introduction to Hasura and to show you how it is flexible enough to be used for front-end development out of the box with almost no learning curve. We will focus on the core concepts and architectural decisions, nevertheless we will end up with a simple but fully functional application.Table of ContentsLocal Hasura setup with Docker ComposeNuxt.js app setupAuthentication with HasuraHasura\u2019s actionsThe whole code for the sample project is available on Gitlab.How do I get started?You can always use Hasura Cloud to get an instant backend, but in this tutorial I will show you how to use it locally.Let's start by taking a look at the install manifests provided by Hasura, here, in the install-manifest \u00a0folder of the graphql-engine repository. We can see they have all common options covered, we will use docker-compose/docker-compose.yaml . Run the following commands in the terminal.mkdir hasura-test\ncd hasura-test\nwget -O server/docker-compose.yaml https://raw.githubusercontent.com/hasura/graphql-engine/master/install-manifests/docker-compose/docker-compose.yaml\ndocker-compose upThat's it, we have our instance running, we can see the admin panel at http://localhost:8080 and we can start hitting the GraphQL API at http://localhost:8080/v1/graphql, of course, we still don't have any model. Don\u2019t worry, this was just a quick start to get a glimpse of how easy it is, but actually we will try to get this inside a Nuxt project. As you just saw, Hasura set up is just a docker-compose file, so let\u2019s create a clean Nuxt.js application and move this file to it.Press Ctrl-C to stop the project and then install Nuxt.js:# yarn create-nuxt-app <project-name>\nnpx create-nuxt-app hasura-as-baas\n\n# This is not the full output, just some config options\n# create-nuxt-app v3.4.0\n# ? Programming language: JavaScript\n# ? Rendering mode: Universal (SSR / SSG)\n# ? Deployment target: Static (Static/JAMStack hosting)\n# \ud83c\udf89  Successfully created project hasura-as-baas\n#  To get started:\n#        cd hasura-as-baas\n#       yarn devBefore start developing let's create a server directory and put our docker-compose.yaml file there:mkdir server\nwget -O server/docker-compose.yaml https://raw.githubusercontent.com/hasura/graphql-engine/master/install-manifests/docker-compose/docker-compose.yamlNow that we have both the front-end and the back-end installed let's help them start talking.Data Models for the applicationWe will create a very basic shopping list with user authentication, that means, some lists will be public and displayed to all users but users will be able to have their own private lists.We will focus on authentication, user experience will lack some basic features but this will allow us to take out a lot of the complexity by removing all front-end talk and, as a consequence, having clear principles that are easier to adapt to your own projects . First, we will use a webhook to get user authentication working.RolesRegisteredAnonymousModels(tables)Roles(Needed to implement roles in Hasura)UsersProductsListsCreating rolesHasura has enum tables that are a more suited version than Postgres enums because they allow more flexibility. We can turn any table into an enum if it follows some rules, it can have two fields, value and description of type Text and must have at least one row, that is why the order for the following tasks is important. You can find more details in the enum docs.Create roles tableCreate the tableOpen the GraphQL console at http://localhost:8080.Click the Data tab on the top.Click the Create table button that is next to Schema title.Write roles as the name.Add a two columns of type Text and click Create tableInteractively add rolesAdd valuesAdd \u2018registered\u2019, \u2018A registered user\u2019 as first row.Add \u2018anonymous\u2019, \u2018A non-registered user\u2019 as second row.Use table as enumSet table as enumGo back to Modify tab and almost at the end of the options there is a toggle Set table as enum, activate it.Creating usersNow we can create the users table, we could have created it before the roles table and then get back to modify it, it is a matter of personal preference but there is no constraint imposed by Hasura, it is as easy to modify it after creation as it is to customize on creation. One really nice thing is that after creating this table in the Insert row tab you will be able to choose role with a dropdown.Create users tableCreate the tableUse users as the name.Add id of type UUID with gen_random_uuid() as default value.Add name, email and role of type Text.Set id and email as unique.Add a foreign key, reference the table roles from role to value.Adding roles as foreign keyEnums are displayed as dropdownRelationships: Products and listsThe creation of this tables is similar, we will need in id, \u00a0name and description for both of them. For list we will additionally need userId that references users. Then, since we have a many-to-many relationship between lists and products we will need a pivot table list_products that has list_id and product_id and references each table respectively.Creating relationshipsOne last thing is to create the relationships between tables, this is done in the Relationships tab of each table and it will allow us to create nested object requests, that is, GraphQL queries and mutations that modify more than one table at once, that will prove very handy in the next sections.There are six relationships that we can create, all of them detected automatically by Hasura and only four of them needed, the last one is created on the pivot table which we won\u2019t usually use by itself.Users\u2019 lists. Allow us to retrieve full list objects for a given user.Lists\u2019 owner and products.Products\u2019 lists.ListProducts\u2019 list and productThere is a catch, when you create this kind of relationships GraphQL schema will wrap the nested object inside of another object, it is not a big deal but you have to keep it in mind when writing your queries. We will talk about this on the next section.Creating sample dataNow that we have our schema in place we can insert some sample data to be able to test our authentication and authorization policies and also to test the service we will add with an action.All code here is meant to be used inside the Graphiql console provided by Hasura. You can use the Explorer to help you write the queries, I\u2019ll make a note here because I found this behavior a bit tricky, to create mutations or subscriptions clear the GraphQL text box and then go select the desired option on the dropdown at the bottom of the panel and click the + button, the content of the panel will change and you\u2019ll be able to start using it.Let\u2019s start by creating an user:mutation InsertUser {\n  insert_users(objects: [\n    {\n      email: \"migsar.navarro@gmail.com\",\n      name: \"Migsar Navarro\",\n      role: registered\n    }\n  ]) {\n    returning {\n      id\n    }\n  }\n}Now that we have a user, we will create a list with some products:mutation InsertList {\n  insert_lists_one(object: {\n    name: \"My shopping list\",\n    description: \"Let's party\",\n    owner_id: \"1f5c2814-a3c0-456e-b528-7490e2f45930\",\n    products: {\n      data: [\n        {\n          product: {\n            data: {\n              name: \"Beers\",\n              description: \"My favorite beers.\"\n            }\n          }\n        },\n        {\n          product: {\n            data: {\n              name: \"Gummies\"\n            }\n          }\n        },\n        {\n          product: {\n            data: {\n              name: \"Salty snacks\"\n            }\n          }\n        },\n        {\n          product: {\n            data: {\n              name: \"Oranges\",\n              description: \"I like oranges\"\n            }\n          }\n        }\n      ]\n    }\n  }) {\n    id\n  }\n}We have hard-coded all the information, we could use variables but let\u2019s keep it simple, one thing to note is how nested objects are a bit convoluted, products is an object with a data prop that contains an array of products, but since it refers only to the pivot table and not the products table itself what we are declaring is an array of references to products from the pivot table, so we need to use a wrapper again.Business Logic FunctionsOnce you\u2019ve decided you can have your full API in Hasura you still need to choose which language and framework you want to use for these business logic functions. It is a really flexible solution since the interface is a regular HTTP service, the implementation is completely up to you.Here we are going to use Node.js Fastify. The overall structure of the function is very similar but there will be some differences in the shape of the payload and in the expected response. Our servers will have a single route, the root path.To get started with this I\u2019ve put together a fastify-docker-starter repository on Github, you can clone it and then remove the git files:cd server\ngit clone https://github.com/migsar/fastify-docker-starter my-function\nrm -rf my-function/.gitThese functions could be executed locally, since Hasura is already using containers it is a better approach to start packing these functions there as well, keeping these things together will help you migrate quickly to a cloud infrastructure provider. We will limit the number of Docker commands we will need and all of them are included in the package.json for the sake of convenience. As a convention we will name the images hasura-baas-* where the * is replaced by the name of the service.You can easily start building your own API in a single container using routes. I prefer to have single route very specific containers because those are usually completely decoupled and easy reused among different projects. It is all about finding the sweet spot for the project at hand.AuthenticationSo far all request are unauthenticated, or put it another way, everyone has admin permissions on the database. This is not the best option for obvious reasons. We will need to enable some form of authentication. Basic authentication flowHasura allows for webhooks and JWT authentication, we will cover only the first in this post. It is important to emphasize that Hasura instance will communicate with our service, not the front-end. Authentication can be very specific to some projects and there are some complex topics as caching and storing tokens that are not covered in this article.Out of the box, Hasura\u2019s installation is open, however Hasura provides a way to skip authentication, it is an environment variable called HASURA_GRAPHQL_ADMIN_SECRET that is set in the docker-compose.yaml, it is already included but commented out, once enabled if it is present it won\u2019t check anything else and allow you to do anything. If it is not it will deny access or look for alternative methods.Webhooks is one of these alternative methods; to configure it we will need to setup two env vars:HASURA_GRAPHQL_AUTH_HOOK: the webhook\u2019s endpoint.HASURA_GRAPHQL_AUTH_HOOK_MODE: the request\u2019s method, GET or POST.We will add all three variables to docker-compose.yaml and configure the admin secret for development and the hook for production. If we want to go a step further we can create a .env file in out root directory, move the environment variables there and reference it from our docker-compose.yaml, be sure to change : to = \u00a0and remove the \" around true\u2019s. I really like this to be sure envvars never made their way into the repository, since .env is in common .gitignore configurations. If you want to learn a bit more check Docker Compose\u2019s documentation.Once you have your .env file in place:# Replate this:\n    environment:\n      HASURA_GRAPHQL_DATABASE_URL: postgres://postgres:postgrespassword@postgres:5432/postgres\n      HASURA_GRAPHQL_ENABLE_CONSOLE: \"true\"\n      HASURA_GRAPHQL_DEV_MODE: \"true\"\n      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log\n\n# By this:\n    env_file:\n      - ../.envOur authentication service will be very basic, but keep in mind that it is a regular HTTP endpoint, that has two important implications. The first is that you are not limited at all to use Node.js, the second is that you can just plug something already battle tested, like passport and any of the strategies it has. We won\u2019t do that here. We will add a header x-email to our requests, if it is present and if the mail is already in our database then it is a registered user, otherwise it is anonymous./server/docker-compose.yaml# Not all the code is displayed\nservices:\n  auth:\n    image: hasura-auth:v1\n    restart: always\n    env_file:\n      - ../.env/server/auth/src/index.mjs\n/ Not all the code is displayed\nimport pg from 'pg';\nconst { Pool } = pg;\n\nconst {\n  AUTH_SERVER_HEADER_KEY: headerKey,\n  HASURA_GRAPHQL_DATABASE_URL: connectionString,\n} = process.env;\n\n// Read-only SQL\nconst getRoleQuery = ({ email }) =>\n  `SELECT role FROM users WHERE email='${email}'`\n\nconst pool = new Pool({ connectionString });\n\nserver.post('/', async (req, reply) => {\n  const { headers } = req.body;\n  if (!headers) {\n    return reply.code(400).send(new Error(Errors.NO_DATA));\n  }\n\n  if (!headers[headerKey]) {\n    // Here we can return any other session variable\n    // Just add another key with the form X-Hasura-*\n    reply.code(200).send({ 'X-Hasura-Role': Roles.ANONYMOUS })\n  }\n\n  try {\n    const { rows } = await pool.query(getRoleQuery({ email: headers[headerKey] }));\n    const role = rows.length > 0 ? rows[0].role :  Roles.ANONYMOUS;\n    reply.code(200).send({ 'X-Hasura-Role': role })\n    reply.code(200).send();\n  } catch (error) {\n    return reply.code(400).send(error)\n  }\n})One final step, we need to create the image with the name we referenced in the docker-compose.yaml file docker build -t hasura-auth:v1 . On /server/auth. Next time we bring Docker Compose up we will have our authentication service.At this point you might ask, \u201cWe have a user, but how will new users sign up?\u201d. It is a fair question, the answer for now is, they won\u2019t someone will have to add them, either a registered user or someone connected with the admin secret header. \u00a0There are different strategies and the truth is that this is highly implementation dependent, so it is hard to come up with a solution that fits all cases, two possible strategies are:Configure the anonymous role to have read-only permission to most assets but write permissions to users table, this could work in internal applications since your users are trusted to some extent.Use a mix of actions and backend-only permission in the write roles to add an additional verification layer.You can always just skip this and create a service that directly writes the user into the database but I think that is the least elegant solution.Authorization / Access ControlWe now have roles in place, the sad thing is that we have not set any clear policy, the only role used in Hasura so far is admin, so when we try to access any thing with our brand new auth service in place we will get an error about things not being found in query root. A feature of Hasura is to have authorization built-in, and it is really nice to don\u2019t have to worry about that, we have a very flexible way of defining the rules and Hasura will enforce them.Let\u2019s assume the admin key will only be used by developers, so we only have the two roles defined above: registered and anonymous.Anonymous is easier to define, it must be able to see public lists (we will need to add a boolean \u00a0is_public column to the lists table to define a list is public), that is easy to accomplish, we have to go to the Permissions tab for the lists table in Data, and add a role anonymous, with the same case as it was defined in the roles table, then click the cross that appears in the select column of the permissions table, then add the following condition and select to share only the name and description columns.Visual editor for permissions{\"is_public\":{\"_eq\":true}}As it is mentioned on the bottom, we must define permissions for the relationships in their own tables, it means we must allow select for lists_products and products table since we want to retrieve all information in a single request. For list_products we will allow access only to public lists:{\"list\":{\"is_public\":{\"_eq\":true}}}For \u00a0products we will allow access to all of them but hide the id and only show name and description.Registered user\u2019s are a bit more complex, we must add the user owned products to select, add insert, update and delete permissions for them. Here is a summary:// User: select, update and delete\n{\"id\":{\"_eq\":\"X-Hasura-User-Id\"}}\n\n// List: select\n{\"_or\":[{\"owner_id\":{\"_eq\":\"X-Hasura-User-Id\"}},{\"is_public\":{\"_eq\":true}}]}\n\n// List: update and delete\n{\"id\":{\"_eq\":\"X-Hasura-User-Id\"}}\n\n// List: insert\n{}Registered users will be able to insert without any check, we have to be careful about having permissions to insert in all related tables, in this case list_products and products otherwise the list will be created but the user will lack the ability to add products to it, there is no easy way to warn you about this because of the flexibility of the tool and the fact of this being related to your business rules. One last thing we can do to make sure our user only creates lists for himself is to use a column preset for the owner_id column.Column preset for owner_idHasura ActionsActions allow Hasura to provide third party functionality inside of the same GraphQL API, it is a great feature because it gives you the ability to use other services in a seamlessly and complaint way. One important thing to consider is that it has some cost, both in terms of infrastructure and abstraction, so it is important to find the sweet spot and if you use a lot of external functionality consider other options like remote schemas or even consuming two unconnected APIs only related through client state.There are some benefits in using actions:Keep the same models. You can use the same entities for a new but related service. Users don\u2019t need to learn new abstractions or convert from one representation to another, it all happens with the same well-know GraphQL syntax and existing resources.Security. The service won\u2019t be accessed from public internet but from your Hasura server, that means infrastructure won\u2019t be exposed even if it is secureSpeed. The call is done at the server, which can be located at a production grade server and optimized to talk with other services. It also mean one less request in the server which could be important to some usersManagement. This one is tricky, I believe in job specialization, as such I think performant teams don\u2019t need to know the whole stack or what it\u2019s happening in other places of the application, that\u2019s why you hire experts on each fields, if you can afford it and the project size worth it. By using actions you keep front-end users focused on front-end and not services.We could use actions for sign up users, email verification or just sending them emails but that is a topic for another post. Just to say that actions are webhooks as well and share a lot of things with the function we just created for authentication.GraphQL Integration with Nuxt.jsWe finally have everything setup with Hasura so let\u2019s head up to configuring Nuxt.js. Our approach here will be to have some setup in which we can have some granularity on permissions, you can switch easily from the admin with access to all queries, that can be useful as a developer, to a user with any of the allowed roles, without sharing the admin key.Our goal is to be able to see public lists listed in the landing page if we are not logged in and to be able to create a new list if logged in. We will set up a very simple application, we won\u2019t use the router ourselves, just the internal one used by Nuxt.Registered user viewFirst we need to add some dependencies, here they are as commands, I\u2019ve group them just to give an idea of the thing they are used for but they can all be installed in a single line.yarn add @nuxtjs/apollo @nuxtjs/dotenvyarn add vue-apollo apollo-link-http isomorphic-fetchyarn add graphql graphql-tagTo keep thing clean we will create a new top-level directory graphql to put all our queries there.We will have a very simple state:const defaultState = {\n  _email: null,\n  user: null,\n  error: null,\n  isLoggedIn: false,\n};Most of these variables are self descriptive, _email is a variable used to hold the unauthenticated user for the authentication request, it could be done setting the role on the user instead, it is a personal preference, error can hold a string or null, if it holds a string that will be the error displayed to the user.Not so human readable error messageApp\u2019s overviewThis application is really simple, it consist in a general Vuex store, a single index view and the following components:Layout - Common code to all views, we include topbar component hereTopbar - A bar with a login/logout area and an error areaShoppingList - A component to display our shopping listThe user can login by typing an email that already exists in the database, if that\u2019s the case it will be able to access private lists, otherwise only public lists will be displayed.Anonymous user viewConfigurationFirst we need to add @nuxtjs/apollo to our modules key in nuxt.config.js and add a new apollo key configuring it:\n/ If it is dev we will use Hasura's admin key\nconst ApolloClientConfigDevelopment = {\n  httpEndpoint: process.env.APP_GRAPHQL_ENDPOINT,\n  httpLinkOptions: {\n    headers: {\n      [process.env.AUTH_SERVER_ADMIN_HEADER]: process.env.HASURA_GRAPHQL_ADMIN_SECRET,\n    },\n  },\n};\n\n// It is not the whole config, just the relevant keys\nexport default {\n  // Modules (https://go.nuxtjs.dev/config-modules)\n  modules: ['@nuxtjs/dotenv', '@nuxtjs/apollo'],\n  \n  // @nuxtjs/apollo configuration (https://github.com/nuxt-community/apollo-module)\n  apollo: {\n    clientConfigs: {\n      default: isProd\n        ? '~/graphql/apollo-client-config.js'\n        : { ...ApolloClientConfigDevelopment },\n    },\n  },\n}  As you can see, we have two different configurations, one for production and one for development, the referenced file for production is here:import { createHttpLink } from 'apollo-link-http';\nimport fetch from 'isomorphic-fetch';\nexport default function ApolloClientConfig(context) {\n  const {\n    env: { AUTH_SERVER_HEADER_KEY },\n    store: { state },\n  } = context;\n  return {\n    defaultHttpLink: false,\n    link: createHttpLink({\n      uri: process.env.APP_GRAPHQL_ENDPOINT,\n      fetch: (uri, options) => {\n        const email = state.user?.email || state._email;\n        options.headers[AUTH_SERVER_HEADER_KEY] = email;\n        return fetch(uri, options);\n      },\n    }),\n  };\n}The important thing to notice here is that in one strategy the token is set at build time and in the other at runtime, I associated that with production and development because Hasura provides an admin key and my opinion is that it should not be used in production, EVER, even if it is a back-end only service. As mentioned you can tweak a bit this configuration depending on your needs and personal preference, this configuration is needed because of the way Nuxt.js is structured for be used with server-side rendering and static sites, but the whole fetch process can be delegated to a vanilla script GraphQL service or library if needed.Queries and mutations with ApolloIf you have configured Nuxt.js to universal mode it will try to resolve request at build time, we need authentication to happen at run time, in the user\u2019s browser. For this we will use fetchPolicy: no-cache in our queries.We have two options to make our requests to the GraphQL API:Inside of componentsInside of storesOne of the things I like about Vue is that it is very flexible, even if it is opinionated it is often more flexible than React in terms of the recommended way to do things, Vue\u2019s state management allow easy access to all parts of the store by some convenient mappings, state, getters, actions and mutations. I prefer to have as much of the logic as possible in the store and have really clean and dumb components whenever it is possible, but here we will cover both ways of fetching GraphQL data.In components you can simply add an Apollo key to your component, it has some loading information, that we won\u2019t use here and at the end it will be available in the scope of the component with no further configuration:<template>\n  <div class=\"container\">\n    <shopping-list v-for=\"list in lists\" :list=\"list\" />\n  </div>\n</template>\n<script>\n// This is a js file that uses graphql-tag\nimport { GetAllLists } from '../graphql/lists';\n\n// Not the whole componet, you must import shopping-list, etc.\nexport default {\n  apollo: {\n    lists: {\n      query: GetAllLists,\n      fetchPolicy: 'no-cache',\n    },\n  },\n}\n</script>If you prefer to do it in the store the client is the same but the access is a bit different:// Only including relevant code\nexport const actions = {\n  async login({ commit }, email) {\n    if (!email) {\n      return;\n    }\n    commit('mailToValidate', email);\n    const { defaultClient } = this.app.apolloProvider;\n    try {\n      const { data } = await defaultClient.query({\n        query: GetUser,\n        variables: { email },\n        fetchPolicy: 'no-cache',\n      });\n      if (data.users.length === 0) {\n        throw new Error('No user found.');\n      }\n      commit('login', data.users[0]);\n      commit('setError', null);\n    } catch(error) {\n      commit('setError', error.message);\n      commit('logout');\n    } finally {\n      commit('mailToValidate', null);\n    }\n  }\n};\nWe need to use this.app.apolloProvider.defaultClient and manually take care of the result.The source code for the working application is here.AfterthoughtsHasura can be a great addition for managing existing databases, it is great for creating databases and easily iterating on them for early prototyping as well. It can be used to set some common grounds between the back-end and front-end team by allowing both teams to comfortably work on the database. It can be used as a gentle introduction to back-end for front-end developers, it is possible to gradually transition from Hasura\u2019s GUI to code and all database information is there accessible with any other tool. Finally, it can be used by developers and entrepreneurs to iterate quickly on back-end ideas and rest assured the work won\u2019t be wasted once you need a more robust solution, in fact, it can be just a step away from production, particularly if you use IaaS.Authentication is a tricky topic, not only from the point of view of security and auditability but from implementation and architecture as well, there is no solution that fits all cases and even using third party identity providers it can be cumbersome to integrate with your code base. I hope this article can work as a very light introduction and solve some technical common problems on the way. There are several strategies and ways around it, I think Hasura\u2019s approach is very clean and elegant, authorization is already built-in and handled for you, and it left the authentication out but provides a well defined interface for it.About the AuthorMigsar Navarro \u00a0is a full-stack javascript developer living in Porto, Portugal, with his girl and their baby daughter. He enjoys creating useful, performant and beautiful web applications, \u00a0He cares as much about developer experience as he does about user \u00a0interaction/experience. His mantra is: Code is meant to be read by \u00a0humans \u00a0but programs are meant to be used without knowledge of the code. He is \u00a0interested in software architecture, design patterns, algorithms, \u00a0documentation and communication. His background is in computer and \u00a0civil engineering. Life for he is about balance. He enjoys running, \u00a0cycling, swimming, landscape photography, reading and learning \u00a0languages."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Schema Migrations with Hasura Console Description: How to use the Hasura Console, a graphical interface, to automate the build out of migrations in our database, and by proxy our Hasura API Server.  Author: Adron Hall"}, {"role": "assistant", "content": "The first part of this series is available \u201cEvolutionary Database Design, GraphQL, APIs, and Database Schema Migrations\u201d. The second post \u201cAutomated Schema Migrations\u201d is available, and in this third post I\u2019ll conclude with the steps to expand our schema migrations using the Hasura Console, a graphical interface in which to automate the build out of migrations in our database, and by proxy our Hasura API Server. A prerequisite to follow along in this post is that you\u2019ve covered the material in the previous two posts, and have the base tables available after executing the first migration. Now we\u2019ll expand on those tables!The original schema we\u2019re working toward looks like this. At this time though, after our first migration has been implemented, we only have the tables themselves and not the relationships as drawn between those tables. In this post I\u2019ll show you how to add those using the console.First, if you don\u2019t have it open already, open the folder structure back up in your preferred editor. I\u2019ll be using Visual Studio Code for the following examples. The folder structure and existing migration, and if you open up the config.yaml file, it should look something like this.From here, to build the next migration open up the locally instantiated Hasura console by executing the following command.hasura consoleIf you\u2019ve got the publicly accessible console open and this newly launched console, they\u2019re indiscernible. However there is one quick way to determine which console is open by noting the URI that is open. The locally instantiated instance, launched by the hasura console command will have a URI of http://localhost:9695/ and the publicly accessible console is a URI that looks something like this https://zee-newest-project.hasura.app/console/. For creating migrations the locally instantiated instance of the console is what we\u2019ll need to use, since it will record all of the changes and mutations that we make.The Next MigrationWith the locally instantiated console open, click on the data tab.Within this tab, any of the changes we make will get put into versioned migrations. Within the data tab, click on the create button to create a new schema.Enter the name of the new schema and then click the create button again to create this new schema. In the upper left the new schema will be selected in the Schema drop down, which will confirm its creation and that any changes at this point will be made to that schema.Now navigate back to the editor, to review the migration that has now been added to the project folder. In the folder, again we have the standard up.sql and down.sql files, except this time the up and down steps have been populated with the generated SQL code necessary to execute those steps.Next up, add a new table via the console. In this example, I\u2019ve added a table with some columns, just to provide an example.When I click add table and then navigate back to my editor to review the migration, I\u2019ll find the following now in my project structure.Notice the new migration, up.sql and down.sql files as expected, and now the create table statement in the up.sql file for creating the table and columns in the table.At this point we have now stepped through some pivotal examples of how the graphical interface of the console can be used to create new migrations. As you step through and create relationships, new tables, and also remove or add changes to those tables, migrations will be created for each of the changes.Additive OnlyMany companies, and practices, often work toward enforcing an additive-only approach to schema migration changes in a database. That is, only additions can be made to tables and data is not mutated in any way during schema migrations. Relationships tend to be open for changes however. The reasoning covers a range of scenarios from ensuring the integrity of data in tables for audits, to the important matter of reducing conflicts in table structure when making changes. Whenever using migrations, this is an important practice to put into place, or to determine you want to put into place. If changes routinely mutate columns, column names, or otherwise a team will find themselves regularly running into conflicts moving back and forth between versions. Even worse, if you find yourself with a wrathful auditor because data has been deleted, the legal ramifications can grow rapidly. With that said, I\u2019d argue, it\u2019s a great idea to enforce additive-only changes for schema migrations and for your data in general.SummaryThis wraps up the series of articles around evolutionary database design and database schema migrations. From the evolutionary ideas presented in the first article, to the manual hand coded SQL of the second article, to this article showing how to use the graphical interface the console provides to create migrations we\u2019ve covered the idea, the practice, and the implementation of database schema migrations using Hasura. It\u2019s a great way to streamline and ensure integrity in your development practices, specifically around integration and cooperation between the database and application tier of development efforts. I hope this series of articles has been of use, and if you\u2019ve got any questions, comments, or suggestions feel free to reach out to us at Hasura and let\u2019s have a conversation!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura 1.4.0.alpha.1 Description: Announcing Hasura 1.4.0.apha1. 1.4 is  not a \u2018small release\u2019. In fact, the list of features is substantive and incredibly exciting. If you\u2019ve attended a recent Hasura Community Call you may well have seen recent demos of these capabilities. Author: Tyler Hannan"}, {"role": "assistant", "content": "It\u2019s not every day that a release as big as this one arrives.Before we get started, a note of caution:This software is released as an alpha. Please keep it away from production. We release to ensure your feedback, bug reports, perspective, etc. are included (as possible) in the final release. Test frequently...but be cautious.As we march towards GA, we will share more the thought-process behind this release, what went into the changes, why it has been a complex -- and lengthy process -- as well as what it sets up for the future. Both for you, the user, and for Hasura, the community (and company).1.4 is \u00a0not a \u2018small release\u2019. In fact, the list of features is substantive and incredibly exciting. If you\u2019ve attended a recent Hasura Community Call you may well have seen recent demos of these capabilities. A, non-exhaustive, list includes:Multiple Postgres Sources - One Hasura, many Postgres. Bringing multiple Postgres databases into one Hasura instance. One Postgres, many Hasura. Multiple versions of Hasura on a single Postgres database (think about advanced versioning, change-management, etc.) Frequently requested...frequently desired...now it\u2019s here.Metadata Separation - Of course the above requires rethinking metadata. Now you can not only achieve the above...but you can have Hasura connect to read-only Postgres. And, importantly, this sets the stage for multiple database support in a scalable way.REST Endpoint Support - Idiomatic REST from GraphQL?? Your saved queries and mutations can be made available (along with a URL template) to expose all of the benefits of GraphQL rapid development to the tooling and ecosystem of REST.Volatile functions as mutations - Writing custom business logic is great. But how about exposing SQL functions that can perform custom insert operations inside Postgres? Now Postgres volatile functions can be used in Hasura to write custom inserts, updates, deletes, etc.Remote schema permissions - Bringing in data from external data sources opens up myriads of use-cases and we don't want to limit them. Expose only a subset of your remote schema using the Hasura authorisation layer...the power of permissions extended to remote schemas.Heterogeneous execution - Multiple data sources, a single query. It used to be that you could query only from Postgres or remote schemas...but not both. Now you can have your proverbial cake and include a query from a remote server at the same time.And many, many more features both large and small!You can get started with the alpha release on Github. Participate in our Github Discussions or join the Hasura Community Discord or Github Discussions to provide feedback!Your perspective matters. Let us know your experience."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build an e-commerce shopping cart reminder with Hasura Events Description: In this tutorial we will discover how to use Hasura Events to store a user preference/wish-list and to send a triggered email from this event.\nWe will also cover the creation of the React app connecting to an Hasura Cloud instance to make it work fully! Author: Harsha"}, {"role": "assistant", "content": "Use Hasura to setup event triggers to call webhooks whenever specific events occur in your DB in less than 2 minutes!This article was written by Riccardo Giorato as part of the Hasura Technical Writer Program. If you would like to publish an article about Hasura or GraphQL on our blog, apply here.In this tutorial we will discover how to use Hasura Events to store a user preference/wish-list and to send a triggered email from this event.We will also cover the creation of the React app connecting to an Hasura Cloud instance to make it work fully!You can try the application on your own here: https://hasura-shopping-reminder.vercel.app/Link to complete application on Github: https://github.com/riccardogiorato/hasura-shopping-reminderhttps://hasura-shopping-reminder.vercel.app/1 \u2014 Setup your Hasura account with Hasura CloudThe first step will be to configure your Hasura server, my preferred choice is using Hasura Cloud or any other cloud hosting provider like Heroku or AWS. I usually prefer to use Hasura Cloud because:It \u00a0has a simple and minimal dashboard to manage your instance, not \u00a0comparable to the complexity of AWS or other hosting providers UI.You get 1 GB/month of data pass through for free.Finally up to 60 requests/minute to power your side/hobby project.2 \u2014 Make Hasura endpoint available to \u201cpublic_user\u201d role!We want \u00a0our wish-list function to work easily without asking any other info to \u00a0the user other than their email. To do so we just need to add a \u00a0environment variable to our Hasura server:add HASURA_GRAPHQL_UNAUTHORIZED_ROLE key value public_user3 \u2014 Create the users_wishlist tableNow we just need to go to the \u201cDATA\u201d section and create the \u201cusers_wishlist\u201d table directly from the Hasura with these fields:email, Textplant and suggestion fields as Textid as UUID and gen_random_uuid() as default valueThe primary key will be the \u201cid\u201d field!4\u2014 Set the permission for the \u201cpublic_user\u201d roleSwitch to the \u201cPermissions\u201d tab and add a new role row:Insert, \u00a0without any checks, Insert permissions for \u201cemail,plant,suggestion\u201d but \u00a0not the \u201cid\u201d field, which will be automatically generated.5 \u2014Configure the React app to connect to Hasura!If you want you can directly download or clone the sample react project from here: https://github.com/riccardogiorato/hasura-shopping-reminderThe main requirement is adding Apollo Client or any other GraphQL client on your client side.To install Apollo Client and configure with Hasura you can follow this React Apollo TutorialWe configured it as it follows: https://github.com/riccardogiorato/hasura-shopping-reminder/blob/main/src/ApolloApp.js6\u2014 Launch the Mutation and save the user Preferences!Inside our main application code we then add the mutation query and configure the Apollo Mutation hook: https://github.com/riccardogiorato/hasura-shopping-reminder/blob/main/src/App.js7 \u2014Here it comes the Hasura Events!Now we can jump back to our Hasura instance to the \u201cEVENTS\u201d tab.We \u00a0will want to add a new trigger called \u201csend_wishlists\u201d linked to the \u00a0shcema \u201cpublic\u201d and the table \u201cusers_wishlist\u201d on the \u201cInsert\u201d trigger.We will need to pass a Webhook URL that will be invoked from our Hasura server when this event happens.Here we passed our custom webook of a Serverless function running on \u00a0Vercel cloud hosting provider but you could choose any server \u00a0environment you prefer. If you want you could pick some examples of \u00a0other providers supported from the Hasura team from the \u201cserverless \u00a0functions example\u201d here.8 \u2014Triggering Serverless functions via Hasura Events!The last step for our tutorial will be to create this serverless function to send the \u201cemail reminder\u201d to the user.But first we need to understand what the request from Hasura will look like.As \u00a0we can see in the body of the request, Hasura will pass various fields \u00a0like the event, user role, type of operation, data, all contained in the \u00a0\u201cpayload\u201d.From the payload we will use only the \u201cevent\u201d object containing both the previous and current values from the modified row.The \u00a0\u201cold/previous\u201d and \u201cnew/current\u201d won\u2019t always present, considering the \u00a0type of the operation, in this case we are getting a newly created row \u00a0so we won\u2019t have the \u201cold\u201d values that are null but oonly the \u201cnew\u201d values.To send the email we choose an external provider in this case Emailjet but you could pick any other provider of choice.The entire function can be found here: https://github.com/riccardogiorato/hasura-shopping-reminder/blob/main/api/sendemail.jsThe main line we are interested in is the line 10, \u00a0we unwrap the data from the request body and we pick all the object \u00a0with all the fields from the Hasura table row. Any other field on this \u00a0newly inserted record would be found here.And voil\u00e0! Our application is done! If we go back in our React app and \u00a0complete the insert process we will receive an email that will look like \u00a0this:the email you will get with your wish-listed plantYou can get a similar email too here: https://hasura-shopping-reminder.vercel.app/(if \u00a0you don\u2019t see any email after you complete the process you might find \u00a0it in the spam folder cause we are using the free tier of Emailjet)9\u2014 In summaryUsing \u00a0Hasura events we didn\u2019t need to develop any custom listener to events \u00a0or build our own trigger detector connected to our PostgreSQL database.Hasura \u00a0can simply hide all this complexity for us to let us create our \u00a0functions and logical features without worrying about the basic things \u00a0to make our infrastructure work.10 \u2014 TLDRLess time spent building custom triggers or watchers!More time spent on the core of our app or ideas thanks to Hasura Events.About the AuthorRiccardo Giorato is a React/JAMStack Developer. In his free time he is a Maker and a Content creator. You can read more about Riccardo here."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Automated Schema Migrations Description: How to create automated schema migrations using the Hasura CLI. Author: Adron Hall"}, {"role": "assistant", "content": "The first part of this series is available \u201cEvolutionary Database Design, GraphQL, APIs, and Database Schema Migrations\u201d and the second is also published now too under \"Schema Migrations with Hasura Console\".There are two primary paths to follow: the automated hand crafted way, as I\u2019ll detail in this blog post and the graphical way that uses the Hasura Console interface that will come in a subsequent post. To start with, both methods start with the Hasura CLI tool. Check out the documentation, and for some extra information I wrote a post Hasura CLI Installation & Notes. Once that prerequisite is taken care of, we can start on the data model schema and what we want to start building.WhyAs discussed in the first post of this series \u201cEvolutionary Database Design, GraphQL, APIs, and Database Schema Migrations: Part 1\u201d I outline why we want to set a good practice around schema migrations. To get into a real world example of setup and usage I\u2019ve created the following database schema.In this schema there are a number of tables and relationships between those tables. One-to-many, many-to-many, and even some recursive (elephant ear) relationships.Inception MigrationFirst, we\u2019ll need to get the basic tables created and then we\u2019ll add the relationships. At least, that will be the easiest way. For this first schema migration I\u2019ve put together the SQL needed to create these tables. We\u2019ll add the relationships and additional elements after the tables are created. This will make up our first migration.CREATE TABLE \"Source\" (\n  \"Id\" uuid PRIMARY KEY,\n  \"Stamp\" timestamp,\n  \"Name\" text,\n  \"Uri\" text,\n  \"Details\" text\n);\n \nCREATE TABLE \"SourceNotes\" (\n  \"SourceId\" uuid,\n  \"NotesId\" uuid,\n  \"Details\" text,\n  \"Stamp\" timestamp\n);\n \nCREATE TABLE \"NoteJot\" (\n  \"Id\" uuid PRIMARY KEY,\n  \"Stamp\" timestamp,\n  \"NoteId\" uuid,\n  \"Details\" text\n);\n \nCREATE TABLE \"Activity\" (\n  \"Id\" uuid PRIMARY KEY,\n  \"Stamp\" timestamp,\n  \"Activity\" json\n);\n \nCREATE TABLE \"Connection\" (\n  \"Id\" uuid PRIMARY KEY,\n  \"Stamp\" timestamp,\n  \"ActivityId\" uuid,\n  \"SourceId\" uuid\n);\n \nCREATE TABLE \"Formatter\" (\n  \"Id\" uuid PRIMARY KEY,\n  \"Stamp\" timestamp,\n  \"ConnectionId\" uuid,\n  \"FormatterMap\" json\n);\n \nCREATE TABLE \"Schema\" (\n  \"Id\" uuid PRIMARY KEY,\n  \"Stamp\" timestamp,\n  \"ConnectionId\" uuid,\n  \"SchemaMap\" json\n);\nAn important detail to note here, that this inception migration breaks a few things that moving forward we won\u2019t want to do. Here are a few of the issues with this first migration;The migration has many parts to it. Generally a goal is to keep each migration to a singular step such as a creation of a table, an added column, or related addition.When creating new tables after this inception migration, we\u2019d want to create the migration so that any table creation, or column addition, would have their respective relationships added with them in that particular step.There is no semblance of ordering of the tables, that in subsequent steps we need to pay close attention to, as a table that has relationships needs to have each table exist before any relationships can be drawn to it, and respectively, drops of tables or other deletions need to be done in the particular reverse order of their creation.But the reason, since this is an inception schema migration, is to quickly get that initial table schema added and then start making more iterative changes after these core tables exist.Getting StartedBefore creating the migration we\u2019ll need to initialize and point our configuration file to our respective Hasura API and database. For this example, I\u2019ll just execute the initialization command without any switches, and accept the default options.hasura initTo create our first schema migration, use the Hasura CLI command. Navigate into the hasura directory just created and execute the following.hasura migrate create \"inception\"The result will return, displaying the name of the migration and the version.Now open up this folder, ideally with something like Visual Studio Code or your preferred editor of choice. In the following image I\u2019ve opened up the folder in Visual Studio Code and opened the up.sql file for the migration we just created.At this point the file is empty, just like the down.sql file. What are these up.sql and down.sql files? Let\u2019s discuss.Migrations Go Up & DownIn every migration, there is a path forward to bring the database schema up to the latest version, and a path backwards to take the database schema back to the previous version. Every single migration requires these two iterative steps. From the Hasura CLI usage, that involves specifically two commands to move iteratively back and forth.To go forward, or execute the migration that we\u2019re about to create we would execute the following command.hasura migrate applyThis executes all up migrations and brings the database schema up to the latest version. If we needed to rollback these changes, and move to the previous schema migration version the following command would execute the down migration one version back.hasura migrate apply --down 1The Up MigrationOk, so these are the steps we\u2019ll need to take, and the steps we\u2019ll need to implement in SQL in order to finish the first migration. \u00a0The above SQL, copy that into the up.sql file.We could execute the hasura migrate apply command right now, but instead let\u2019s build out the drop commands for the down.sql first. That way, if any errors occur we would be able to roll back immediately. Even though that isn\u2019t super important in this first step, it\u2019s a good practice to always build out both the up and down steps before executing the migration.Since none of these tables are created with relationships at this point, the drop commands can be in any order. If we had drawn any relationships, these relationships would require the drop command list the tables drops in the order in which the relationships could be removed. Usually running schema migrations we wouldn\u2019t run into this problem, as the migration itself would be a small enough unit of work that we wouldn\u2019t run into complexity issues. Only the relationship, or only the table would be being created at any particular point in time.The Down MigrationThe drop commands to drop each of the tables is as follows.DROP TABLE \"Source\";DROP TABLE \"SourceNotes\";DROP TABLE \"NoteJot\" ;DROP TABLE \"Activity\";DROP TABLE \"Connection\";DROP TABLE \"Formatter\";DROP TABLE \"Schema\";Add each of those to the down.sql file.Point & ExecuteThe next step would be to execute the migration against the Hasura instance. To get your own instance and try this out yourself, I\u2019ve got a quick 5 minute and 37 second video on setting up a free tier Hasura instance. I\u2019ll presume at this point you\u2019ve got your own instance now. In order to execute the migration now the URI for the instance is needed. Navigate to the console of the instance and get the URI as shown in this image.Now open up the config.yaml file that is located at the root of our project folder structure, and change out the localhost URI and add our new URI path. Now remove the path on the URI, ` /v1/graphql` so that we only have the base URI in the config.yaml file. It\u2019ll look something like this.Save that, and then execute the migration with the Hasura CLI.hasura migrate applyNavigating over to the console, click on the data tab, and we will now see the following tables.Now if we wanted to roll back, issue the following command.hasura migrate apply --down 1 to migrate back one step. Once that is done if we take a look back at the data tab again, click refresh on the browser to ensure it\u2019s updated, and the tables will have been dropped.Go ahead and apply those changes, and we\u2019ll use this as a base for the next part of this series. In part 3, we\u2019ll get into how to use the console itself to build out our migrations. We\u2019ll add some relationships between tables, and other schema changes, covering what is and is not done through the migration process and touch on how the metadata pairs with the Hasura migrations to provide full iterative versioning and movement back and forth between changes in the database and changes on the Hasura API server itself.SummaryWith these steps done, the first migration is now completed. A recap of the steps covered here, even if at first they seem simple, open up a wide expanse of what can be done from here. At the start I provided a schema to implement. Next the SQL that would need to be executed to create each of the tables in that schema. Finally, I covered the creation of our schema migrations project folder structure using hasura init, and the creation of the first migration folder and up.sql and down.sql files. From there, we worked together to add the SQL needed for the up and down migration steps, repointed the config.yaml URI to our respective Hasura API servers, and executed our first up migration. We then wrapped up with a down migration just to compare before and after, and then ended with an up migration, leaving us prepared for the next post!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build JAMStack Sites With JungleJS and GraphQL Description: We will look at how to build JAMStack sites with JungleJs (Svelte) and Hasura GraphQL APIs and deploy it to Netlify. Author: Hasura"}, {"role": "assistant", "content": "This article was written by Oluwaseun Raphael Afolayan as part of the Hasura Technical Writer Program. If you would like to publish an article about Hasura or GraphQL on our blog, apply here.IntroductionJAMStack, as you may already know, is an approach to building websites using Javascript, API, and Markup. In this article, we'll take a look at JungleJs and how it helps you build fast and light-weight static sites using Svelte and GraphQL. By the end of this article, you should be familiar with JungleJS enough to understand what makes it super awesome. Check out earlier posts from the JAMStack series.What is Svelte?Suppose you're not familiar with it already, Svelte is a front-end javascript framework similar to React and Vue but with a crucial difference: compiling your code into javascript at build time. This difference implies that your app works super fast as it does not have to interpret your code at runtime.Read more: Build and Deploy Realtime Svelte 3 Apps using GraphQLHow JungleJs worksJungleJs is a static site generator similar to Gatsby or Hugo. JungleJs works by pulling data from a source such as a JSON or CSV file, any headless CMS, APIs such as WordPress, Ghost, etc. When working with JungleJS, you structure the different parts of your application using svelte components. In turn, Jungle builds the site and compiles it to static HTML content, ready for deployment on any web server of your choice.GraphQL Data LayerBefore we go into the GraphQL Data layer and how it works, let\u2019s take a brief look at GraphQL itself. According to it\u2019s official documentation:GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools. Learn more about GraphQL.The GraphQL data layer is a tool JungleJs utilizes in development, and it serves as temporary storage for data imported into your JungleJS app. When data gets imported into our Jungle app, the GraphQL Data Layer converts it to static data. It is worthy of note that there is no real-time connection between the data layer and the source from where the data is imported. What this implies is that you need to re-generate the website to get the latest data updates.A lazy sketch of how jungleJs uses the GraphQL Data layerSince JungleJs works at build time, you only need to add a QUERY and a QUERYRES to access data in your svelte files, as shown in the example below.<script>\nconst QUERY = `\n        query {\n                items {\n                        property\n                        another_property\n                }\n        }\n`;\nconst QUERYRES = {};\n...\n</script>Then you can access QUERYRES in the component like this:{#each QUERYRES.items as item}\n        <p> {item.property} </p>\n{/each}Installing JungleJSTo create a new JungleJS project, first, clone the starter project using degit and navigate into the directory where the files were downloaded by running:npx degit junglejs/template jungle-app\ncd jungle-app Secondly, install the required dependencies by runningnpm install Running the Jungle AppTo run the Jungle App, start the app using npm run startAt the time of writing this article, hot-reloading doesn't work in JungleJS yet. To reload the app with your changes, you need to stop and run the process once more. If you want to generate a build folder containing the compiled static files for deployment anywhere, you can do so by running npm run buildJungleJS app directory structureIf you're familiar with frameworks like Sapper, Nuxt, and the likes, their folder structure is pretty similar to that of JungleJs. However, here are some things to keep in mind, especially when working with JungleJs for the first time.If you look in the installed starter files' root directory, you will find a file named jungle.config.js. This file is where configurations for your app are defined. This file contains a dataSources array which specifies the sources Jungle\u2019s GraphQL Data Layer should pull its data. These data sources could be from a dynamic JSON source, a static object, or even markdown files. The default starter template currently contains two example data sources.The /jungle/build/ folder generated when you run either npm run start or \u00a0npm run build is the directory to be deployed when hosting a JungleJs project.You should store static files such as images in the /static folder. When \u00a0NPM generates the app's build folder, JungleJS will insert the content of our app's static folder into the build folder so the rest of the file can use them.Components for use in multiple routes should be stored in the src/components directory.The src/routes directory holds the routes for our application.ComponentsComponents like in any frontend development framework allows you break your app in to smaller parts so they can be managed. A typical web app for example can be broken down into components such as the Navbar, footer etc. The default Jungle installation already presents us with a Nav component, so in order to have a better feel of this, let\u2019s create a footer component for our web app. To get started, create a Footer.svelte file in the src/components \u00a0folder and add the following code in it:<div class=\"footer_area\">\n    <p>All rights reserved, Example developer &copy 2021</p>\n</div>Once the markup is written, your component is ready for use in any section of your app in this format:<script> \n// First import the component in your script tag\n    import Footer from \"../components/Footer.svelte\";\n<script> \n\n<div>\n// inject it anywhere suitable\n<Footer/>\n</div>Creating pages and Linking RoutesTo setup a page in a jungle app, simply create the [pagename].svelte file in the src/routes directory. Next, write the markup as you would in a normal svelte file. In order to create an about page in the default jungle starter code for example, create a file named about.svelte and add the content from your favorite text editor (eg, vs code)//about.svelte\n<script>\n    import Nav from \"../components/Nav.svelte\";\n// importing the Footer component we created earlier\n    import Footer from \"../components/Footer.svelte\";\n</script>\n<svelte:head>\n    <title>About Page</title>\n</svelte:head>\n<Nav page=\"/about\"/>\n<main>\n    <h1>This is the About Page</h1>\n    <p> JungleJS is Easy to use!</p>\n</main>\n<Footer/>You can proceed to add the About route to the Nav component as such//src/components/Nav.svelte\n...\n <a class={page == '/about/' ? 'active' : ''} href=\"/about/\">About</a>\n...StylingJungleJS inherits its styling syntax from svelte, and as such, you can write CSS files in your svelte files using <style> </style> tags. One thing i find so handy about Jungle is that styles defined in Svelte components are scoped by default. When compiled, Svelte generates unique class identifiers to ensure that styles for elements do not conflict with each other. you can learn more about styling in Svelte here. \u00a0To see this in action, let\u2019s add a bit of styling to the Footer component we created earlier.// src/components/Footer.svelte\n   ...\n<style>\n    .footer_area {\n        position: fixed;\n        left: 0;\n        bottom: 0;\n        width: 100%;\n        background-color: #4caf50;\n        color: white;\n        text-align: center;\n    }\n</style>\n     ...DeploymentOnce you run the npm run build command, all you need to do is deploy the resulting jungle/build folder in an hosting service of your choice. If, for example you want to deploy your site to Netlify, you can head over to your netlify dashboard and then drag/drop the folder unto your dashboard for a quick manual deploy. If on the other hand, you like to do things a bit more organised, you could setup your jungle app in a github repository and hit the \u201cNew site from git\u201d button.Next, Select the repo containing your jungle site and click on it to continue.Once you have selected your preferred repo, make sure to modify your publish directory to reflect the the jungle/build folder like in the screenshot below, before you finally hit the deploy button.SummarySo far, we have covered the steps involved in building a simple site using JungleJS and GraphQL.We got started by setting up a JungleJS scaffolding, next we added an about page to our site, we then created a footer component while how to style a Jungle app. Finally, we took a look at the deployment process involved in a JungleJs app. If you have any questions, feel free to reach out to me on Twitter.About the AuthorOluwaseun Raphael Afolayan is an established software craftsman and technical author from Nigeria."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: TimescaleDB 2.0 with Hasura Part 2 - Multi-Node Description: Multi-node is the most interesting feature of version TimescaleDB 2.0 that provides the ability to create a cluster of TimescaleDB instances to scale both reads and writes. \n Author: Toan Nguyen"}, {"role": "assistant", "content": "Our #GraphQLJanuary continues with blog posts, live streams, Discord Q&A, office hours, and more. For a schedule of upcoming events, join the community or register at https://hasura.io/graphql/graphql-january/.In this post, Hasura engineer Toan shares his experience with using Hasura alongside TimescaleDB 2.0 and exploring what works...and what doesn\u2019t. Learning from the experience of others exploring workarounds to achieve their goals is always instructive.This post is part of a 3-part series exploring TimescaleDB with Hasura.Multi-node is the most interesting feature of version TimescaleDB 2.0 that provides the ability to create a cluster of TimescaleDB instances to scale both reads and writes. A multi-node TimescaleDB cluster consists of:One access node to handle ingest, data routing and act as an entry point for user access;One or more data nodes to store and organize distributed data.In this post, I will try setting up this Multi-node feature and running it with Hasura.The example code is uploaded on GithubInfrastructure setupTimescaleDB Multi-node diagramCluster requirements:Network infrastructure is ready. It is necessary for the access node to connect to data nodes when adding them.Authentication between nodes. The access node distributes requests to data nodes through the client library, so it still needs authentication credentials.There are 3 authentication mechanisms:Trust authenticationPassword authenticationCertificate authenticationIn this demo, we will use the first mechanism. You may think that this approach is insecure. I don't disagree. But it is fine for the purpose of this exploration. And we will place the TimescaleDB cluster into a private cloud, and expose only the access node to the internet. The Docker environment can emulate it.You can read more detail about authentication in official TimescaleDB docs.First of all, we need to register data nodes to the access node.SELECT add_data_node('data1', host => 'timescaledb-data1');\nhost can be IP or DNS name. In docker, you can use alias name.Note: this function requires running in AUTOCOMMIT mode. So you can only run it with psql, or custom migration CLI. However, I don't recommend using migration, because Multi-node cluster configuration is different if you run multiple development environments (dev, staging, production...).You can add nodes manually with psql. Fortunately, the Postgres docker image supports initialization hooks /docker-entrypoint-initdb.d. You can use a bash script to add them automatically.# multinode/scripts/add-data-nodes.sh\npsql -v ON_ERROR_STOP=1 -U \"$POSTGRES_USER\" -d \"$POSTGRES_DB\" <<-EOSQL\n  SELECT add_data_node('$node', host => '$node');\nEOSQL\nCan we register reversely from data node to access node?It is possible. However you have to customize Postgres image. The temporary Postgres server for initialization scripts isn't exposed to external network (source). Therefore we can't run remote psql to register a node.Other solutions are:Create an initialization container that waits for all Postgres servers to be online, then run psql script.Use Patroni with postInit hooks.Create Distributed hypertableAs a next step, we need to create a distributed hypertable instead of normal, non-distributed hypertable. The difference of Distributed hypertable is that the data is stored across data node instances. The access node stores metadata of data nodes and distributes the requests and queries appropriately to those nodes, then aggregates the results received from them. Non-distributed \u00a0hypertable still stores data locally in the current server.The function is similar to hypertable. We run it after creating tableCREATE TABLE conditions (\n  time        TIMESTAMPTZ       NOT NULL DEFAULT NOW(),\n  location    TEXT              NOT NULL,\n  temperature DOUBLE PRECISION  NULL,\n  humidity    DOUBLE PRECISION  NULL\n);\n\nSELECT create_distributed_hypertable('conditions', 'time');\nHere we encounter another issue.Create Distribution Hypertable ErrorPrepared transactions are disabled by default. timescaledb-tune doesn't automatically enable this setting so we have to set it manually.max_prepared_transactions = 150\nenable_partitionwise_aggregate = on\nYou can mount a manual postgres.conf file in Docker container. However, it can't take advantage of timescaledb-tune. Fortunately the Postgres image also supports config flags. These can be set as arguments.timescaledb-data1:\n  image: timescale/timescaledb:2.0.0-pg12\n  command:\n    - \"-cmax_prepared_transactions=150\"\n  # ...\nRestart services and run the function again. You can verify by querying hypertable information:select \n  hypertable_name, \n  num_chunks, \n  is_distributed, \n  replication_factor, \n  data nodes \nfrom timescaledb_information.hypertables;\n\n\n\nhypertable_name\nnum_chunks\nis_distributed\nreplication_factor\ndata_nodes\n\n\n\n\nconditions\n2\nf\n1\n{timescaledb-data1,timescaledb-data2}\n\n\n\nYou may notice the is_distributed column that represents whether this hypertable is distributed or not. On data nodes, this column is false or non-distributed. That means if the hypertable is detached, it can work as a normal hypertable.Work with dataIn this test, I will run SQL script to insert 1 million rows. Let's see how long the execution takes and the number of rows per data node.INSERT INTO conditions\n    SELECT time, 'US', (random()*30)::int, random()*80\n    FROM generate_series(\n      '2020-01-01 00:00:00'::timestamptz, \n      '2020-01-01 00:00:00'::timestamptz + INTERVAL '999999 seconds', \n      '1 second')\n    AS time;\n\nPlanning Time: 1.297 ms\nExecution Time: 11763.459 ms\n\n\n\naccess\ntimescaledb-data1\ntimescaledb-data2\n\n\n\n\n1,000,000\n395,200\n604,800\n\n\n\nThe distribution ratio isn't usually 1:1. The variance is larger when we insert large number of concurrent requests. The insert speed is very fast. It takes about 11 seconds to insert 1 million rows. In contrast, UPDATE and DELETE operations take a very long time. The below table show performance comparison between non-distributed hypertable and distributed hypertable, in milliseconds.\n\n\nOperation\nHypertable\nHypertable (JIT)\nDistributed hypertable\nDistributed hypertable (JIT)\n\n\n\n\nINSERT\n3,707.195\n4,586.120\n12,700.597\n11,763.459\n\n\nUPDATE\n4,353.752\n4,404.471\n184,707.671\n204,340.519\n\n\nSELECT\n174.012\n210.479\n2,011.194\n1,057.094\n\n\nDELETE\n737.954\n883.095\n159,165.419\n184,945.111\n\n\n\nDistributed hypertable performance is much worse than non-distributed one. In my opinion, the access node has to distribute requests to data nodes through the network. This causes higher latency cost than inserting directly into disk. Moreover, the access node takes more computing power to query from all data nodes, then aggregate the final result.UPDATE and DELETE operations are extremely slow. We should avoid modifying data.JIT mode isn't really boosting performance. From the above table, the performance is slightly improved on Distributed hypertable, but slower on non-distributed one.However, the main purpose of distributed hypertable is handling more write requests. The slower performance is an expected trade-off.Foreign key and RelationshipFor example, we create a table locations. conditions table reference locations through location column.CREATE TABLE \"public\".\"locations\"(\n  \"id\" Text NOT NULL, \n  \"description\" text NOT NULL, \n  PRIMARY KEY (\"id\") \n);\n\nALTER TABLE conditions ADD CONSTRAINT conditions_locations_fk \n  FOREIGN KEY (location) REFERENCES locations (id);\n-- ERROR:  [timescaledb-data1]: relation \"locations\" does not exist\nThe foreign key can't be created. Because locations table is available in the access node only. Data nodes don't know about it.However, JOIN query is working on the access node. TimescaleDB is smart enough to map relational data.SELECT * FROM conditions JOIN locations ON conditions.location = locations.id;\n\n\ntime\nlocation\ntemperature\nhumidity\nid\ndescription\n\n\n\n\n2020-12-31 16:32:42.328627+00\nUS\n10\n1\nUS\nUnited State\n\n\n2020-12-31 16:34:01.648958+00\nUS\n12\n1\nUS\nUnited State\n\n\n2020-12-31 16:34:35.241304+00\nUS\n13\n13\nUS\nUnited State\n\n\n\nContinuous Aggregate ViewWe can't create a Continuous Materialized view (see Caveats below).postgres= CREATE MATERIALIZED VIEW conditions_summary_minutely\npostgres-     WITH (timescaledb.continuous) AS\npostgres-     SELECT time_bucket(INTERVAL '1 minute', time) AS bucket,\npostgres-            AVG(temperature),\npostgres-            MAX(temperature),\npostgres-            MIN(temperature)\npostgres-     FROM conditions\npostgres-     GROUP BY bucket;\nERROR:  continuous aggregates not supported on distributed hypertables\nFortunately we still can create View or Materialized view. Therefore we can do a workaround with View or the combination of Materialized view and a scheduled job. The downside is a \u00a0performance hit compared to the continuous view.CREATE MATERIALIZED VIEW conditions_summary_hourly  AS\n    SELECT time_bucket(INTERVAL '1h', time) AS bucket,\n           AVG(temperature),\n           MAX(temperature),\n           MIN(temperature)\n    FROM conditions\n    GROUP BY bucket;\n    \nCREATE OR REPLACE PROCEDURE conditions_summary_hourly_refresh(job_id int, config jsonb) LANGUAGE PLPGSQL AS\n$$\nBEGIN\n  REFRESH MATERIALIZED VIEW conditions_summary_hourly;     \nEND\n$$;\n\nSELECT add_job('conditions_summary_hourly_refresh','1h');\nSQL functionCustom SQL functions for GraphQL queries work well. These functions are created on the \u00a0Access node only. However, the query engine is smart enough to get data from data nodes.CREATE OR REPLACE FUNCTION search_conditions(location text) \n    RETURNS SETOF conditions \n    LANGUAGE sql STABLE\nAS $$\n    SELECT * FROM conditions \n        WHERE conditions.location = location;\n$$\nHowever, Triggers don\u2019t appear to work. The access node doesn't automatically create SQL functions and Triggers to data nodes. You have to create them on each data node.There are more consideration in the Caveats section.Run with Hasura GraphQL EngineIt isn't different from running with Postgres. GraphQL Engine service only needs connecting to the access node's database URL. There are still issues that are similar to non-distributed hypertable. View and SQL query function works as expected.Hasura GraphQL Engine Console dataHasura GraphQL Engine Console QueryHowever, because Foreign keys doesn't work on a distributed hypertable, GraphQL Engine can't automatically suggest relationships. It is necessary to define them manually.Manual RelationshipRelationship QueryCaveatsBeside non-distribution hypertable caveats, distributed hypertable has more considerations:Besides hypertable, other features aren't distributed: background jobs, Continuous aggregate view, compression policies, reordering chunks...Joins on data nodes are not supported.Consistent database structure required between access and data nodes.If you create SQL functions, foreign keys,... you have to create them in all data nodes. The access node can't automatically do it for you.Native replication is still in experiment.The root cause is in the access node that can't manage consistent metadata with data nodes yet. It can't automatically sync database structure to data node except distributed hypertables. However, TimescaleDB 2.0 is still in early stage. The caveats still have chance to be solved in next versions.You can read more detail in TimescaleDB docsSo, should I use it?Yes, if you really need to scale write performance, and don't use Continuous aggregate view, compression policies features, at least in near future. In theory, you still can create them manually in each data node, since these hypertables in data nodes are normal one. However this workaround could lead to inconsistent database structures between nodes.The access node is still a TimescaleDB server. You still can create tables and local hypertables and use them along with distributed hypertables.In the next, and final, part of this series I will discuss High Availability in greater detail. Thanks to the TimescaleDB team for their work and advice."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Metadata SDK Description: The Hasura Metadata SDK is a collection of type definitions for Hasura metadata in various programming languages. Users can also generate definitions in other languages if their preferred language is not readily available. Author: Gavin Ray"}, {"role": "assistant", "content": "Our #GraphQLJanuary continues with blog posts, live streams, Discord Q&A, office hours, and more. For a schedule of upcoming events, join the community or register at https://hasura.io/graphql/graphql-january/.What is it?The Hasura Metadata SDK is a collection of type definitions for Hasura metadata in various programming languages. Users can also generate definitions in other languages if their preferred language is not readily available.These pre-generated SDK's included:TypeScript (also published on NPM, as @hasura/metadata-types)Python 3.7 Dataclasses + MyPyGolangHaskellWhy was it built, what problems does it solve?Hasura's workflow is flexible. Some users prefer to interact with & develop their project through our web interface. Other users prefer to stick to purely text-based flows, or integrate with the existing editor tooling they have configured.We saw two areas where we felt we could better serve the needs of the community and enhance our user experience by augmenting the tooling around metadata:Providing our users with a platform to build more complex tooling, and programmatic workflows, around interacting with metadataProviding our users with enhanced integration to maintain flow-state while working in their editor environments on their Hasura projectsTo the first point, we felt that publishing fully type-safe and documented SDK's would allow users to more confidently build the tooling they needed, while feeling comfortable they weren't making mistakes.Type-safe and documented SDK'sTo the second point, we realized that users in most modern editors would be able to take advantage of JSON/YAML Schema integrations, so that when working on, or referencing metadata files, there is intellisense, type-checking, and doc-comments on hover.JSON/YAML schema integrationsHow might I use this?Below is an example to demonstrate the common use cases you may encounter when wanting to script your interactions with metadata. It includes:Loading tables.yaml, and actions.yaml filesAdding a new tableCreating a JSON and text diff of tables.yaml, and writing it to a diffs folderRepeating the above process for metadata.json (could be metadata.yaml as well)import { Convert } from \"./customMetadataConverter\"\nimport {\n  TableEntry,\n  Action,\n  CustomTypes,\n  HasuraMetadataV2,\n} from \"../generated/HasuraMetadataV2\"\n\n// Read \"tables.yaml\" file as text from filesystem\nconst tablesMetadataFile = fs.readFileSync(\"./metadata/tables.yaml\", \"utf8\")\n// Convert it to JSON object with type annotation using loadYAML utility\nconst tablesMetadata: TableEntry[] = Convert.loadYAML(tablesMetadataFile)\ntablesMetadata.forEach(console.log)\n\n// Read \"actions.yaml\" file as text from filesystem\nconst actionMetadataFile = fs.readFileSync(\"./metadata/actions.yaml\", \"utf8\")\n// Convert it to JSON object with type annotation using loadYAML utility\nconst actionMetadata: {\n  actions: Action[]\n  custom_types: CustomTypes\n} = Convert.loadYAML(actionMetadataFile)\nactionMetadata.actions.forEach(console.log)\nconsole.log(actionMetadata.custom_types)\n\n// Make a new table object\nconst newTable: TableEntry = {\n  table: { schema: \"public\", name: \"user\" },\n  select_permissions: [\n    {\n      role: \"user\",\n      permission: {\n        limit: 100,\n        allow_aggregations: false,\n        columns: [\"id\", \"name\", \"etc\"],\n        computed_fields: [\"my_computed_field\"],\n        filter: {\n          id: { _eq: \"X-Hasura-User-ID\" },\n        },\n      },\n    },\n  ],\n}\n\n// Clone the tables for comparison after changes using diff()\nconst originalTablesMetadata = Convert.clone(tablesMetadata)\n// Add the new table to tables metadata\ntablesMetadata.push(newTable)\n\n// Generate a structural and text diff from the changes between original and now\nconst tableDiff = Convert.diff(originalTablesMetadata, tablesMetadata)\n// Write the diffs to /diffs folder, will output \"tables.json\" and \"tables.diff\"\nConvert.writeDiff({ folder: \"diffs\", file: \"tables\", diffs: tableDiff })\n// Ouput the updated \"tables.yaml\" to filesystem\nfs.writeFileSync(\n  \"./tables-updated.yaml\",\n  Convert.metadataToYAML(tablesMetadata)\n)\n\n// Read \"metadata.json\"\nconst metadataFile = fs.readFileSync(\"./metadata.json\", \"utf-8\")\n// Convert.to<typeName> does runtime validation of the type\nconst allMetadata: HasuraMetadataV2 = Convert.toHasuraMetadataV2(metadataFile)\nconsole.log(allMetadata)\n\n// Clone, add table\nconst beforeMetadataChanges = Convert.clone(allMetadata)\nallMetadata.tables.push(newTable)\n\n// Diff, write diff\nconst metadataDiff = Convert.diff(beforeMetadataChanges, allMetadata)\nConvert.writeDiff({ folder: \"diffs\", file: \"metadata\", diffs: metadataDiff })\nClosing WordsWe hope that you enjoyed and learned something from this article, and in this SDK find a valuable tool for your own workflow.We invite you to check out the Github repo for more information, play with it, and give us feed back on what works & what doesn't =)Lastly, we hope you'll join and become a part of our online community."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Asia - Announcing Second Round of Speakers Description: The second round of speakers and workshops for GraphQL Asia 2021.  Author: Hasura"}, {"role": "assistant", "content": "Asia\u2019s largest gathering of GraphQL enthusiasts is going online for its third edition on 24th & 25th Feb 2021. Tickets are free! You can register here.The first round of speakers was announced last week, you can read more about them here. Today, we added 2 more in-depth workshops and 11 more fantastic speakers to our line up.WorkshopsGraphQL in the JamstackTamas Piros, CloudinaryType-safe GraphQL apps with TypeScriptAaron Powell, MicrosoftTalksMonorepo development using Hasura role and GraphQL Code GeneratorYudai Shinnoki, WASD Inc.Intelligent Caching with GraphQL Gateway using CDCRoshan Alexander, PratilipiConnect your enterprise to Cloud with Hasura and Apache KafkaWojciech Trocki, Red HatGraphQL: Wrong Answers Only!Peter Piekarczyk, DraftbitReintroducing Schema Stitching in 2021Greg MacWilliam, Vox MediaData Science with Weaviate and GraphQLLaura Ham, SeMI TechnologiesGraphQL & Microservices - the different approaches & challengesToan Nguyen, HasuraAdopting GraphQL in a legacy NodeJS APIPranay Kalro, TescoA Tale of Inherited Projects and why we kept HasuraBrooke Swanson, OutpaceYou can read more about the talks -- and the incredible speakers giving them -- over at the GraphQL Asia homepage."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing - Hasura Advanced Tutorial Description: For quite a long period of time, our GraphQL Tutorials (colloquially referred to as \u2018Learn\u2019) have been one of the most highly trafficked areas of the website. We are pleased to introduce the Hasura Advanced Tutorial. Author: Tyler Hannan"}, {"role": "assistant", "content": "Our #GraphQLJanuary continues with blog posts, live streams, Discord Q&A, office hours, and more. For a schedule of upcoming events, join the community or register at https://hasura.io/graphql/graphql-january/.For quite a long period of time, our GraphQL Tutorials (colloquially referred to as \u2018Learn\u2019) have been one of the most highly trafficked areas of the website. This content, alongside other resources like our GraphQL hub, form the basis for how many people first learn about GraphQL and then, ultimately, explore whether Hasura is the right product for their needs.As with any other rapidly growing, open-source project -- Did you hear we surpassed 100M downloads? -- there comes a point where tutorials expand from conceptual (Intro to GraphQL and What is GraphQL?) to implementation (front-end and back-end tutorials). And, then suddenly something more is needed.We are pleased to introduce the Hasura Advanced Tutorial.This course is meant to be taken by users of Hasura who need to optimise their application for production use cases. If you are new to Hasura, head to the Hasura Basics tutorial to get a fair idea of setting up Hasura and leveraging the fundamental features before diving into this tutorial.Of course, with that said, understanding optimisation and implementation best-practices can be an important part of learning about and vetting a solution.Because we are the sort of people for whom open-source software makes sense, each of these tutorials is MIT-licensed. We welcome your feedback, participation, and pull-requests. Join us on the https://github.com/hasura/learn-graphql/ repository to participate.As part of #GraphQLJanuary, we will be live-streaming this tutorial over the course of the upcoming weeks. Join us on the Hasura Twitch channel to participate in the stream. Or, catch-up after the fact on the Hasura Youtube channel.Happy learning!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The New Data Fabric - Enterprise GraphQL Description: In October of last year, Hasura held a conference entitled \u2018Enterprise GraphQL Conference\u2019 and one of the sessions was \n\"The New Data Fabric, 2020 as a trigger for innovation\" Author: Tyler Hannan"}, {"role": "assistant", "content": "Our #GraphQLJanuary continues with blog posts, live streams, Discord Q&A, office hours, and more. For a schedule of upcoming events, join the community or register at https://hasura.io/graphql/graphql-january/.While discussing the #GraphQLJanuary that is ongoing with a friend, we began discussing how (in 2020) the shifting world situation had changed the way that people do business. I found myself saying: \u201cCompanies that had developers on staff (internal) were able to deliver new workflows in reaction on the challenges\u201d and...in a moment...I knew that I had heard that assertion somewhere before.In October of last year, Hasura held a conference entitled \u2018Enterprise GraphQL Conference\u2019 and one of the sessions was The New Data Fabric, 2020 as a trigger for innovationThe industry is quickly moving to a model of higher level of services and abstraction, in all layers of the stack, from compute to storage to networking and application deployment and delivery. While we've seen a ton of innovation in data and data management, we're now seeing an acceleration in that space, with developer experience as a key driver for the new platforms and capabilities. Programming languages are becoming more data friendly, and data services are increasingly being defined by new APIs, notably GraphQL. The pendulum is swinging from 3 tier application models to 2 tier, with rich interactive front ends orchestrating from a palette of back end services - client/serverless is a thing. In this talk James will discuss how enterprises can build more powerful data driven apps using new technologies and methods, putting all of this change into context.In this talk, James Governor of Redmonk discussed how the industry is changing and my \u2018insight\u2019 from above was a poor summation of an early point in his talk.Why do I bring this talk up now?I find that, often, we become so consumed with the \u2018how\u2019 of a technical topic that we forget about the \u2018what\u2019 of a technical topic. As I relistened to James session, I was reminded of some of the fundamental shifts that are ongoing (of which things like GraphQL are a part). And framing the education we are doing throughout the month in context of this greater trend was deeply meaningful.To share a quote from the talk:\u201cWe need to have a platform team creating rich services, curating those service that are then consumed by developers. <snip> And, for me, that\u2019s one of the reasons why GraphQL is so interesting because it begins to create a separation of concerns. Who are building the APIs and who are consuming the APIs are a different set of folks.\u201dThis is the introduction to a section that discusses GraphQL in context of performance, predictability, portability, control, and maintainability.Whether, over the next 3 weeks, you join our Discord Q&A, one of the many livestreams, read a blog, fire up a cloud instance, participate in office hours, etc. I encourage you to do so with the context shared above in mind. It is not just about \u2018How do I GraphQL?\u2019 but also about \u2018What are we achieving with GraphQL?\u2019The talk is available to view, in its entirety, at the Enterprise GraphQL talks page. What is shared above is only a summary of a small section. The discussion of the Data Fabric, more broadly, is one in which not every developer is expected to do everything but the \u2018separation of concerns\u2019 allows for richer experiences and expedited innovation."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: TimescaleDB 2.0 with Hasura Description: TimescaleDB 2.0 is a major version upgrade that has many improvements from version 1. It introduces new interesting features and capabilities, especially horizontal multi-node scaling. Because it is a PostgreSQL extension, it mostly works well with Hasura. Author: Toan Nguyen"}, {"role": "assistant", "content": "Our #GraphQLJanuary continues with blog posts, live streams, Discord Q&A, office hours, and more. For a schedule of upcoming events, join the community or register at https://hasura.io/graphql/graphql-january/.In this post, Hasura engineer Toan shares his experience with using Hasura alongside TimescaleDB 2.0 and exploring what works...and what doesn\u2019t. Learning from the experience of others exploring workarounds to achieve their goals is always instructive.This post is part of a 3-part series exploring TimescaleDB with Hasura.TimescaleDB is an open-source database designed to make SQL scalable for time-series data. The most valuable features of TimescaleDB is hypertable, a high-level table that provides automatic partitioning across time and space (partitioning key).TimescaleDB 2.0 is a major version upgrade that has many improvements from version 1. It introduces new interesting features and capabilities, especially horizontal multi-node scaling that can solve the limitation of write performance.Because it is a PostgreSQL extension, it mostly works well with Hasura. However, there are several limitations. This article will share some known issues and workarounds.The example code is uploaded on Github.Migrations and Breaking changesFrom 1.x to 2.0To upgrade new version, you can read the official guide hereThe following table shows syntax comparison between TimescaleDB 1.7 and 2.0:\n\n\n1.7\n2.0\nComment\n\n\n\n\nadd_compress_chunks_policy\nadd_compression_policy\nAdd compression policy\n\n\nremove_compress_chunks_policy\nremove_compression_policy\nRemove compression policy\n\n\nadd_drop_chunks_policy\nadd_retention_policy\nAdd retention policy\n\n\nremove_drop_chunks_policy\nremove_retention_policy\nRemove retention policy\n\n\nCREATE VIEW ... WITH (timescaledb.continuous)\nCREATE MATERIALIZED VIEW ... (timescaledb.continuous); SELECT add_continuous_aggregate_policy(...);\ncreate materialized view (continuous aggregate)\n\n\nDROP VIEW <view_name> CASCADE\nDROP MATERIALIZED VIEW <view_name>\nDrop materialized view (continuous aggregate)\n\n\nSELECT * FROM timescaledb_information.hypertables WHERE table_name = '<table_name>'\nSELECT * FROM hypertable_size('<table_name>')\nGet hypertable size\n\n\n^\nSELECT hypertable_index_size('<table_name>')\nGet hypertable index size\n\n\n^\nSELECT hypertable_detailed_size('<table_name>')\nGet hypertable index detailed size\n\n\nSELECT * FROM timescaledb_information.compressed_chunk_stats WHERE hypertable_name = '<table_name>'\nSELECT * FROM hypertable_compression_stats('<table_name>')\nGet compression stats\n\n\n\nMany of the functions and SQL syntaxes are renamed. timescaledb_information view structures are very different. You can't list all hypertable sizes with timescaledb_information.hypertables, timescaledb_information.compressed_hypertable_stats but SELECT each table with hypertable_size, hypertable_compression_stats functions.In version 2.0, there are 2 new interesting features:Scheduled job. Now you can run cronjob in Postgres to do many things, such as automatically refreshing Materialized view.Multi-node. It helps us scaling read/write into multiple data nodes.Because of the scheduled job. Continuous Aggregate Materialized View has a big refactor, as well as breaking changes, that we will explore more in the next section.Materialized View (Continuous Aggregate)Continuous Aggregate Materialized View is Materialized View with auto refresh and partitioning. From a migration point of view, high level SQL definition is translated to internal statements.Input:\nCREATE MATERIALIZED VIEW conditions_summary_minutely\n    WITH (timescaledb.continuous) AS\n    SELECT time_bucket(INTERVAL '1 minute', time) AS bucket,\n        AVG(temperature),\n        MAX(temperature),\n        MIN(temperature)\n    FROM conditions\n    GROUP BY bucket;\n\nOutput:\nCREATE OR REPLACE VIEW \"public\".\"conditions_summary_minutely\" AS\nSELECT\n  _materialized_hypertable_5.bucket,\n  _timescaledb_internal.finalize_agg(\n    'avg(double precision)' :: text,\n    NULL :: name,\n    NULL :: name,\n    '{{pg_catalog,float8}}' :: name [],\n    _materialized_hypertable_5.agg_2_2,\n    NULL :: double precision\n  ) AS avg,\n  _timescaledb_internal.finalize_agg(\n    'max(double precision)' :: text,\n    NULL :: name,\n    NULL :: name,\n    '{{pg_catalog,float8}}' :: name [],\n    _materialized_hypertable_5.agg_3_3,\n    NULL :: double precision\n  ) AS max,\n  _timescaledb_internal.finalize_agg(\n    'min(double precision)' :: text,\n    NULL :: name,\n    NULL :: name,\n    '{{pg_catalog,float8}}' :: name [],\n    _materialized_hypertable_5.agg_4_4,\n    NULL :: double precision\n  ) AS min\nFROM\n  _timescaledb_internal._materialized_hypertable_5\nWHERE\n  (\n    _materialized_hypertable_5.bucket < COALESCE(\n      _timescaledb_internal.to_timestamp(_timescaledb_internal.cagg_watermark(5)),\n      '-infinity' :: timestamp with time zone\n    )\n  )\nGROUP BY\n  _materialized_hypertable_5.bucket\nUNION ALL\nSELECT\n  time_bucket('00:01:00' :: interval, conditions.\"time\") AS bucket,\n  avg(conditions.temperature) AS avg,\n  max(conditions.temperature) AS max,\n  min(conditions.temperature) AS min\nFROM\n  conditions\nWHERE\n  (\n    conditions.\"time\" >= COALESCE(\n      _timescaledb_internal.to_timestamp(_timescaledb_internal.cagg_watermark(5)),\n      '-infinity' :: timestamp with time zone\n    )\n  )\nGROUP BY\n  (\n    time_bucket('00:01:00' :: interval, conditions.\"time\")\n  );\n\nIt's frustrating to reuse definitions in console if you lose the original script. Moreover, it doesn't support CREATE OR REPLACE to replace current definition, so you need to DROP and CREATE the view.From TimescaleDB 2.0, the new API and framework for continuous aggregates separates policy from the core functionality (#2118). The continuous view creation syntax is changed:-- old syntax\nCREATE VIEW conditions_summary_minutely\nWITH (timescaledb.continuous,\n    timescaledb.refresh_lag = '1h',\n    timescaledb.refresh_interval = '1h')\nAS\n    SELECT time_bucket(INTERVAL '1 minute', time) AS bucket,\n        AVG(temperature),\n        MAX(temperature),\n        MIN(temperature)\n    FROM conditions\n    GROUP BY bucket;\n\n-- new syntax\nCREATE MATERIALIZED VIEW conditions_summary_minutely\n    WITH (timescaledb.continuous) AS\n    SELECT time_bucket(INTERVAL '1 minute', time) AS bucket,\n           AVG(temperature),\n           MAX(temperature),\n           MIN(temperature)\n    FROM conditions\n    GROUP BY bucket;\n\n-- the continuous aggregate policy is a separated job\nSELECT add_continuous_aggregate_policy('conditions_summary_minutely',\n    start_offset => INTERVAL '2 h',\n    end_offset => INTERVAL '1 h',\n    schedule_interval => INTERVAL '1 h');\n\nThe new continuous view separates continuous aggregate policy into another function add_continuous_aggregate_policy instead of parameters in one CREATE VIEW statement. Moreover, it leads another issue:CREATE MATERIALIZED VIEW ... WITH DATA cannot run inside a transaction blockFatalError: ... cannot run inside a transaction blockThere are functions that require running in AUTOCOMMIT mode. Every statement is automatically committed once submitted. psql turns on autocommit by default. Ever notice that psql executes SQL statements one by one? AFAIK, this error is thrown in these operations:Admin commands: CREATE/DROP database, tablespace,...CREATE INDEX CONCURRENTLYNow, this error appears on new Continuous materialized view too. It isn't a problem if you create views with psql. However, Hasura migration CLI becomes unusable, as it uses transaction to apply migration files.It isn't impossible to run a migration with database driver, if it satisfies both conditions:Executes SQL statement without transaction.Only one statement per request.Unfortunately it is hard to change GraphQL Engine source code, because it relates to core functionality of the engine. So, I came up with an idea. It is easier to customize the CLI with native Go's lib/pq option.This is a hack, so it isn't official in the main upstream. You can download the customized CLI here, or try it in the single node example.hasura migrate apply --disable-transaction --database-url \"<url>\"There are also caveats:Migration files are applied right away one by one, not applied as bulk SQL in one transaction. Therefore the migration can't be canceled. For example, you apply 3 migrations A, B, C. If there is any error in migration C, A and B were still applied.There must be only 1 SQL statement in up.sql if you use a special statement such as CREATE MATERIALIZED VIEW.GraphQLBecause TimescaleDB is an extension of Postgres, it is compatible with GraphQL Engine. However, there are several limitations of TimescaleDB that affect Hasura:Hypertable doesn't require a Primary key. Therefore <hypertable_name>_by_pk queries, mutations and subscriptions are disabled.GraphQL Engine doesn't support UPSERT for hypertable. In fact, TimescaleDB supports UPSERT , but with infer indexes only. Meanwhile popular Postgres query builder engines, ORMs use ON CONFLICT ON CONSTRAINT constraint_name statement.Foreign key can't be point into a hypertable (although that data model isn't very common). \u00a0The common is for the hypertable to have a foreign key to point to a relational table (think an object_id in the time-series is a Foreign Key to a metadata table about the object).Although we can create a manual relationship between hypertables, the query performance should be considered.(Thank Mike Freedman (@michaelfreedman) for the feedback)Console and Hasura CLITimescaleDB SQL API isn't supported by the Hasura console. We have to use Raw SQL or create migration manually. In theory hypertable is the high level of the table, we can create it in console. However, hypertable uses timestamp or number column as partition key. It requires including that column as primary key. Therefore we have to ignore primary key or use composite keys, and in practice we choose ignoring it. The issue is, Hasura console forces Primary key on table creation #6235. Manual migration creation is unavoidable.However, Continuous Aggregate View can't be deleted by console UI. Behind the scene it requests DROP VIEW SQL execution. The correct statement is DROP MATERIALIZED VIEW.Drop materialized view error TimescaleDB and HasuraDue to the optional Primary key in hypertable, we can't view detail, update and delete rows in the data table.TimescaleDB and Hasura ConsoleTherefore, most TimescaleDB functions have to be run in raw SQL. The console doesn't have much help here.Should I upgrade?Yes, if you aren't afraid of migration breaking changes. The new Continuous Materialized View is also another concern.TimescaleDB 2.0 is also worth to upgrade with Multi-node scaling solution. We will explore in the next part of series coming soon."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Asia - Announcing First Round of Speakers Description: Asia's largest GraphQL conference is happening online on 24th & 25th. Featuring a fireside chat with GraphQL's co-creator, workshops, and talks in multiple Asian languages. Author: Tyler Hannan"}, {"role": "assistant", "content": "GraphQL Asia is the largest gathering of GraphQL enthusiasts across Asia...and, now, it is online.http://graphql.asia \u00a0On the 24th and 25th of February, the third edition of GraphQL Asia is occurring. It has, and remains, deeply important to the Hasura team to ensure that there is a regional conference discussing the successes, challenges, and providing education material about GraphQL in Asia.To make good on our vision, we are having several talks this year in multiple Asian languages. It is, as of today, somewhat unclear how we will handle live translation to English. But, the only benefit of an online event is that most talks are pre-recorded. And, as such, there is flexibility.It is not just about GraphQL adoption.It is about expanding the community in Asia.In the languages that countries in Asia speak.(As an aside, there is already a channel in our community Discord for Japanese speakers. Please feel free to join us.)The 2021 edition of GraphQL Asia is proving to be very exciting. We have, today, announced 4 talks:The State of GraphQLTanmai Gopal, HasuraDan Schafer, FacebookEvolution of GraphQL@Tokopedia with increase in ScaleAditi Singh, TokopediaBuilding high performing applications using GraphQL for heterogeneous systemSunitha N, SAPGraphQL Error Handling Done RightKewei Qu, FacebookIn addition, we have listed 3 Workshops:Server-side Authentication in GraphQLRoy Derks, VanderbronGraphQL on SQL Server with HasuraIntroduction to HasuraThere have been enough CFP submissions to support a week long event. So the event team is hard at work sorting through all of the submissions. Expect a steady stream of updates over the upcoming weeks.If we can\u2019t meet in person, at least we can ensure that the best possible content is highlighted at the event. Registrations are open and we look forward to seeing you there.Our #GraphQLJanuary continues with blog posts, live streams, Discord Q&A, office hours, and more. For a schedule of upcoming events, join the community or register at https://hasura.io/graphql/graphql-january/. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: 2020 Year In Review Description: Understanding the recent history of a project can help ensure that you have context. Looking back on 2020 for Hasura and looking forward to what comes in the future. Author: Tyler Hannan"}, {"role": "assistant", "content": "Our #GraphQLJanuary continues with blog posts, live streams, Discord Q&A, office hours, and more. For a schedule of upcoming events, join the community or register at https://hasura.io/graphql/graphql-january/Any learning journey you undertake, be it technical or otherwise, should usually start with exploring the fundamentals (check out today\u2019s stream on Twitch for more) and then quickly followed with a bit of archaeology. Understanding the recent history of a project can help ensure that you have context.Context for expanding on your base of knowledge. Context for what technical decisions are made.Context for how you will build your solution.Recently, Praveen streamed a retrospective (I like to think of it as an archaeological survey) of 2020 for Hasura. That video is embedded below.The year of 2020 was a big year for Hasura. There is value in exploring what happened, by month.January - Hasura v1.0.0 Stable\u201cThe Hasura GraphQL Engine fronts a Postgres database instance and can accept GraphQL requests from your client apps. It can be configured to work with your existing auth system and can handle access control using field-level rules with dynamic variables from your auth system.\u201d - What Is Hasura?February - Series A Funding & GraphQL Asia\u201cOur entire team is super excited with what we\u2019ve been able to achieve in the last few months. Hasura started as something we built ourselves to ease data access and accelerate development. Over time, our user base picked Hasura up and built a passionate, positive and extremely helpful community around Hasura, and that has become the bedrock on which Hasura has matured.\u201d - Hasura 2020: The next phase of our journey\u201cEngineers from all over the world will be coming down to Bangalore to share their experiences, best practices and insights about working with GraphQL.\u201d - What to expect from GraphQL Asia 2020??  (Author\u2019s note: Little did we know that this would be the last in-person event for quite a while. We look forward to having the community gather, in physical proximity, at some point in the future and are delighted, for now, that we can still host events remotely.)March - Typescript Migration\u201cAt Hasura, we wanted to introduce a statically typed frontend language for quite some time now. We discussed which one we should choose and evaluated options such as PureScript, TypeScript, ReasonML, and Elm. After all of the discussions, we decided to choose TypeScript. \u201c - Why we chose TypeScript for the Hasura ConsoleApril - #ActionPackedApril\u201cHasura has recently implemented a way to create custom mutations called Actions. Want to handle complex business logic? Actions are the way to go!\u201d - #Actions on Hasura blog \u00a0#ActionPackedApril on twitterMay - Scheduled Triggers\u201cScheduled Triggers are used to execute custom business logic, via webhooks, at specific points in time. It extends the idea of Event Triggers (which are driven by table events) to a flexible API that can be used to schedule events at some time(s) for any payload and webhook.\u201d - Introducing Scheduled Triggers: API Driven Cron Jobs and Scheduled EventsJune - HasuraCon 2020 / Hasura Cloud Released\u201cWe\u2019ve used the core Hasura GraphQL engine as a base, made sure that we\u2019re able to provide Hasura as a cloud offering where you don\u2019t think about the number of instances, cores, memory, concurrent users, high-availability, realtime monitoring, caching, tracing, rate-limiting and other things that are infrastructure concerns and not really things you want to spend time on.\u201d \u00a0- Announcing Hasura Cloud: Managed GraphQL for your database and services\u201cHasura Con is a free, online conference. Join us for a week of talks and hand-on workshops about pushing the boundaries of GraphQL forward!\u201d - The Hasura User ConferenceJuly - Hasura v1.3.0 Stable\u201cAfter a few weeks of being in beta, Hasura v1.3.0 is now released. This is one of the biggest Hasura releases till date, with 3 huge features being launched - Remote Joins, Relay support and Scheduled Triggers. See the changelog for more details!\u201d - Announcing Hasura GraphQL v1.3 - Data Federation, Relay, Scheduled Triggers and moreAugust - PDV Refactor Server\u201cThis pull request is a complete rearchitecting of the way we build GraphQL schemas and parse GraphQL queries. It implements some of the ideas in #2801, but the approach is rather different from the one described there (as that approach turned out to be too na\u00efve in various ways).\u201d - GraphQL-engine issue 4111September - Series B Funding\u201cHasura has been driven by a passionate team and an active community, solving difficult problems and helping evolve the product. And it's amazing that we have the opportunity to do even more of this in the future!Keep us honest, share your frustrations and share your successes. That's our fuel which makes us go, and it's our carrot \ud83e\udd55and it's our stick. :)\u201d - Announcing our $25M Series B financingOctober - Enterprise GraphQL Conference\u201cA conference about the role of GraphQL in unlocking data and modernizingdata access to accelerate product delivery in organizations. Curated for enterprise developers & teams.\u201d - Enterprise GraphQL ConferenceNovember - Office Hours, Webinars, Streams Began\u201cA live informal 30 minute session with the Hasura Team. We will be hanging out on Zoom, and you can join us any time to ask your questions.\u201d - Hasura Office Hours\u201cJoin a Hasura event, from anywhere in the world. Learn more about our live and recorded events below. You can also sign up to our community newsletter to stay up-to-date on our events.\u201d - Hasura EventsThat moment when a team begins to show their face and have conversations. (Schedule is live and updated) - Livestreams on TwitchDecember - Regions, HIPAA, SOC2 Type 1\u201cWe\u2019re excited to announce that you can now deploy your Hasura Cloud projects to different geographies. \u00a0This has been something many of you have asked for - based on your feedback more regions are now available\u201d - Use Hasura Cloud In 5 Regions\u201cTo achieve our SOC2 Type 1 certification, we implemented an exhaustive list of security controls including technical safeguards like penetration testing by an independent security firm, vulnerability scans and encryption, and process measures such as security training and disaster recovery planning.\u201d - Announcement: Hasura Cloud Achieves SOC2 Type 1 and HIPAA Compliance CertificationsIn SummaryAny retrospective would be incomplete without looking forward in anticipation of the future. 2021 -- beginning with #GraphQLJanuary -- promises to be even more exciting. Keep your ears tuned and eyes out for multiple roles, remote schema permissions, generalised remote joins, actions permissions, relay updates, to name just a few. And, of course, support for multiple databases is coming soon...very soon."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Contributor Days - #GraphQLJanuary Description: The goal of Contributor Days is to not only give important updates to the community, but also a forum to provide a voice for anyone using GraphQL to have direct access to core contributors and those who have implemented GraphQL solutions at a variety of company stages and sizes. Author: Tyler Hannan"}, {"role": "assistant", "content": "In our blog post titled \u2018GraphLQ January with Hasura\u2019 we shared plans for a month of content focused on learning, expanding, and exploring GraphQL. We believe, strongly, that GraphQL will continue to be a key skill for developers (of all types) to learn. And, of course, we believe that Hasura has some interesting, and unique, approaches to GraphQL implementation and allows you to unlock your data.The plans, as laid out in the prior blog post, remain the same. But, some housekeeping is in order.For the Live Q&A, we will be hosting this in the #general channel of the Hasura Community Discord weekly on Wednesdays. 13 Jan - 1500 CET, 1930 IST, 0900 EST20 Jan - 2100 CET, 1500 EST, 1200 PST27 Jan - 1500 CET, 1930 IST, 0900 EST3 Feb - 2100 CET, 1500 EST, 1200 PSTThe easiest way to keep track of all the things happening in context of #GraphQLJanuary is to join the platform of your choice. We will be sharing content via Twitter, Discord, LinkedIn, Facebook, Twitch, Youtube, etc. And, of course, the Hasura community team (Adron, Praveen, Vish, Tyler) are available to take your recommendations for content or questions as they arise.Join us to continue, and influence, the conversation! If you are new to GraphQL, we have collated helpful resources that you can check out on hasura.io/graphql.More updates and scheduling details to come over time. For now, let\u2019s start learning together!GraphQL Contributor DaysWe are the kind of people for whom open-source makes sense. - Tanmai Gopal, CEO HasuraFor me, this statement carries great weight. We could discuss, at some length, what \u2018open-source\u2019 means and how to accurately define its impact. But, personally, any great open-source project is influenced by the breadth of community that engages in building, driving, and discussing the implementation.The goal of Contributor Days is to not only give important updates to the community, but also a forum to provide a voice for anyone using GraphQL to have direct access to core contributors and those who have implemented GraphQL solutions at a variety of company stages and sizes. It remains humbling that Hasura is able to help facilitate this dialog.In this session, hosted in November of last year, a rotating group of approximately 20 people spent time discussing a variety of topics. For convenience, I have linked timestamps below.What\u2019s new with GraphQL (foundation updates, tooling, etc.)@defer and @stream directivesOther upcoming work and proposalsCustom scalarsGraphQL with microservices and Remote/Federated GraphQL executionAPI lifecycle in, and alongside, GraphQLError handling and integration with APM toolsEcosystem suggestions & ideas (new spec, tooling, etc) The conversation was compelling, and lasted for nearly 4 hours. While consuming the entirety of the recording is an investment in time, it is also worth it. I begin #GraphQLJanuary by sharing this content recognising that many of the themes discussed will surface again throughout the month."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building GraphQL APIs with Laravel, PostgreSQL and Hasura Remote Joins Description: We will be building GraphQL APIs in Laravel using PostgreSQL as a database and Hasura for query and Remote Joining multiple tables in the database. Author: Hasura"}, {"role": "assistant", "content": "This articles was written by Chimezie Enyinnaya as part of the Hasura Technical Writer Program. If you would like to publish an article about Hasura or GraphQL on our blog, apply here.Remote Joins in Hasura extend the concept of joining data \u00a0across tables, to being able to join data across tables and remote data \u00a0sources. Once you create relationships between types from your database \u00a0and types created from APIs, you can then \u201cjoin\u201d them by running GraphQL \u00a0queries.Remote joins can join across the database and APIs. \u00a0These APIs can be custom GraphQL servers you write, 3rd party SaaS APIs, \u00a0or even other Hasura instances.In this article, we will be building APIs in Laravel using PostgreSQL as a database and Hasura for query and Remote Joining multiple tables in the database.PrerequisitesKnowledge of LaravelBasic knowledge of PostgreSQL and GraphQLSetting upTo start, we will need to install Laravel on our computer and set it up. Open your command line and type in the below command:bash\ncomposer create-project laravel/laravel hasura-graphql\ncd hasura-graphql\nphp artisan serveNext, we set up an online PostgreSQL database. I will be using ElephantSQL for my database. You can use other online PostgreSQL databases if you would like.After setting up our ElephantSQL account and creating your database, we will then set up a Hasura.io account and create a new project. If you are new to Hasura, you can learn how to set up a new project here. Then we proceed to database setup, copy your PostgreSQL database URL and paste in the Enter your database URL input.Note: the URL starts with postgres:// so look out for a URL like that in your online PostgreSQL platform.Go to your database and create a todos table and populate it with some data.With that done, we are ready to test our database. We go back to our project and configure our .env file to add our database configuration.DB_CONNECTION=pgsql\nDB_HOST=<DATABASE_HOST>\nDB_PORT=5432\nDB_DATABASE=DATABASE_NAME>\nDB_USERNAME=<DATABASE_USERNAME>\nDB_PASSWORD=<DATABASE_PASSWORD>\nAfter the configuration is complete, we need to install lighthouse php to use GraphQL in our Laravel application. Lighthouse is a framework for serving QueryQL on Laravel. To install use the below code:bash\ncomposer require nuwave/lighthouse\nThen we publish the Schema using:bash\nphp artisan vendor:publish --tag=lighthouse-schema\nNext, let\u2019s install GraphQL Playground to test out our GraphQL API. To do that, simply use:bash\ncomposer require mll-lab/laravel-graphql-playground\nThen we can open it using http://127.0.0.1:8000/graphql-playground. But before that, we need to configure our Lighthouse and create a Todo Model. We do that using the below command:bash\nphp artisan make:model Todo\nThis will create a new model in your app/Models directory. Configure it with the code:php\n<?php\n\n\nnamespace App\\Models;\n\n\nuse Illuminate\\Database\\Eloquent\\Factories\\HasFactory;\nuse Illuminate\\Database\\Eloquent\\Model;\n\n\nclass Todo extends Model\n{\n    use HasFactory;\n\n    protected $table = 'todos';\n    protected $primaryKey = 'id';\n\n\n    protected $fillable = ['title', 'short_description', 'content'];\n}Then, in our graphql/schema.graphql file we can add the schema for the Todo type as well as a couple of GraphQL queries:type Query {\n    todos: [Todo!]! @all\n    todo(id: ID @eq): Todo @find\n}\n\ntype Todo {\n    id: ID!\n    title: String!\n    short_description: String!\n    content: String!\n    created_at: DateTime!\n    updated_at: DateTime!\n}With that done, we can open our Graphql-playground and test out our database using http://127.0.0.1:8000/graphql-playgroundRemote JoinsNow we are done with setting up, we need to create our remote joins, to do this we will beModifying our todo table and added users tablecreating a relationship between the users table and todo table (manually populating data)Testing out our remote joinsTo modify the database, go to your Hasura console and create a new Users tableThen create a relationship between todos table and users table. To do that, follow the instruction here. Once the relationship is created between todos and users, we can then test on our Hasura console.As the above code shows, we have joined our todos table to our users table and we are fetching all todos with their corresponding user.ConclusionIn this tutorial, we have Created a new instance of LaravelInstalled and used Lighthouse and GraphQL playground for our GraphQL queryWe then created our ElephantSQl database and tied it to our Hasura account.Configured our Laravel application to interact with our ElephantSQL and then created a remote join for our todos and users table. You should now understand the basics of building Laravel APIs with PostgreSQL and Hasura Remote Joins. This articles was written by Chimezie Enyinnaya as part of the Hasura Technical Writer Program. If you would like to publish an article about Hasura or GraphQL on our blog, apply here.About the AuthorChimezie Enyinnaya is a software developer and content creator based in Lagos, Nigeria. He is the founder of adonismastery.com where he teaches about AdonisJS, a fully-featured Node.js framework."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Vue Tutorial: Implement Infinite Scroll in Vue.js Using Apollo and Hasura GraphQL Description: In this article, we're going to implement infinite scroll in a Vue.js app using Apollo and Hasura GraphQL APIs. Author: Anthony Gore"}, {"role": "assistant", "content": "This articles was written by Anthony Gore as part of the Hasura Technical Writer Program. If you would like to publish an article about Hasura or GraphQL on our blog, apply here.\n\"Infinite Scroll\" is a UI feature that allows a website to show a long list of objects to a user (posts, comments, images, etc) without overwhelming their browser or network connection.\nYou'll be familiar with this feature if you've used social media apps like Twitter, Reddit, or Instagram which all have an implementation of this.\nA key aspect of this feature is to call data on demand. This is trivial to implement using Hasura's query pagination.\nIn this article, we're going to implement infinite scroll in a Vue.js app using Apollo and Hasura.\nHere's a break-down of the steps we'll cover:\n\nQuick set up of a Vue/Apollo app using Vue CLI\nSet up local Hasura with Docker\nSeed database with mock data\nPaginated database calls with Apollo's fetchMore feature\nImplement scroll loading in frontend\n\nBy reading this article, you'll gain a better understanding of the power of Hasura's GraphQL server and how it can be used effectively in a web app stack.\nYou'll also have a blueprint for implementing infinite scroll in your own JavaScript apps.\n\nIf you'd like to see the complete code, check out this GitHub repo.\n\nPrerequisites\nI assume you are familiar with the following:\n\nBasics of Vue & Vue CLI\nBasics of GraphQL\n\nYou'll need the following software on your system:\n\nDocker\nNode.js & NPM\nVue CLI\n\nCreating an app with Vue CLI\nTo get a frontend app up and running quickly, Vue CLI is a great option. Let's create a new project by going to the terminal and typing:\n$ vue create infinite-scroll\n\nFor this tutorial, you can instruct the CLI tool to create a Vue 2 app with the default settings.\nOnce installation completes, change into the project directory:\n$ cd infinite-scroll\n\nAdding Vue Apollo\nIn case you aren't familiar with Apollo, it's a GraphQL client implementation that makes it easy to bridge between a frontend app and a GraphQL server like HasuraDB.\nVue Apollo is, of course, a plugin for Vue.js that adds the Apollo client as a service that is easy to access from any Vue component.\nVue CLI makes it dead-simple to add the Vue Apollo plugin to your project:\n$ vue add apollo\n\nThis will not only install the plugin but also automatically configure it!\nClearing out boilerplate files\nBefore we continue, let's remove some of the boilerplate provided by Vue CLI so we're ready to build our own app.\nFirstly, let's delete the HelloWorld component. We'll also empty out the App.vue component, though we won't delete it completely as we'll add new content to it soon.\n$ rm src/components/HelloWorld.vue\n$ > src/App.vue\n\nIn a moment we're going to set up our frontend app. Before we do, let's set up our Hasura database and add some mock data to it.\nSetting up a local Hasura database with Docker\nAs explained, we're going to get data via GraphQL from a Postgres database using Hasura.\nTo get up and running quickly, we're going to use Hasura in a local Docker environment. We can simply download the premade docker-compose file into our project root:\n$ wget https://raw.githubusercontent.com/hasura/graphql-engine/stable/install-manifests/docker-compose/docker-compose.yaml\n\nThen run docker-compose up -d to get it running.\nIf it worked, you should be able to access the Hasura console here: localhost:8080/console.\nConnecting Vue Apollo to HasuraDB\nFor our Vue Apollo app to access the Hasura GraphQL server, there is one bit of configuration we must provide - we must create a .env file and specify the URLs as follows:\n.env\nVUE_APP_GRAPHQL_HTTP=http://localhost:8080/v1/graphql\nVUE_APP_GRAPHQL_WS=ws://localhost:8080/v1/graphql\n\nData model\nIn this app, we're going to display a list of \"posts\" - think of Reddit, Twitter, or Instagram. Let's now create a table in our database to store these posts before we create some with mock data.\nThe structure of our posts table will be:\n\nid (Integer, auto-increment)\nurl (Text)\ntitle (Text)\ncreated_at (Timestamp)\n\nTo create this table, go to the Hasura console. Go to the \"Data\" section and click on \"Add Table\".\nSet the table name as posts. Create the columns listed above. Set the primary key to id. Finally, click \"Add Table\" and you'll be done.\nAdding dummy data\nTo add some mock data for our API, let's generate it using the Mockaroo dummy data generator at https://mockaroo.com.\nTo use this, you need to tell it what fields you're looking for and the kind of data you want to use. Try the following:\n\nid (Row Number)\nurl (Dummy Image URL)\ntitle (Animal Common Name)\ncreated_at (Datetime)\n\nNow you need to tell it the kind of data you want to return. I suggest you select at least 100 rows in format JSON.\nTo enter this data into our table, we'll need it as a JavaScript object, but Mockaroo only gives us the JSON option.\nNo problem, though - we can convert JSON to JavaScript using an online conversion tool: https://www.convertonline.io/convert/json-to-js\nSo copy your JSON data from Mockaroo, then paste it in the JSON to JavaScript convertor.\nInserting data\nTo insert the data we just created into our Hasura database, we'll use the GraphQL API explorer of the console. So, navigate your browser to localhost:8080/console/api-explorer\nCreate this query:\nmutation MyMutation {\n  insert_posts(objects: []) {\n    affected_rows\n  }\n}\n\nReplace the array [] assigned to objects with the array of mock JavaScript objects above. Run the query and you should now have the mock data in the database.\nCreating posts component\nLet's return now to our Vue Apollo app. To display each post we're going to create a simple component. These components will be fed the data from the Hasura - we'll see how to do that in the next section.\nFirst, create the component:\n$ touch src/components/Post.vue\n\nNotice that we use a prop post to supply the data, then we use the properties url and title in the template.\nsrc/components/Post.vue\n<template>\n  <div class=\"post\">\n    <div>\n      <img :src=\"post.url\" :alt=\"post.title\"/>\n    </div>\n    <div>\n      <h3>{{ post.title }}</h3>\n    </div>\n  </div>\n</template>\n<script>\nexport default {\n  props: ['post']\n}\n</script>\n<style>\n  .post {\n    display: flex;\n    flex-direction: row;\n  }\n</style>\n\nDisplaying posts\nNext, we're going to display the posts in the app. Open up the file src/App.vue (which we emptied earlier). Add a template section where we'll iterate the post component, using the id property as a key, and binding the post object to the post prop.\nIn the script section, we'll import and register the Post component. Go ahead and add a post data property as well, which will be an array. You can hard-code one or more of your mock data objects from the previous step to see if it works correctly. Of course, we won't need those in a moment when we load posts from the GraphQL server.\nsrc/App.vue\n<template>\n  <div id=\"app\">\n    <post v-for=\"post in posts\" :key=\"post.id\" :post=\"post\" />\n  </div>\n</template>\n\n<script>\nimport Post from '@/components/Post';\n\nexport default {\n  name: 'App',\n  components: { Post },\n  data() {\n    return {\n      posts: [\n        {\n          id: 1\n          title: 'Euro Wallaby'\n          url: 'http://dummyimage.com/134x161.bmp/5fa2dd/ffffff'\n        }\n      ]\n    }\n  }\n}\n</script>\n\nIf you haven't already, start your Vue CLI dev server: npm run serve. Since the Hasura GraphQL server is occupying localhost port 8080, Vue CLI will start on 8081.\nNavigate to localhost:8081 in your browser and you'll see our basic app skeleton.\nSetting up pagination\nLet's now use Apollo to make a query to grab the first \"page\" of our paginated posts. In the next section, we'll work on automatically grabbing the rest to properly implement the scroll load.\nThe first thing we'll do is create a const pageSize and set its value to 10 (or whatever integer you like). This defines how many posts are on one page. This can be defined outside the component state since it does not need to be reactive.\nNext, we'll add a property page to the reactive component state. This keeps track of the current pagination page. Let's initialize to 0.\nsrc/App.vue\n...\n\nconst pageSize = 10;\n\nexport default {\n  ...\n  data() {\n    return {\n      posts: []\n      page: 0\n    }\n  }\n}\n\nFetching the first page of data\nNow we use Apollo to query the GraphQL server and retrieve posts from the database. First, import the graphql-tag package. This would have been automatically added by Vue CLI.\nNow we're going to create a query. First, access the apollo option on the component definition and assign it an object. Create a sub-property posts which tells Vue Apollo we want to use this query to add data to the posts reactive data property.\nAssign an object to posts and give it a sub-property query. We'll assign to this our GraphQL query by using the gql package so we can define the query in a JavaScript string.\nThe query will be called GetPostPages and will have two arguments $offset and $limit. These values will be used to set the offset and limit arguments of the post object (from which we'll grab all available fields).\nIn Hasura, limit specifies the number of rows to include in the results offset determines where to slice the result set.\nThe final thing to do is declare these variables in Apollo by adding a variables sub-property to the posts object where we specify the initial values of these variables.\nsrc/App.vue\n...\n\nimport gql from 'graphql-tag';\n\nexport default {\n  ...\n  apollo: {\n    posts: {\n      query: gql`\n        query GetPostPages ($offset: Int, $limit: Int) {\n          posts (offset: $offset, limit: $limit, order_by: {created_at: asc}) {\n            id\n            url\n            title,\n            created_at\n          }\n        }\n      `,\n      variables: {\n        offset: 0,\n        limit: pageSize,\n      },\n    },\n  }\n}\n\nLet the dev server rebuild your app and take a look at the page. Now the mock data in the Hasura database will be displayed on the page!\nNote that you'll see the most current 10 posts since we ordered results by the created_at date, and we set the limit (page size) to 10.\nFetching subsequent pages\nNow we want to create a method that will fetch the next results and combine them with our existing results.\nFirst, create a new Vue method fetchMore. In this step, we'll define the method, in the next step, we'll see how to call it.\nIn this method, we're going to increment the page reactive value. We'll then use the fetchMore method Apollo provides to each query for pagination. This method triggers a new query and combines the results with previous results. It's perfect for use cases like pagination.\nfetchMore receives a config object as an argument. We'll give it a variables property where we'll provide the updated variable values to pass to the subsequent query.\nupdateQuery will tell Apollo how to store the new incoming posts with the existing posts. We're simply going to merge them all into a new array.\nexport default {\n  ...\n  methods: {\n    fetchMore() {\n      this.page++;\n      this.$apollo.queries.posts.fetchMore({\n        variables: { \n          offset: (page * pageSize), \n          limit: pageSize \n        },\n        updateQuery: (existing, incoming) => ({\n          posts: [...existing.posts, ...incoming.fetchMoreResult.posts]\n        }),\n      })\n    },\n  },\n}\n\nAdding scroll load\nIn the last step, we defined a method for fetching more posts from the server. Now we want to call this anytime the user scrolls to the bottom of the page.\nHow do we know if the user has reached the bottom of the page? There are various browser APIs we can use for this, but it's easier to use the package scrollmonitor to do it for us.\nSo first, install scrollmonitor using NPM.\n$ npm i scrollmonitor -S\n\nNext, add an element with id sensor to the App component underneath the list of posts. Scroll monitor will tell us when this element is visible on the page.\nIf the element is visible, it means the user has scrolled to the bottom!\nsrc/App.vue\n<template>\n  <div id=\"app\">\n    <post v-for=\"post in posts\" :key=\"post.id\" :post=\"post\" />\n    <div id=\"sensor\"></div>\n  </div>\n</template>\n\nIn the App script, import scrollmonitor. We'll then use scroll monitor to set up a callback that fires any time the element enters the page. The perfect place to set this up is in the mounted hook of the component.\nHow it works:\n\nWe use a DOM query to get a reference to the sensor element\nWe create an instance of scroll monitor to watch this element\nWe can trigger a callback when the enterViewport event of scroll monitor fires. In this callback, we'll call fetchMore (note we only call it conditional on posts being defined otherwise it may call too early).\n\nsrc/App.vue\n...\nimport scrollMonitor from 'scrollmonitor';\n\nexport default {\n  ...\n  mounted () {\n    const el = document.getElementById('sensor');\n    const elementWatcher = scrollMonitor.create(el);\n    elementWatcher.enterViewport(() => {\n      if (this.posts)  {\n        this.fetchMore();\n      }\n    });\n  }\n}\n\nWith that in place, go see your app in the browser. The infinite scroll feature means every time you reach the bottom more items appear.\nThis articles was written by Anthony Gore as part of the Hasura Technical Writer Program. If you would like to publish an article about Hasura or GraphQL on our blog, apply here.\n\nAbout the Author\nAnthony Gore provides short and sweet explanations of the latest development trends at DevNow.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL January Description: Beginning January 2021, Hasura will provide a wide variety of content, tutorials, streams, and more with which you can expand your GraphQL familiarity and start your adoption journey Author: Tyler Hannan"}, {"role": "assistant", "content": "As 2020 draws to a close, everyone at Hasura finds themselves preparing for 2021. For some, last minute holiday plans, for others budgets and hiring, for all of us...a series of lengthy conversations about you, our users (or those yet to become).We believe, strongly, that GraphQL will continue to become a key skill for developers (of all types) to learn, understand, and explore over the next year.With that in mind, we are pleased to announce:GraphQL January with Hasura - #GraphQLJanuaryBeginning 11 January 2021, the team at Hasura will be providing you with a wide variety of content, tutorials, streams, and more with which you can expand your GraphQL familiarity and start your adoption journey. This will include:Daily blogsDaily blog posts on the general theme of learning GraphQL. We know that we are all at different stages in our journey of understanding. Posts will range from a discussion of recorded talks, treatise on the future of GraphQL, beginner content, intermediate deep-dives, and more.LivestreamsOnce weekly we will host language specific livestreams on our Twitch channel. We will build a full stack app with a variety of frameworks and keep your ears open for some special guests joining us.Tutorial VideosWe all learn differently. Once a week we will take one of the front-end GraphQL tutorials hosted on our learning portal and walk you through it in 30 minutes. These will be, initially, delivered by email but later hosted for future reference on the learning portal and YouTube.Live Q&ACome join the team from Hasura in a once a week live (text) chat in our community Discord. Bring your questions...we will bring engineers and answers. The schedule will be shared via email to all registrants and, as the dates draw closer, promoted on our social media.Office HoursAre you familiar with our Hasura Office Hours? If you have questions about how Hasura applies to the GraphQL journey in your company register for this weekly, live (but still informal) Zoom chat with members of the Hasura team.Are you interested?Do you want to know what content is coming...and when? Register here for us to keep you up to date. Of course, it wouldn\u2019t be a Hasura event without an eye towards interestingly designed swag. More to come on this as the month progresses. But, trust me, you will want to be signed up."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Creating a Data Graph with GraphQL Mesh and Hasura Remote Joins Description: In this example, we will look at the ability to add an API source that GraphQL Mesh creates as a Remote Schema in Hasura. Going one step further we will try to establish relationships between Hasura's GraphQL API and APIs sourced via GraphQL Mesh. Author: Praveen Durairaju"}, {"role": "assistant", "content": "This post is a part of our Remote Joins series. Remote Joins in Hasura allows you to join data across tables and remote data sources. Data Federation using Hasura Remote Joins is now available from v1.3.0 stable release.Try it out on Hasura Cloud and let us know what you think!Creating a Data Graph with GraphQL Mesh and Hasura Remote JoinsData is typically sourced from different services. GraphQL Mesh allows you to use GraphQL query language to access data in remote APIs. The remote APIs can be Swagger/openapi, oData, gRPC, SOAP and others.GraphQL Mesh can be used as a gateway to other services, or run as a local GraphQL schema that aggregates data from remote APIs.What we are going to explore in this example is the ability to add this API source that GraphQL Mesh creates as a Remote Schema in Hasura. Going one step further we will try to establish relationships between Hasura's GraphQL API and APIs sourced via GraphQL Mesh.GraphQL Mesh can be used asSDK in Node.js codeGateway to serve GraphQL APIWe will look at using Mesh as a Gateway that exposes an endpoint that can be added as a Remote Schema in Hasura.Initialise a GraphQL Mesh projectWe can initialise a new project using yarn.yarn init -y\nyarn add graphql @graphql-mesh/cli @graphql-mesh/openapi\nWe are setting up the mesh CLI and adding the openapi client.Create a config file .meshrc.yml to specify the data source and let's add the Currency Open API.sources:\n  - name: CurrencyOpenAPI\n    handler:\n      openapi:\n        source: <path-to-open-api-schema.json>Download the Open API Spec for this example and replace the source path appropriately. This is a spec for API served at https://api.exchangerate-api.com/v4.Run GraphQL MeshRun the GraphQL Mesh instance with the following command:yarn mesh serve\nThis will run a GraphQL API at http://localhost:4000.To add this as a remote schema to the Hasura Cloud project, we will have to deploy this on a public endpoint. I'm going to make use of Codesandbox to try this out.You can create a new Sandbox/fork the above with Node.js and put in the mesh config file over there.In your Hasura Cloud project, add the graphql endpoint from codesandbox. In the above example, it would be https://00e5c.sse.codesandbox.io/graphql. Replace this if necessary with your own forked sandbox URL.Alright, now we have configured GraphQL Mesh to serve a GraphQL API over an existing OpenAPI spec.Add GraphQL Mesh Endpoint as Remote SchemaTry out the GraphQL Queryquery {\n  getLatestBaseCurrency(baseCurrency: \"USD\") {\n    base\n    date\n    rates\n    timeLastUpdated\n  }\n}\nGraphQL Query for underlying OpenAPI\u00a0This should fetch data from the underlying currency API.Remote Relationship with MeshAssuming that we have a users table with columns id, name and currency. Let's add a remote relationship called currency_rates from inside of users table.Remote Relationship with GraphQL MeshNow the GraphQL query to fetch this data in a single API call would look like the following:query {\n  users {\n    id\n    name\n    currency\n    currency_rates {\n      date\n      rates\n    }\n  }\n}Notice that, the nested query currency_rates come from exchange rate API following the OpenAPI spec. It will apply the filter of users.currency = currency_rates.baseCurrency, there by only giving exchange rates related to the current user's currency.We had directly used the currency API through the OpenAPI to GraphQL library earlier and demoed it over there. The advantage of using this via GraphQL Mesh is that, you can now add multiple sources to this mesh and you get a single GraphQL endpoint for all of these different sources combined.Should I use GraphQL Mesh as a Remote Schema?Yes, typically the frontend clients will be querying Hasura's GraphQL Endpoint and GraphQL Mesh can be added as a Remote Schema as a way to proxy to other data sources. What Hasura gives you is a neat declarative Authorization layer on top of these GraphQL APIs and hence there's a lot less code to be written. This means that any usage of Hasura as an ORM inside Mesh will lose out on these benefits and we highly recommend using the GraphQL Mesh as a remote schema.Uri Goldshtein, the creator of graphql-mesh gave a talk at HasuraCon earlier this year talking about how Hasura and GraphQL Mesh fits in together and can serve as a gateway to a bunch of remote API sources. Watch it below:Check out the various examples in the github repo that can be added as a remote source to GraphQL Mesh (and it turn available for querying via Hasura).There's another blogpost from the community talking about using GraphQL Mesh as an SDK. Do check that out as well.Do let us know in the comments on what your typical use case for GraphQL Mesh has been and how it fits in with Hasura."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcement: Hasura Cloud Achieves SOC2 Type 1 and HIPAA Compliance Certifications Description: Hasura Cloud has reached a new milestone by achieving SOC2 Type 1 and HIPAA compliance certifications by an independent auditor.  Author: Steven Yi"}, {"role": "assistant", "content": "Hasura Cloud has reached a new milestone by achieving SOC2 Type 1 and HIPAA compliance certifications by an independent auditor. With our Standard plan, you can now be confident in Hasura Cloud\u2019s ability to securely store and transmit your data, and pass the requirements of your organization\u2019s IT department.Hasura Cloud provides several additional capabilities on top of our open-source version:Managed Service: Hasura Cloud takes care of all the infrastructure, providing high-availability and auto-scaling, so you don\u2019t have to set up and maintain servers and infrastructure. \u00a0Signing up and creating a project literally takes one minute!Additional Features: Caching, analytics, and application performance monitoring (APM) integration with services like Datadog are available to increase performance and monitor your workload.Hasura Cloud is secure, highly available, and will maintain the confidentiality of your data. \u00a0Below are some additional details that illustrate why it\u2019s time to consider it for your application architecture whether you\u2019re in healthcare, financial services, manufacturing, or retail.SOC2 ComplianceTo achieve our SOC2 Type 1 certification, we implemented an exhaustive list of security controls including technical safeguards like penetration testing by an independent security firm, vulnerability scans and encryption, and process measures such as security training and disaster recovery planning. \u00a0Hasura Cloud delivers security, high availability, and confidentiality - you can be confident you can make it part of your mission-critical infrastructure and pass the requirements of your organization\u2019s IT department. \u00a0The audit report is available on request, under an NDA. As a further demonstration of our commitment to ensuring security for all our users, Hasura Cloud will also be undergoing a SOC2 Type 2 audit in Q2 2021.HIPAA ComplianceHasura Cloud\u2019s HIPAA compliance has been audited by an independent third party, and includes controls \u00a0just as stringent as that of healthcare organizations to protect the privacy, security, and integrity of protected health information (PHI). \u00a0Examples of PHI include names, addresses, phone numbers, Social Security numbers, medical records, and financial information.This means that you can safely consider Hasura Cloud for healthcare initiatives like processing electronic health records and medical billing to drive efficiencies and create better patient health outcomes and user experiences. \u00a0As part of Hasura Cloud\u2019s Enterprise plan, we can also sign Business Associate Agreements (BAA) with our customers as needed.Get Started!If you haven\u2019t tried Hasura Cloud, I encourage you to give it a try and see how quickly you can create a GraphQL API from an existing database. \u00a0Check out our Getting Started guide, as well as our online tutorials!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Data Fetching with SWR React Hooks and Hasura Description: The idea behind this example is to demonstrate usage of useSWR, mutate and trigger hooks of SWR in combination with GraphQL. Author: Praveen Durairaju"}, {"role": "assistant", "content": "From SWR's github repo:The name \u201cSWR\u201d is derived from stale-while-revalidate, a cache invalidation strategy popularized by HTTP RFC 5861. SWR first returns the data from cache (stale), then sends the fetch request (revalidate), and finally comes with the up-to-date data again.GraphQL requests are just HTTP POST requests with a JSON payload at the end of the day. It doesn't need the typical setup overhead for data fetching. Popular GraphQL clients have made it look slightly complicated than a normal fetch request counterpart, but for added benefits like caching.The SWR API is simple and looks like the one below:const { data, error, isValidating, mutate } = useSWR(key, fetcher, options)We will focus on the arguments key and fetcher. In a GraphQL API implementation with SWR, the GraphQL document will be the key that will be used. In REST APIs, the URL endpoint will usually be the key (something like /api/users. The fetcher function is an async function that returns the data. It gets the key and the logic of data fetching is written inside this. We will split our logic between HTTP POST requests and native websocket connections here.The unique key ensures that requests are not sent multiple times if used in different components in the same page.We can use simple data fetching libraries like graphqurl (read about Graphqurl 1.0) or graphql-request to send a GraphQL query. In the example below, I'm sticking to native fetch library for making the HTTP POST API calls and native WebSocket client.The idea behind this example is to demonstrate usage of useSWR, mutate and trigger hooks of SWR in combination with GraphQL. We need a GraphQL API and we are going to make use of Hasura Cloud to get it instantly.Deploy Hasura to get a GraphQL API\n\n\nClick on the following button to deploy GraphQL engine on Hasura Cloud including Postgres add-on or using an existing Postgres database:\n\n\n\nOpen the Hasura console\nClick on the button \"Launch console\" to open the Hasura console.\n\n\nCreate table users\nHead to the Data tab and create a new table called users with columns:\n\nid (text)\nname (text)\ncreated_at (timestamp now()).\n\n\n\nTry out a GraphQL Query\nquery {\n    users {\n        id\n        name\n        created_at\n    }\n}\n\n\n\nWe are going to make use of the above users query inside our react frontend.Clone the swr-graphql repo and head to the libs directory that contains the wrapper for both fetch and websockets. Change the GraphQL endpoint to point to the Hasura Cloud project URL.GraphQL QueryThe useSWR hook gets a graphql query as the key and the fetcher function is defined in getData. We have written a custom fetch method in the libs directory.import useSWR from 'swr'\n\nconst query = {\n  'query': 'query { users(limit:10, order_by:{created_at: desc}) { id name } }'\n};\n\nconst getData = async(...args) => {\n  return await fetch(query);\n};\n\nexport default () => {\n  const { data, error } = useSWR(query, getData);\n  if(error) {\n    return <div>Error...</div>\n  }\n  if(!data) {\n    return <div>Loading...</div>\n  }\n}The hook returns the data / error and the respective states can be handled on the rendering logic.GraphQL MutationOnce a mutation is done, you would want to update existing state by modifying data (addition / removals / updates). This is possible via the mutate method, where a key can be passed (GraphQL query will be the key for us) and the new data to be used along with a boolean to specify whether you want to revalidate or not.export default () => {\n  const [text, setText] = React.useState('');\n  const { data } = useSWR(query, getData)\n\n  async function handleSubmit(event) {\n    event.preventDefault()\n    // mutate current data to optimistically update the UI\n    mutate(query, {users: [...data.users, {id: uuidv4(), name: text}]}, false)\n    // send text to the API\n    const mutation = {\n      'query': 'mutation users($name: String!) { insert_users(objects: [{name: $name}]) { affected_rows } }',\n      'variables': { name: text}\n    };\n    await fetch(mutation);\n    // revalidate\n    trigger(mutation);\n    setText('')\n  }\n  return ...\n}Once the mutation is complete, we revalidate it with a trigger.GraphQL SubscriptionsFinally for subscriptions with realtime data, we have a custom wrapper defined in libs. Using the native WebSocket instance, we are creating an initial message that will be sent to the GraphQL server (containing the payload like headers).const ws = new WebSocket(GRAPHQL_ENDPOINT_WS, \"graphql-ws\");\nconst init_msg = {\n  type: 'connection_init',\n  payload: { headers: headers }\n};There's no native Subscription hook with SWR, but we will reuse useSWR to call a fetcher that initiates a new WebSocket connection.Pre-renderingAll of the above examples assumes that data loading happens on the client side. But if you are building an app which requires pre-rendering for SEO/Caching benefits, then you can pass the pre-fetched data as the initial value to the initialData option.For example:const { data, error } = useSWR(query, getData, { initialData: props.users } );This combined with getStaticProps will take care of the pre-rendering of data at the server side.Source Code - https://github.com/praveenweb/swr-graphqlDo try it out and let us know in the comments on what frontend library you prefer using for data fetching via GraphQL."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Community Tooling at Hasura Description: All the ways to keep up with the latest news, join & interact with the Hasura community and connect with the Hasura Team Author: Vishwa Mehta"}, {"role": "assistant", "content": "Intro\u201cCommunity is core to the identity of Hasura. We are the kind of people for whom open-source makes sense.\u201dWe\u2019ve written this before. It still holds true.One of the opportunities -- perhaps challenges -- in a fastly growing community is choosing the appropriate tools for interaction with users all over the globe. Recently, we\u2019ve made some changes to our community tooling and want to share our thought process as well as the outcome.Where can you find us?tl;dr:Discord - A real-time, synchronous relationship building chat with other Hasura users including quick discussions, watercooler, and language specific (japanese, spanish, etc) chat.GH Discussions - Need to troubleshoot, initiate a long-form technical discussion, show off your work, or interact with our engineering team? This is where to start.Hasura Community Newsletter - Hasura community & product highlights once monthly. It isn\u2019t spam, or a marketing list, we promise.Hasura Community Call - A monthly \u2018what\u2019s on the roadmap\u2019, early feature demos, and product updates from our engineering teamHasura Office Hours - A live, informal 30 min Zoom meeting. Bring your questions, we\u2019ll bring answers.And, of course, in your social platform of choiceWhy GitHub Discussions?Wasn\u2019t Discord good enough? Why introduce another tool? The reasons, most simply, include:Searchability of the discussion topics - It is important to ensure that answers to common questions remain discoverable and that your contributions aren\u2019t lost to chat history so that others can benefit from the questions you ask.Ease of access to a platform that is open and doesn\u2019t require joining \u201cyet another\u201d Discord serverThe ability to convert a valuable/complex discussion into an open issue and work on fixing it for our usersThe team can tend to queries and questions faster and with ease because of the UI/UX.Decision making processOf course, such a decision is not taken lightly. Such a change brings to mind a number of important questions. While a detailed answer to each question would be quite lengthy, the summary of response below should be sufficient to share our thought process. Of course, if you have more questions, you can always reach out to us to discuss (in public or private)What other options exist?What does this change mean for our community?What happens to Discord after moving to Discussions for support?If we can provide support on Discussions, why do we need Discord for? Do we need it anymore?What other options did we consider?Multiple tools exist for Q&A style interaction including Discourse (who also has an amazing team) and 3rd-party sites like Stack Overflow. While we will participate in Stack Overflow as we can based on resourcing...the primary motivation for GH is that we know that you, our user, spends a fair amount of time interacting with the tool. In addition, the ability to take an ongoing discussion and convert to an issue, maintain history, and provide tracking from question to issue to PR to merge and release was too good to ignore.What does this change mean for our community?The primary place to interact with a Hasura engineer on a technical question is Github Discussions. It is that simple.What happens to Discord after moving to Discussions? What changes?Does this change mean Discord will go away? No! We are still committed to using Discord to keep the community spirit that we have alive. However, as we add more platforms to our suite of community tools, we will be modifying channels to more accurately reflect the intent for how each tool will be used.Discord is still the central hub for our community to hang out, learn, and collaborate. We would love for you all to start moving your technical questions about using Hasura to GitHub Discussions going forward so that others can find them easily and benefit from them as well. If we recommend a new location for a question, don\u2019t take offense it isn\u2019t personal. And you can help us by recommending GitHub Discussions to new users as well. :)As part of this migration, you will probably see us begin to streamline the channel list in Discord and focus our efforts on providing a vibrant place for you to interact with the community team and other Hasura users.Also, helping new members of the community know that Discussions are now available is super useful. Please help us in this effort.If we can provide support on Discussions, why do we need Discord for? Do we need it anymore?Discord will be focused around our community, collaborations and -- hopefully someday soon -- local communities where people can meet other Hasura users who live near them.ConclusionA community is only as strong as its members. The tooling, while important, is simply a means to an end. We appreciate your patience, and support, as we grow. Thank you for being a part of the community. If you have any concerns or questions we are always available to discuss them either publicly or privately."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Parser Combinators: a Walkthrough Description: Understand Parsec better by building a small JSON parser from scratch.  Author: Antoine Leblanc"}, {"role": "assistant", "content": "Or: Write you a Parsec for Great GoodMost people\u2019s journey with Haskell will follow the same first steps. First, you need to get acquainted with the language itself, especially if you arrive at it without any knowledge of functional programming: its unusual syntax, laziness, the lack of mutable state\u2026 From there, you can move to functions and type declarations, familiarize yourself with the Prelude functions (and their known warts), and start writing your first programs. The next big step is, of course, monads, the infamous m-word[1], and how we use them to structure our programs.\nBut then, when you\u2019ve started to develop an intuition for monads\u2026 the path becomes unclear. There\u2019s a lot more that could be covered, but none of it is as fundamental. There\u2019s roughly three categories of topics one might want to explore:\n\nGoing further with the language itself: learning about language extensions, the theory behind them, and the advanced features they provide.\nLearning more about the different ways to structure a program (such as monad transformers or effect systems).\nExploring some of the most used libraries, and understanding what makes them so ubiquitous: QuickCheck, Lens\u2026 and Parsec.\n\nToday, I want to explore Parsec, and most specifically how Parsec works. Parsing is ubiquitous, and most Haskell programs will use Parsec or one of its variants (megaparsec or attoparsec). While it\u2019s possible to use those libraries without caring about how they work, I think it\u2019s fascinating to develop a mental model for their inner design; namely, monadic parser combinators, as it is a simple and elegant technique that is useful beyond the use case of parsing raw text. At Hasura, we have recently used this technique to rewrite the code that generates a GraphQL schema and checks an incoming GraphQL query against it.\nTo develop an intuition for this technique, in the course of this article, we will first reimplement a simplified version of Parsec, and then use it to write a parser for JSON values[2]. Our goal here won\u2019t be to develop a fully-fledged library, quite the opposite: we will implement the strict minimum that we need, to focus on the core ideas.\nChoosing a type representation for parsers\nFrom a high level perspective, a parser can be thought of as a translation function: it takes as input some loosely-structured data (most of the time: text), and attempts to translate it into structured data, following the rules of a formal grammar. A compiler will transform a series of characters into an abstract syntax tree, a JSON parser will transform a series of characters into an equivalent Haskell representation of the JSON value. There are several algorithmic approaches to this problem; parser combinators are an example of recursive descent: we parse each term of our grammar by recursively calling the parsers for each sub-term.\nWhat that means is that, by \u201cparser\u201d, we both mean the overall high level transformation and each of its individual steps, as each parser is expressed recursively as a combination of other parsers.\nA minimal viable implementation\nLet\u2019s build up an understanding of the type of a parser step by step. First, we have established that a parser is a function, from some given input to whatever it is that we\u2019re trying to parse. Throughout this project, we will be using String as input, for simplicity[3]. We could therefore decide to use the following to represent our parsers:\ntype Parser a = String -> a\n\nBut this type isn\u2019t enough for our use case. First and foremost: a parser can fail, if the input doesn\u2019t match what our grammar dictates. We will need to define an appropriate type to represent errors, and a parser needs to include this possibility at the type level.\ntype Parser a = String -> Either ParseError a\n\nFurthermore, parsing is a sequential operation: to parse a JSON object, one must first parse an opening curly bracket, then parse entries, then parse a closing curly bracket; recursively, to parse an entry, one must first parse a key, then a colon, then a value. Our parser type will be used to represent each of those steps, and we don\u2019t expect each step to fully consume the input string: each parser must therefore also return something that indicates where we are in the input stream, how much of it was consumed, or what\u2019s left to process.\nSince we\u2019re using String as our input stream, it is enough for each parser to simply return what\u2019s left of the input string[4]. The Parser type that we will use is thus:\nnewtype Parser a = Parser {\n  runParser :: String -> (String, Either ParseError a)\n}\n\nWriting elementary parsers\nAs is common in Haskell, we start by covering the \u201caxiomatic\u201d base cases, and we then generalize / extrapolate from there. Given that our stream is a String, and our individual tokens are just the characters of said string, our two base cases simply correspond to the two constructors of a list: there are tokens left in the input stream, or we\u2019ve reached the end. We call those two functions any and eof, respectively:\nany :: Parser Char\nany = Parser $ \\input -> case input of\n  -- some input left: we unpack and return the first character\n  (x:xs) -> (xs, Right x)\n  -- no input left: the parser fails\n  []     -> (\"\", Left $ ParseError\n    \"any character\"        -- expected\n    \"the end of the input\" -- encountered\n  )\n\neof :: Parser ()\neof = Parser $ \\input -> case input of\n  -- no input left: the parser succeeds\n  []    -> (\"\", Right ())\n  -- leftover data: the parser fails\n  (c:_) -> (input, Left $ ParseError\n    \"the end of the input\" -- expected\n    [c]                    -- encountered\n  )\n\nThose are the only two basic parsers that we need! They cover the two base cases: we have a character to parse, or none. Everything else can be expressed in terms of those two, and by using combinators.\nSequencing parsers\nAs mentioned earlier, parsing is sequential. We will often need to express something along the lines of: I want to apply some parser A, then some parser B, and use their results. Let\u2019s look at how we would implement a parser for a JSON object entry, for instance: we need to parse a json string, then a colon, then a json value (we will assume those individual parsers already exist). The resulting code is\u2026 suboptimal: repetitive, error-prone, and hard to read:\njsonEntry :: Parser (String, JValue)\njsonEntry = Parser $ \\input ->\n  -- parse a json string\n  case runParser jsonString input of\n    (input2, Left err)  -> (input2, Left err)\n    (input2, Right key) ->\n      -- on success: parse a single colon\n      case runParser (char ':') input2 of\n        (input3, Left err) -> (input3, Left err)\n        (input3, Right _)  ->\n          -- on success: parse a json value\n          case runParser jsonValue input3 of\n            (input4, Left err)    -> (input4, Left err)\n            (input4, Right value) ->\n              -- on success: return the result\n              (input4, Right (key, value))\n\nThis is made quite cumbersome by both the fact that each step might fail, and by the fact that each step returns the input for the next one. Furthermore, it forces us to know about the internal structure of a parser to know how to chain them. To make things easier, we can extract that pattern into a standalone function:\nandThen :: Parser a -> (a -> Parser b) -> Parser b\nparserA `andThen` f = Parser $ \\input ->\n  case runParser parserA input of\n    (restOfInput, Right a) -> runParser (f a) restOfInput\n    (restOfInput, Left  e) -> (restOfInput, Left e)\n\nThis function allows us to rewrite our JSON entry parser in a way that no longer requires explicitly introspecting each parser along the way.\njsonEntry :: Parser (String, JValue)\njsonEntry =\n  -- parse a json string\n  jsonString `andThen` \\key ->\n    -- parse a single colon\n    char \u2018:\u2019 `andThen` \\_ ->\n      -- parse a JSON value\n      jsonValue `andThen` \\value ->\n        -- create a constant parser that consumes no input\n        constParser (key, value)\n\nBy the power of monads!\nTo some readers, this andThen pattern will feel familiar; that\u2019s because andThen is exactly the same as Monad\u2019s bind operator:\n(>>=) :: Parser a -> (a -> Parser b) -> Parser b\n\nBy making our Parser type an instance of Monad, we can leverage the power of all the functions that come with it and the much more comfortable syntax of do notation for our parsers[5]. At last, our jsonEntry function can be rewritten in a concise and straightforward manner:\njsonEntry :: Parser (String, JValue)\njsonEntry = do\n  key   <- jsonString\n  _     <- char \u2018:\u2019\n  value <- jsonValue\n  return (key, value)\n\nAs a sidenote, it is worth mentioning that while do notation makes the sequencing explicit, we will also make use of operators throughout the code (this wouldn\u2019t be \u201ctrue Haskell\u201d otherwise :P). Most notably, the following Functor and Applicative operators:\n(*>) :: Parser a -> Parser b -> Parser b\n(<*) :: Parser a -> Parser b -> Parser a\n(<$) :: a -> Parser b -> Parser a\n\n-- ignore leading spaces\nspaces *> value\n\n-- ignore trailing spaces\nvalue <* spaces\n\n-- substitute a value\nTrue <$ string \u201ctrue\u201d\n\nRecap\nWe have so far defined our two elementary parsers, and seen how chaining is a monadic operation. The stage is set: our basic needs are covered. We can now start expressing more interesting parsers. To start with, let\u2019s introduce satisfy: a small wrapper around any that allows us to check whether the next character in the input meets some arbitrary requirement.\nsatisfy :: String -> (Char -> Bool) -> Parser Char\nsatisfy description predicate = try $ do\n  c <- any\n  if predicate c\n    then return c\n    else parseError description [c]\n\nThis function, while straightforward, uses other functions we haven\u2019t encountered yet, such as try. It\u2019s our first combinator!\nCombining parsers\nFormally, a combinator is a function that does not rely on anything but its arguments, such as (.) the function composition operator:\n(f . g) x = f (g x)\n\nBut informally\u2026 a combinator is something that combines other things. And it is in that informal sense that we use it here! In our context, parser combinators are functions on parsers: functions that combine and transform parsers into other parsers, to handle such things as backtracking, or repetition.\nChoice, errors, and backtracking\nWe know that parsers can fail: that\u2019s why we have the Either in their type after all. But not all errors are fatal: it is possible to recover from some of them. Imagine, for instance, that you are trying to parse a JSON value: it could be an object, or an array, or a string\u2026 To parse such a value, you could try to parse the opening curly braces of an object: if that succeeds, you can continue with parsing an object; but if that fails immediately, you can instead try to parse the opening bracket of an array, and so on. It is with error and backtracking that we can implement our first \u201cadvanced\u201d feature: choice.\nTo distinguish between a real error that happened further down the line or something that simply was the wrong choice, we make a distinction between parsers that fail without consuming any input, that fail immediately, and parsers that fail later on: we assume that if any input was consumed, then we were on the right \u201cbranch\u201d. But looking only one character ahead will not always be enough to decide whether a branch is correct, and in that case we need to be able to backtrack should a branch fail: we need a backtracking combinator. This is what try does: it transforms a parser into a backtracking one: one that consumes no input on failure, that restores the state to what it was. It makes sense to use it for satisfy: if the predicate fails, we haven\u2019t encountered a single character we could unpack, and we should leave the input string unmodified.\ntry :: Parser a -> Parser a\ntry p = Parser $ \\state -> case runParser p state of\n  (_newState, Left err) -> (state, Left err)\n  success               -> success\n\nThroughout this project, we name our combinators the exact same way Parsec does. The operator that Parsec uses to represent choice is the same as the one defined by the Alternative typeclass: (<|>). Its semantics are simple: if the first parser failed without consuming any input[6], then try the second one; otherwise, propagate the error:\n(<|>) :: Parser a -> Parser a -> Parser a\np1 <|> p2 = Parser $ \\s -> case runParser p1 s of\n  (s', Left err)\n    | s' == s   -> runParser p2 s\n    | otherwise -> (s', Left err)\n  success -> success\n\nWith this operator, we can finally implement a much more convenient combinator: choice. Given a list of parsers, try them all until one succeeds.\nchoice :: String -> [Parser a] -> Parser a\nchoice description = foldr (<|>) noMatch\n  where noMatch = parseError description \"no match\"\n\nRepetition\nOne last group of combinators we will need for our project is repetition combinators. Those do not require any internal knowledge of our Parser type, and are a good example of what kind of high-level abstraction we can now write. As usual, we use the same names as Parsec: many is the equivalent of a regex\u2019s star and matches zero or more occurrences of the given parser, while many1 is the equivalent of plus: it matches one or more occurrences:\nmany, many1 :: Parser a -> Parser [a]\nmany  p = many1 p <|> return []\nmany1 p = do\n  first <- p\n  rest  <- many p\n  return (first:rest)\n\nThanks to those, we can also implement sepBy and sepBy1, which match repeated occurrences of a given parser with a given separator between them:\nsepBy, sepBy1 :: Parser a -> Parser s -> Parser [a]\nsepBy  p s = sepBy1 p s <|> return []\nsepBy1 p s = do\n  first <- p\n  rest  <- many (s >> p)\n  return (first:rest)\n\nRecap\nThat\u2019s it: those seven combinators are all we need to implement a JSON parser from scratch; at this point, we already have a good enough minimal reimplementation of a Parsec-like parser combinators library!\nOf course, a fully-fledged library would implement much more: more primitives for handling characters, combinators for optional parsers, more specific repetition combinators, support for better error messages\u2026 But this is beyond the scope of this exercise.\nParsers in practice: parsing JSON\nTo put together everything we\u2019ve seen so far, let\u2019s walk through the process of building the grammar of a JSON parser, step by step. We will do it from the bottom up: starting with parsers for individual characters, then moving on to parsers for syntax, then for scalars\u2026 until we can finally express a parser for an arbitrary JSON value. The goal here is to showcase how at each step we can leverage the simpler abstractions we have built, in order to compose something more complex. This section is a bit more code-heavy, but it is my hope that, if you\u2019ve followed everything so far, you will find it as pleasant to read as I do!\nWe will use the following representation for JSON values, which is very close to what Aeson[7] defines:\ndata JValue\n  = JObject (HashMap String JValue)\n  | JArray  [JValue]\n  | JString String\n  | JNumber Double\n  | JBool   Bool\n  | JNull\n\nRecognizing characters\nUsing the functions of Data.Char, let\u2019s start defining our grammar by defining the kind of characters that we want to recognize: this is a straightforward use of our satisfy function:\nchar c = satisfy [c]     (== c)\nspace  = satisfy \"space\" isSpace\ndigit  = satisfy \"digit\" isDigit\n\nLanguage syntax\nTo go further, we can define convenient syntax functions, using the aforementioned Applicative operators:\nstring = traverse char\nspaces = many space\nsymbol s = string s <* spaces\n\nbetween open close value = open *> value <* close\nbrackets = between (symbol \"[\") (symbol \"]\")\nbraces   = between (symbol \"{\") (symbol \"}\")\n\nsymbol\u2019s implementation here is based on Parsec\u2019s definition of a lexeme (in their language definition library), which always skips trailing whitespace; every parser can therefore be safely written with the assumption that there is no leading whitespace it needs to take into account. Thanks to this approach, there are very few explicit mentions of whitespace in our JSON grammar.\nScalars\nHere is where our implementation will differ from the JSON standard. For the sake of simplicity, our parser for numbers will only match natural numbers:\njsonNumber = read <$> many1 digit\n\nFor booleans, we simply match the two possible cases:\njsonBool = choice \"JSON boolean\"\n  [ True  <$ symbol \"true\"\n  , False <$ symbol \"false\"\n  ]\n\nAs for strings, we match a sequence of characters between two double quotes, but we have to handle the possibility that some characters might be escaped. We only handle a small subset of escaped characters, implementing the rest of the specification is left as an exercise to the reader. :)\njsonString =\n  between (char '\"') (char '\"') (many jsonChar) <* spaces\n  where\n    jsonChar = choice \"JSON string character\"\n      [ try $ '\\n' <$ string \"\\\\n\"\n      , try $ '\\t' <$ string \"\\\\t\"\n      , try $ '\"'  <$ string \"\\\\\\\"\"\n      , try $ '\\\\' <$ string \"\\\\\\\\\"\n      , satisfy \"not a quote\" (/= '\"')\n      ]\n\nArrays and objects\nJSON values are by definition recursive: arrays and objects contain other JSON values\u2026 To continue with our bottom-up approach, we will be assuming the existence of a top-level jsonValue parser, which will be defined last.\nA JSON object is a group of individual entries, separated by commas. We first parse them as a list, using our repetition combinator, then transform said association list into a HashMap:\njsonObject = do\n  assocList <- braces $ jsonEntry `sepBy` symbol \",\"\n  return $ fromList assocList\n  where\n    jsonEntry = do\n      k <- jsonString\n      symbol \":\"\n      v <- jsonValue\n      return (k,v)\n\nFinally, an array is simply a group of values, between brackets, separated by commas:\njsonArray = brackets $ jsonValue `sepBy` symbol \",\"\n\nPutting it all together\nAt last, we can express the top-level parser for a JSON value:\njsonValue = choice \"a JSON value\"\n  [ JObject <$> jsonObject\n  , JArray  <$> jsonArray\n  , JString <$> jsonString\n  , JNumber <$> jsonNumber\n  , JBool   <$> jsonBool\n  , JNull   <$  symbol \"null\"\n  ]\n\nAnd\u2026 that\u2019s it!\nWrapping up\nThis article is the result of my personal experience trying to understand the inner workings of Parsec, after using it for years. I tried my hand at writing a small JSON parser from scratch, on the side, as I was learning about what makes Parsec tick, to gain a better understanding by putting it in practice. I was astonished, once done, by how small and concise the resulting code was, and by how straightforward the JSON grammar was. It is my sincere hope that this walkthrough will prove useful to you, and will give you an insight into the beauty of parser combinators!\nAll the code in this article can be found in this GitHub gist.\nFor a more advanced example of using parser combinators, if you want to read more about how we used this technique to reimplement our GraphQL schema generation, you can have a look at the description of the PR that introduced it.\nHasura is, of course, hiring. If discussions like the above are something you find compelling, do look at our open roles and apply. If you would like to keep an eye on what the team is building, where we are speaking, and an occasional baby animal gif... the Hasura Community Newsletter is a monthly delivery without the traditional marketing spam.\nThank you for reading!\n\n\n\nOn a personal note: a hill I am ready to die on is that monads are not complicated, but often poorly taught, or taught in a way that is not properly tailored for the audience. I am seriously considering running some kind of \u201can intuition for monads if you already know at least one other programming language, in less than 15 minutes or your money back\u201d program. \u21a9\ufe0e\n\nFor the sake of simplicity, the code in this article will not be fully standard-compliant: we will only handle natural numbers, and a reduced set of escapable characters. \u21a9\ufe0e\n\nParser combinator libraries do not directly use String as an input; partly because there are better types available to represent text, such as Text, but also because a parser isn\u2019t always the first translation step: often, such as in the case of a compiler, a lexer or tokenizer will have already transformed the input text into a sequence of lexemes or tokens. What matters for a parser is that the input data can be linearly iterated upon; Parsec calls this a Stream: a String is a stream of Char, and the output of a tokenizer would likewise be a stream of tokens. \u21a9\ufe0e\n\nMost libraries choose to pass along a State record that wraps the input streams and carries additional information, such as current position, which allows for (among other benefits) much better error messages. \u21a9\ufe0e\n\nWhile Parsec and its derivatives all use this monadic approach, it is possible to build parsers that only rely on Applicative. It is a trade-off: monadic parsers are more powerful, as parsing can branch depending on what was parsed before, but applicative parsers being more restricted allow for static introspection of values. \u21a9\ufe0e\n\nDoing string comparison to check whether the parser failed to consume any input is extremely inefficient, and an unfortunate consequence of our simplified design. If instead of simply passing a String along as state we were to use a proper State record, we could implement this in a more performant manner, by comparing a value that represents our position in the input stream for instance. \u21a9\ufe0e\n\nAeson is the most widely used Haskell library for JSON manipulation. \u21a9\ufe0e\n\n\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcement: Hasura Cloud Available In 8 Regions Description: Today we have added 3 new regions where you can deploy your Hasura Cloud projects: London, Montreal, and Sydney. Author: Steven Yi"}, {"role": "assistant", "content": "Last month, we announced that you could deploy your Hasura Cloud projects to 5 different regions. \u00a0Today we have added 3 new regions - Sydney, Montreal, and London. \u00a0Here's a list of the 8 regions we now support:Asia Pacific (Mumbai)Asia Pacific (Singapore)Asia Pacific (Sydney) - NEW \ud83d\udca5 Canada East (Montreal) - NEW \ud83d\udca5 Europe (Frankfurt)Europe (London) - NEW \ud83d\udca5 US East (Ohio)US West (Northern California)Region selection has the potential to improve the performance of your applications by reducing data latency between your web front-end, your Hasura Cloud GraphQL endpoint, and the databases and services you\u2019re connecting. \u00a0Deploying your app closer to your user base will also reduce response time and create a better experience for your users. \u00a0Deploying your Hasura Cloud project in a specific region may help with requirements you have for data residency or data localization.Try It OutSelecting a region is very easy. \u00a0Just go to cloud.hasura.io - create a project and then you can select a region to deploy to. \u00a0Free projects will only be able to deploy to US-East, but if you upgrade to a commercial plan you can deploy to any region.Migrating existing projects to different regions is just as easy - just go to the project settings! \u00a0In the future we\u2019ll continue to add new regions.Easily select a region in the Hasura Cloud dashboardGet StartedIf you haven\u2019t tried Hasura Cloud, I encourage you to give it a try and see how quickly you can create a GraphQL API from an existing database. \u00a0Check out our Getting Started guide, as well as our online tutorials!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How to secure GraphQL APIs with Hasura Cloud Description: In this post, we will look at how to secure GraphQL APIs with various defense mechanisms available in Hasura Cloud. Without the right protections, you are opening up your application network to malicious attacks and potentially exposing data in some cases. Author: Praveen Durairaju"}, {"role": "assistant", "content": "In this post, we will look at how to secure GraphQL APIs with various defense mechanisms available in Hasura Cloud. Without the right protections, you are opening up your application network to malicious attacks and potentially exposing data in some cases.Why security in GraphQL is different?GraphQL is fundamentally different from REST APIs in the way it is served - API is served over a single endpoint. This means that URL based filtering cannot be applied to GraphQL APIs. Morever REST APIs rely on request methods like GET, POST, PUT and DELETE where as GraphQL is typically served over POST (or websockets for realtime). Attackers can crawl GraphQL endpoint (typically served at /graphql) and exploit interface consoles like GraphiQL if not protected behind auth.There's more to this, but this gives us an initial idea of why the approach to GraphQL needs to be different.Let's begin by looking at what constitutes a GraphQL query:query {\n  user(id: 1) {\n    id\n    name\n  }\n}The above is the GraphQL document and the full http request/response would look something like the one below:The JSON body with the query object will dictate what is to be returned back to the client. So can I ask for whatever I want from the server? Hmm, ideally not. Let's look at various attack vectors below.IntrospectionGraphQL Introspection queries expose the underlying schema and lets you try out all combinations of queries/mutations/subscriptions that are available in the endpoint. Although this is useful during development, it is highly recommended to disable Introspection in production. There's one thing to notice though. The schema exposed via Introspection queries is typically protected behind auth and hence not everything is exposed by default.With Hasura it is possible to enforce this through Allow List. Once an Allow List is enabled, only the queries explicitly pre-approved will work. So in case, you want introspection queries to work, you will need to explicitly add them to the list. Once this is configured, you don't need to worry about exposed UI interfaces like GraphiQL.Allow List can be populated by selecting the actual queries made to the server instead of manually entering them.Allow Lists in HasuraAuthenticationAuthentication in GraphQL is not much different than what you would typically do in REST APIs. You can authenticate with any provider (email/password, social sign-in) and pass in Authorization header on every request. Hasura supports Authentication via different methods.JWTWebhooks where you can write your own custom logicUnauthenticated / public accessThis is infact the pre-GraphQL layer and any production ready Authentication system should work well with GraphQL (and Hasura) to fix this security level.AuthorizationNow comes the tricky bit, Authorization. Hasura supports role-based access control at row and column level of your database schema.Role based permission system with HasuraTaking the example of the above GraphQL query, we can select which fields are allowed to be queried and which role is allowed to query. Hasura supports boolean expressions to configure row level checks and this makes it powerful. For example, we can traverse through relationships to make a check and also combine multiple conditions using operators like _and and _or.{\n  \"_and\": [\n    {\n      \"user\" : {\n        \"id\": {\n          \"_eq\": \"x-hasura-user-id\"\n        }\n      }\n    },\n    {\n      \"user\": {\n        \"karma\": {\n          \"_gte\": 500\n        }\n      }\n    }\n  ]\n}Read more examples of using Hasura's permission system effectively or build a slack clone using Hasura's Authorization layer.Admin SecretTo make sure that your GraphQL endpoint and the Hasura console are not publicly accessible, you need to configure an admin secret key. If you want to give restricted access to the console in an organisation team setup, you can add collaborators to your Hasura Cloud Project and give them access to only view the metrics of the project and not allow GraphQL API access.Read more on how to add an admin secret to a Hasura project.Tip: Do not ever use an admin secret on a frontend client.API LimitsPublic facing APIs are subject to DDoS attacks. To prevent abuse, we need to limit the number of API calls being made for a particular GraphQL query or any query. In some cases, this will be applicable for a role and in some case this will be applicable for a given user id.Role based: API limits can be configured at the role level i.e the number of requests per minute that can be made by a given role. Parameter based: To make it more granular, you can also specify parameters like IP address and session variables like x-hasura-user-id or custom x-hasura- variables to filter.Depth based: Depending on the relationship depth, the graphql operation can go n level deep and can create a really complex SQL join and slow down the API responses. Based on the application requirement, the maximum depth of a graphql operation should have a reasonable limit to prevent abuse.Configure GraphQL API Limits per roleSome of these settings are a bonus if Allowed List is configured right, since most of the filtering happens at that layer.External APIsApart from the instant GraphQL APIs, you have custom business logic written in different servers that are added to Hasura via Actions, Remote Schemas and Events. For example with Actions and Event triggers, the REST API configured can be called directly if not protected by a shared secret between Hasura and the API server. In case of a remote schema, appropriate authorization header needs to be configured for the initial loading of the schema. But to ensure granular permissions, Forwarding client headers can be enabled and the API server can handle the request based on respective client.Restrictive CORSBy default, Hasura allows all CORS requests. For example, if your application is hosted on a domain, say https://example.com, you can allow any requests to come from this and any of its subdomains by enabling the config HASURA_GRAPHQL_CORS_DOMAIN=\"http://*.example.com\" Of course this restriction applies only on the client side (browser).TimeoutsThe other attack vector is to make a query which takes a really long time to execute. There are broadly two types of timeouts:Database timeout: In case a database connection or a query takes a long time to respond, Hasura can return a timeout error. There's a statement_timeout setting in Postgres to abort any statement execution if it's taking longer than specified milliseconds.HTTP timeout: If you are using Actions/Remote Schemas/Events, you would have configured http endpoints for respective requests. You can configure a timeout setting as well to abort the query execution. Http endpoints make different queries in their underlying logic and configuring a timeout is recommended to abort connections and respond quickly back to the client.Hasura's production check-list is a good resource to go over to address top level security concerns before going live with your application.Resources:Authorization and security patterns for GraphQLHasura Authorization System through ExamplesAccess Control Patterns with Hasura"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing the Open-Source Hasura Data Dictionary Description: The Hasura Data Dictionary is an open-source project to make it easy to understand your database relationships, GraphQL schemas, and create complex GraphQL queries. Author: Steven Yi"}, {"role": "assistant", "content": "We\u2019re excited to release the initial version of the Hasura Data Dictionary, an open-source project to make it easy to understand your data graph - which includes your database relationships, and GraphQL schemas. It also helps you easily create complex GraphQL queries. \u00a0Clone the code from this Github repo, which even includes a Docker compose file to get started in minutes on your local machine or infrastructure.Gavin Ray, Technical Evangelist at Hasura, took me through how it works in this short video interview.We created this project for three reasons:Help developers and teams quickly reference and understand the tables and relationships of the databases they work with, and the generated GraphQL schemas in their Hasura projectsQuickly build complex GraphQL queries through an intuitive, easy to use interfaceCreate a resource for the community to contribute, and use as a starting point to extend for their own specific needs, such as for content management.Quickly Understand Entity & Schema RelationshipsIn the Data Dictionary, the Data Models gives you a view of the tables in your databases and also includes search capability. \u00a0Selecting a table will take you to the detail view, showing the table columns, field data types and indexes. \u00a0You\u2019ll also see the Query and Mutate GraphQL operations available to execute on that table - fetch, delete, insert, and update.At the bottom of the Data Models view, you also get a visual entity-relationship view that illustrates the table relationships, making it easier to understand how the database is structured. \u00a0The Data Graph view shows the entire macro view of all data relationships:Build Complex GraphQL QueriesThe GraphiQL tab makes it easy to experiment and create complex queries that would otherwise be difficult to do by hand. \u00a0In the example below, we\u2019ve constructed a GraphQL query that aggregates all the invoices for a specific customer and sums the total simply by checking the fields to report on and the available operations. \u00a0In this example we selected invoices_aggregate, and selected the sum of the total to generate our query - we didn\u2019t need to memorize the exact GraphQL syntax to get it right.Extend and ContributeWe hope you find the Data Dictionary useful and accelerates your development, or even embedded in your apps to help with configuration and administration. \u00a0Contribute to the project if you think of any great features! \u00a0There are two components to the Data Dictionary - the front end which we\u2019ve shown you here, and a backend API that provides the metadata information. \u00a0Right now the Data Dictionary supports Postgres databases, but in the near future there will be support for MySQL and SQL Server. \u00a0Get Started!Download the code in our Github repo to get started, which includes setup instructions and a Docker compose file to quickly get started!If you\u2019re not on Hasura Cloud yet, get a free account and follow this easy tutorial to create a GraphQL endpoint in minutes."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Hasura Monitoring with DataDog Description: We\u2019re excited to announce that you can monitor your Hasura projects with Datadog.  Available for Hasura Cloud\u2019s Standard Plan and with Hasura Enterprise for on-prem or private cloud deployment, this feature lets you send operation logs to your organization's Datadog dashboard. Author: Steven Yi"}, {"role": "assistant", "content": "We\u2019re excited to announce that you can now monitor your Hasura projects with Datadog. \u00a0Available for Hasura Cloud\u2019s Standard Plan and with Hasura Enterprise for on-prem or private cloud deployment, this feature lets you send operation logs to your organization's Datadog dashboard. \u00a0Our docs have a great tutorial on getting started.In Datadog you can observe all the GraphQL queries and mutations made against your Hasura project's API, and well as query success, failure, errors, and any exceptions that may occur. \u00a0With this integration, you\u2019ll now be able to monitor and quickly troubleshoot every tier and component of your application\u2019s stack - from the UI, back-end database, and now your Hasura data access and logic layer.Getting this configured in Hasura Cloud is simple and takes just a few minutes. \u00a0To set this up In Hasura Cloud, your project settings have a new tab called Integrations and you\u2019ll see the option to add Datadog monitoring.All you need to do is enter the Datadog API key, and a few fields that you can define yourself - Host, Tags, and Service Name - Datadog will use these to help you identify and search for Hasura metrics in their dashboard.Get Started!Read the docs on how to export metrics from your Hasura Cloud project. \u00a0If you haven't already, sign up for a Hasura Cloud Standard Plan to start monitoring your Hasura project with Datadog.We also have an always-free plan and great getting started tutorial if you\u2019re new to Hasura and GraphQL. \u00a0Try it out!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: RedwoodJS with Hasura = \u2764\ufe0f Description: How to build a fullstack, JAMstack application using RedwoodJS, GraphQL and Hasura. Author: Vilva Athiban P B"}, {"role": "assistant", "content": "\nRedwoodJS is the new sensation in full-stack application development. It is highly opinionated with a choice of technologies.  This blog is a step by step tutorial on how to use Hasura with Redwood JS and make it double awesome.\nBefore we move forward, let\u2019s have a small introduction about our two participants Redwood and Hasura.\nRedwood JS:\n\nRedwood is an opinionated, full-stack, serverless web application framework that will allow you to build and deploy JAMstack applications with ease.\n\nRedwood JS uses a lot of the latest bleeding technology and abstracts all the complexity away from the developers. It uses React with Apollo for the frontend and has a lot of code generators like pages, cells, etc which makes development very easy. It also takes care of configuring tests, storybooks, etc out of the box. It uses Prisma for its GraphQL backend and enables you to create quick backends with DashBoard/GUI support. Sounds fascinating? Not just that, it supports out of the box deployment with Netlify, AWS, etc.\nIn one line, the developer has to write only the styles and business logic. Everything else has been taken care of.\nHasura:\n\nInstant GraphQL with built-in authorization for your data\n\nHasura is a futuristic tool that lets you built GraphQL backends instantly from your existing database. It makes creating GraphQL backends as simple as a walk in the park.\nThe Problem\nMy equal love for both the tools created a desire for using them together. But Redwood JS was highly opinionated and didn't have any provisions to do it out of the box. This made me explore their codebase and figure out simple steps to start building applications with RedwoodJS + Hasura.\nWhy Hasura?\nIf we just focus on frontend parts of Redwood, it gives you literally everything you need, out of the box. It lets you create pages and control the routes manually as well (in case of customization), it lets you create cells along with stories and tests pre-configured. They will support React Native and Electron in the future. So, it's kind of an ideal frontend with all complexity abstracted (similar to Next.js, but more GraphQL focused).\nAny frontend engineer who uses redwood would start loving it. But the constraint is, it's highly opinionated and you are obliged to use default integrations and this can be a lot of work. Hasura, on the other hand, is truly a one-stop GraphQL solution with redwood. It can be directly consumed by the Web side (what redwood calls its frontend) and also it supports auth/webhooks etc. Especially when the DB is Postgres, Hasura is pure black magic. Also with Prisma it's again the developer's responsibility to look after scaling/performance etc while Hasura does it out of the box.\nStep 1: Build a Hasura GraphQL backend\nRefer to this detailed tutorial by Hasura to build a GraphQL backend here. I am using one of their demo apps for this tutorial - https://learn-graphql.demo.hasura.app/console.\nThis backend gives me a set of queries on Super Heros data. We will use the following query to fetch the required data:\n    query getSuperHeros {\n      superhero_aggregate {\n        nodes {\n          name\n          team {\n            name\n          }\n        }\n      }\n    }\n\nStep 2: Bootstrap a Redwood JS app\nTo bootstrap a Redwood JS app, run the following command.\nyarn create redwood-app my-redwood-app\n\nThis will create a full-stack boilerplate with two subfolders web and api. Since we are going to use Hasura for the backend, we will not require this api folder. Hence, this folder can be deleted.\nStep 3: Create a Page\nA Page is a screen/route of the app. Each Page gets loaded in a particular route. To create a Page, run the below command:\nyarn redwood generate page SuperHero /\n\nThis will create a new Page called SuperHero inside the pages folder and will load the page in the route / ie: root route. You can replace / with /custom-route to load it in the desired path.\nThe page looks like this:\nimport { Link, routes } from '@redwoodjs/router'\n\nconst SuperHeroPage = () => {\n  return (\n    <>\n      <h1>SuperHeroPage</h1>\n      <p>\n        Find me in \n        <code>./web/src/pages/SuperHeroPage/SuperHeroPage.js</code>\n      </p>\n      <p>\n        My default route is named <code>superHero</code>, link to me with `\n        <Link to={routes.superHero()}>SuperHero</Link>`\n      </p>\n    </>\n  )\n}\n\nexport default SuperHeroPage\n\nStep 4: Setup Hasura in RedWoodJS\nIn the generated Redwood JS files, the src/index.js file has the following code:\nReactDOM.render(\n  <FatalErrorBoundary page={FatalErrorPage}>\n    <RedwoodProvider>\n      <Routes />\n    </RedwoodProvider>\n  </FatalErrorBoundary>,\n  document.getElementById('redwood-app')\n)\n\nHere, the entire App is wrapped inside RedWoodProvider which is from the redwoodjs library. Reading through the source code, this provider uses ApolloProvider underneath.\n/**\n * Create a GraphQL Client (Apollo) that points to the `apiProxyPath` that's\n * specified in `redwood.toml`.\n */\nexport const createGraphQLClient = (config: GraphQLClientConfig) => {\n  return new ApolloClient({\n    uri: `${window.__REDWOOD__API_PROXY_PATH}/graphql`,\n    cache: new InMemoryCache(),\n    ...config,\n  })\n}\n\n/**\n * A GraphQL provider that instantiates a client automatically.\n */\nexport const GraphQLProvider: React.FC<GraphQLProviderProps> = ({\n  config = {},\n  ...rest\n}) => {\n  return <ApolloProvider client={createGraphQLClient(config)} {...rest} />\n}\n\nand the provider gets the config as props in RedWoodProvider as below:\n/* Modified a bit to shorten the code */\nconst RedwoodProvider: React.FC<RedwoodProviderProps> = ({\n  useAuth = window.__REDWOOD__USE_AUTH,\n  graphQLClientConfig,\n  children,\n  ...rest\n}) => {\n    return (\n      <GraphQLProvider config={graphQLClientConfig} {...rest}>\n        <FlashProvider>{children}</FlashProvider>\n      </GraphQLProvider>\n    )\n}\n\nConsidering the above piece of source code, we can override the existing set up by passing the config in props of RedWoodProvider.  The src/index.js is modified accordingly:\nReactDOM.render(\n  <FatalErrorBoundary page={FatalErrorPage}>\n    <RedwoodProvider graphQLClientConfig={{\n      // URL of your Hasura backend\n      uri: 'https://learn-graphql.demo.hasura.app/v1/graphql' \n    }}>\n      <Routes />\n    </RedwoodProvider>\n  </FatalErrorBoundary>,\n  document.getElementById('redwood-app')\n)\n\nNote: You can pass any config supported by Apollo Client through the graphQLClientConfig props.\nStep 5: Generate a Cell\n\nCells are a declarative approach to data fetching and one of Redwood's signature modes of abstraction.\n\nCells are auto-generated files, that have scaffolding to fire a GraphQL query and display loading/error states or data based on the response of the query. It reduces the efforts of retyping the boilerplate code. You can create a cell using the following command:\nyarn rw g cell SuperHeros\n\nThis command will generate a Cell inside the src/components folder along with scaffolded tests and storybooks. The file looks like this:\nexport const QUERY = gql`\n  query SuperHerosQuery {\n    superHeros {\n      id\n    }\n  }\n`\nexport const Loading = () => <div>Loading...</div>\nexport const Empty = () => <div>Empty</div>\nexport const Failure = ({ error }) => <div>Error: {error.message}</div>\nexport const Success = ({ superHeros }) => {\n  return JSON.stringify(superHeros)\n}\n\nHere we will also have to change the query and the components based on our UI/UX requirements. Let\u2019s modify the above file to consume our query:\nexport const QUERY = gql`\n  query getSuperHeros {\n    superhero_aggregate {\n      nodes {\n        name\n        team {\n          name\n        }\n      }\n    }\n  }\n`\nexport const Loading = () => <div>Loading...</div>\nexport const Empty = () => <div>Empty</div>\nexport const Failure = ({ error }) => <div>Error: {error.message}</div>\nexport const Success = ({ superhero_aggregate }) => {\n  return (\n    <div>\n      {superhero_aggregate.nodes.map((hero) => (\n        <li>{hero.name} - {hero.team.name}</li>\n      ))}\n    </div>\n  )\n}\n\nNow lets import and use it in our pages/SuperHeroPage/SuperHeroPage.js file as below:\nimport SuperHerosCell from 'src/components/SuperHerosCell';\nconst SuperHeroPage = () => {\n  return (\n    <>\n      <h1>SuperHeroPage</h1>\n      <SuperHerosCell />\n    </>\n  )\n}\nexport default SuperHeroPage\n\nExecute the command yarn rw dev to run the application. The app opens in http://localhost:8910/ by default. The output looks like this:\n\nTadaa! We have successfully created an app with RedwoodJS and Hasura.\nBonus:\nRedWood JS is actively building a lot of tools and to take advantage of all the tools and support, we will also have to modify graphql.config.js file.\nReplace the contents of the file with the below code and you are ready to enjoy all the benefits RedwoodJS offers along with Hasura's superpowers!\nmodule.exports = {\n  // URL of your Hasura backend\n  schema: `https://learn-graphql.demo.hasura.app/v1/graphql`, \n}\n\nA sample implementation of Hasura + Redwood can be found here. Happy coding\n\nAbout the author\nThis blog post was written by Vilva Athiban P B, a JavaScript developer working for Omio as part of the Hasura Technical Writer Program.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: AWS Aurora: Is it for you? Description: Aurora is the new kid in the block. It leverages AWS\u2019s infrastructure to deliver a 1-click enterprise-grade database system. You\u2019ll have in this post the ins-and-outs of Aurora, whether it works with Hasura (hint: it does), and how to set it up with Hasura Cloud. Author: Gunar Gessner"}, {"role": "assistant", "content": "Aurora is the new kid in the block. It leverages AWS\u2019s infrastructure to deliver a 1-click enterprise-grade database system. You\u2019ll have in this post the ins-and-outs of Aurora, whether it works with Hasura (hint: it does), and how to set it up with Hasura Cloud.\nWhat is Aurora?\n\nAurora Storage Subsystem\nServerless\nHow much does it cost\nWhat are its limitations\nDoes it work well with Hasura and Hasura Cloud\n\n\nWhat is Hasura Cloud\n\nHow much does it cost\n\n\nSetting up AWS Aurora with Hasura Cloud\n\n1. Set up an Aurora database\n2. Set up a Hasura Cloud account\n3. Accessing the Hasura Console\n4. Clean up\n\n\nConclusion\n\nWhat is Aurora?Aurora is a SQL database-as-a-service offering from Amazon AWS. You can think of it as a fully managed relational database engine that is compatible with MySQL and Postgres. It leverages AWS\u2019s infrastructure to deliver higher performance, scalability, and reliability. You might know AWS RDS already, so how is Aurora different? In two big ways, its storage subsystem and its serverless capabilities.Aurora Storage SubsystemThe Aurora storage subsystem is a distributed storage architecture. It really does leverage all of AWS\u2019s infrastructure to squeeze out every bit of availability and performance.No need to provision storage space ahead of time\u2014storage space scales automaticallyAutomatic six-way replication of the data across multiple Availability ZonesContinuous backup to Amazon S3Greater than 99.99% availabilityTransparent recoveries from storage failuresInstance failover in less than 30 secondsAbility to restore to a point-in-timeServerlessAurora has two engines. Provisioned and serverless.The provisioned engine works similar to what you\u2019d expect from a managed SQL database. You specify the instance class size (e.g. db.r5.large) and if you need to increase read throughput, you can create read replicas.The serverless engine is different. It doesn\u2019t require you to specify an instance class size. You just create a database endpoint and AWS will automatically scale it up and down according to the workload. It\u2019s recommended for intermittent or unpredictable workloads.Whenever we say \u201cserverless,\u201d you might think of scaling-to-zero. Meaning the service would pause itself automatically after receiving no requests for some time. And while you can indeed set up Aurora Serverless to scale to zero, you probably don\u2019t want to do it in production. The warm up time for a paused database takes almost a minute, and I\u2019m going to assume your users aren\u2019t willing to wait that long for a request to resolve. That being said, it\u2019s easy to disable the feature, which will have your cluster running a minimum of 1 Aurora Capacity Unit (ACU) at all times.How much does it cost?Aurora is priced per DB instance-hour consumed. Then if you want to save some cash\u2014and who doesn\u2019t\u2014you can look into purchasing a Reserved Instance, in a contract of 1 or 3 years. But that\u2019s besides the point. The real question is, how much money does it save? In order to set up a globally distributed database system like Aurora does, it would take you hundreds of engineering hours to development and maintenance.It\u2019s a good deal from a pure costs point of view. And even though Aurora is not open-source, there isn\u2019t really a lock-in problem. You can always take out your data, move it to a Postgres database, and host anywhere.What are its limitations?Aurora Postgres is just Postgres. As you would expect. It works with any Postgres-compatible software. In terms of scalability, its storage constraints (e.g. tables up to 32 terabytes) and network constraints are so generous you\u2019ll probably never hit any of those limits.However, the Serverless engine does have some limitations you should be aware of.Does it work well with Hasura and Hasura Cloud?Yes.The provisioned engine works out-of-the-box. All you\u2019ll need to do is create a public Aurora Provisioned cluster. You\u2019ll find the step-by-step instructions below.The serverless engine works with Hasura but unfortunately does not work with Hasura Cloud at this moment. The problem is that an Aurora Serverless DB cluster can\u2019t have a public IP address, which means you can\u2019t point Hasura Cloud to it. Hosting your own Hasura service still works though. Just deploy Hasura to EC2 on the same VPC as your Aurora cluster\u2014but you\u2019ll lose all the benefits below. VPC Peering support in Hasura Cloud would, of course, address this issue. Keep your eyes open as more will be coming on that front soon.What is Hasura Cloud?Hasura Cloud is a fully managed Hasura service. As you\u2019d expect, Hasura Cloud scales automatically, guarantees high availability, and offers global distribution. But having a managed infrastructure is just half.In addition to all the features of the open-source Hasura software, the Cloud version offers:Distributed tracingAutomatic caching of shared dataRate limitingRegression testsRead replicas\u2026and moreHow much does it cost?It\u2019s free.Hasura Cloud has a free-forever tier. I myself have one instance online at all times for personal use. It\u2019s super useful. You can use it to learn about Hasura, to experiment with new features, and to create prototypes for new applications. It\u2019s as easy as it gets.For production, the Pay-as-you-go tier starts at $99/mo. It\u2019s an auto-scaling, high availability offering\u2014and includes all the free features. The starting price includes 20GiB of data ingress/egress, after which costs $2 per gigabyte.See full pricing page here: https://hasura.io/pricing/Setting up AWS Aurora with Hasura CloudFollow the steps below to set up an AWS Aurora cluster and connect it to a free Hasura Cloud account.1. Set up an Aurora databaseCreate database:On the AWS Console, go to RDS.Click: \u201cCreate database\u201dChoose a database creation method:Select: \u201cStandard create\u201d\u2014 For the purpose of this post, we\u2019ll pick the \u201cStandard create\u201d option as it offers some customization we\u2019re going to be needing. For example, we\u2019ll need to set the database to public, so that Hasura Cloud can access it.Engine options:Engine type: Amazon Aurora\u2014 Otherwise what are we even doing here!?Edition: PostgreSQL\u2014 Although you could try out MySQL as well.Capacity type: Provisioned\u2014 As I\u2019ve mentioned above, as of today it\u2019s impossible to give a Serverless cluster a public IP address\u2014which we\u2019d need in order to allow Hasura Cloud to connect to it.Engine version: Aurora PostgreSQL (Compatible with PostgreSQL 11.18)\u2014 Hasura works with Postgres versions 9.5 and above.Templates:Templates: \u201cDev/Test\u201d\u2014We won\u2019t need high availability or any of the other super cool Aurora features for our present goal of testing Aurora and Hasura.Settings:DC cluster name: aurora-hasura-experiment\u2014 You\u2019re welcome to choose a different name.Master username: postgres\u2014 In production environments you\u2019ll want to use a randomly generated username for increased security.Auto generate a password: Yes\u2014 This tells AWS to generate a password for us (d\u2019oh). Alternatively, use your own password generator.DB instance size:DB instance class: db.t3.medium\u2014 I\u2019ve picked the smallest class available, which will set you back roughly $60/mo.Availability & durability:Multi-AZ deployment: Don\u2019t create an Aurora Replica\u2014 Unless you\u2019re setting up a production environment, you won\u2019t need an Aurora Replica.Connectivity:Virtual private cloud (VPC): Default VPC\u2014 I\u2019ve picked my default VPC but you might already be using that one, in which case go ahead and select your desired VPC.Subnet group: Default\u2014 Same rationale as above.Public access: Yes\u2014 We\u2019re going to need the database cluster to be publicly accessible, so that Hasura Cloud is able to reach it.Security group: Default\u2014 I\u2019ve picked the default security group here. Further down in this post we\u2019ll add our rules to it.Database port: 5432\u2014 I\u2019m going with the default Postgres port to keep things simple.Database authentication:Database authentication options: Password authentication\u2014 Password authentication is enough as Hasura Cloud uses password to connect to the Postgres engine.Additional configuration:Initial database name: mydatabase\u2014 You\u2019ll need to remember the name for later. If you don\u2019t create a database now you\u2019ll have to connect to your newly created Aurora cluster and create a database manually. Better to just create one here and get on with it.Click: \u201cCreate database\u201dConnection details:After clicking \u201cCreate database\u201d above the AWS Console should\u2019ve taken you to the RDS dashboard. On the top, you should see the confirmation message.Click: \u201cView credential details\u201dCopy: Master usernameCopy: Master passwordCopy: Endpoint\u2014 We\u2019re going to use this information later, to point Hasura Cloud to Aurora.Click: \u201cClose\u201dVPC configuration:Next, you\u2019ll need to allow Hasura Cloud to connect to our database cluster. A few things need to be in place for this. The default VPC has, by default, an internet gateway attached to it\u2014so you won\u2019t need to do that. And if you followed the instructions so far, you created the Aurora cluster with the option Engine set to Provisioned, and Public Access set to Yes. Now all that\u2019s left for you to do is to create a Security Group, which will allow your DB cluster to be connected from Hasura Cloud\u2019s IP.The quickest way to that from the RDS dashboard screen is as follows:Click: On the DB instance (i.e. second row)Click: On the VPC security group that is active (i.e. default, probably)Security Groups:Click: \u201cInbound rules\u201dClick: \u201cEdit inbound rules\u201dInbound rules:Now you\u2019ll create the Inbound rules. See the image below.The first rule allows Hasura Cloud to connect to our DB cluster. Hasura Cloud\u2019s IP is 3.23.154.30. The interface will automatically append \u201c/32\u201d to it, which makes it a valid CIDR notation\u2014basically saying \u201cthis IP and no other.\u201dThe second rule will allow traffic coming from my machine. This is not necessary for Hasura Cloud to work but it's nice to have in case you want to check things out locally.Type: As aboveClick: \u201cSave rules\u201dYou\u2019re done!Great! The AWS part of our work here is done.Before moving forward, I\u2019d recommend you go back to the AWS Console and make sure the cluster\u2019s status changed from Creating to Available. It might take a few minutes and you\u2019ll have to remember to refresh the page.If you\u2019d like to test the cluster locally\u2014and if you\u2019ve added the \u201cMy IP\u201d rule as above\u2014you can now use a tool like psql to test the connection to the new database.Now onto Hasura Cloud.2. Set up a Hasura Cloud accountCreate an account: Go to cloud.hasura.io\u2014 It\u2019s free to create an account and also to use the free tier of the Cloud offerings.Connect Your Database:Once you\u2019re signed in:Click: \u201cI have an existing Postgres database\u201dIn the next screen, you\u2019re going to insert the Postgres Database URL. A Postgres Database URL has the following format postgres://<username>:<password>@<hostname>:<port>/<database name>. Here\u2019s what mine looks like: postgres://postgres:FFNMI3SpmeTV20sU4hSS@aurora-hasura-experiment.cluster-cgaopxl80gyv.us-east-1.rds.amazonaws.com:5432/mydatabase.Fill in: Postgres Database URLClick: Create ProjectWait: Until the button on the top-right hand changes from \u201cInitializing\u2026\u201d to \u201cLaunch Console\u201d\u2014 It only takes a few secondsIf for some reason you get a red button with an error message, here\u2019s what you could do:Double-check your Postgres Database URL. Make sure there were no copy and paste mistakesDouble-check your Inbound rules in the Security group \u2014 For example, if you have allowed connections from your \u201cMy IP\u201d, you could run `psql postgres://\u2026` just to confirm the cluster is accessible.Click: Launch Console3. Accessing the Hasura ConsoleIf everything worked, you should find yourself on the Hasura Console. You\u2019re done!4. Cleaning upIf this cluster you\u2019ve set up was for experimentation only, you\u2019ll want to shut it all down before clocking out. Otherwise AWS will charge you upwards of $60/mo. I\u2019m going to show you now how to turn it all off.Did you know that Heroku Postgres has a free plan and it\u2019s integrated with Hasura Cloud?Cleaning up AWS:Go to: The RDS DashboardSelect: Your cluster instance\u2014 By deleting the instance we\u2019ll be deleting the whole cluster as wellClick: Actions > DeleteDisable: Create final snapshot\u2014 By the way, snapshots are paidClick: I acknowledgeType: \u201cdelete me\u201dCleaning up Hasura Cloud:Hasura Cloud is free. So you could keep this project running for zero costs. But if you\u2019ve deleted your Aurora cluster you might want to delete your Hasura Cloud project as well, in order to keep things tidy.Go to: https://cloud.hasura.io/projectsClick: Gear icon (Settings)Click: Delete Project (at the bottom)ConclusionAurora is fantastic. I\u2019m astounded with its storage subsystem. It's so simple. With just a few clicks, you get enterprise-level performance, durability, and availability. Combine Aurora and Hasura Cloud and you get a globally distributed stack\u2014both the database and the API layers geographically close to your end users\u2014for an unprecedented price. That's impressive. This setup would normally cost hundreds of engineering hours to set up and countless more for maintenance. If you need a distributed database system, look no further.Let us know on twitter or in the comments if you have any questions, suggestions, or feedback!About the authorThis blog post was written by Gunar Gessner - a seasoned Software Engineer and Product Manager based in London, under the Hasura Technical Writer Program."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Use Hasura Cloud In 5 Regions Description: We\u2019re excited to announce that you can now deploy your Hasura Cloud projects to different geographies.  Based on your feedback these regions are now available:\nAsia Pacific (Mumbai), Asia Pacific (Singapore), Europe (Frankfurt), US East (Ohio), and US West (Northern California). Author: Steven Yi"}, {"role": "assistant", "content": "We\u2019re excited to announce that you can now deploy your Hasura Cloud projects to different geographies. \u00a0This has been something many of you have asked for - based on your feedback these regions are now available:Asia Pacific (Mumbai)Asia Pacific (Singapore)Europe (Frankfurt)US East (Ohio)US West (Northern California)Region selection has the potential to improve the performance of your applications by reducing data latency between your web front-end, your Hasura Cloud GraphQL endpoint, and the databases and services you\u2019re connecting. \u00a0Deploying your app closer to your user base will also reduce response time and create a better experience for your users.Here is the latest on regions for Hasura Cloud (May 2021).Try It OutSelecting a region is very easy. \u00a0Just go to cloud.hasura.io - create a project and then you can select a region to deploy to. \u00a0Free projects will only be able to deploy to US-East, but if you upgrade to a commercial plan you can deploy to any region.Migrating existing projects to different regions is just as easy - just go to the project settings! \u00a0In the future we\u2019ll continue to add new regions.Easily select a region in the Hasura Cloud dashboardGet StartedIf you haven\u2019t tried Hasura Cloud, I encourage you to give it a try and see how quickly you can create a GraphQL API from an existing database. \u00a0Check out our Getting Started guide, as well as our online tutorials!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Evolutionary Database Design, GraphQL, APIs, and Database Schema Migrations Description: As a recent joiner to the Hasura team, I find myself learning, and exploring, GraphQL through the lens of my prior experience. In this series, I am going to share my point-of-view on Evolutionary Database Design, GraphQL, and schema migrations. Author: Adron Hall"}, {"role": "assistant", "content": "NOTE: Check out the subsequent parts of this post in \"Automated Schema Migrations\" and \"Schema Migrations with Hasura Console\".IntroductionAs a recent joiner to the Hasura team, I find myself learning, and exploring, GraphQL through the lens of my prior experience. As I delve into the technology more deeply, I find it intriguing to explore the amazing things we are building through my full stack and database background.In this series, I am going to share my point-of-view on Evolutionary Database Design, GraphQL, and schema migrations. I have begun exploring this topic in a livestream of which the first VOD (Video on Demand) is available in \u201cDatabase Migrations: Patterns and Best Practices\u201d.Evolutionary Database DesignEvolutionary Database Design is a practice that has been around for a long time and can be applied to any database storage medium. It could be a fancy pants NoSQL database like Apache Cassandra, Mongo, Neo4j, one of a gazillion relational databases like Postgres, MySQL, Maria DB, SQL Server, DB2, or the ole' Oracle DB. Whichever database, Evolutionary Database Design -- both in concept and practice -- seeks to help with several key requirements for any database:It synchronizes and provides better integration for those responsible for management of the database(s), likely the Database Administrators (DBAs) in enterprises, to work more concisely and effectively with those responsible for application development against the database(s).With almost all databases, developers each get their own database or at least database model mocks to develop against.The database schema itself is versioned, providing a way to accurately and safely iterate during application and database development or refactoring.As with code of the application, the migrations for the databases are managed in the same or parallel repositories, thus version controlled. Keeping a synchronized versioning across the application and database for deployment and in some cases rolling back to previous versions.As refactoring, iterative development, or other mutation of the code and database occur automation should be applied to manage these changes moving forward and for rolling back. As I've often paraphrased; \"manually doing this is ok, do twice and ponder automation, do thrice and it's time to automate.\"Evolutionary Database Design doesn't stop there. There are additional offshoots of database management that can benefit from the patterns and process of Evolutionary Database Design. These include, are not limited to, but include the following;Managing multiple versions of databases for developers, production, etc.Shipping changes with application through automation and related efforts.Using multiple applications with a database, or multiple databases with an application, or multiple databases with multiple applications interlined across each other.Check out the references at the end of this blog entry for further material to read about Evolutionary Database Design.My 2 Cents on Evolutionary Database DesignWith some of the hostility towards Agile (upper case), agile (lower case), scrum, and some of the ideas often mentioned along with Evolutionary Database design it is extremely important to not conflate this practice and pattern of development with those ideas and practices. There is no tight coupling between these things, Evolutionary Database Design, Database Schema Migrations, and related practices around database development and refactoring are not tightly coupled, they stand on their own and respectively can be used as such.With that stated, it is important to note that this design acumen and practice around managing one's databases, their development, and the respective APIs and application development related to those databases is extremely effective. It's going on 20 plus years and is still a trusted and liked practice to keep development in an always known state. It folds well into modern mindsets of development as well as new infrastructure capabilities and development ideas such as Infrastructure as Code (IaC).GraphQL and APIsMany of the relational databases have a plethora of options focused around accessing and managing data in a database. Very specifically relational databases like Postgres, SQL Server, MySQL, Oracle, and the like. These could be Object Relationship Mappers, or ORMs, Active Record, or a number of other patterns. A modern approach that has risen from all of these various patterns is to use GraphQL via an API as the access pattern over whatever the database is. One can implement a GraphQL Server themselves, per spec, over whatever data source they have, and there are many options that are built that offer server capabilities via API (Apollo, Hasura, and others).With the advent of this layer, where GraphQL is provided via API access, there is a need to manage the metadata and API schema of the system along with any database schema itself. What this manifests as, when working with an evolutionary database design practice, is to work with the API as if it is the database itself. This way during development one would iterate on the API and the respective schema of the API in conjunction with the database versioning done with the schema migrations. With each solution one might have to build custom tooling to do this, but in this post as I move into examples of process and flow around database schema migration I'll be using the Hasura CLI. This CLI gives us prebuilt capabilities to keep the API schema and database schema in versioned synchronization.Enough theory and ideas, let's talk about some real examples.What\u2019s Next: Database Schema MigrationsIn upcoming posts, I will work through two discrete scenarios. One building on the other. The first is a scenario in which a green field application, just getting started, needs to be initialized and set up for schema migrations. There are two primary examples that I've provided in the following: using the user interface to build out or start the initial schema migration, and a secondary option to build all elements of the schema migration process manually. Both of these options use the Hasura CLI to cover metadata and related collateral for organizing the iteration, version, and folder structure creation.ReferencesEvolutionary Database DesignMartin Fowler's & Pramod Sadalage's Evolutionary Database Design PostRefactoring Databases by Pramod Sadalage and Scott AmblerWikipedia Database RefactoringDatabase Refactoring SitePramod Sadalage's and Scott Ambler's Twitter AccountsDatabase Schema Migrations & Metadata MgirationsDocs: Hasura Migrations & Metadata v2Migration Katas: Common Database and Migration Workflows with HasuraHasura Migrations [GraphQL engine, pragmatic overview, 2020]Migrations on the Hasura GraphQL EngineSchema and Metadata migrations with Hasura"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Design Philosophy Description: In this post, I\u2019m going to talk about the product vision of Hasura, our technical design philosophy, how Hasura compares to other technologies in the landscape and finally some FAQs about Hasura. Author: Tanmai Gopal"}, {"role": "assistant", "content": "Our design philosophy, technical goals, & FAQs.In this post, I\u2019m going to talk about the product vision of Hasura, our technical design philosophy, how Hasura compares to other technologies in the landscape and finally some FAQs about Hasura.If you just want to start quickly, then our Hasura Tutorials provide tutorial content across a variety of frontend frameworks, Hasura backend tutorial, and also an introduction to GraphQL fundamentals. \n The problem\n Hasura\u2019s technical design\n\n Inspiration & prior work \n Hasura architecture \n\n\n Key Hasura capabilities\n    \n Production ready in minutes \n Instant GraphQL \n Authorization model \n Application level caching \n Driving stateless business logic: Transactions and eventing \n Realtime APIs \n Change management & CI/CD \n Data and control plane federation \n No-ops \n Avoiding language/frame-work lock-in \n\n\n Hasura FAQ\n    \n What if my data models change? Isn\u2019t Hasura deeply coupled to the database? \n How do I customise Hasura\u2019s APIs? \n Does Hasura force my consumers to use GraphQL? \n How do I use Hasura for existing applications? \n\n\n\n1. The ProblemAt Hasura, our mission is to make application development faster than ever before. As the technology landscape today evolves, we believe the key bottleneck that needs to be addressed is making data accessible.Especially in enterprise environments, adopting best-of-breed technologies, modernizing applications or building new capabilities is critically dependent on being able to work with online/realtime (OLTP) data.Two important trends have emerged over the last few years:Data is no longer in a single data-source. Operational data is increasingly split across multiple sources viz. workload-optimised databases, SaaS solutions and even internal servicesDevelopers are consuming data in insecure & unauthorized compute environments (eg: web/mobile apps) and increasingly stateless & concurrent compute environments (eg: web/mobile apps, containers, serverless functions).This necessitates building a data API that is able to connect to multiple data-sources, absorb some domain-specific logic, and provide the required security and performance/concurrency.This is tedious work and a solid time-suck. It is work that is semi-repetitive (eg: integrating different sources, exposing CRUD APIs) and also simultaneously domain specific (eg: authorization rules).Hasura\u2019s crazy idea is to convert the tedious aspects of building this API into a service. Instead of spending months writing code to map thousands of tables, embed authorization rules and building APIs, Hasura allows developers to declaratively configure a web-server in minutes and expose a production-ready API. From building something by hand to getting that \u201cas a service\u201d.Hasura provides the data API \"as a service\" on your data sources.This allows Hasura users to move extremely fast in environments where they wouldn\u2019t have imagined it possible. From indie devs, startups, to Fortune 10 organizations we hear again and again that Hasura has allowed them to deliver their APIs and build modern apps in record time with record predictability. This is especially valuable for enterprise where all new capabilities (apps or APIs) have to connect to existing online data and they\u2019re not just building entirely greenfield applications.2. Hasura\u2019s technical designDesigning a product like Hasura is an extremely dangerous and risky affair. Capturing what developers are used to doing by hand and automating portions of that to become a service is not something one should just wade into.As we started to build Hasura (many years ago, before even GraphQL was launched as an open-specification!), we looked into what kinds of products and technologies have done something similar that we could take design inspiration from.2.1. Prior work & design inspirationIf we take a step back and think about technology that was able to massively increase developer productivity, by abstracting away the repetitive portions of work into a service and also simultaneously capture domain-specific requirements, the best examples that come to mind are: Databases!A database provides a metadata engine at runtime that is able to capture the domain-specific requirements provided by a developer and then provide a service that allows the developer to operate on data - without the tedious work of actually dealing with raw files. Consider, for example, the difference between Lucene and Elasticsearch. :)Using its metadata, a database engine is able to operate entirely at runtime (as a service) without any build-time work on the part of the developer. A declarative metadata system is useful, because the database engine can effectively \u201cthink about itself\u201d and automatically abstract away the grunt work over a predictable but flexible API. Furthermore, a database server is now able to provide guarantees and SLAs on performance, security, availability and on other operational aspects out of the box.When we started thinking about technical design primitives for Hasura, we took heavy inspiration from the database world, having worked extensively with databases like Postgres, building search services with Lucene, synchronizing data from normalized to de-normalized data stores and other similar OLTP data tasks.A convenient way to think about Hasura is that it is the frontend of a distributed database engine that works at the web-tier to support, ahem, web-scale loads. Check out the table below that draws analogies from a database world to the Hasura GraphQL engine.\n\n\n Database \n Hasura \n\n\nSpeaks SQL over TCP\nSpeaks GraphQL over HTTP\n\n\nExpose models & methods over a relational algebra\nExpose data models and methods over a unified JSON abstraction\n\n\nMetadata captures modelling rules\nMetadata captures mapping, relationships and authorization rules\n\n\nEngine talks to storage directly\nEngine talks to other databases and sources over the network\n\n\nImplements transaction engine and a write-ahead-log to drive complex database operations with fault-tolerance\nImplements an event-engine to drive stateless business logic with atleast-once or exactly-once semantics.\n\n\n\n2.2. Hasura approachHasura implements a metadata engine that can capture the domain-specific requirements of a web service that provides data access. Using that, Hasura provides out-of-the-box web APIs that allow flexible and secure operations on data.At the same time, Hasura is able to provide the SLAs and guarantees on so-called NFRs (non-functional requirements) to ensure smooth operations.In the diagram below, you can see how Hasura maps web API concepts into a declarative web-service aka \"a GraphQL engine\".Let\u2019s dig into details of these capabilities and features and we\u2019ll see how Hasura\u2019s design is able to tackle these technical challenges.3. Key Hasura Capabilities3.1. Production ready in minutesThe most important business value that Hasura provides is reducing time to market, by unblocking developers. Everything that we do at Hasura is through the lens of making sure that getting productive is hyper-fast. We've designed Hasura and the developer tooling around it to make sure grokking Hasura concepts and operating Hasura is as easy and fast as possible. 3.2. Instant (GraphQL) APIs on your dataOnce the Hasura service starts running, developers can dynamically configure its metadata. This configuration can be done using a UI, or using an API call, or using code in a git repo that is \u201capplied\u201d to Hasura to integrate best with your CI/CD pipeline.This metadata captures a few key things:Connections to data sourcesMapping models from the data sources into an APIRelationships between models within the same data-source and across data sourcesAuthorization rules at a model or method levelUsing this metadata, Hasura generates a JSON API schema and presents a web API that can be consumed. Hasura provides this unified JSON API over GraphQL. GraphQL is a particularly great fit, since it is native to JSON. As an API spec it can capture the flexibility required to operate on a large number of data models with a variety of methods, while at the same time provide a controlled boundary required by a web service.Although Hasura presents itself as a web-service, Hasura is quite a JIT compiler, Hasura takes incoming GraphQL API calls over HTTP and then tries to achieve theoretically optimal performance while delegating the data fetches to downstream data-sources.Hasura can avoid N+1 issues entirely, common with GraphQL backend implementations. It can memoize aggressively, and issue the minimal number of hits to upstream data sources.For a detailed deep-dive into how Hasura does this with Postgres & SQL read this blogpost: Architecture of a high-performance GraphQL to SQL web server3.3. Built-in authorizationHasura\u2019s metadata and query engine are able to effectively present your entire world of data as a unified JSON document that is queryable. However, data access can not be automated and solved for unless without embedding authorization rules.Developers are consuming data in insecure environments. Unlike many decades ago where a bank building would have the database, and the Java applications would be running on trusted machines being used by trusted employees. As owners of data today, we want to be able to give developers access to the right scoped data (and methods on that data) wherever they are and whatever they may be doing with it.\n\n\nTypes of developers\nTypes of APIs\n\n\nDevelopers outside the org\nPublic APIs\n\n\nDevelopers in different teams\nInternal APIs but with governance and QoS controls\n\n\nDevelopers building apps for external users\nExternal facing APIs, but with restricted surface area to prevent abuse by end-users\n\n\n\nHasura has an authorization engine that allows developers to embed declarative constraints at the model level. This allows Hasura to automatically use the right authorization rules for any given API call that may access lists of models, parts of the models, models through relationships. Because these authorizations are declarative and entirely available at runtime (via APIs), it allows developers and Hasura as a product, to provide additional observability into the authorization system. Examples:Authorization audit: What authorization rules were used/triggered in a particular API call by and end consumerTooling to review and manage authorization rulesHasura\u2019s authorization system resembles the declarative system of RLS (or Row Level Security) that some database engines offer but brought to the application layer. Incidentally, we built the first version of Hasura\u2019s authz system at the same time as Postgres RLS was first introduced many years ago. A happy coincidence! \ud83e\udd13Authorization rules are conditions that can span any property of the JSON data graph and its methods:\nAny property of the data spanning relationships\n\t    \nEg: Allow access if document.collaborators.editors contains current_user.id\n\n\nAny property of the user accessing the data\n        \nEg: Allow access if accounts.organization.id is equal to current_user.organization_id\n\n\n\n    Rules can mask or tokenize or encrypt portions of the data model or the data returned by a method and are labelled. These labels are called \"roles\"\n    \nThis allows Hasura to provide RBAC, ABAC, hierarchical multi-tenant permissions etc. For example, this blogpost describes how you can setup a Google Cloud IAM like permissions system with Hasura: https://hasura.io/blog/authorization-rules-for-multi-tenant-system-google-cloud/There are significant advantages to a declarative authorization system because it allows Hasura to perform several optimizations automatically:Authorization + data access in a transaction: Often the data fetch and the authorization rule check are present in the same system. This allows the fetch and the authz check to happen \u201catomically\u201d so that the authz rule is not invalid by the time the data is fetched.Authorization predicate push-down: Wherever possible, Hasura can automatically push-down the authz check in the data query itself. This provides a significant performance benefit and avoids additional lookups where it can be avoidedAuthorization caching: Hasura can automatically update and cache authentication information or authorization checks as required which is work that one would have to do for hot queriesAutomated private-data caching: Because Hasura can \u201cthink\u201d about the authorization rules, Hasura can also automatically determine if multiple GraphQL queries made by different users will actually end up fetching the same data. This part of application level caching, aka knowing \u201cwhat to cache\u201d which developers would manually build can now be automated by Hasura.Notably, Hasura doesn\u2019t provide a built-in authentication mechanism. In most existing environments, especially enterprise, the authentication service is often an external system. Also with the rise of amazing SaaS and open-source identity management and authentication \"as-a-service\" providers Hasura can now continue to solve for what it does best. For example, Hasura works seamlessly with Keycloak, Okta, Auth0, Azure AD, Firebase Auth and entirely custom community-built solutions like hasura-backend-plus that provide an authentication service for Hasura. The list above is not exhaustive in the least, but is intended to convey the variety of authentication solutions that exist and that Hasura can integrate well with!3.4. Application level cachingApplication level caching is one of the major reasons why developers often need to manually build web APIs that provide data access manually. Certain API calls are extremely frequent or extremely expensive and computing the data fetch is too expensive for the upstream data source. Developers then build caching strategies that use domain knowledge of knowing what queries to caching, for which users (or user groups) and use a caching store like Redis to provide API caching.Because Hasura\u2019s metadata configuration contains detailed information about both the data models and the authorization rules which contain information about which user can access what data, Hasura can implement API caching for dynamic data automatically.Cache invalidation, which is the second part of this problem, is a HARD problem. Most commonly, to avoid worrying about caching invalidation vs consistency, as developers we use TTL based caching and let the API consumers deal with the inconsistency. Hasura has deep integrations into the data sources, and since all data access can go through Hasura or using the data-source\u2019s CDC mechanism, Hasura can theoretically provide automated cache invalidation as well. This part of the caching problem is analogous to the \"materialized view update\" problem.We\u2019ve taken our first steps to automate application level caching and have a long way to go, but the future and potential is extremely exciting :)I gave a detailed talk about building automated caching strategies at GraphQL Summit a few months ago which might be interesting to you if you\u2019d like to learn more about how we think about caching at Hasura!3.5. Driving stateless business logicDriving stateless business logic is becoming increasingly harder in the microservices and polyglot data landscape that we have.Event driven patterns have emerged as a solution to deal with high-volume transactional loads and to orchestrate a sequence of business logic actions that execute exactly once and handle failures without requiring roll-backs. These patterns also allow business logic to be as stateless as possible. Business logic resembles \u201cpure compute\u201d that is invoked on an event. Any data required during the execution is accessed over HTTP APIs.However, the plumbing required to do this is prohibitive to getting started.Hasura implements an eventing system that provides developers an easy way to write stateless business logic that is integrated with Hasura over HTTP.Hasura captures and delivers events over HTTP and makes certain guarantees (atleast-once, exactly-once) to help create workflows that allow developers to be as productive with stateless business logic as they would have been writing code in a stateful monolith! Hasura's eventing integrations:Change Data Capture integrations with data-sources to atomically capture and reliably deliver data events over HTTP. Read more.API events: API calls made by end-users are mapped to events that are delivered to backing business logic, over HTTP. Hasura can provide a synchronous or asynchronous boundary. Read more.Time based events: Hasura can trigger events based on a time schedule or a timestamp and deliver them over HTTP to business logic. Read more.For example, a data change event can be captured by Hasura and delivered to a REST endpoint that is backed by a serverless function or a microservice.Supporting traditional transactions: However, when an API consumer is speaking to a single data-source which supports transactions Hasura can also expose transactions over websockets and GraphQL. This allows business logic authors to use transactions the way they normally would and gradually move to an event based system if required.3.6. Realtime APIs (aka Subscriptions)Hasura integrates with databases and can provide a scalable realtime API to end API consumers. This is kind of like a WATCH API or a GraphQL subscription to watch changes on a particular resources.Because of its authorization system, Hasura can ensure that it can send the right events to the right consumers.Realtime APIs over websockets are notoriously painful to manage in production because they are surprisingly stateful and expensive. Websockets also require implementing another layer of web security.Read more about how Hasura provides a scalable subscriptions API on Postgres here:https://github.com/hasura/graphql-engine/blob/master/architecture/live-queries.md3.7. Change management & CI/CDIf you\u2019re familiar with the JAMStack ecosystem, you would have come across the idea of \u201cpreview URLs\u201d. These are endpoints that host versions of your static app in various stages of your development process. These are extremely useful during development, staging, QA etc for human stakeholders.Blue/green deployments or A/B rollouts are also a joy because these are static assets available at dedicated endpoints. Via some simple configuration, DNS rules, or load-balancer configuration you can benefit from this ability to publish multiple versions of your app simultaneously and split traffic between them.Hasura\u2019s goal is to bring this ability to dynamic APIs!Because Hasura is an API system that is entirely configured at runtime and can isolate it\u2019s metadata configuration in an external system, you can do the following with ease:Deploy a new version of your API, against the same data-source to preview a metadata change (like new model mappings or authorization rules)Run multiple versions of Hasura against the same data-sources to ensure a reliable roll out.3.8. Federated data & control planeIn case you missed it, Hasura\u2019s metadata system is entirely dynamic and configurable via APIs at runtime.We\u2019re building an authorization system on the Hasura metadata APIs. This allows you as an owner of the API to federate control of the metadata to relevant data-source owners.Your online data is already present across multiple sources which need to be federated. Hasura makes that tractable by providing the missing link, a federated control plane.Read more about data federation with Hasura and Remote Joins here.3.9. No lock-in to a specific language/frameworkHasura presents itself as a service and provides integration points to other services over the network viz. Native connections to data systems and over HTTP for API consumption or for driving business logic.This allows Hasura users to bring their favourite language, runtime, framework and hosting vendor when using Hasura.This also allows us to build Hasura in a relatively insulated and decoupled environment while encouraging adoption of the latest and greatest technology trends.3.10. No-opsOur vision as a product, and also critical to ensure that Hasura adds recurring business value, is to allow developers to move as quickly to the \u201cNo Ops\u201d future :)Because Hasura is a self-contained service, we\u2019re able to build an auto-scaling experience that actually works, building all the right integrations with APM and tracing tooling so that the data API service doesn\u2019t impose a heavy operational burden.Even more important, is Hasura\u2019s ability to natively integrate with the health-check, availability, replication, scalability and circuit breaking patterns required by upstream data-sources.4. FAQ\n4.1. What if my data models change? Isn\u2019t it a bad idea to couple the API to the data sources?Hasura does not derive it\u2019s API from a database. Technically, Hasura derives its APIs from the metadata configured by the user. This allows Hasura to handle the all the problems that emanate from exposing data models directly, while still benefitting from the approach.Data models are meant to change, which could happen for several reasons:Data models evolve (additively or destructively) with business requirementsNormalize to make writing data easierDenormalize to make reading data fasterChange databases entirelyHasura\u2019s metadata engine allows developers to redefine their model mappings while preserving the end APIs. While this could use database concepts like views, this can be done without requiring any additional DDL on the database as well, because Hasura metadata can contain the right mappings. For example, suppose an entire table ceased to exist, Hasura\u2019s metadata can contain a named query (similar to a view) that continues exposing the same API model but now mapped to a named query to ensure that data is now fetched from multiple tables instead of the one.Another example, on Postgres specifically, we\u2019re working on being able to create the same GraphQL API whether you use a normalized set of tables, or denormalized tables with JSON columns.4.2. How do I customize Hasura\u2019s APIs?Hasura auto-generates a bulk of the required APIs using metadata you provide and then allows you to extend the API by mixing in your business logic.Hasura encourages a CQRS model while adding custom APIs by providing all the underlying plumbing and you can then add your own API in your favourite language/framework/hosting-provider and plug it into Hasura as a set of REST endpoints.4.3. Does Hasura force my consumers to use GraphQL?Based on popular demand, especially from our enterprise users, we\u2019re working on adding support for REST APIs soon, just the way Hasura already provides GraphQL :) Keep a lookout!4.4. How do I use Hasura for existing applications?We\u2019ve designed Hasura to instantly add value to any existing online data store. You can point Hasura to an existing data source (database, GraphQL, REST), add some configuration and start using it.Very commonly, our users use Hasura to add high-performance GraphQL queries and subscription support to their existing applications and continue using their existing APIs for writes.Hasura is designed for incremental adoption without having to rip-and-replace or entirely rebuild your stack. Get in touch with us if you need help thinking through your architecture.4.5. I use X web-framework to build APIs. How do I use Hasura?Same as 4.2 and 4.4!5. Comparisons to other tech in the landscapeComing soon. We'll take a detailed look at how Hasura compares to other technologies in your stack and what the pros/cons of Hasura are."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Contributor Days is back again and online! Description: An online discussion about the state of GraphQL and important updates from core contributors and authors of various libraries and frameworks in the GraphQL Community. Author: Hasura"}, {"role": "assistant", "content": "Photo by Tim Marshall on UnsplashConnect with your fellow GraphQL enthusiasts at GraphQL Contributor Days, in partnership with This Dot Media!The goal of Contributor Days is to not only give important updates to the community, but also provide a forum for anyone writing GraphQL to have direct access to core contributors and authors of various libraries and frameworks through live chat and an online broadcast.Some exciting topics on our agenda include:What's new with GraphQL (GraphQL Foundation Updates, Tooling, etc.)Authorisation and Security patterns for GraphQLUsing GraphQL with MicroservicesThe API Lifecycle: documentation/exploration; regression and testing; deprecation; monitoring/usage-analytic; process changes (for debugging/performance)We love the opportunity to host these events quarterly with those working closely with GraphQL. While we usually host these invites in-person, we are excited to reach the larger GraphQL community by moving it online!Our featured guests include\u2026Tracy Lee: Founder,\u2008This\u2008Dot\u2008&\u2008RxJS\u2008Core\u2008TeamTanmai Gopal: Founder, HasuraTyler Hannan: Head of Community, HasuraGago Frigerio: Staff Engineer, Coursera Jon Wong: Staff Software Engineer,\u2008CourseraConor Hastings: Javascript Engineer - Algorithms, Stitch FixSasha Solomon: Sr. Software Engineer, Twitter Eve Porcello: Co-Founder & Instructor, Moon HighwayAlex Banks: Co-Founder & Instructor, Moon HighwayEran Kampf: Director Of Engineering, TwingateIvan Goncharov: GraphQL consultant, APIs.guruJoey Nenni: Web Engineering Lead (GraphQL)Natalia Tepluhina: Staff frontend engineer, GitLabUri Goldshtein: Founder, The GuildAnd many more to be announced!Sign up to receive the livestream link here.Have a topic you\u2019d like us to talk about? Leave us a comment below, and let us know!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Optimizing your GraphQL API with Postgres Description: This post shows how to optimize your GraphQL API using some of Postgres' most popular features. These will help with data validations, data representation and performance enhancement. Author: Marion"}, {"role": "assistant", "content": "This blogpost introduces Postgres and some of its most popular features that you can use to enhance your Hasura GraphQL API.\nAbout Postgres\nPostgres features\n\nPostgres constraints\nPostgres views\nPostgres functions\nPostgres triggers\nPostgres indexes\n\n\nConclusion\n\nAbout PostgresPostgres (or PostgreSQL) is a general-purpose object-relational database management system that uses and extends the SQL language. Postgres is free and open source and has been developed continuously over the last 30 years. Postgres strives to be reliable, robust and performant.Postgres supports advanced data types and advanced performance optimization, features that are otherwise only available in expensive commercial database systems.Postgres featuresPostgres has some features that can be used to extend your Hasura GraphQL API. They are described in the following.Postgres constraintsPostgres constraints are used to define rules for columns in a database table. They ensure that no invalid data is entered into the database.There are different types of constraints that can be used with Postgres:Primary key constraintsA PRIMARY KEY is used to identify each row of a table uniquely.Example: Identify the author\u2019s id as the primary key of the authors table:CREATE TABLE authors(\n  id INT PRIMARY KEY,\n  name           TEXT    NOT NULL\n);Foreign key constraintsA foreign key constraint specifies that the values in a column must match the values appearing in a row of another table. Foreign key constraints are used to create relationships between tables.Example: Define the author_id in the articles table as a foreign key to the id column in the authors table:CREATE TABLE authors(\n  id SERIAL PRIMARY KEY,\n  name           TEXT    NOT NULL,\n  email          TEXT    UNIQUE\n);\n\nCREATE TABLE articles(\n  id SERIAL PRIMARY KEY,\n  title          TEXT    NOT NULL,\n  author_id INTEGER,\n  FOREIGN KEY (author_id) REFERENCES authors (id)\n);Not-null constraintsA not-null constraint allows you to specify that a column\u2019s value cannot be null.Example: Validate that an author\u2019s name cannot be null:CREATE TABLE authors(\n  id SERIAL PRIMARY KEY,\n  name           TEXT    NOT NULL\n);Unique constraintsUnique constraints prevent database entries with a duplicate value of the respective column.Example: Validate that an author\u2019s email is unique:CREATE TABLE authors(\n  id SERIAL PRIMARY KEY,\n  name           TEXT    NOT NULL,\n  email          TEXT    UNIQUE\n);Check constraintsCheck constraints allow you to specify a Boolean expression for one or several columns. This Boolean expression must be satisfied (equal to true) by the column value for the object to be inserted.Example: Validate that an author\u2019s rating is between 1 and 10:CREATE TABLE authors(\n  id SERIAL PRIMARY KEY,\n  name           TEXT    NOT NULL,\n  rating         INT     NOT NULL CHECK(rating > 0 AND rating <= 10)\n);Postgres constraints & HasuraPostgres constraints will help avoid writing custom logic code for basic data validation in Hasura.Most Postgres constraints (primary key, foreign key, not-null and unique constraints) can be added to Hasura natively when creating tables.Postgres check constraints can be used as a form of data validation in Hasura and can be added as described here.Postgres viewsA Postgres view is a virtual table in Postgres. It represents the result of a query to one or more underlying tables in Postgres. Views are used to simplify complex queries since these queries are defined once in the view, and can then be directly queried via the same.Standard viewsExample 1: View with authors whose rating is larger than 6:CREATE VIEW popular_authors AS\n  SELECT name, rating\n  FROM authors\n  WHERE rating > 6;The created view can now be queried as follows:SELECT name, rating from popular_authors;Example 2: View with authors ordered by their rating:CREATE VIEW authors_ordered_by_rating AS\n    SELECT name, rating\n    FROM authors\n    ORDER BY rating;The created view can now be queried as follows:SELECT name, rating from authors_ordered_by_rating;Materialized viewsCompared to the standard view described above, materialized views do store data physically in the database. Materialized views are used if data from complex queries needs to be accessed quickly.Example: Materialized view with authors whose rating is larger than 6 and who are active, ordered by rating:CREATE MATERIALIZED VIEW popular_active_authors AS\n    SELECT name, rating\n    FROM authors\n    WHERE rating > 6 AND is_active = TRUE\n    ORDER BY rating;The created materialized view can now be queried as follows:SELECT name, rating from popular_active_authors;Refreshing materialized viewsMaterialized views don\u2019t always have the most recent data. Since the result of a query is stored in a materialized view like in a cache, you need to make sure to refresh it periodically:REFRESH MATERIALIZED VIEW popular_active_authors;Materialized views can be refreshed periodically using Postgres triggers.Postgres views & HasuraYou can use Postgres views with Hasura to expose a subset of fields in a table, as well for using Postgres clauses like ORDER_BY and GROUP_BY. \u00a0After creating a view, you can expose it over your GraphQL API and query it like a normal table.See here how to create and expose views in Hasura.Postgres functionsPostgres functions allow you to customize your database schema by defining a set of operations that can include several statements such as declarations, assignments and conditional workflows. Postgres functions are similar to views but allow more procedural computations and can take arguments.Example: Searching articles:We can create the following function that we can call later to search articles based on the input text argument search.CREATE FUNCTION search_articles(search text)\nRETURNS SETOF article AS $$\n    SELECT *\n    FROM article\n    WHERE\n      title ilike ('%' || search || '%')\n      OR content ilike ('%' || search || '%')\n$$ LANGUAGE sql STABLE;Let\u2019s break this function apart:Function name: search_articlesParameters: there is one parameter where search is the name and text is the typeReturn type: SETOF articleFunction body: Block from SELECT until the end of the WHERE clauseLanguage: The response is returned in the sql languagePostgres functions & HasuraIn Hasura, Postgres functions can be used to either encapsulate custom business logic or extend the built-in SQL functions and operators. Postgres functions can be exposed in Hasura\u2019s GraphQL schema. They are typically used for performing custom business logic in the database.Refer to this page for more use cases and for instructions on how to create and expose Postgres functions in Hasura.Postgres triggersPostgres triggers are used to invoke previously defined Postgres functions before or after a specific database event (e.g. INSERT) occurs.Example 1: Trigger a Postgres function before an article is inserted or updated:Let\u2019s say we want to check if an author is active before a corresponding article can be inserted or updated. We can do so with the following Postgres function:CREATE FUNCTION check_author_active()\n    RETURNS trigger AS $BODY$\n    DECLARE active_author BOOLEAN;\n    BEGIN\n    SELECT author.is_active INTO active_author FROM \"authors\" author WHERE author.id = NEW.\"author_id\";\n    IF active_author != TRUE THEN\n        RAISE EXCEPTION 'Author must be active';\n    END IF;\n    RETURN NEW;\n    END;\n    $BODY$ LANGUAGE plpgsql;Now we want to have this function executed whenever a new article is about to be inserted or updated. We can create a Postgres trigger as follows:CREATE TRIGGER insert_article BEFORE INSERT OR UPDATE \nON \"articles\" FOR EACH ROW EXECUTE PROCEDURE check_author_active();If someone now tries to insert an article for an author that is not active, the following error will be thrown:unexpected : Author must be activeExample 2: Refresh a materialized view when an author gets inserted:Let\u2019s say we want to refresh a materialized view whenever a new author is inserted.The following Postgres function refreshes a materialized view:CREATE FUNCTION refresh_materialized_view()\n  RETURNS trigger AS $BODY$\n  BEGIN\n  REFRESH MATERIALIZED VIEW popular_active_authors;\n  RETURN NULL;\n  END;\n  $BODY$ LANGUAGE plpgsql;Now, to make sure this function gets called whenever a new author is inserted, we can create the following Postgres trigger:CREATE TRIGGER update_materialized_view AFTER INSERT \nON \"authors\" FOR EACH ROW EXECUTE PROCEDURE refresh_materialized_view();Postgres triggers & HasuraPostgres triggers can be used as a form of data validation in Hasura and can be added as described here.Postgres indexesPostgres indexes are a way of increasing performance on a column that is queried frequently. The concept is similar to the one of an index in a book. It helps accessing the data you\u2019re looking for more quickly by maintaining additional metadata.Example: Create an index on the column name in the table authors:Let\u2019s say the database receives a large number of requests of authors being queried by their name, for example:SELECT * FROM authors WHERE name = 'J.K. Rowling';We can now create an index on the name column of the authors table:CREATE INDEX author_name_index ON authors (name);Since the database is now able to look up the result of these queries more quickly, the performance of these queries will increase significantly.Postgres indexes & HasuraIndexes can be used to optimize query performance in Hasura. Refer to this page for information about query performance and how to add Postgres indexes to Hasura.ConclusionWe've seen how the Hasura GraphQL API can be extended and improved using some of the Postgres features. For more detail about these features, check out the Postgres documentation."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Joins Hacktoberfest: 3rd year in a row! Description: Hasura joins Hacktoberfest 3rd year in a row to celebrate open-source with the community and encourage contributions. Author: Vishwa Mehta"}, {"role": "assistant", "content": "It's that time of the year. DigitalOcean is back with Hacktoberfest 2020. And Hasura will be part of it again with a lot of great opportunities to contribute to open-source and win some amazing swag!Last year, the Hasura Hacktoberfest challenge received an amazing response from our community! We had 130+ PRs ranging across all the different components of the hasura/graphql-engine & hasura/learn-graphql repos.Below is a sneak-peek of the awesome work our community did during last year\u2019s Hacktoberfest:[Community]: Ruby examples for triggers on AWS[CLI]: Diff command and dry run flag addition to Hasura GraphQL Engine CLIAbout HacktoberfestHacktoberfest encourages open source contributions all over the world, and it\u2019s growing larger every year. It emphasizes that everyone can get involved in open source, no matter what job they\u2019re doing or how much experience they have.In order to participate, head over to Digital Ocean and register through the website. The first 75,000 contributors who manage to complete the challenge will get a free and exclusive Hacktoberfest t-shirt from the organisers. You can read more about the challenge and the rules here!#HacktoberfestWithHasuraWe have our own version of Hacktoberfest!You can contribute to Hasura in the following ways:Submit a PR for one of our issues with the label hacktoberfest: For all valid PRs attempting to close an issue labelled hacktoberfest, we\u2019ll send out some amazing Hasura - Hacktoberfest swag.Keep an eye out for our surprise Hasura swag reveal on Twitter and don't forget to vote for your favourite choice of swag!#Hacktoberfest is just round the corner & Hasura is participating for the 3rd time in a row! \ud83d\ude80What exclusive Hasura swag do you want for Hacktoberfest this year? \ud83c\udf81\u2014 Hasura (@HasuraHQ) September 24, 2020\n\nOpen a spontaneous PR for a small fix, example: a typo fix.For such PRs, we\u2019ll send out a sticker pack to the authors.If you have a feature or any other change in mind that you feel should be implemented, feel free to open an issue. The team will then decide to add the Hacktoberfest label.We will ship out all swag packs latest by November end so that we give ourselves enough time to go over all your PRs please bear with us in the meantime. \ud83d\ude42Getting startedRepositories & ComponentsYou can open PRs against one of the following repositories:1. hasura/graphql-engine: Hasura GraphQL engine gives you instant realtime GraphQL APIs, so you can build modern apps and APIs faster. Hasura connects to your databases, REST and GraphQL servers as well as third party APIs (eg: Stripe, Salesforce) to provide a unified realtime GraphQL API across all your data sources instantly.2. hasura/learn-graphql: Hasura\u2019s Learn tutorials are community maintained tutorials that introduce developers to the basics of GraphQL & Hasura.We\u2019ve curated collections of issues suitable for Hacktoberfest participants for various components of the above mentioned repositories:DocsCommunityLearnConsoleCLIServerHasura\u2019s core engine is built in Haskell and we, at Hasura, have a special place in our hearts for the language and the community. Check out our recent blog post by Tanmai Gopal to learn more about how Hasura is committed to invest in the Haskell community. We especially welcome any Haskell contributions on our server, and we are happy to help you get started.Contributing guidesBefore you get started, read through the contributing guide of the component you\u2019d like to work on:DocsCommunityLearnConsoleCLIServerGuidelinesHere are a few guidelines that will help make the Hacktoberfest experience a fun experience for everyone!Working on an issueHacktoberfest is a great way to encourage open source contributions and help beginners get started. To ensure that everyone gets a fair chance to contribute, we won\u2019t be assigning issues to a single contributor. To show that you're working on a certain issue, comment on the issue of interest and make sure to reference it in your PR so that other participants can see the progress.Code of conductOpen source is all about learning with and giving back to the community. To facilitate this, it is pivotal for us to create and maintain an inclusive space so that everyone can learn and grow together.Please go through to our Code of Conduct thoroughly and adhere to it. Any violations of the Code of Conduct will not be tolerated and will further lead to disqualification from the challenge.Quality over quantityAs the old saying goes, the more the merrier \u2014 we truly appreciate, and hence, encourage you to contribute to as many issues as you can. However, we also believe that quality trumps quantity and thus, abide by the official Hacktoberfest quality standards while reviewing PRs. PRs that do not match these quality standards will not be considered as eligible for completion of the challenge.First time contributors, we got you!Hacktoberfest is an amazing opportunity for newcomers to get started with open-source! We want to make it a streamlined process for newbies as they make their first ever contribution so they can focus on learning and having fun with the community!We have tagged issues suitable for newcomers with the label good first issue across all our open source projects. Here is a list of good first issues for newcomers to check out!Need help?If you need help getting started with contributing to Hasura, we\u2019ve got your back! You can get in touch with us on your preferred channels:Open a new GitHub Discussion (after checking if your question has already been posted by someone else)Talk to us on the #contrib channel on DiscordTweet at us!We are happy to answer your questions and to help you get started with contributing to Hasura!Office hoursIf you have questions about issues or if you need help getting started, you can also join our office hours. This will be a live Q&A happening on Discord where you can ask all your questions.The exact date and time of these office hours will be updated on this blogpost closer to the event.We want you to win!We are super stoked to have you all participate and celebrate open-source yet again with the Hasura Community!Follow us on Twitter to keep an eye out for updates (like the surprise swag reveal \ud83d\ude09) & join us on our Discord to come hang out with the rest of the Hasura community!Share your excitement!Finally, share your excitement with the community by tweeting at us @HasuraHQ with the hashtag #HacktoberfestWithHasura!(tip: everyone loves GIFs and emojis)Pick a tweet idea for your awesome #HacktoberfestWithHasura tweet:This is my ____ year of #HacktoberfestWithHasura! What about you all?@HasuraHQSuper excited to be participating in #HacktoberfestWithHasura for the ____ time this year! What are you contributing to?@HasuraHQCan\u2019t wait to get started with #HacktoberfestWithHasura!What about you? @HasuraHQI am excited to contribute to @HasuraHQ on the (*insert the collection of issues you\u2019re excited to contribute to eg: CLI, Docs, Haskell, Learn, Console*) collection!What are y\u2019all contributing to?Or just wing it yourself! We can\u2019t wait to see all your fun tweets!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL in Production with Hasura Cloud Description: In this post, we will look at various aspects of taking a GraphQL API to production and how Hasura helps you through the process seamlessly with it's advanced security and production ready features. Author: Praveen Durairaju"}, {"role": "assistant", "content": "In this post, we will look at various aspects of taking a GraphQL API to production and how Hasura helps you through the process seamlessly with it's advanced security and production ready features.As we dive into what Hasura offers to be production ready, we will also step back to look at potential issues related to performance, security and reliability in building apps with GraphQL.\n Performance\n\n N+1 problem\n Efficient Queries\n Scalable Subscriptions\n Response Caching\n Scaling Database\n\n\n\nSecurity\n\n Service Level\n Authentication and Authorization\n Defense mechanisms\n Collaborator Management\n\n\n\nReliability\n\n Monitoring\n Observability\n Regression Testing\n Availability\n\n\n\nGraphQL Infrastructure\n\nPerformanceN+1 problemThe most common bottleneck with GraphQL performance is the N+1 query problem. GraphQL query execution typically involves executing a resolver for each field. Let's take an example of fetching artists and their related albums.Naive Approach with N+1 problemIn the example query above, we would invoke a function to fetch the albums and then for each of these albums, we would invoke a function to fetch the tracks, the classic N+1 query problem. The number of queries grows exponentially with the depth of the query.The queries executed on Postgres would be as follows:SELECT name FROM artists;This gives us all the artists. Let the number of artists returned be N. For each artists, we would execute this query (so, N queries):SELECT title FROM albums WHERE artist_id = <artist-id>This would be a total of N + 1 queries to fetch all the required data. As you can imagine, this hits the database exponentially with higher levels of nesting. The other approach to consider here is the Dataloader approach which hits the database linearly with the level of nesting. Batching of query happens at each nested layer reducing the exponential queries made in the naive approach.Efficient QueriesSo what would be a better solution? Let's look at this SQL:SELECT artists.name as name\nFROM artists\nJOIN albums\nON (artists.id = albums.artist_id)Instead of making multiple SQL queries, this SQL plan hits the database just once! This compiler based approach allows Hasura to form a single SQL query for a GraphQL query of any depth.GraphQL Parser -> GraphQL AST -> Internal AST -> SQL AST -> SQLHasura follows the Don't resolve, compile instead approach for the database queries. You can read more about the architecture of GraphQL to SQL server. For other remote sources like Remote schemas (custom GraphQL server), it follows the dataloader approach for batching and combining queries.Scalable SubscriptionsIf you are building a realtime app, scaling websocket connections for live queries is a critical requirement for the app. When there are enormous number of realtime queries, Hasura can handle them by grouping similar queries and batching them during the compilation step, yet maintaining permissions.Batching Live QueriesRead more about how Hasura leverages batching to scale to 1 million active GraphQL Subscription queries.Response CachingCaching GraphQL queries at the server side needs a different approach compared to a typical REST API architecture. In REST, you can cache queries at the HTTP layer using GET requests and serve it in front of a CDN. Typically GraphQL queries are POST requests and caching needs to be handled differently.Query Caching: Hasura already offers query caching in the OSS version, where the internal representation of the fully qualified GraphQL AST is cached, so that it\u2019s plan need not be computed again. For Postgres, Hasura makes use of Prepared Statements to make query execution faster. Read more about Hasura's fast GraphQL query execution.Data Caching: Hasura Cloud goes a step further and adds support for data caching on top of the query caching that's already available. Hasura has enough information about the data models across data sources, and the authorization rules at the application level letting users cache authenticated GraphQL API calls that works for shared data.Cache invalidation is handled by using a TTL configuration so that state data is refreshed within a time interval. So clients can query with a @cached directive for data caching.Learn more about the usage of @cached directive in this video - An approach to automated caching for public & private GraphQL APIs.Scaling Database (Postgres)The final bottleneck when it comes to performance is the underlying database (i.e Postgres). Hasura Cloud supports horizontal scaling of Postgres, letting you load balance between multiple instances for read queries. You canCreate read-replicas of your postgres instancesConfigure routing, connection pooling, and load balancingAnd ofcourse, the regular optimisation of Postgres like adding the right indexes, analyzing the output of Explain remains the same on both the OSS version and on the Cloud.GraphQL Performance Checklist ComparisonHand rolling your own GraphQL server is really hard if you are serious about performance. To top it off, Hasura also adds a declarative Authorization layer to make the job easier for authenticated requests and we will dive in to this in the next section.SecurityNext up is Security. As much as we care about performance, any complex production app will involve authenticated requests, a sophisticated access control system and some form of defense mechanism for preventing malicious attacks.Service levelTo cover the bases right, we will need to protect the Hasura GraphQL endpoint with some form of an admin secret. Hasura can be configured with an admin secret for direct backend access without restrictions, but also to prevent any public access of the API. CORS policies should be configured to whitelist only the application domains, unless it's a public facing SaaS API.Maintaining different logging and debugging modes for development and production environment is important to prevent exposing sensitive information in server logs. For example, dev mode should be disabled in prod. Read more about these in the Hasura Production Checklist.Authentication and AuthorizationAdding to the admin secret, for real world apps, you might want to integrate an authentication provider either via JWT or through Webhooks. If applicable, \u00a0ensure auth tokens are consistent with the user db via events or webhooks (example: match user IDs on create). Some applications will also require unauthenticated public access and for those use cases, you can configure UNAUTHORIZED_ROLE.Hasura comes with built-in authorization system. You can define role based permission rules to restrict data access. Permissions can be set per role, per schema, per table and per operation type and access control rules (records and fields) for insert, select, update, and delete.Role based permission checkDefense mechanismNow despite setting up security rules and configuration, there are different vectors that need to be controlled to prevent any malicious attempts. With Hasura, you can:Enable allow lists: Define a list of allowed operations (GraphQL queries, mutations or subscriptions) for your project to only allow a set of operations to be performed. In Hasura Cloud, you can configure Allow Lists for different environments. For example, during development of a new feature you can define a new set of allowed operations on a staging environment and apply the same configuration to a production environment easily after the necessary testing is done.GraphQL Query Capture: Hasura cloud captures queries (optionally stripping out session and query variables since they might be sensitive) and makes it available to you for 2 main uses:Regression testingBootstrapping and managing an Allow ListThe allow list can be configured with one click from the record of past operations. This way, you can quickly define an allow list on an existing project by just monitoring the queries that are made to the server.\nRead more about configuring Allow Lists in Hasura.Set rate limits: Configure your reverse proxy to limit by session variable or client IP; configure your GraphQL service to limit by query depth or complexity. Rate limiting restricts number of GraphQL operations per minute. This uses a sliding window approach. This means whenever Hasura receives a request, it will count the rate of that client starting from the current time to last one minute. Read more about configuring API Limits in Hasura.Set response limits: Limit the number of rows returned, access to aggregation queries. This can be configured at the permission layer.Effort required for Security ChecklistCollaborator ManagementIn Hasura Cloud, you can share access to the projects by configuring roles having certain privileges. The Admin role has complete access to the console i.e they can change the schema and the GraphQL engine metadata. The User role has limited privileges, depending on what's allowed.The Execute GraphQL permission allows running queries, mutations and subscriptions from the GraphiQL tab of the console.The View Metrics permission allows inspecting operation data and metrics from the PRO tab of the console.This granularity makes it easier to share the management of APIs in teams without using a shared admin secret or a shared account credential.ReliabilityFinally, for any production app at scale, the APIs need to be reliable for data access and manipulation. Hasura Cloud makes it easy to leverage performance and sensibile security rules. Let's look at reliability for maintaining and monitoring the APIs in production.MonitoringIn a production app, it is important to monitor the API usage to detect any anamoly. For example, if you have deployed a new schema change to production, you want to ensure if the APIs continue to work fine. With Hasura Cloud you can monitor errors, websocket connections, subscriptions, with a drill-down into individual operations.Health Check: You can also configure external tools with the Health Check API that Hasura exposes. The Health API will respond with a status code 500 when something is wrong with the server and you can setup alerting/notifications for the same.Distributed Tracing: Hasura Cloud has support for distributed tracing, a technique for debugging Hasura in production in the context of the services it works with. These services might include your own Postgres database, any remote schemas, event trigger webhook providers, action providers or authentication hooks. Distributed tracing attempts to give a unified view into the performance characteristics of all of these components of your architecture. \nSubscription workers can also be monitored using Hasura Cloud. You can visualize usage statistics like Average execution time, Total number of Postgres queries and Total number of subscriptions among other metrics.Subscription workersAPI access to metrics: Hasura Cloud exposes APIs to access various metrics like the errors, websocket connections, subscriptions etc that can in turn be integrated with an APM tool of choice.ObservabilityObservability (in software world) means you can answer any questions about what\u2019s happening on the inside of the system just by observing metrics from outside of the system, without having to modify the working deployment to support this.Hasura Cloud Metrics offers insights into:Query payload, hash, and timestampQuery execution timeConnection type (http or ws)Response status codes & size (kB)GraphQL server versionOrigin IP address.Inspect Query MetricsYou can also configure these metrics with APM tools with the API access exposed by Hasura for further drill down of usage.Specifically for Hasura GraphQL Engine, apart from the Cloud offering, you can also track usage ofSession variables such as X-Hasura-Role, X-Hasura-User-IdQuery metadataRegression TestingHasura Cloud includes a comprehensive test bench that lets you seamlessly compile a test suite on each project, executable on any GraphQL Engine instance (ex: staging, prod). Broadly, we need toCreate a suite of critical operations (based on the application use case)Add regression test suite to workflows, CI/CD.The workflow involves, creating an initial regression test suite for production environment, and whenever there is a schema or Hasura metadata change, before moving to production, it will go through an automated test suite in CI/CD and only if the tests pass we will promote the changes to production.Hasura Cloud makes it easier to configure these tests and run the test suite on demand.\nRead more about Regression Testing with Hasura Cloud.AvailabilityHigh availability of the API is critical for a busy (continous network requests) \u00a0application.Configure GraphQL server load balancingOnce you configure read replicas for your Postgres instance, the load balancing at the database layer is automatically taken care. At the Hasura API layer, load balancing is done seamlessly with multiple instances of Hasura running in different regions* across the globe.Configure Postgres high availabilityThis can be achieved by using a hosted Postgres Cloud vendor like Amazon RDS, Google Cloud SQL, Azure Postgres among others.Here's a quick comparison of hand rolling your own GraphQL server and Hasura Cloud to ensure reliability of your API and application.Effort required for Reliability ChecklistGraphQL InfrastructureIn production, hosting of custom GraphQL servers will typically involve Dev-Ops to manage infrastructure, scaling and to be highly available. In Hasura Cloud, each instance of Hasura is a heavily multi-threaded server that exploits shared memory within the instance to aggressively optimise handling GraphQL queries at runtime. If you are running Hasura yourself or any other custom GraphQL server, then you will have to figure out how much CPU or RAM is required for optimal performance and high availability.If you\u2019re running on Hasura Cloud, you don\u2019t need to care! Hasura Cloud doesn\u2019t ask you to think about the number of instances, cores, memory, thresholds etc. All you need to do is to connect the database and let Hasura Cloud handle the infra for you.You can keep increasing the number of concurrent users and the number of API calls and Hasura Cloud will figure out how much resources to allocate and will auto-scale for you.Now that you have learnt what it takes to run and maintain a production ready GraphQL server, to reap the benefits of performance, security and reliability, make the switch to Hasura Cloud from your OSS version. Read more on Moving from OSS to Hasura Cloud."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Turn your Java REST API to GraphQL using Hasura Actions Description: In this post, we will look at using Hasura Actions to convert your Java REST API written in Spring Boot to GraphQL. Author: Vishwa Mehta"}, {"role": "assistant", "content": "This post is a part of a series which goes over how to convert an existing REST API to GraphQL by defining types and configuring endpoints.IntroductionIn this post, we will learn how to use Hasura Actions to convert your Java REST APIs to GraphQL.Typically, writing a GraphQL server from scratch involves setting up the GraphQL schema, writing type definitions, and wiring it up with the resolvers in the language or framework of your choice. The hard part about this server component is the resolver code for someone who is used to writing a REST API with the simplicity of using GET/POST and handling authentication logic for each type; which becomes more and more complex as the app grows.Hasura reduces bulk of the complexity of setting up a GraphQL API from scratch by giving you instant GraphQL CRUD for databases (currently Postgres) which should cover most of the data-fetching and real-time subscription use-cases.In case of custom business logic, Hasura lets you extend the graph in several ways:1. Remote Schemas: If you are comfortable writing GraphQL servers, you can add a custom GraphQL server as a Remote Schema and Hasura merges it automatically for you.2. Actions: In case you are looking to write a new or keep your existing REST API for the custom logic, you can do so using Actions which is what we will focus in this post.In this blog post, we'll explore the Actions route and learn how to use Hasura Actions to convert a new or existing Java REST API to GraphQL.New REST APIIn case you are starting from scratch, we will begin by defining the GraphQL types required for an Action and create a simple Spring Boot app to handle the resolver bit. Let's take the example use-case of registering a new user.Creating a new Hasura Cloud project\nBefore we begin, let's create a project on Hasura Cloud to setup the Action. Click on the Deploy to Hasura button below, signup for free and create a new project.\n\nHasura requires a Postgres database to start with. We can make use of Heroku's free Postgres database tier to try this app.\n\nAfter signing in with Heroku, you should see the option Create project.\n\nOnce you are done creating the project, click on Launch the Console button on the Projects page for the newly created project and this will take you to the Hasura Console. By creating a new project, you get a GraphQL endpoint for the Postgres database that is globally available, secure, and scalable from the beginning.\nNow let's head to the Actions tab on the Hasura Console and define the GraphQL types for our app.\n\nAction Definition\ntype Mutation {\n  registerUser (\n    name: String!\n    email: String!\n    password: String!\n  ): UserOutput\n}\n\nTypes Definition\ntype UserOutput {\n  id : uuid!\n}\n\nHere we are defining the Mutation type registerUser which accepts the name, email and password arguments and returns the id of the user.\nWe can configure the handler URL later. Click on Create to create the Action.\nNow let's verify the generated GraphQL mutation by trying out the sample mutation in the GraphiQL tab.\nmutation {\n  registerUser(\n    email: \"user@site.com\"\n    name: \"Person Name\"\n    password: \"mypass\"\n  ) {\n    id\n  }\n}\n\nThe above mutation should obviously give a http exception since the Handler URL has not been configured yet. But this test is to verify that Hasura generates mutations for you based on the types defined to query on the same GraphQL endpoint.\n\nNow let's go ahead and define our Java app and try out the mutation.\nCodegen: Auto generate boilerplate code\nNow head over to the Codegen tab to autogenerate the boilerplate code for your Java server. Since we are generating the API from scratch, we need the full server setup instead of just the POST handler.\n\nThe console gives you options to generate code for different frameworks including but not limited to nodejs-express, nodejs-zeit, nodejs-azure-function etc.\nWe're using the Java framework Spring Boot for this example. However, any Java server that can serve a POST endpoint can be used to swap with this example.\nBusiness logic required:\n\nReceive the action arguments name, email and password on request.\nConvert the plaintext password input into a hashed secure password.\nSend a mutation to Hasura to save the newly created user with the hashed password.\nReturn the created user object to signal success, or else error.\n\nThe generated codegen from Hasura console (registerUser.java and registerUserTypes.java) can be copied to your filesystem to start making the changes to the handler.\nOnce you've copied the codegen, you can setup Spring Boot and run the server.\nSetting up Spring Boot\n\nStart by creating a new Spring Boot project by using start.spring.io.\n\nNote: For this, you will need an IDE such as IntelliJ IDEA, Spring Tools, Visual Studio Code, or Eclipse, and JDK (Java Development Kit) installed. Learn more here.\n\n\nAdd your copied codegen to a new file called demo.java.\n\n\nBuild and run the project from your terminal by running the ./mvnw spring-boot:run command.\n\n\nYou will see an output similar to this:\n\nVoila! You have set up and started your Spring Boot project. The above command starts Apache Tomcat that acts as a webserver at http://localhost:8080/. But we need to update our handler URL for the Action so that the HTTP call works. Add the registerUser endpoint that handles the mutation. So the final handler URL will look like http://localhost:8080/registerUser.\nSince we want our Hasura Cloud instance to communicate with our Java server, we need to expose the Java server on a public URL. We can host this server on a cloud instance and point to that URL/IP. But for the sake of brevity for this tutorial, we will use ngrok, which gives public URLs for exposing your local servers to communicate with a cloud instance.\nAssuming the use of ngrok, running ngrok http 8080 on your terminal should give a unique URL you can use. Go back to the Modify tab of the registerUser Action that was created on the Hasura Console. Update the handler URL to one generated by ngrok.\nFinally let's try out the same mutation through GraphiQL and the connection should work returning a dummy < value > for id.\n\nAlright! We have a working GraphQL API that is resolved using a Java REST API in the background.\nNow you can modify your handler code as required to do any business logic; like connecting to a different API, connecting to a database (preferrably using Hasura's APIs) or using an ORM for a different databases etc. Hasura will take care of proxying the GraphQL mutation to the right REST API handler internally.\nPermissions\nWhat about permissions? Who will be able to make this GraphQL mutation from the client? Right now, it is configured to be admin only. We can configure roles to allow other kind of users to perform this mutation.\n\nIn the above example, I have created a role called public that is allowed to make this mutation. To read more about how the whole Authentication and Authorization works with Hasura, you can checkout the docs.\nRelationship Data\nNow consider that the Postgres database has users table. We can connect the id of the registerUser output to the id of the users table.\n\nThe related data also conforms to the permissions defined for the respective table. (ie. users)\nExisting REST API\nYou might be wondering - what if you already have a Java server with a bunch of endpoints written to handle custom business logic? In that case, as long as it's a POST endpoint, you can define the necessary GraphQL types and make some modifications to the way you handle request body to get it working.\nThe request payload is of the format:\n{\n  \"action\": {\n    \"name\": \"<action-name>\"\n  },\n  \"input\": {\n    \"arg1\": \"<value>\",\n    \"arg2\": \"<value>\"\n  },\n  \"session_variables\": {\n    \"x-hasura-user-id\": \"<session-user-id>\",\n    \"x-hasura-role\": \"<session-user-role>\"\n  }\n}\n\nSo in our example, the input arguments - name, email and password was wrapped inside an input object. If you can make necessary modifications to your existing REST API to handle this request body, your GraphQL API will work as expected.\nQuery Action\nThe above example showcased how to perform a GraphQL mutation. The same workflow can be extended to perform a GraphQL query which proxies to a Java REST API in the background. Typical use cases for query actions include computed fields, data enrichment, data transformations, fetching from multiple databases and API sources etc.\nSupported Frameworks\nThe REST API can be in any framework like JAX-RS, Restlet, Jersey, RESTEasy, Apache CFX, or serverless functions. After all, Hasura just requires a reachable POST endpoint that it can forward the query to. In case you are going to deploy the API to a serverless function, you will need to follow the format of the cloud provider.\n\nAdditional Reading:\nCreating a GraphQL API for Notion with Hasura Actions\nCheck out the Java backend course. In this course, you will learn how to integrate Java in a GraphQL backend server stack with Hasura."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing our $25M Series B financing Description: I\u2019m happy to announce our $25M Series B financing led by Lightspeed (Techcrunch) with participation of our existing investors Vertex, Nexus, SAP.io & Strive. Author: Tanmai Gopal"}, {"role": "assistant", "content": "I\u2019m happy to announce our financing of $25M led by Lightspeed (Techcrunch) with participation of our existing investors Vertex, Nexus, SAP.io, & Strive.As part of this round, we are delighted to welcome Gaurav Gupta -- \u00a0who comes with a background in great infrastructure and open-source companies like Splunk, Elastic, and Grafana as an executive and a VC -- to our board as well.Although we\u2019ve been well capitalised, with a good revenue stream, building an open-source infrastructure product and company is not easy. With this recent fundraise, we\u2019re in an even better place to invest in our community, build faster, and support our customers even better than today. \ud83d\udd25I thought I'd take this opportunity to share how we think about this financing, our business, and the road ahead!tl;dr - Here are our focus areas:Continue investing our communitySolve hard problemsLaunch new DB integrations (MySQL for example)Actively contribute to OSS communities we depend on: the Haskell, GraphQL and OSS communityCompany building and setting stronger foundations for a distributed and remote futureMake our users successful!1. Community is core to the identity of HasuraWe are the kind of people for whom open-source makes sense. It is easy to lose sight of this when raising money but our commitment to investing in our community and in open source does not change.To that end, we are continuing to invest in building community. The goals are simple: to listen to our users and enable them, and to nurture an active community. A community is made up of companies, contributors, users, customers, learners, followers, and so much more. The stewardship of this community belongs to each of us\u2026 and not just to the company. Each of you reading this plays an important role in building, and maintaining, a healthy, welcoming, safe, diverse environment. Your participation matters and we value it!Community has been a company responsibility, and now we have a dedicated team within Hasura so that we're giving it our full attention.2. Continue solving hard problems & solve harder onesHasura is an infrastructure component, a web service that presents your entire universe of application data as interlinked JSON documents that can be queried and operated on over the web. This involves solving some hard problems, some which we\u2019ve tackled and a few others on our roadmap. Some of these may be non-obvious, so we've put together a detailed post that enumerates our technical design and the problems we're solving to provide more visibility into our approach and thinking.Read the post here: https://hasura.io/blog/how-hasura-works/With this financing, we are investing in our engineering team. Both in humans to create amazing code but also in infrastructure, tooling, and processes that will allow us to collaborate more effectively so that we can keep delivering on complex problems!3. We \u2764\ufe0f HaskellHaskell is, and remains, a secret weapon that has allowed us to work in parallel on a large and complex codebase. Hasura is one of the few teams in the world building and running cloud infrastructure on Haskell.This would not be possible without our amazing engineering team and the community, and ecosystem, built around Haskell. We rely heavily on this open-source ecosystem to make sure we stay productive. We are going to be spending more time exchanging ideas, improving tooling, and helping maintain open-source libraries along with the broader community.Among other things, we are partnering with Well Typed to work on improving Haskell tooling together!It has been a dream for us to be the kind of company that can keep giving back to the open-source community and we're taking better steps towards that every day :)4. Support for multiple databasesIn addition to this financing, we released alpha support for MySQL today, and SQL Server is actively underway.This process has enabled us to think about how we build the underlying abstractions for supporting multiple, different databases. We are going to be accelerating our work with the community to rapidly build, and maintain, native & no-code connectors to more SQL, NoSQL, NewSQL databases and other data-sources.If you\u2019d like to stay updated, you can sign up for our mailing list here.5. Hasura is its people and its processesHasura is its people. We share a mission to solve hard problems, enable our users, drive bottom-line impact for our customers alongside shared cultural values of trust, empathy, and \u2018Doing The Right Thing\u2019.But, working with a group of 5 people is very different from a group of 50 (and growing). Distributed, or \u2018all-remote\u2019 teams are especially hard (we span 20 hours of timezones!). But, it is a forcing function for doing the right things. It demands more of us as people.As we grow as a global team working towards a shared goal, we cannot stay the same or, even worse, regress to the mean. We have to communicate more. We have to be more transparent. We have to be vastly more empathetic. To keep our amazing culture and to keep innovating we are investing in people, process, and technology to make sure we are able to grow as a team.6. Making our mission-critical users successfulAt Hasura we have the opportunity of having tremendous business impact for users who're building mission-critical application and trusting Hasura with their workloads. Especially as our cloud adoption and enterprise adoption grow, we will continue to invest in our people and processes to ensure our users running mission-critical applications get the kind of reliability, performance, security and support that they need to be successful.In SummaryHasura has been driven by a passionate team and an active community, solving difficult problems and helping evolve the product. And it's amazing that we have the opportunity to do even more of this in the future! Keep us honest, share your frustrations and share your successes. That's our fuel which makes us go, and it's our carrot \ud83e\udd55and it's our stick. :)As always, I\u2019d love to hear about what you\u2019ve been building and welcome any input you have for us!I\u2019m at @tanmaigo on twitter. Don\u2019t hesitate to reach out!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Partnering with Well Typed and investing in the Haskell community Description: Hasura\u2019s core engine is built in Haskell. Although a seemingly risky choice initially, our choice of spending our innovation token / golden ticket to build with Haskell had paid off well. Author: Tanmai Gopal"}, {"role": "assistant", "content": "Hasura\u2019s core engine is built in Haskell. Although a seemingly risky choice initially, our choice of spending our innovation token / golden ticket to build with Haskell had paid off well. :)github.com/hasura/graphql-engineHaskell is particularly well suited for complex concurrent programming tasks (hello web servers!) and to build compilers. A lot of what Hasura does can be boiled down to a performant JIT compiler that is presented as a web-server handling large concurrent loads. Haskell has allowed us to collaborate on a large codebase and has allowed us to move with startup-like speeds on infrastructure software without breaking things.State of the Haskell ecosystem - github.com/Gabriel439Over the years, as our codebase has become more and more complex we\u2019ve needed better tools to understand how and why our code is behaving in particular ways in particular situations. Especially as we\u2019ve been working on our cloud offering, innovatively called Hasura Cloud (we had just the one innovation token), greater visibility and better operational tooling has become critical!I\u2019m excited to announce an engineering collaboration and partnership with the great folks at Well Typed to be working with them on improving open-source tooling in the Haskell ecosystem.A collage of logos I put together \ud83e\udd13Most recently, we\u2019ve been investigating memory fragmentation in GHC Haskell and found limitations in the profiling tools available, so we're planning to support ongoing development efforts on ghc-debug and help improve GHC's DWARF support for tracking running code back to source file locations. This will unlock many possibilities for easier profiling of production Haskell code.We\u2019re excited to be working with David Eichmann, Ben Gamari and the team at Well Typed over the coming months. And special thanks of course to Adam Gundry for helping this collaboration come together!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Turn your Go REST API to GraphQL using Hasura Actions Description: In this post, we will look at using Hasura Actions to convert your Go REST API to GraphQL. Author: Gavin Ray"}, {"role": "assistant", "content": "This post is a part of a series which goes over how to convert an existing REST API to GraphQL by just defining types and configuring endpoints.Introduction\nIn this post, we will look at using Hasura Actions to convert your Go REST API to GraphQL.\nTypically writing a GraphQL server from scratch will involve setting up the GraphQL schema, type definitions and wiring it up with the resolvers in the language/framework of choice. The hard parts about this server component is the resolver code for someone who is used to writing a REST API with GET or POST and handling auth logic for each type; which becomes complex as the app grows bigger.\nHasura reduces bulk of this complexity of setting up a GraphQL API from scratch by giving you instant GraphQL CRUD for databases (currently Postgres) which should cover most of the data fetching and realtime subscription use cases.\nIn case of custom business logic, Hasura lets you extend the Graph through different ways. If you are comfortable writing GraphQL servers, you can totally add a custom GraphQL server as a Remote Schema and Hasura merges it automatically for you. In case you are looking to write a new or keep your existing REST API for the custom logic, you can do so using Actions which is what we will focus in this post.\nNew REST API\nIn case you are starting from scratch, we will define the GraphQL types required for the Action and create a simple Express app to handle the resolver bit. Let's take the example of registering a user.\nBefore we begin, let's create a project on Hasura Cloud to setup the Action. Click on the Deploy to Hasura button below, signup for free and create a new project.\n\nHasura requires a Postgres database to start with. We can make use of Heroku's free Postgres database tier to try this app.\n\nAfter signing in with Heroku, you should see the option to Create project.\n\nOnce you are done creating the project, click on Launch the Console button on the Projects page for the newly created project and the Hasura Console should be visible. By creating this project, you have already got a GraphQL endpoint for the Postgres database that is globally available, secure and scalable from the beginning.\nNow let's head to the Actions tab on the Hasura Console and define the GraphQL types for our app.\n\nHere we are defining the Mutation type registerUser which accepts the name, email and password arguments and returns the id of the user.\nWe can configure the handler URL later. Click on Create to create the Action.\nNow let's verify the generated GraphQL mutation by trying out the sample mutation in GraphiQL.\nmutation {\n  registerUser(\n    email: \"user@site.com\"\n    name: \"Person Name\"\n    password: \"mypass\"\n  ) {\n    id\n  }\n}\n\nThe above mutation should obviously give a http exception since the Handler URL has not been configured yet. But this test is to verify that Hasura generates mutations for you based on the types defined to query on the same GraphQL endpoint.\n\nNow let's go ahead and actually define the Go app and try out the mutation.\nCodegen: Auto generate boilerplate code\nNow head over to the Codegen tab to autogenerate the boilerplate code for your Golang server. Since we are generating the API from scratch, we need the full server setup instead of just the POST handler.\n\nThe console gives you options to generate code for different frameworks including but not limited to nodejs-express, nodejs-zeit, nodejs-azure-function etc.\nLet's run the application locally and test that it works. Create a file, for example main.go or app.go in a directory, and paste the contents of registerUser.go and registerUserTypes.go into it.\nThen, running go run <filename>.go will start a REST API on http://localhost:3000\nYou can quickly check that it's functioning correctly by hitting the endpoint with curl and simulating an Action payload:\n$ curl http://localhost:3000/registerUser \\\n    --data '{ \"input\": { \"name\": \"Person Name\", \"email\": \"user@site.com\", \"password\": \"mypass\" } }'\n$ {\"id\":\"<sample value>\"}\n\nNow we need to expose our local API so that our Cloud app can reach it. To do this, we can use a service like https://ngrok.com to create a publically accessible tunneled URL to our local app.\nAssuming the use of ngrok, running ngrok http 3000 on your terminal should give a unique URL you can use. Go back to the Modify tab of the registerUser Action that was created on the Hasura Console. Update the handler URL to one generated by ngrok.\nFinally let's try out the same mutation through GraphiQL and the connection should work returning a dummy  for id.\n\nAlright! We have a working GraphQL API that is resolved using a Go REST API in the background.\nNow you can modify your handler code as required to do any business logic; like connecting to a different API, connecting to a database (preferrably using Hasura's APIs) or using an ORM for a different databases etc. Hasura will take care of proxying the GraphQL mutation to the right REST API handler internally.\nPermissions\nWhat about permissions? Who will be able to make this GraphQL mutation from the client? Right now, it is configured to be admin only. We can configure roles to allow other kind of users to perform this mutation.\n\nIn the above example, I have created a role called public that is allowed to make this mutation. To read more about how the whole Authentication and Authorization works with Hasura, you can checkout the docs.\nRelationship Data\nNow consider that the Postgres database has users table. We can connect the id of the registerUser output to the id of the users table.\n\nThe related data also conforms to the permissions defined for the respective table. (ie. users)\nExisting REST API\nNow you might be wondering what if I already have a Go server with a bunch of endpoints written to handle some custom business logic. In that case, as long as it's a POST endpoint, you can just define the necessary GraphQL types and make some modifications to the way you handle request body to get it working quickly.\nThe request payload is of the format:\n{\n  \"action\": {\n    \"name\": \"<action-name>\"\n  },\n  \"input\": {\n    \"arg1\": \"<value>\",\n    \"arg2\": \"<value>\"\n  },\n  \"session_variables\": {\n    \"x-hasura-user-id\": \"<session-user-id>\",\n    \"x-hasura-role\": \"<session-user-role>\"\n  }\n}\n\nSo in our example, the input arguments - name, email and password was wrapped inside an input object. Now if you can make necessary modifications to your existing REST API to handle this request body, your GraphQL API will work as expected :)\nQuery Action\nThe above example showcased how to perform a GraphQL mutation. The same workflow can be extended to perform a GraphQL query which proxies to a Go REST API in the background. Typical use cases for query actions include computed fields, data enrichment, data transformations, fetching from multiple databases and API sources etc.\nSupported Frameworks\nThe REST API can be in any framework like Gin, Echo, fasthttp, or serverless functions. After all, Hasura just requires a reachable POST endpoint that it can forward the query to. In case you are going to deploy the API to a serverless function, you will need to follow the format of the cloud provider.\nAdditional Reading:\nCreating a GraphQL API for Notion with Hasura Actions\n\nCheck out the Go backend course. In this course, you will learn how to integrate Go in a GraphQL backend server stack with Hasura.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Data Federation in GraphQL using Hasura Remote Joins and Gatsby Source Plugins Description: We will look at how to use Gatsby Source Plugins as a Remote Schema source and federate data using Hasura Remote Joins for a unified data model and GraphQL API Author: Praveen Durairaju"}, {"role": "assistant", "content": "This post is a part of our Remote Joins (available from v1.3) series. Remote Joins in Hasura allows you to join data across tables and remote data sources.Before we dive into the actual use cases, let me introduce some terminologies and technologies being used.What is Data Federation?Data federation is a concept that allows for aggregating data from distributed sources together into a single, common data model but \"pushes\" down the execution to underlying sources. It doesn't contain the actual data. Instead, it contains metadata about where different data sources are located and how to fetch them on demand. Think of it as accessing a virtual database that contains all the data ever required for your application.With Data Federation, there is a clear separation of concerns. Teams can independently work on their service and incrementally adopt federation. There are a lot of benefits that comes along but that's for a different post.Hasura Remote JoinsRemote Joins in Hasura is the feature that enables Data Federation via GraphQL. Remote joins can join data across databases and other sources like APIs. These APIs can be custom GraphQL servers you write, 3rd party SaaS APIs, or even other Hasura instances.GraphQL enables federation more easily by design, via a schema and a type system. In Hasura, you would declaratively configure your external data sources as Remote Schemas. You can then go one step further to connect these remote schemas with the database or establish relationships between different remote schemas.The eventual goal is to aggregate every data requirement into a single graph that is type-safe, documented automatically and lets all clients access them with ease. Hasura becomes that Data gateway powered by GraphQL.Gatsby Source PluginsAlright, let's switch focus towards the example use case for this post - Gatsby Source Plugins. Gatsby allows you to pull data into your site using GraphQL and source plugins. The data in Gatsby sites can come from anywhere: APIs, databases, CMSs, local files, etc. Source plugins are written for different data sources by the community or by companies behind various SaaS APIs. For example, the gatsby-source-filesystem plugin knows how to fetch data from the file system and the gatsby-source-contentful plugin knows how to fetch data from the Contentful source and so on.Gatsby uses GraphQL to enable the app to declare what data it needs. All of your source plugins are merged to a unified schema. Though the catch is that, Gatsby uses GraphQL at build time and not for live sites. But they do serve the GraphQL endpoint during development. We are going to make use of this endpoint as an experiment to see what use cases open up.Hasura Remote Joins with Gatsby Source PluginsTry it out on Hasura CloudAlright. Let's try this out. Start by creating a project on Hasura Cloud. Click on the Deploy to Hasura button below, signup for free and create a new project.Hasura requires a Postgres database to start with. We can make use of Heroku's free Postgres database tier to try this app.Setup Hasura with Heroku PostgresAfter signing in with Heroku, you should see the option to Create project.Generate Postgres connection stringOnce you are done creating the project, click on Launch the Console button on the Projects page for the newly created project and the Hasura Console should be visible.Add Gatsby as Remote Schema in HasuraGatsby serves their graphql endpoint at /___graphiql that loads the GraphiQL UI tool on GET request and works as a graphql endpoint on POST request.In Hasura, under Remote Schemas, we will add this endpoint to explore the different data sources added via Gatsby.Deploy Gatsby Dev Server to HerokuFor Hasura Cloud to communicate with Gatsby's GraphQL endpoint, we will deploy the dev server of Gatsby on a public endpoint. Install the Heroku CLI.On any Gatsby project, run the following commands to deploy on Heroku.# create a heroku app\nheroku create\n\n# git push to deploy\ngit push heroku masterThis will create a Heroku app and deploy the dev server. The app starts by using the start script npm start as the entrypoint. Typically the start script for Gatsby is gatsby develop and that runs the dev server exposing the GraphQL endpoint.Once you have the Heroku app deployed, copy the app URL and add the suffix /___graphiql . For example if your Heroku app URL is https://gentle-reef-97253.herokuapp.com, then the GraphQL endpoint for the same would be https://gentle-reef-97253.herokuapp.com/___graphiql.You can add this endpoint as a Remote Schema in Hasura.Gatsby Remote Schema added to HasuraIf you want to try out Hasura and Gatsby locally, run gatsby in development mode - gatsby develop and add the endpoint http://host.docker.internal:8000/___graphiql as the remote schema endpoint. (where host.docker.internal can be used in Mac and localhost for linux)Gatsby Remote Schema in local devUse CasesLet's look at some of the data federation use cases that are possible with Hasura and Gatsby's GraphQL APIs.CMS ProvidersGatsby has support for lots of headless CMS options and lets you bring data from any of the popular providers. Here's a list of headless CMS providers that have a source plugin with Gatsby. For example, Contentful has a source plugin. You can configure this with the following in gatsby-config.js{\n      resolve: `gatsby-source-contentful`,\n      options: {\n        spaceId: `your_space_id`,\n        // Learn about environment variables: https://gatsby.dev/env-vars\n        accessToken: process.env.CONTENTFUL_ACCESS_TOKEN,\n      }\n},You can now join data from contentful with data in Postgres. Let's say you have a common id stored in both Postgres and Contentful for an entity product. You can now establish a relationship between these two nodes via the schema exposed by Gatsby.FilesystemThe gatsby-source-filesystem creates File nodes from the files present in your local filesystem. In combination with transformer plugins, you can perform specific transformations, \u00a0like using gatsby-transformer-json to transform JSON files into JSON data nodes and using gatsby-transformer-remark you can transform Markdown files into a MarkdownRemark node to get a HTML representation of the markdown.Source: Gatsby TutorialIn your gatsby-config.js you can add plugins and configure data sources. For example the filesystem plugin will have a configuration like:{\n      resolve: `gatsby-source-filesystem`,\n      options: {\n        name: `blogs`,\n        path: `${__dirname}/src/blogs/`,\n      }\n}Here, you specify which directory you want to create Nodes for and Gatsby will take care of the plumbing required to serve it via GraphQL.Now under Hasura's GraphQL endpoint, you should be able to query this directly. This gets interesting, when you can map files to a user record in Postgres. Yes! This is where Remote Joins come in.Remote RelationshipsThe users table has the following fields.idfull_namepicture_pathUsing the picture_path column, we can establish a relationship to the file query type in Gatsby's schema.Cloud File Storage Access (S3)The filesystem plugin gave you access to your local directories. There are also plugins to access files from cloud storage services like Amazon S3 and Dropbox etc. You will be using gatsby-source-s3-image for S3 and gatsby-source-dropbox for dropbox.Using the file URL, you can access metadata about the file from S3. Again, a simple example; this can be joined with users data to get metadata about the user's profile picture. Accessing Git reposgatsby-source-git can be used to pull files from arbitrary git repositories. For example, you can make a query like{\n  allFile(filter: { sourceInstanceName: { eq: \"graphql-engine\" } }) {\n    edges {\n      node {\n        extension\n        name\n        dir\n        extension\n        modifiedTime\n      }\n    }\n  }\n}And get access to the repo's tree structure. The remote join can be used to merge blog posts written in markdown in the repo tagged to a user.Git repo becomes the source of your markdown files and each user in Postgres is tagged to a set of markdown files as blog posts.RSS Feedsgatsby-source-rss-feed plugin lets you pull data from an RSS feed into Gatsby's GraphQL API. A user can subscribe to multiple blogs and data can be fetched from the subscribed blog. A typical query with remote relationship for this plugin will look like the one below.query {\n  users {\n    id\n    full_name\n    blogs {\n      nodes {\n        id\n        title\n        link\n        isoDate\n      }\n    }\n  }\n}With Hasura remote joins configured, you can make the above query to fetch the list of items in the feed. If you are following blogs of favorite sites, there will be a RSS feed for most blogs and topics. Since this is public data, it can be fetched on demand.There are more such categories of source-plugins, like social APIs of Twitter/Facebook and content sites like Wikipedia that can be leveraged. CaveatsThe GraphQL API exposed by Gatsby only runs on a development server. Since Gatsby compiles a site into static assets, the production server doesn't expose or need to expose a GraphQL API to the client.There's an RFC to add support for this by running an additional server using something like gatsby api-server. Until there is an official merge/release for that, you will have to run gatsby develop which might consume more resources and is probably not optimised for production. But from the limited experience of using this, it should be good enough for most small apps. If there's a complex use case, you might want to add some layer of caching.The Nodes are created during the initial load of the gatsby server and the data along with it. In case there are updates, you might need to restart the server to reflect the changes and hence in a highly dynamic scenario this solution might not scale. The gatsby dev server needs to be put in front of a proxy which will do basic token authorization. If you are self-hosting both Hasura and Gatsby dev server in the same cluster, you can choose to not expose Gatsby to the outside world.Community supported solutions like graphql-gatsby is a decent alternative, where you can choose to run a custom Node server with direct access to Gatsby's GraphQL schema and resolvers. This also lets you configure some form of authentication between Hasura and Gatsby's server and you can continue to use Hasura Cloud.SummaryGraphQL and its ecosystem enables Data Federation more easily than before. Gatsby source plugins opens up a lot of possibilities to add data sources from different places and makes it an interesting combination to federate data using Hasura remote joins.Gatsby's plugin ecosystem is huge with over 2000+ plugins of which 600 are source plugins alone. Do note that, this approach is more or less an experimentation till we have an official and stable api-server from Gatsby or the community.Read more about other Remote Join use cases.What kind of Gatsby Source plugins are you using the most? Which data federation use-case do you think will be relevant for you to perform with Hasura and Gatsby? Do let us know about it in the comments!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Turn your Ruby on Rails REST API to GraphQL using Hasura Actions Description: In this post, we will look at using Hasura Actions to convert your Ruby on Rails REST API written in Flask to GraphQL. Author: Marion"}, {"role": "assistant", "content": "This post is a part of a series which goes over how to convert an existing REST API to GraphQL by defining types and configuring endpoints.Hasura gives you instant GraphQL CRUD for databases (currently Postgres) which should cover most of the data fetching and real-time subscription use cases. However, sometimes we want to add custom business logic to our API. There are different ways of doing this with Hasura:Remote schemas: If you have an existing GraphQL server or you're comfortable writing one, you can add this GraphQL server as a Remote schema and Hasura merges it automatically. Actions: If you'd like to write a new or keep your existing REST API for custom business logic, Hasura actions are the way to go. Actions are a way to extend Hasura\u2019s schema with custom business logic using custom queries and mutations. Actions can be added to Hasura to handle various use cases such as data validation, data enrichment from external sources and any other complex business logic.In this post, we will focus on Hasura actions and we'll see how to use them to convert a Ruby on Rails REST API to GraphQL.Create a Hasura Cloud ProjectBefore we begin, let's create a project on Hasura Cloud to set up the action. Click on the Deploy to Hasura button below, sign up for free and create a new project.Hasura requires a Postgres database to start with. We can make use of Heroku's free Postgres database tier for the purpose of this app.Hasura Cloud Database SetupAfter signing in with Heroku, you should see the button Create project.Heroku Database Provisioning via Hasura CloudOnce the project creation is done, click on the Launch the Console button on the Projects tab for the newly created project and the Hasura Console should be visible. By creating this project, you have already got a GraphQL endpoint for the Postgres database that is globally available, secure and scalable from the beginning.Create an actionNow that we have our Hasura project up and running, let's create an action by heading to the Actions tab on the Hasura console and then clicking the Create button. Our action will look as follows:Action Definitiontype Mutation {\n  registerUser (\n    name: String!\n    email: String!\n    password: String!\n  ): UserOutput\n}Types Definitiontype UserOutput {\n  id : uuid!\n}We just defined a new mutation type called registerUser which accepts the name, email and password arguments and returns the id of the user.We can add the correct handler URL later. For now, we can just click the Create button to create the action.Let's verify if the custom mutation was generated by heading to the GraphiQL tab and trying it out:mutation {\n  registerUser(name: \"Marion\", email: \"marion@hasura.io\", password: \"mysecretpassword\") {\n    id\n  }\n}You will get the following http exception:The reason for the error is that the handler URL has not been configured yet. But this test confirms that the custom mutation was successfully generated based on the types defined to query on the same GraphQL endpoint.Now let's go ahead and actually define the Ruby on Rails app, so that this mutation will eventually succeed.Codegen: Auto generate boilerplate codeBack on the Actions tab and on our newly created action, click on the Codegen tab to auto-generate the code for our Ruby on Rails server. Since we are generating the API from scratch, we need the full server setup instead of just the POST handler.Business logic requiredThe following business logic will need to be added in the handler for this action:Receive the action arguments name, email and password on request.Convert the plaintext password input into a hashed secure password.Send a mutation to Hasura to save the newly created user with the hashed password.Return the created user object to signal success, or else error.Run the actionThe generated codegen (registerUser.rb) can be copied to your filesystem to make it possible to make the necessary changes to the handler.begin\n  require \"bundler/inline\"\nrescue LoadError => e\n  $stderr.puts \"Bundler version 1.10 or later is required. Please update your Bundler\"\n  raise e\nend\n\ngemfile(true) do\n  source \"https://rubygems.org\"\n  gem 'rails', '~> 6.0.0'\nend\n\nrequire \"action_controller/railtie\"\n\nclass App < Rails::Application\n  routes.append do\n    post \"/registerUser\" => \"hasura#register_user_handler\"\n  end\n\n  config.consider_all_requests_local = true # display errors\nend\n\nclass HasuraController < ActionController::API\n  def register_user_handler\n    request_data = params[:input]\n    puts request_data\n    render json: request_data\n  end\nend\n\nApp.initialize!\nRack::Server.new(app: App, Port: 3000).startregisterUser.rbOnce you copy the codegen, you can set up Rails and start running the server:If you haven't already, install Ruby Run the Ruby server:ruby registerUser.rbThis will first install all the Ruby gems and will then start up a server that is running on port 3000.Ruby development serverWith this, we need to update our handler URL for the action so that the HTTP call works. The registerUser endpoint needs to be added and exposed, so that the handler URL will look as follows http://localhost:3000/registerUser.Since we need a way to communicate with our Ruby server from the Hasura Cloud instance, we need to expose the Ruby server on a public URL. We can host this server on a cloud instance and point to that URL/IP. But for brevity of this tutorial, we can use ngrok, which gives public URLs for exposing local servers, so that they can be contacted by a cloud instance.Once you have ngrok installed, in a different tab, you can run the following command:ngrok http 3000This should expose your local Ruby server running on port 3000 on a public URL (something like http://b4318819ea61.ngrok.io).Go back to the Modify tab of the registerUser action that we created earlier on the Hasura Console. Update the handler URL to the above one.It would look like http://b4318818ea61.ngrok.io/registerUser. Note the usage of registerUser since that's the endpoint we are handling the mutation in.Test the new endpointFinally let's try accessing the /registerUser endpoint through GraphiQL. We want to make the connection work by returning a dummy value for id.First, we need to make some changes to the handler in registerUser.rb. Because we're using ngrok, we need to add the following line to the Rails application config:config.hosts = nilIt will then look like this:class App < Rails::Application\n  routes.append do\n    post \"/registerUser\" => \"hasura#register_user_handler\"\n  end\n  config.hosts = nil\n  config.consider_all_requests_local = true # display errors\nendAlso, make the handler endpoint return a dummy value for id:class HasuraController < ActionController::API\n  def register_user_handler\n    request_data = params[:input]\n    puts request_data\n    render json: {id: 1}\n  end\nendAfter that, restart the Rails server.If we now go back to GraphiQL on the Hasura console, and run the mutation again, the dummy id will be returned:registerUser mutation served by RubyAlright! We have a working GraphQL API that is resolved using a Ruby REST API written using Rails.Now you can modify your handler code as required to do any business logic; like connecting to a different API, connecting to a database (preferably using Hasura's APIs) or using an ORM for a different databases etc. Hasura will take care of proxying the GraphQL mutation to the right REST API handler internally.Connecting to database from inside RailsYou can connect to the database and perform any operations. The easier way to perform reads and writes to the database is to use the auto-generated GraphQL API of Hasura inside the handler. By making use of an admin secret you can perform any query from the backend as you will be simulating an admin permission with no restrictions.Permissions & relationshipsThe access control logic for this GraphQL API can be declaratively configured using Hasura's role based permission system. Head over to the Permissions tab on the action page to add a role and allow access.In the above example, we created a role called public that is allowed to execute this mutation. To read more about how authentication & authorization works with Hasura, you can check out the docs.This permission can be applied to both custom queries and mutations. Let's look at how this will apply for a relationship. Now consider that the Postgres database has a users table. We can connect the id of the registerUser output to the id of the users table.Relationship from action to postgresThe related data also conforms to the permissions defined for the respective table (i.e. users).Existing REST APISo far, what we have been looking at is creating a GraphQL API from scratch by mapping it to a new REST endpoint. What if there is an existing REST API endpoint that you want to reuse as a GraphQL API?It's quite possible to do this, as long as you are able to handle the request body format that Hasura sends to the endpoint. The format looks something like this:{\n  \"action\": {\n    \"name\": \"<action-name>\"\n  },\n  \"input\": {\n    \"arg1\": \"<value>\",\n    \"arg2\": \"<value>\"\n  },\n  \"session_variables\": {\n    \"x-hasura-user-id\": \"<session-user-id>\",\n    \"x-hasura-role\": \"<session-user-role>\"\n  }\n}In our example, the input arguments - name, email and password - were wrapped inside an input object. Now if you can make necessary modifications to your existing REST API to handle this request body, your GraphQL API will work as expected :)Deploying a Ruby APIThe Ruby API can be deployed as a serverless endpoint or lambda function if you want to architect your application in the 3factor architecture (read more about building 3factor apps with event-driven programming). It really doesn't matter where you deploy it since at the end of the day, Hasura just needs an HTTP POST endpoint for each action handler. This way, you can host a bunch of endpoints in the same server or hand off to serverless functions for each modular functionality. The GraphQL type system and metadata lives at the Hasura layer and hence your serverless function or endpoint doesn't need to worry about that part of the logic.Additional Reading: Creating a GraphQL API for Notion with Hasura Actions"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Turn your Python REST API to GraphQL using Hasura Actions Description: In this post, we will look at using Hasura Actions to convert your Python REST API written in Flask to GraphQL. Author: Praveen Durairaju"}, {"role": "assistant", "content": "This post is a part of a series which goes over how to convert an existing REST API to GraphQL by just defining types and configuring endpoints.In this post, we will look at using Hasura Actions to convert your Python REST API to GraphQL.Typically, setting up a GraphQL server from scratch would involve writing the GraphQL schema, type definitions and wiring it up with the resolvers. With Hasura Actions, the complexity of writing resolvers is reduced as one can just write a REST API POST endpoint and use that as a resolver for the GraphQL query or mutation.Hasura gives you instant GraphQL CRUD for databases (currently Postgres) which should cover most of the data fetching and realtime subscription use cases. But when there's some business logic that needs to be written, we can make use of Hasura Actions to map a REST API to be a custom resolver. We will look at how to do this with Python in this tutorial.Create a Hasura Cloud ProjectBefore we begin, let's create a project on Hasura Cloud to setup the Action. Click on the Deploy to Hasura button below, signup for free and create a new project.Hasura requires a Postgres database to start with. We can make use of Heroku's free Postgres database tier to try this app.Hasura Cloud Database SetupAfter signing in with Heroku, you should see the option to Create project.Heroku Database Provisioning via Hasura CloudOnce you are done creating the project, click on Launch the Console button on the Projects page for the newly created project and the Hasura Console should be visible. By creating this project, you have already got a GraphQL endpoint for the Postgres database that is globally available, secure and scalable from the beginning.Create an ActionActions are a way to extend Hasura\u2019s schema with custom business logic using custom queries and mutations. Actions can be added to Hasura to handle various use cases such as data validation, data enrichment from external sources and any other complex business logic.Now let's head to the Actions tab on the Hasura Console and define the GraphQL types for our app.Action Definitiontype Mutation {\n  registerUser (\n    name: String!\n    email: String!\n    password: String!\n  ): UserOutput\n}Types Definitiontype UserOutput {\n  id : uuid!\n}Here we are defining the Mutation type registerUser which accepts the name, email and password arguments and returns the id of the user.We can configure the handler URL later. Click on Create to create the Action.Let's verify if the custom mutation was generated by heading to the GraphiQL tab and trying it out.mutation {\n  registerUser(name: \"Praveen\", email: \"praveen@hasura.io\", password: \"mysecretpassword\") {\n    id\n  }\n}The above mutation should obviously give a http exception since the Handler URL has not been configured yet. But this test is to verify that Hasura generates mutations for you based on the types defined to query on the same GraphQL endpoint.Now let's go ahead and actually define the Python app and try out the mutation.Codegen: Auto generate boilerplate codeNow head over to the Codegen tab to autogenerate the boilerplate code for your python flask server. Since we are generating the API from scratch, we need the full server setup instead of just the POST handler.We are using the python framework Flask for this example, there's also python-fast-api boilerplate that can be used to replicate a similar example. In fact, any python server that can serve a POST endpoint can be used to swap with this example.Business logic required:Receive the action arguments name, email and password on request.Convert the plaintext password input into a hashed secure password.Send a mutation to Hasura to save the newly created user with the hashed password.Return the created user object to signal success, or else error.The generated codegen (registerUser.py and registerUserTypes.py) can be copied to your filesystem to start making the changes to the handler.from registerUserTypes import registerUserArgs, UserOutput\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route('/registerUser', methods=['POST'])\ndef registerUserHandler():\n  args = registerUserArgs.from_request(request.get_json())\n  print(args)\n  # business logic here\n  return UserOutput(id=\"1\").to_json()\n\nif __name__ == '__main__':\n  app.run(debug = True, host = '0.0.0.0')registerUser.pyfrom dataclasses import dataclass, asdict\nfrom typing import List, Optional\nfrom enum import Enum, auto\nimport json\n\n@dataclass\nclass RequestMixin:\n    @classmethod\n    def from_request(cls, request):\n        values = request.get(\"input\")\n        return cls(**values)\n\n    def to_json(self):\n        return json.dumps(asdict(self))\n\n@dataclass\nclass UserOutput(RequestMixin):\n  id: any\n\n@dataclass\nclass Mutation(RequestMixin):\n  registerUser: Optional[UserOutput]\n\n@dataclass\nclass registerUserArgs(RequestMixin):\n  name: str\n  email: str\n  password: strregisterUserTypes.pyOnce you copy the codegen, you can setup flask and start running the server:Setup FlaskYou can start by installing flask:pip3 install flaskand then run the python server using:python3 registerUser.pyWith this you should start seeing your flask server running on port 5000.With this, we need to update our handler URL for the Action so that the HTTP call works. Add the registerUser endpoint that handles the mutation. So the final handler URL will look like http://localhost:5000/registerUser.Since we need a way to communicate with our python server from the Hasura Cloud instance, we need to expose the python server on a public URL. We can host this server on a cloud instance and point that URL/IP. But for brevity of this tutorial, we can use ngrok, which gives public URLs for exposing your local servers to be communicated by a cloud instance.Once you have ngrok installed, you can run the following command:ngrok http 5000This should expose your local python server running on port 5000 on a public URL (something like http://b4318818ea61.ngrok.io).Go back to the Modify tab of the registerUser Action that was created on the Hasura Console. Update the handler URL to the above one.It would look like http://b4318818ea61.ngrok.io/registerUser. Note the usage of registerUser since that's the endpoint we are handling the mutation in.Finally let's try out the same mutation through GraphiQL and the connection should work returning a dummy <value> for id.registerUser Mutation served via PythonAlright! We have a working GraphQL API that is resolved using a Python REST API written using Flask.Now you can modify your handler code as required to do any business logic; like connecting to a different API, connecting to a database (preferrably using Hasura's APIs) or using an ORM for a different databases etc. Hasura will take care of proxying the GraphQL mutation to the right REST API handler internally.Connecting to database from inside flaskYou can connect to the database and perform any operations. The easier way to perform read and writes to the database is to use the auto-generated GraphQL API of Hasura inside the handler. By making use of Admin Secret you can perform any query from the backend as you will be simulating an admin permission with no restrictions.Permissions & RelationshipsThe access control logic for this GraphQL API can be declaratively configured using Hasura's role based permission system. Head over to the Permissions tab on the Action page to add a role and allow access.In the above example, I have created a role called public that is allowed to make this mutation. To read more about how the whole Authentication and Authorization works with Hasura, you can checkout the docs.This permission can be applied to both custom queries and mutations. Let's look at how this will apply for a relationship. Now consider that the Postgres database has users table. We can connect the id of the registerUser output to the id of the users table.Relationship from action to postgresThe related data also conforms to the permissions defined for the respective table. (ie. users)Existing REST APISo far, what we have been looking at is create a GraphQL API from scratch by mapping it to a new REST endpoint. What if there is an existing REST API endpoint that you want to reuse as a GraphQL API?It's quite possible to do this, as long as you are able to handle the request body format that Hasura sends to the endpoint. The format looks something like this:{\n  \"action\": {\n    \"name\": \"<action-name>\"\n  },\n  \"input\": {\n    \"arg1\": \"<value>\",\n    \"arg2\": \"<value>\"\n  },\n  \"session_variables\": {\n    \"x-hasura-user-id\": \"<session-user-id>\",\n    \"x-hasura-role\": \"<session-user-role>\"\n  }\n}So in our example, the input arguments - name, email and password was wrapped inside an input object. Now if you can make necessary modifications to your existing REST API to handle this request body, your GraphQL API will work as expected :)Supported FrameworksThis was an example with Python Flask. But as you can imagine, this would work with any python server framework that can serve a POST Endpoint. So you can technically use frameworks like Django, Bottle, gunicorn, Pyramid, Tornado, web2py and so on.Deploying Python APIThe python API can be deployed as a serverless endpoint, lambda function if you want to architect your application in the 3factor architecture (read more about building 3factor apps with event-driven programming). It really doesn't matter where you deploy since at the end of the day, Hasura just needs a HTTP POST endpoint for each Action handler. This way, you can host a bunch of endpoints in the same server or hand off to serverless functions for each modular functionality. The GraphQL type system and metadata lives at the Hasura layer and hence your serverless function or endpoint doesn't need to worry about that part of the logic.Additional Reading: Creating a GraphQL API for Notion with Hasura ActionsCheck out the Python backend course. In this course, you will learn how to integrate Python in a GraphQL backend server stack with Hasura."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a Calendar Scheduling App Backend with Hasura Scheduled Triggers Description: In this post, we will look at how a calendar app backend with complex scheduling logic can be implemented with Hasura GraphQL and Scheduled Triggers API in Hasura. Author: Praveen Durairaju"}, {"role": "assistant", "content": "In this post, we will look at how a calendar app backend with scheduling logic can be implemented with Hasura GraphQL and Scheduled Triggers API in Hasura.The core functionalities of this app would follow the event-driven programming paradigm and hence, would involve:Scheduling a one time event with cancellation logic.Scheduling a recurring event with cancellation logic (one instance of the event or the whole event)Event creator should be able to invite other users to their event.Check availability of the user to be invited.The hard parts about building this app would be about confirming event based on availability, sending email reminders / notifications a few minutes before the event begins and handling for any changes (like modified timestamp) etc.I'm taking the simplest of schema and will be talking about where logic fits in and how scheduling can be done.Database SchemaUsers want to schedule an event for a specific time and invite users to their event. The simplest schema for scheduling an event would have three tables.user (users of the app)event (contains the list of events created by any user)event_attendee (all the users who are participants of the event)Event Database SchemaFor recurrence pattern, iCal RFC is being followed which will have the format like FREQ=DAILY;INTERVAL=10;COUNT=5. We will make use of this format later to validate and schedule in our custom code.Hasura GraphQLWe are going to make use of Hasura to build the backend with GraphQL. Hasura auto-generates CRUD APIs for the app, allows for declarative authorization rules and lets you schedule events in the future.The database schema in Postgres along with the Hasura project setup is available here on github.Data FetchingLet's look at different data fetching queries that are required for a calendar app.Fetch all events of a user for a given day/week/month/yearThe fundamental requirement of the UI would be to look at the schedule (either upcoming or past) for a given date range. This can be simply retrieved using the following query:query fetchEvents($from: timestamptz!, $to: timestamptz! ) {\n  event(where: {\n    start_date_time_utc: {_gte: $from},\n    end_date_time_utc: {_lte: $to}\n  }) {\n    title\n    start_date_time_utc\n    end_date_time_utc\n    recurrence_pattern\n    is_recurring\n    is_all_day\n  }\n}\nNote the usage of $from and $to variables which can be substituted with the timestamp value. These variables can be passed in the right values depending on date range (day/week/month/year), typically part of any calendar / event booking UI.Business LogicThe read queries require no code to be written since we are making use of the Instant GraphQL APIs. But there's more to this app than just using the auto-generated API. For example, Validating if the event can be scheduled based on availability and checking if dates are valid.Sending invitation emails upon event confirmation.Schedule an email or notification reminder just few minutes before the event begins.This requires hooking into the mutation for inserting an event.Scheduling EventsLet's come to the core part of the app; Scheduling. The ability to get notified a few minutes before or just on time of the event is one of the core parts of the app that we are covering in this post. This is trivial to do with Scheduled Triggers with Hasura.Hasura has two types of scheduled events:Cron triggersOne-off scheduled eventsScheduled Triggers in HasuraAs the name suggests, Cron triggers are used to schedule events on a cron schedule. (For example, run every day at 12AM). In a calendar app, this is useful to setup reminders/notifications for a recurring event.One-off scheduled eventsOne-off scheduled events are used to reliably trigger an HTTP webhook to run custom business logic at a particular point in time. This is useful to setup a webhook to run only once in the future. This is a more common use case for our calendar app.Actions for custom mutationThe logic of scheduling events will take place immediately after creating an event. Hasura gives you GraphQL mutations as part of its auto-generated API. But since we have some custom logic to check if event is valid and some scheduling of tasks to be done, we will make use of Actions to write a custom mutation that handles this.The createEvent custom mutation will look something like:mutation createEvent($object: eventInput! ) {\n  createEvent(object: $object) {\n    id\n  }\n}This is how the types look like for the Action:type Mutation {\n  createEvent (\n    object: eventInput!\n  ): eventOutput\n}If you look at the input type eventInput, we have just derived the types from the original mutation that was auto-generated by Hasura and added one more object called attendees so that we can tag who are all attending the event in the same mutation call.input eventInput {\n  created_by : Int\n  end_date_time_utc : timestamptz\n  is_all_day : Boolean\n  is_recurring : Boolean\n  recurrence_pattern : String\n  start_date_time_utc : timestamptz\n  title : name\n  attendees : [attendeeIds]\n}\n\ninput attendeeIds {\n  id : Int\n}\n\ntype eventOutput {\n  id : uuid!\n}Alright! Now that the types are out of the way, let's write the resolver. In Hasura Actions, your resolver is basically a HTTP POST endpoint and you don't need to write a GraphQL server from scratch.The codegen tab under an Action gives you boilerplate code to get started. You don't need to even set this up, no kidding :DValidationsOnce you have the boilerplate setup, the first thing to start writing would be validations. We have a few validations to be performed.Check if the event start date and end date are future date/time and valid.Check if the user who is creating the event and the user who is attending the event are available during that time. Now this may not be a blocking validation and we can totally allow multiple events for the same timeframe.Create a scheduled eventOnce the mutation is complete, we would like to schedule an event a few minutes before the start time (start_date_time_utc) of the event.The request body for this one-off scheduled event will roughly look like:{\n    \"type\" : \"create_scheduled_event\",\n    \"args\" : {\n        \"webhook\": \"<my_webhook_url>\",\n        \"schedule_at\": moment(start_time).subtract('10','minutes'),\n        \"payload\": {\n            attendees_data\n        }\n    }\n}The payload contains the attendees data (in this case the user_id) to actually send out email notifications.Cancellation LogicThe application typically allows for cancelling scheduled events. In this case, we need to be able to handle this logic inside the scheduling webhook. The webhook can check the status of event and event_attendee to verify if the event is still valid to be executed.For example, if the status column on the event table says cancelled, then the scheduled event can be ignored.Note: You don't need to delete the scheduled event, but rather put the logic in place inside the webhook to understand if the event is valid at the time of execution.Webhook for SchedulingThe webhook URL that you specified during the creation of scheduled event must handle the cancellation logic mentioned above. This will also be responsible for sending out emails/reminder notifications as part of the app logic.Fetch the event details and check the status columnIf event is cancelled, don't proceed furtherIf valid, send email to all notificationsPermissionsWhat about security? Hasura gives you role based access control that can be configured declaratively. Depending on the app we might want to restrict who is able to read the event, who is able to create an event etc.We would like to enable anybody who is logged in to be able to create an event. Let's assume logged in users are assigned the role user.For example: The insert permission for event looks like the following:Event Insert PermissionSo who can invite attendees to the same event? Most likely the creator of the event.Note that we are making use of relationships here and applying permission. If the requirement is to allow any existing participant to invite other participants, then we can apply the following insert permission:This is a boolean expression which lets either the creator of the event or the attendee of the event to invite other users to participate in the event. The permission rules can be tweaked quickly according to the app requirement.For the custom mutation createEvent we need to allow the role user to be able to execute the mutation. This can be done via the Actions manage tab.We can also configure who can update the event; If all participants of the event can update or just the creator of the event. With flexible boolean expressions, the permission can nest to different relationships and conditions.SummaryThe freedom of writing custom logic in language of choice but making configuration declarative is a great balance to build complex apps. Hasura's eventing system helps build asynchronous logic away from the core API, and with serverless, you don't need to worry about scaling and cost.The permission layer allows you to build custom logic via Actions, yet keep it secure via metadata configuration. Role based access control lets you focus on the scheduling logic of the app than the permissions.Good ReadsRecurrence"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Experiments with Arduino and Hasura Description: The goal of this project is to measure the room temperature using a combination of Arduino Uno / LM35 Temperature Sensor and keep pushing it to a Hasura Cloud instance every one minute, over a 12-hour period. Author: Meera Sundar"}, {"role": "assistant", "content": "What is Arduino?Arduino is an open-source electronic prototyping platform, usually consisting of a single-board microcontroller. There are a bunch of different variants at the entry-level: Uno, Nano, Leonardo, Due, Mega etc. In this post, we\u2019ll use the Arduino Uno. The Arduino and the Arduino-IDE come with native support for C & C++ languages.Say hello to Johnny-five!Johnny-five is a Javascript robotics & IoT platform. It is an easy way for web developers to get up-and-running with IoT, without any worries about learning a new language. Let\u2019s get started!There is a bit of initial board setup that is required. Johnny-Five uses something called the StandardFirmataPlus firmware to communicate with the Arduino board. We need to set this up before we start executing our nodejs code:Download the Arduino IDE for your machine from here and install itConnect your Arduino board to your machine and open up the IDENavigate to File -> Examples -> Firmata -> StandardFirmataPlusThis would open a \u201csketch\u201d in the IDE. Load this on to the board.Next, create a new project directory from your machine\u2019s Terminal / command line: mkdir hello-blinky-projectMake sure you have nodejs installed on your computerSwitch to the project directory and install the johnny-five library:cd hello-blinky-projectnpm install johnny-fiveThe traditional \u201cHello World!\u201d program becomes the \u201cHello Blinky!\u201d hereRun this program in the Terminal: node blinky.js.This should make the on-board LED(or an external LED pin connected to port #13) to blink, once every 500 milliseconds.That was pretty simple. Let\u2019s move on to cooler stuff now.Hasura-controlled LEDsLet\u2019s do more LED blinking, but this time based on the result of a GraphQL query. We have an instance setup on Hasura Cloud. The schema is super simple, and has a couple of tables: authors and books. We use Apollo-Client to send out a query for a particular author in our table. If the author row is present, the LED blinks every 1000 milliseconds. If the author row is not in our database, the LED blinks every 3000 milliseconds(!)Testing out Hasura Cloud-Apollo Client-Arduino setupThis is a very basic way to test out our Hasura-Apollo-Arduino setup. Enough with the LEDs now, let\u2019s move on to more serious stuff!Room Temperature Sensor + HasuraThe goal of this project is to measure the room temperature and keep pushing it to a Hasura Cloud instance every one minute, over a 12-hour period.This is our setup-Arduino UnoLM35 Temperature SensorBreadboard and jumper wiresThe circuit diagram and sensor related code can be found hereWe again have an instance setup on Hasura Cloud. Our schema has a single temperature table with these fields:We continue using Apollo Client and Johnny-Five in our code.Notice that we configure the thermometer sensor at the beginning. The frequency is set to 60000 milliseconds ( = 1 minute). We use the on(\u201cdata\u201d) and off parameters from the Thermometer APIEvery 60 seconds, the temperature object\u2019s on(\u201cdata\u201d) function is called. Within this is our GraphQL mutation, which sends out the current room temperature in celsius, fahrenheit and kelvin. This is a snapshot of the data pushed, as seen from the Hasura console:Room Temperature Data over a 12 hour periodAs one might expect, there isn\u2019t much variation in the temperature values that are measured every minute. This can be fixed by reducing the thermometer object\u2019s frequency and running this for 24+ hours.That\u2019s it! We now have a homegrown weather service! The full code is available here.There are a ton of cool ways to integrate the Arduino with GraphQL and Hasura. Here are a couple of ideas:Flame sensor + Hasura + Event TriggersDIY GPS + Hasura + PostGISLet us know what you\u2019ve been building!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Moving From Django to Hasura and Next.js Description: Read about an experience of moving from Django to Hasura with the outcome of better developer experience, better performance and lower costs. Author: Vishwa Mehta"}, {"role": "assistant", "content": "Or how to save money by speeding up your systemHello Django\u201cI\u2019m sorry Django, it\u2019s not you, it\u2019s me.\u201d Such could be a start of a cliched tech article or conference talk. \u201cIt was back in 2010 when we first met, and you looked great, probably because there weren\u2019t many others on the market to consider.\u201d A less romantic statement could follow. Indeed back in 2010 we migrated our news publishing app from .NET to Django and we were thrilled. We didn\u2019t like the locked down nature of the Microsoft universe, PHP was already uncool, and Java frameworks were only for banks, insurances, or something. Besides those, there were only Ruby on Rails and Django on the market of open source frameworks. And given the simplicity and likeability of Python as well as in-house Python expertise, Django was the obvious winner.Django was great: mature and stable, amazing ORM, built in authentication and authorization, automatically built admin interface - almost the whole CMS for free, a superb plugin ecosystem, or as Djangouts call them - \u201capps\u201d. We were a great new couple, happily in love, went to conferences together, yada yada yada. What went wrong?As years went by, developers in the team kept increasing their Javascript skills for all things web dev, and even built desktop apps with Electron, mobile apps, etc. It was a matter of time for server side JS to pick up the pace as well, so everyone started flirting with Node, Express, and similar. We started building smaller services on the side and over time \u201cnpm i\u201d and \u201cnpm start\u201d started feeling much more natural than \u201cpip install -r requirements.txt\u201d, \u201cuwsgi\u201d started feeling strange, upgrading Celery felt like nightmare that could take days, setting up local dev environments with Python 3 alongside mandatory Python 2.7 was awkward, also - Django templates vs. React on the front end? Ugh.The whole thing also of course had very real implications on the app performance and cost on AWS. The countless days we spent staring into AWS charts, the experiments, just didn\u2019t bring in improvements that we felt were possible, the AWS cost kept on increasing, and first we attributed it to more and more traffic on our app, and around 15 employees really hammering it well, with around 15k daily users also being very active. But something just didn\u2019t feel right, we knew we must get better performance and lower cost. Worst of all, our DB on RDS would just randomly go berserk, the CPU up to 100%, for no obvious reason. Then spin up an even bigger RDS instance, dive into logs, charts, redeploy the app, are we hacked? Is it DDoS? We tried everything under the sun to fix it, even with some Django community celebs, but nothing would really cut it.Given all this, we were constantly on the lookout for something in the NodeJS community to pop along that would let us try and seamlessly move from Django, but somehow for various reasons, none of the frameworks seemed really up to the task, and we tried quite a few.Hello HasuraIt was in May, springtime in Paris, the perfect time to fall in love again. I was at a React conference in Paris and I attended a GraphQL / Hasura workshop by Vladimir Novick. At first thinking it\u2019s just another plug for someone\u2019s open source project, I was blown away in minutes. The plan was quickly hatched to try and move the frontend part of our app to NextJS, fed by GraphQL from Hasura, connected to the PostgreSQL database, the same DB that would still be in use by Django for the admin part. So in stage one, we would just move the front end to NextJS, and leave the admin part on Django, and someday later also move the admin part to Node.Three goals: Better developer experience, performance at least slightly better, cost at least slightly lowerFirst experimentsWe wanted to make sure what we\u2019re doing, and wanted to do extensive tests and experiments on staging first, before deciding to use Hasura + Next JS in production too. We did a proof of concepts in three steps. Which had to bring benefits in three areas, if it did, we would then go and port the app. We wanted better developer experience, cost savings on AWS, and at least a bit of improvements on performance with ability to further tweak it in an easier fashion than Django app.Everything worked out of the box on the first tryStep 1 - set up Hasura to expose GraphQL (let\u2019s see if it even works with our DB)We set up the Hasura on our staging DB, and first thing we noticed that everything strangely worked out of the box on first attempt. Something like this very rarely happens, not in the open source world, and also not with paid products. To throw new middleware technology against a huge, legacy DB, and everything from installation to correctly resolving all the foreign keys, constructing GraphQL out of the box, it was short of a miracle. Took us maybe one hour in total and we had a working GraphQL API for hundreds of tables and relationships. Wow.On the figure below you can see all these database relationships recognized on the left, and the visual, auto-complete query builder with JSON-esque GraphQL syntax.Step 2 - build few pages to display the dataSo with GraphQL working, it was time to build a frontend to test it out. We decided to rebuild the home page, the news listing page and a news detail page, only this time in NextJS instead of Django templates. We knew React, so we had that down pretty quickly, in a matter of two days, our app with three pages was working. Step 3 - benchmark and compare to DjangoFirst of all we did a few experiments on the staging app, with just enough UI to test the system. We wanted to be sure that we\u2019ll get some benefits in performance before going to port the production system.We used a few benchmarks to see if the new stack is indeed going to bring:Apache BenchLighthouseApache bench tests started giving much better results than Django and there were very significant improvements in Lighthouse too. In fact it was so much better, we thought we might be making a mistake, we\u2019re not measuring correct things. So for weeks we kept on hammering the app with more and more requests, trying to slow it down, break it in any way possible, but at the end it was obvious that \u201cit just works\u201d.But still, production is a different beast, and we knew it could bring all sorts of new issues, unforseen on staging.It worked great on staging, but we knew production is a different beastProductionEncouraged by the results on staging experiments we finally decided to move the production to the same stack. So the backend admin part would be left as-is on Django, but the frontend part would move to Hasura and NextJs. Below is a simplified diagram of how we set it up on AWS.It is too complex to explain all the details of the setup, there are docker files, ngnix configs, DNS settings on Area 15, build systems, etc. Also important to npt is that Hasura is used as read-only middleware for now, we are not using mutations to save the data to DB but special API\u2019s on Django to accommodate certain features for front end, like registration, login, content upload, which still happens by Next.js calling the Django API. This is obviously something we would like to get rid of in the future and directly call the GraphQL mutations, but for the time being, and given it works nicely, it is good enough.Production showed even bigger benefitsDXIt took us a bit more than three months to rewrite all the frontend code. It was really a pleasure moving from Django templates and writing code in React, we could split the frontend into components tested in Storybook, Jest tests, use all the other familiar JS toolsets, everyone was immediately familiar how to set up and run the project locally, frontend devs could easily set up and modify GraphQL queries, something that in the past wasn\u2019t easy. The DX improvement was clearly achieved. Developers were smiling again.Then came the big day. Moving things to production is always scary, so we set up one certain weekend to get it done, test, and if needed revert back. Weekends still have lots of visits but very few users and no employees are uploading content, so it was the perfect time to test things at scale but without the fear of breaking people\u2019s workflows and ruining their day. \u00a0In about an hour, some fiddling with Postgres and DNS\u2019s, the site was live, and quickly we jumped on CloudWatch, staring into charts like maniacs. The results were stunning. The charts mostly speak for themselves so I\u2019ll just add a short commentary. Database CPU PerformanceThe most problematic part of the stack is the database, the single source of truth, with no real dynamic scaling possibilities on AWS, has to run all the time with all the data baggage accumulated over the years. It is sort of like the heart, if it stops, everything stops. Under Django this was often under stress for no obvious reasons so this was the very first metric we were interested in.10x performance boost for \u00bc of the priceApplication CPU Performance The situation with application servers was now a bit different, because we have two apps - Django for the backend, Next.js/Hasura for the front end. So we established two different environments on ELB, each with its own autoscaling rules but we used the same instance types for both. Left chart is the Django app and the right is Hasura / Next.js. You can see that after the switch Django CPU fell from ~30 to 4%, but that was expected since it is now only doing the backend work, running the Django admin. The new frontend app requires somewhere between 15 to 35%, sometimes it spikes to 60% but rarely above. Here as well, we reduced the server size from one ELB environment with m4.large instances to 2 environments with t3a.small or t3a.medium, we\u2019re still experimenting a bit on what is the best, but roughly this brings us EC2 savings of some 30%.Other Performance MetricsApache Bench is how it all started so this is a bit to consider. Running the following command showed approximately a 5x performance boost.:ab -n 100 -c 3 \u201chttp://our.url\u201d\n\nLighthouse speed score went from single digits to comfortably in the 30\u2019s, about a 10x boost.Latency on load balancer went from 1500-ish ms, down to ~30ms, so 50x better.Request count on all systems on AWS went from ~5k/s to ~80k/s, so roughly.About the authorThis blog post was written under the Hasura Technical Writer Program by Alen Balja - Full stack tech lead and data scientist with experience in aerospace, health sciences and gaming."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Scheduled Triggers: API Driven Cron Jobs and Scheduled Events Description: We are happy to announce the stable release of v1.3 bringing in a slew of features like Data Federation via Remote Joins, Support for Relay and Scheduled Triggers. Author: Praveen Durairaju"}, {"role": "assistant", "content": "We are happy to announce the stable release of v1.3 which brings a slew of features like Data Federation via Remote Joins, Support for Relay and Scheduled Triggers.Scheduled Triggers are used to execute custom business logic, via webhooks, at specific points in time. It extends the idea of Event Triggers (which are driven by table events) to a flexible API that can be used to schedule events at some time(s) for any payload and webhook.Scheduled Triggers are a great fit with serverless because each event can be mapped to an individual function which can scale infinitely with no-ops!Scheduled Triggers comes with two types of scheduling: Cron Triggers - used to execute periodic events according to a cron schedule and One-off Scheduled Events - used for one-time execution, typically created inside some business logic.API driven schedulingScheduled Triggers can be created from both the console and through metadata APIs. You may use the console to create a Cron Trigger which has a fixed payload on a static schedule. You may also use the console to create any adhoc one-off scheduled events.But suppose your event payload is dynamic (say depending on the user), then you can also schedule an event via the metadata API. This metadata API would typically be called from inside your application code. The simplest API call to schedule a one-off scheduled event will look like:{\n  \"type\": \"create_scheduled_event\",\n  \"args\": {\n    \"webhook\": \"https://my-awesome-serverless-fn.com/send-email\",\n    \"schedule_at\": \"2022-07-20T12:45:00Z\",\n    \"payload\": {\n      \"email\": \"bob@ross.com\"\n    }\n  }\n}API for creating a scheduled eventYou can also create cron triggers dynamically via metadata API, e.g. when you want to perform periodic actions for each user.How does it work?When you create a scheduled trigger, with a cron schedule or timestamp, Hasura will create event(s) in the database with a timestamp associated with it. There is a worker thread running in the background which fetches events at the appropriate time that need to be delivered. There is another thread which generates future events for Cron triggers. The event delivery mechanism is similar to Event Triggers i.e. atleast-once guaranteed. Hasura also stores the full logs of every invocation for later retrieval.Fault ToleranceIf Hasura is down, newer events won't be generated (for Cron triggers) or existing events wouldn't be processed temporarily. But what happens when the server is back up and running? In this case, Hasura will rehydrate the events and existing events that were scheduled to run will be (re)triggered based on a customisable tolerance configuration. For example: If an event was scheduled for 12am and Hasura went down at 11:59pm and came back up at 12:01am, depending on the tolerance config (default: 6 hours), Hasura will attempt to deliver these existing events.Hasura also gives a flexible retry configuration to retry the event in case of HTTP failures. This includes number of retries and interval between each retry (to avoid overloading your webhook).Metadata workflowCron triggers will usually be part of your Hasura metadata as it's configuration will typically be fixed. This will ensure that it can be migrated between different environments (like staging, production). If your cron trigger is dynamic (say contains user info in payload), then you can skip including it in metadata as well.One-off scheduled events will typically be API-driven and can be archived once the event is delivered. Hence, it doesn't need to be part of your metadata workflow and can remain local to the environment where it is created.Actions & Scheduled TriggersScheduled triggers goes well with Hasura Actions as you would typically create a scheduled event as part of a mutation with custom business logic. Let's take the example of scheduling a tweet: You will create a custom mutation through Actions and define the logic to validate and insert the tweet into the database. Once the payload is validated, you will be calling the create_cron_trigger or \u00a0create_scheduled_event for the specified timestamp and the event delivery will be taken care by Hasura.Both Actions and Scheduled Triggers fit nicely in the serverless pattern as the source of data and logic is spread across different APIs. Of course you are not limited to serverless runtimes, since all Hasura needs is a valid HTTP POST endpoint.Use CasesThe typical uses for cron triggers are recurring events like:Daily reportsAutomated weekly newslettersRecurring remindersUse cases for one-off scheduled event:Schedule a tweet for tomorrow at 2:30pmSend a feedback email to user after a week post order delivery in an e-commerce appWe did a demo in one of our community calls earlier and you can check it out to see how events can be scheduled both via Console and through an API.Hasura Community Call - Scheduled Triggers DemoTry it out!Get started with Scheduled Triggers on Hasura Cloud today!If you're already running Hasura, make sure you're on version 1.3 or above and you're good to go!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura GraphQL v1.3 - Data Federation, Relay, Scheduled Triggers and more Description: After few weeks of being in beta, Hasura v1.3.0 is now released. This is one of the biggest Hasura releases till date, with 3 huge features being launched - Remote Joins, Relay support and Scheduled Triggers. Author: Tirumarai Selvan"}, {"role": "assistant", "content": "After few weeks of being in beta, Hasura v1.3.0 is now released. This is one of the biggest Hasura releases till date, with 3 huge features being launched - Remote Joins, Relay support and Scheduled Triggers. See the changelog for more details! Let's talk about these features in the broader context of what they enable.Data Federation Data Federation enables a unified data layer for all your data and a federated way to execute requests on it. With Hasura Remote Joins, data federation becomes easy. All you need to do is point your data sources to Hasura and a unified GraphQL API is created automatically. But the real magic of Remote Joins is that you can create relationships across these various data models (with different underlying sources) using declarative configuration and get \"joined\" data to your clients instantly.In this release, you will now be able to create relationships across tables and remote schemas. Imagine building an e-commerce website with a user's order page getting data from Postgres as well as Stripe in a single query. Or having multiple domain databases and performing a join across them similar to a normalized table join.#JoinAllTheThingsYou can learn more about Remote Joins from the announcement blogpost here and the docs for getting started!RelayRelay allows you to combine React and GraphQL to build highly modular and performant Javascript applications. For this, Relay comes with it's own spec on top of GraphQL which Hasura now serves over Postgres! This is a great addition to the arsenal for frontend developers to build data-driven apps using the power of GraphQL.We have released Relay as a beta endpoint in v1.3.0 as there are more things we are going to include in future releases before promoting it to v1. You can learn more about Relay and Hasura in this announcement blogpost and the docs for getting started!Scheduled TriggersExtending the eventing capabilities of Hasura, you can now trigger webhooks based on a cron schedule or timestamp. Scheduled Triggers open up a whole new set of use-cases in Hasura like generating daily reports or sending post-signup emails.Trigger webhooks based on time scheduleRead more about Scheduled Triggers in the announcement blogpost and the docs for getting started!Other improvements and road aheadThis release also extends various existing functionalities like accessing session variables inside computed fields, automatic dependency cascade in the console, a new seed command in the CLI, among others. Read the changelog for the full list of changes.Our plans for the near future is to continue to improve and extend existing features while also bringing new features like GraphQL transactions and few surprises. Stay tuned!Try it out!Get started with Scheduled Triggers on Hasura Cloud today!If you're already running Hasura, make sure you're on version 1.3 or above and you're good to go!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing another set of GraphQL Fireside Chats: a Special Series of Online Events Description: A series of live streamed panel discussions about GraphQL and its ecosystem. This edition explores using GraphQL along with Angular, React, Vue and Relay.  Author: Hasura"}, {"role": "assistant", "content": "Photo by Joris Voeten on UnsplashWe are thrilled to be partnering with This Dot Media for another special series of GraphQL online events focused on various front-end frameworks!Starting on July 28th, 2020, we will be co-hosting weekly \u201cGraphQL Fireside Chats\u201d, where each week we\u2019ll focus on GraphQL + Flutter, Vue, Angular, and Relay! Join the panel of special guests for a roundtable discussion and lively Q&A.The purpose of these events is for the GraphQL community to come together and discuss some key topics in the GraphQL ecosystem and facilitate cross collaboration amongst the community. We are thrilled to be able to connect with fellow GraphQL enthusiasts while also practicing safe social distancing!Check out the schedule below:July 28th, 2020: GraphQL + Flutter with special guests Chris Sells, \u00a0Tanmai Gopal & more!August 4th, 2020: GraphQL + Vue with special guests Debbie O'Brien, Natalia Tepluhina, Tanmai Gopal, & more!August 11th, 2020: GraphQL + Angular with special guests Bonnie Brennan, Chris Whited, Tanmai Gopal, Uri Goldshtein, & more!August 18th, 2020: GraphQL + Relay with special guests Joe Savona, Tanmai Gopal & more!Don\u2019t forget to sign up to receive the live link the day of!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Turn your Node.js REST API to GraphQL using Hasura Actions Description: In this post, we will look at using Hasura Actions to convert your Node.js REST API to GraphQL. Author: Praveen Durairaju"}, {"role": "assistant", "content": "In this post, we will look at using Hasura Actions to convert your Node.js REST API to GraphQL.Typically writing a GraphQL server from scratch will involve setting up the GraphQL schema, type definitions and wiring it up with the resolvers in the language/framework of choice. The hard parts about this server component is the resolver code for someone who is used to writing a REST API with GET or POST and handling auth logic for each type; which becomes complex as the app grows bigger.Hasura reduces bulk of this complexity of setting up a GraphQL API from scratch by giving you instant GraphQL CRUD for databases (currently Postgres) which should cover most of the data fetching and realtime subscription use cases.In case of custom business logic, Hasura lets you extend the Graph through different ways. If you are comfortable writing GraphQL servers, you can totally add a custom GraphQL server as a Remote Schema and Hasura merges it automatically for you. In case you are looking to write a new or keep your existing REST API for the custom logic, you can do so using Actions which is what we will focus in this post.ActionsActions are a way to extend Hasura\u2019s schema with custom business logic using custom queries and mutations. Actions can be added to Hasura to handle various use cases such as data validation, data enrichment from external sources and any other complex business logic.Hasura Actions ArchitectureAn action can be either a query or a mutation. We will look at how to implement both.New REST APIIn case you are starting from scratch, we will define the GraphQL types required for the Action and create a simple Express app to handle the resolver bit. Let's take the example of registering a user.Before we begin, let's create a project on Hasura Cloud to setup the Action. Click on the Deploy to Hasura button below, signup for free and create a new project.Hasura requires a Postgres database to start with. We can make use of Heroku's free Postgres database tier to try this app.After signing in with Heroku, you should see the option to Create project.Create a new project on Hasura CloudOnce you are done creating the project, click on Launch the Console button on the Projects page for the newly created project and the Hasura Console should be visible. By creating this project, you have already got a GraphQL endpoint for the Postgres database that is globally available, secure and scalable from the beginning.Now let's head to the Actions tab on the Hasura Console and define the GraphQL types for our app.Action DefinitionHere we are defining the Mutation type registerUser which accepts the name, email and password arguments and returns the id of the user.We can configure the handler URL later. Click on Create to create the Action.Now let's verify the generated GraphQL mutation by trying out the sample mutation in GraphiQL.mutation {\n  registerUser(name: \"Praveen\", email: \"praveen@hasura.io\", password: \"mysecretpassword\") {\n    id\n  }\n}The above mutation should obviously give a http exception since the Handler URL has not been configured yet. But this test is to verify that Hasura generates mutations for you based on the types defined to query on the same GraphQL endpoint.Try out generated mutation in GraphiQLNow let's go ahead and actually define the Node.js app and try out the mutation.Codegen: Auto generate boilerplate codeNow head over to the Codegen tab to autogenerate the boilerplate code for your nodejs-express server. Since we are generating the API from scratch, we need the full server setup instead of just the POST handler.Codegen for Node.js REST APIThe console gives you options to generate code for different frameworks including but not limited to nodejs-express, nodejs-zeit, nodejs-azure-function etc. Let's quickly deploy the nodejs-express app to Glitch so that we can add this request handler to the server. Click on Try on Glitch to remix an express app on Glitch.Once you are done, add the generated action handler code from the Codegen tab to the server.js file. Finally we need to update our handler URL for the Action so that the HTTP call works.Capture the Glitch URL which will look something like this https://stealth-puddle-cowl.glitch.me/ and add the registerUser endpoint that handles the mutation. So the final handler URL will look like https://stealth-puddle-cowl.glitch.me/registerUser.Go back to the Modify tab of the registerUser Action that was created on the Hasura Console. Update the handler URL to the above one.Finally let's try out the same mutation through GraphiQL and the connection should work returning a dummy <value> for id.Mutation with Action Handler ConfiguredAlright! We have a working GraphQL API that is resolved using a Node.js REST API in the background.Now you can modify your handler code as required to do any business logic; like connecting to a different API, connecting to a database (preferrably using Hasura's APIs) or using an ORM for a different databases etc. Hasura will take care of proxying the GraphQL mutation to the right REST API handler internally.PermissionsWhat about permissions? Who will be able to make this GraphQL mutation from the client? Right now, it is configured to be admin only. We can configure roles to allow other kind of users to perform this mutation.Permission for ActionIn the above example, I have created a role called public that is allowed to make this mutation. To read more about how the whole Authentication and Authorization works with Hasura, you can checkout the docs.Relationship DataNow consider that the Postgres database has users table. We can connect the id of the registerUser output to the id of the users table.Relationships for Action Type to Database TableThe related data also conforms to the permissions defined for the respective table. (ie. users)Existing REST APINow you might be wondering what if I already have a Node.js server with a bunch of endpoints written to handle some custom business logic. In that case, as long as it's a POST endpoint, you can just define the necessary GraphQL types and make some modifications to the way you handle request body to get it working quickly.The request payload is of the format:{\n  \"action\": {\n    \"name\": \"<action-name>\"\n  },\n  \"input\": {\n    \"arg1\": \"<value>\",\n    \"arg2\": \"<value>\"\n  },\n  \"session_variables\": {\n    \"x-hasura-user-id\": \"<session-user-id>\",\n    \"x-hasura-role\": \"<session-user-role>\"\n  }\n}So in our example, the input arguments - name, email and password was wrapped inside an input object. Now if you can make necessary modifications to your existing REST API to handle this request body, your GraphQL API will work as expected :)Query ActionThe above example showcased how to perform a GraphQL mutation. The same workflow can be extended to perform a GraphQL query which proxies to a Node.js REST API in the background. Typical use cases for query actions include computed fields, data enrichment, data transformations, fetching from multiple databases and API sources etc.Supported FrameworksThe REST API can be in any framework like Express.js, Next.js or serverless functions. After all, Hasura just requires a reachable POST endpoint that it can forward the query to. In case you are going to deploy the API to a serverless function, you will need to follow the format of the cloud provider. For example, Next.js has API routes which can be deployed as serverless functions and each API route follows a format. Here's a quick example following the Next.js function format and can be deployed to Vercel.ExamplesHead over to hasura-actions-examples for more practical real world use cases like validations, payments, emails etc where a Node.js REST API was used to convert to GraphQL using Actions.We have a Youtube Playlist with a bunch of examples going over writing out a REST API in Node.js to be used via Actions. You can also check them out.FAQHow do I protect my action endpoint from being called directly?Though the Action Endpoint is not visible to the public, it is a good practice to use a secret that can be forwarded from Hasura through headers to identify if Hasura is making the request. If not, you can reject the request with a 401 unauthorized.What's the best way to deploy my REST API?Your Node.js API can be deployed on any cloud provider. If it's an express.js app, you can write multiple action endpoints using the same app and deploy one server or compose them individually into serverless functions which can be deployed on Vercel / AWS Lambda and so on. Hasura just needs a HTTP POST endpoint for each Action handler.Can I connect to the database inside my Node.js handler?You can connect to the database and perform any operations. The easier way to perform read and writes to the database is to use the auto-generated GraphQL API of Hasura inside the handler. By making use of Admin Secret you can perform any query from the backend.Can I use a CLI to generate the action boilerplate code and not use the UI?Of course! You can make use of the Hasura CLI to generate an action handler that clones the boilerplate code from the framework you choose, all from your commandline inside your Hasura project.Additional Reading: Creating a GraphQL API for Notion with Hasura ActionsCheck out the Node.js backend course. In this course, you will learn how to integrate Node.js in a GraphQL backend server stack with Hasura."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: A REST View of GraphQL Description: In this article we will understand what REST & GraphQL are and the trade offs involved in using one or the other. Author: Gautam BT"}, {"role": "assistant", "content": "Software designers often compare GraphQL, a language specification for defining, querying, and updating data, and REST, an architectural style that describes the Web. We'll explore why this comparison doesn't make sense, and what questions we should be asking instead. In this article, we will talk about:What does the REST architectural style mean?How has REST been implemented in practice?What is GraphQL? What problems does it solve?How is GraphQL incompatible with REST?What is REST?The REST architectural styleREST is an architectural style that came out of Roy Fielding's PhD thesis in 2000. The work examined the properties that made the World Wide Web successful, and derived constraints that would preserve these properties. Roy Fielding was also part of the HTTP/1.0 and HTTP/1.1 working committees. Some of these constraints were added to the HTTP and HTML specs. What constitutes the Web?Before understanding REST, it is useful to look at the different kind of participants on the web:Websites: Programs that serve content for humans to consume and interact with on a browser.Browsers: Programs meant for (mostly) humans to interact with websites.API Service Providers: Programs meant to enable other programs to consume and interact with dataAPI Clients: Programs written to consume and interact with data from an API Service Provider.Note that a program can act in multiple roles. For example: an API service provider can also be a client consuming API's from another API service provider. Also note that the internet and the World Wide Web are not the same. There are other participants on the internet that we don't talk about here (mail servers, torrent clients, blockchain based applications, etc)What is an \"architectural style\"?An architectural style is a named, coordinated set of architectural constraints. An architectural constraint is a restriction placed on the components of an architecture so that a desirable property is achieved. An example of this is the uniform pipe and filter architecture used in the design of UNIX utilities. A recommended practice for a UNIX utility is:Each utility should run as a standalone processCommunication between utilities is done only using stdin and stdout via a text interfaceFollowing these constraints would bring these benefits:Simplicity: Easy for a newcomer to use utilities togetherRe-usability: Allows mixing and matching any two utilities if the second can process the data from the first. For example, I can pipe the output from cat or ls or ps to grep. The author of grep does not worry about where the input is coming from.But in following this constraint, we add latency to the processing since each utility must write its output to the stdout and the next utility must read it from stdin.An alternate design can be for ls, grep, cat, etc to be libraries with well defined interfaces. The end user is then expected to write programs integrating different libraries to solve their problem. This system would be more performant and roughly as reusable as the earlier system, but would be more complicated to use. In general, every constraint we add to the design will have trade-offs. Software design is about identifying the set of design constraints that best serve the requirements.Architectural constraints for the WebLet's talk about the constraints that you should follow to build a \"RESTful\" service:\nUse the client/server pattern for communication\nKeep the server stateless by having each request send everything required to process that request.\nResponses to requests must be labelled as cacheable or not\nUniform interface\n\nServers must expose resources with unique IDs\nResources must be retrieved and manipulated using representations (media-types)\nRequests and responses must have all the information to be able to interpret them, i.e they must be self-descriptive\nHypermedia as the engine of application state (HATEOAS): Clients must rely only on the response to a request to determine the next steps that can be taken. There must be no out-of-band communication related to this.\n\n\nLayered system: Each component in a system must only rely on the behaviour of systems it immediately interacts with.\nCode on demand: This is an optional constraint. A server can send code to be executed by the client to extend the functionality of the client (JavaScript, for example).\n\nThe purpose of these constraints is to make the web simple to develop on, scalable, efficient, and support independent evolution of clients and servers. This article explains these constraints in more detail.REST in PracticeHTTP has been the protocol of choice for implementing the REST architectural style. Some of the constraints such as the client/server pattern, marking resources as caching, and a layered system are baked into HTTP. \u00a0Others need to be explicitly followed.The uniform interface & HATEOAS in particular are the most frequently violated REST constraints. Let us look at each of the sub-constraints:The different components of a REST RequestServices must expose resources with unique IDsBy convention the URI plays the role of the resource ID, and HTTP methods are the uniform set of actions that can be performed on any resource. The first constraint is automatically followed by definition when using HTTP. Most backend frameworks (Rails, Django, etc) also nudge you in the direction of following the second constraint. Resources must be retrieved and manipulated using representationsClients retrieve and manipulate resources using media types such as HTML or JSON. A media type such as HTML can also contain the actions the client can perform on the resources, e.g. using forms. This allows independent evolution of servers and clients. Any client that understands a particular representation can be used with any server that supports that representation. This constraint is useful if you are expecting to have multiple services serving similar types of data, and multiple clients accessing them. For example, any web browser can render any page with content type text/html. Similarly, any RSS reader will work with any server that supports application/rss+xml media type.However, a lot of scenarios do not require this kind of flexibilitySelf-descriptive messages: requests and responses must have all the information to be able to interpret themAgain, this allows services and clients to independently evolve, since clients do not assume a particular response structure. This works well in case of web browsers or RSS readers. However, most API clients are built to access a particular service and are tied to the semantics of the response. In these cases, the overhead of maintaining self-descriptive messages is not always useful. Hypermedia as the engine of application state (HATEOAS)REST Response with possible next actions the client can makeThis constraint is also expected to allow services and clients to independently evolve, since clients don't hard-code the next steps available. HATEOAS makes perfect sense if the consumer is an end user using a browser. A browser will simply render the HTML along with the actions (forms and anchor tags) that the user can take. The user will then understand what's on the page and take the action they prefer. If you change the URL or form parameters for the next set of actions available to the user, nothing has to change in the browser. The user will still be able to read the page, understand what's happening (maybe grudgingly) and take the right action.Unfortunately, this won't work for an API client:If you change the parameters required for an API, the developer for the client program most likely has to understand the semantics of the change, and change the client to accommodate these changes.For the client developer discovering API's by browsing through them one by one is not very useful. A comprehensive API documentation (such as a Swagger or Postman collection) makes more sense.In a distributed system with micro services, the next action is often taken by a completely different system (by listening to events generated fired from the current action). So returning the list of available actions to the current client is useless.So for API Clients, following the HATEOAS constraint does not imply independent evolvability. A lot of API clients are written to target a single backend, and some amount of coupling will always exist between an API client and the backend. In these cases we can still reduce the amount of coupling by making sure:Adding new parameters to an API call should not break existing clientsAdding new fields to a response should not break existing clientsIt should be possible to add a new serialization format (Say JSON to HTML or protobuf) for a new client. #1 and #2 above are usually achieved by API versioning and Evolution. Phil Sturgeon has great posts on \u00a0API Versioning and Evolution that talk about various best practices. #3 can be achieved by respecting the HTTP Accepts headerWhat is GraphQL?GraphQL is a language for defining data schemas, queries and updates developed by Facebook in 2012 and open sourced in 2015. The key idea behind GraphQL is that instead of implementing various resource endpoints for fetching and updating data, you define the overall schema of the data available, along with relationships and mutations (updates) that are possible. Clients then can query the data they need.Here's how a sample GraphQL schema looks:# Our schema consists of products, users and orders. We will first define these types and relationships\ntype Product {\n  id: Int!\n  title: String!\n  price: Int!\n}\n\ntype User {\n  id: Int!\n  email: String!\n}\n\ntype OrderItem {\n  id: Int!\n  product: Product!\n  quantity: Int!\n}\n\ntype Order {\n  id: Int!\n  orderItems: [OrderItem!]!\n  user: User!\n}\n\n# Some helper types for defining the query types\ntype ProductFilter {\n  id: Int\n  title_like: String\n  price_lt: Int\n  price_gt: Int\n}\n\ntype OrderFilter {\n  id: Int\n  userEmail: String\n  userId: Int\n  productTitle: String\n  productId: Int\n}\n\n# Define what can be queried\ntype Query {\n  #Query user\n  user(email: String!): User!\n\n  #query products\n  products(where: ProductFilter, limit: Int, offset: Int): [Product!]!\n\n  #query orders\n  orders(where: OrderFilter, limit: Int, offset: Int): [Order]\n}\n\n# Helper types for defining mutations (updates)\ntype OrderItemInput {\n  product: Product!\n  quantity: Int!\n}\n\ntype OrderInput{\n  orderItems: [OrderItemInput!]!\n  user: User!\n}\n\nscalar Void\n\n# Define possible updates\ntype Mutation {\n  insertOrder(input: OrderInput!): Order!\n  updateOrderItem(id: Int!, quantity: Int!): OrderItem!\n  cancelOrder(id: Int): Void\n}Here is how sample GraphQL queries and responses look like:Sample GraphQL Requests and ResponsesWhy use GraphQL?The key advantage of GraphQL is that once the data schema and resolvers have been defined:Clients can fetch the exact data they need, reducing the network bandwidth required (caching can make this tricky--more on this later).Frontend teams can execute with very little dependency on the backend teams, since the backend pretty much exposes all the data that is possible. This allows front end teams to execute faster.Since the schema is typed, it is possible to generate type-safe clients, reducing type errors.However, on the backend, writing resolvers involves some challenges:To really achieve #1 and #2, resolvers will usually need to take various filter, pagination, and sort options. Optimizing resolvers is tricky because different clients will request for different subsets of the data.You need to worry about avoiding N+1 queries while implementing resolvers. It is easy for the client to construct complex nested (and potentially recursive) queries that make the server to a lot of work. This can lead to DoS for other clients.These problems are described in more detail in this article by Paypal.GraphQL makes it effortless for frontend developers to iterate, but at the expense of backend developers having to put in additional effort upfront. With Hasura, you do not have to worry about the first 3 problems since Hasura compiles your GraphQL queries directly to SQL queries. So you do not write any resolvers while using Hasura. The generated queries use SQL join to fetch related data avoiding the N + 1 query problem. The allowed list feature also provides a solution to #4.GraphQL vs RESTHaving understood what GraphQL and REST are, you can see that \"GraphQL vs REST\" is the wrong comparison. Instead, we need to be asking the following questions:How does GraphQL break the REST architectural style?GraphQL is consistent with the REST constraints of client/server, statelessness, layered system and code on demand since GraphQL is usually used with HTTP and HTTP already enforces these constraints. But it breaks the uniform interface constraint, and to some extent, the cache constraint.GraphQL and CachingThe cache constraint states that responses to requests must be labelled as cacheable or not. In practice, this is implemented by using the HTTP GET method, and using Cache-Control, Etags and If-None-Match headers. In theory, you can use GraphQL and remain consistent with the cache constraint if you use HTTP GET for the query and use the headers correctly. However, in practice, if you have vastly different queries being sent by different clients, you will end up with a cache key explosion (since each queries result will need to be cached separately) and the utility of a caching layer will be lost.GraphQL clients like Apollo and Relay implement a client-side cache that solves these problems. These clients will decompose the response to a query and cache the individual objects instead. When the next query is fired, only the objects not in the cache need to be refetched. This can actually lead to better cache utilization than HTTP caching as even parts of the response can be reused. So, if you are using GraphQL in a browser, Android or iOS, you do not have to worry about client side caching.If you need shared caching (CDN's, Varnish, etc) however, you need to make sure you are not running into a cache key explosion.Hasura Cloud supports data caching by adding an @cached directive to your queries. Read more about how Hasura supports both query caching and data cachingGraphQL & Uniform interfacesGraphQL breaks the Uniform resource constraint. We've discussed above why it is okay for API clients to break this constraint in certain scenarios. The uniform resource constraint is expected to bring the following properties:Simplicity - Since everything is a resource and has the same set of HTTP methods applicable to itIndependent evolution of frontend and backendDecoupling of frontend and backendIf you are building a GraphQL server you should still aim to model your APIs as resources with unique IDs and have a uniform set of actions to access them. This makes it easier for developers to navigate your APIs. The Relay server spec enforces and is good to follow if you are building a backend GraphQL server.We've recently added Relay support to Hasura. Since Hasura auto generates GraphQL queries and mutations from your database schema you also automatically get a uniform set of actions for each of your resources. When should I use GraphQL?The web of the 1990's and 2000's is different from today's web:Front end applications we build today are far richer and offer far more functionality. They are also increasingly built in javascript framework such as React, Vue, Angular instead of rendering templates from the backend. The javascript application then becomes an API client to the backend. Front end applications are also increasingly accessed over mobile networks which are often both slower and flakyWe need to take these into account while designing current systems. GraphQL helps build performant applications in this context.As described in the GraphQL section, the key advantage of using GraphQL is that data fetching on the front end becomes easier and this makes iterating on the front end much faster. The tradeoffs are:The uniform resource constraint is broken. For the vast majority of API clients, this should not be a problem. \u00a0You need to make sure every possible query that can be fired will be served efficiently.HTTP Cache infrastructure will most likely not work.With Hasura you can solve #2 above by using the permission system to disallow aggregate queries or by setting an explicit list of allowed queries.Hasura cloud also comes with a solution for the caching problem.Can't I just use query parameters to specify the exact data I need without breaking REST?Yes, you can, for example by supporting the sparse field-sets spec. You are better off using GraphQL in practice though since:Any sort of query language will need parsing and implementation on the backend. GraphQL has much better tooling to do this.With GraphQL, the response shape is the same as the request shape, making it easier for clients to access the response.However if you do need the uniform interface constraint to be followed, this is the approach you should take. Note that using sparse field-sets will also potentially lead to a cache key explosion, breaking shared caching.ConclusionWe've looked at both GraphQL and REST, and whether it makes sense to compare them. System design goes through the following process:Understand both functional and non-functional requirementsDerive design constraints to meet the requirements (especially the non-functional ones)Choose technology that helps us implement those constraintsREST describes these constraints for the Web. As such, most of these constraints are applicable to most systems. For example, organizing your APIs by resources, having IDs for each of your resources, and exposing a mostly common set of actions are all best practices for GraphQL API design as well (and for that matter an RPC based system as well). If you want to try out GraphQL, head over to the learn course. Hasura is a quick way to get started and play with GraphQL, since your backend is set up automatically."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Add 5 login methods to your Hasura app in 5 minutes with OneGraph\u2019s AuthGuardian Description: AuthGuardian by OneGraph is live on Product Hunt today, check it out Author: Vishwa Mehta"}, {"role": "assistant", "content": "\nAuthGuardian by OneGraph is live on Product Hunt today, check it out here!\n\nAuthGuardian is a free service by OneGraph that recently hit public release, and it\u2019ll let you both:\n\nAdd sign-on with dozens of services like GitHub, Twitch, Stripe, Salesforce, and more\nEasily, visually describe authentication and authorization rules for your app, API, or service.\n\nThe best part? It speaks native Hasura authorization out of the box!\nThat means you can set a user\u2019s X-Hasura-User-Id to their Spotify userId, add an admin  default role if they\u2019re part of your Stripe organization, set a session variable of discount: 0.10 if they\u2019ve contributed to your open-source project on GitHub to use in your custom Hasura functions\nSo let\u2019s see how once you\u2019re in the groove with AuthGuardian, you can do all of this in ~5 minutes:\n\nAdd login with 5 different services, including:\n\nStripe\nGitHub\nTwitter\nSalesforce\nTwitch\n\n\nSet a user as admin if they\u2019re part of your GitHub org or your Stripe org\nSet a session variable with a 10% discount for a user if they\u2019ve starred the Hasura graphql-engine repository on GitHub\nDeploy a AuthGuardian React starterkit to Netlify for testing\nUse Hasura\u2019s remote schemas to query into the service API\nConfigure Hasura to accept AuthGuardian JWTs\n\nStep 1: Add login with 5 services!\nLet\u2019s start with configuring AuthGuardian to allow Hasura sign-in for five different services!\nFirst, sign up for AuthGuardian (fun fact, the AuthGuardian sign up is powered by AuthGuardian!) with your GitHub or Netlify account, or just plain ol\u2019 email.\nThat\u2019ll take you straight to the AuthGuardian settings panel:\n\nHere you can configure a series of rules: Each rule has conditions (\u201cIf this user is logged into GitHub and has starred my repository\u201d), and if the condition is true , then there are also effects that run to create your final JWT (\u201cThen set the Hasura user id to their GitHub userId and create a session variable of is-fan=true\").\nLogin 1: Stripe!\nFor our first rule, we\u2019ll build:\n\nIf this user is logged in to Stripe, then set their Hasura user-id to their Stripe accountId\n\n\nStripe login - test it out!\nNext we\u2019ll log into Stripe and use our JWT preview to make sure the result looks like what we expect:\n\n\ud83d\udca5BOOOOOOOOOOOOOOMMMMM\ud83e\udd2f\n\u2705Stripe login: Done!\nStep 2: All the others!\nLet\u2019s do the same for GitHub:\n\nAnd Twitter:\n\nAnd Salesforce:\n\n\u2026.Twitch\u2026\n\nAnd why not throw in Spotify login as a bonus while we\u2019re at it?\n\nStep 3: Set Hasura session variables\nBut we can take it further! Hasura lets you set session variables for use in authorization permissions, and in custom Postgres functions [TODO: FIX THIS LINK]!\nLet\u2019s set a session variable of is-fan to true if (and only if!) they\u2019ve starred the Hasura GraphQL Engine repository on GitHub:\n\nStep 4: Setting Hasura auth roles\nIn our application we want admins to be anyone who\u2019s a member of our Stripe or GitHub org - easily done!\n\nWhen this user on GitHub is a member of an org called \u201cHasura\u201d, then add admin  to their list of allowed Hasura roles\n\n\nAnd if they\u2019re a member of our Stripe org, then we\u2019ll make them use the admin role by default:\n\nWhen this user on Stripe has an account id (organization id) equal to <my-stripe-account-id>, then set their default Hasura role to admin\n\n\nStep 5: Log in ALL THE SERVICES!\nWhat happens in our JWT preview if we log into all of the services we just set up?\n\nOur JWT has our custom Hasura claims set up exactly as we expect - success!\nFinal step: Configure Hasura and AuthGuardian for great harmony\nThe last step is configuring Hasura to accept JWTs signed by OneGraph\u2019s AuthGuardian - no worries, you\u2019re covered there as well! AuthGuardian has a config generator under Auth Services => Settings that will give you a copy/paste line for updating the JWT auth settings for Hasura on Heroku or standalone.\n\nAs an advanced option, you can also change the JWT generation method here in case you\u2019d like to use shared-secret instead of public-key signatures.\nBonus: Deploy a live React app with your logins preconfigured\nUnder the Install tab, you can click on the React starterkit to deploy a pre-configured app live to either Netlify or Vercel in just a few seconds!\n\nThe future: Remote schemas, joins, tigers and bears, oh my!\nThe JSON web tokens generated by AuthGuardian are also compatible with Hasura\u2019s excellent remote-schemas feature: Bring in OneGraph as a remote schema, have your users log in with AuthGuardian, and immediate query into Salesforce, Stripe, GitHub, and more.\nAnd AuthGuardian tokens are perfectly forward compatible with planned Hasura features like the mind-blowing remote-joins as well, so you can build your apps today knowing that you\u2019re set up to take advantage of Hasura\u2019s incredible development pace.\nTry it on your own Hasura project!\nYou can try AuthGuardian with your own Hasura project by signing up on the AuthGuardian Landing Page - we\u2019d love to hear what you think!\nAbout the author\nThis blog post was written by Sean Grove - Founder of onegraph.com, under the Hasura Technical Writer Program.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hasura Cloud: Managed GraphQL for your database and services Description: I\u2019m delighted to announce the public beta of Hasura cloud! You don\u2019t think about the number of instances, cores, memory, concurrent users, high-availability, realtime monitoring, caching, tracing, rate-limiting and other things that are infrastructure concerns  Author: Tanmai Gopal"}, {"role": "assistant", "content": "I\u2019m delighted to announce the public beta of Hasura cloud! Try it out here: cloud.hasura.io This has been something we\u2019ve been working on since January this year. We\u2019ve used the core Hasura GraphQL engine as a base, made sure that we\u2019re able to provide Hasura as a cloud offering where you don\u2019t think about the number of instances, cores, memory, concurrent users, high-availability, realtime monitoring, caching, tracing, rate-limiting and other things that are infrastructure concerns and not really things you want to spend time on.Drake says, you ought to focus on building things, not running them!For those new to Hasura, the Hasura GraphQL engine is an open-source service that uses a metadata engine to connect to new or existing data-sources (like Postgres, REST APIs, GraphQL APIs etc) and provide a managed and secure GraphQL endpoint that internal or external applications can connect to.We want to make GraphQL invisible to API developers, and give API consumers a GraphQL API they love. The logical next step for us, was to make Hasura GraphQL engine available as a managed cloud service.From hosting and running Hasura yourself, to Hasura Cloud.Auto-scale and serverless GraphQLEach instance of Hasura is a heavily multi-threaded server that exploits shared memory within the instance to aggressively optimise handling GraphQL queries at runtime. If you\u2019re running Hasura yourself, you can first scale a few Hasura instances vertically and then horizontally. At what CPU or memory threshold should you scale up or scale out? Well\u2026.it depends. Hasura\u2019s footprint is different for different kinds of workloads. If you\u2019re running on Hasura Cloud, you don\u2019t need to care! Hasura cloud doesn\u2019t ask you to think about the number of instances, cores, memory, thresholds etc. Hook up your database and other services, and it just works.You can keep increasing your number of concurrent users and the number of API calls and Hasura Cloud will figure out how to make sure that your GraphQL API just keeps on giving. \ud83e\udd13Ultimately, your API performance will bottleneck on your database. Hasura Cloud ships with monitoring and tracing features that help you understand database query performance and give you insight into what to do. Most DBaaS vendors make it really easy to scale your postgres vertically, or add read replicas to your primary Postgres server.Use SQL query analysis built in to Hasura to improve your database performanceIncidentally, if your upstream services are also written in a stateless way, especially if you\u2019re using Actions with serverless functions or auto-scaling containers, then these will get automatically scaled by the cloud vendor too. \ud83e\udd18GraphQL caching (\ud83d\udc4b @cached)TL;DR:Hasura GraphQL Engine already supports query cachingHasura Cloud supports data caching by adding an @cached directive to your queriesGraphQL query cachingHasura GraphQL Engine already offers query caching where the internal representation of the fully qualified GraphQL AST is cached, so that it\u2019s plan need not be computed again. For example, when a GraphQL query with variables and session variables comes in, the generated SQL is already prepared as a statement and exact end-user session based variables and the dynamic API variables (query variables) are zoomed through to the database. This makes Hasura\u2019s queries extremely fast.GraphQL data (response) cachingHasura cloud goes a step further and adds support for data caching. We\u2019ve taken the first step, and there\u2019s lots more to do, but here\u2019s how Hasura cloud thinks about caching which is something we API developers usually build inside our app server.There are many purposes to application level caching, but the most common requirement is usually to reduce the load on the underlying data-source. Caching authenticated API calls automatically is very hard/painful, because of 2 main problems:Knowing what to cache given an incoming API call made by a specific end-user:If the API call doesn\u2019t depend on a property of the user, then it is straightforward to say that all API calls will fetch the same data. The cached data can be retrieved from a cache and sent to the API consumers.If the API call depends on the property of the user, then we need to know if because of the identity of a user, they would end up fetching the same data that some other user has already cached. Let\u2019s say everyone in the same area fetches the same list of restaurants on their app. This is a dynamic rule, because users can change their current areas and hence which cached entities are fetched depend on who is making the API call at that point.There are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors.\u2014 Leon Bambrick (@secretGeek) January 1, 2010\n\nKnowing how to update the cacheCache invalidation is hard :)When users make API calls that perform writes, busting the cache that the same API server or that other instances of the API server are using, is hard.This is a version of the materialized-view-dynamic-update problem, but at the application level which is potentially fetching from multiple data sources. This is typically why most application server developers using something like Redis typically set a TTL using information from #1 (Knowing what to cache) and deal with the temporary staleness on the UI side.Because Hasura has enough information about the data models across data sources, and the authorization rules at the application level Hasura has enough information to offer end to end application caching by solving all the problems above. Today, we\u2019ve launched support for caching authenticated GraphQL API calls that works for shared data (problem 1). How do we handle cache invalidation today? We use a TTL configuration so that state data is refreshed within a time interval. The good news is that since API writes and workflows pass through Hasura (GraphQL mutations), Hasura can eventually handle automatic cache invalidation too!GraphQL MonitoringApart from monitoring successes errors (and partial errors) from GraphQL API calls and their responses, Hasura Cloud also adds great support for monitoring websocket connections and subscriptions!GraphQL Tracing with OpenTracingStrange things happen in production and it\u2019s hard to know where the problems came from. Tracing requests and responses, end to end and through the various components in your system in production is critical.Hasura acts as a fulcrum and coordinating point between your GraphQL API consumer, your data models and your custom business logic or external services that you bring in via HTTP (GraphQL Remote Schemas or REST API Actions or Event Triggers).Hasura Cloud adds a deep integration with OpenTracing, so that it can create traces, or respect incoming trace spans. Hasura also forwards trace-ids and span-ids to services so that a distributed trace can easily be collected across your entire system. For example, this is what a trace collected from Hasura forwarding to an Action, that interacts with the database, whose response is then enriched by Hasura looks like when it's visualised end to end.Today Hasura Cloud supports Datadog as a target to send traces to, and we\u2019ll rapidly be adding more vendors that support OpenTracing over time. GraphQL Rate limitingWhen you\u2019re running a GraphQL API in production, you might need to add some rate-limiting rules. Especially if your API is a developer facing API and not just used by an app. Rate-limiting allows you to make sure that your server and your data sources are protected from sudden spikes from malicious users and also allows you to make sure that your customers or tenants consume your API within reasonable limits. Depending on what you want rate limiting to do, you need to maintain some state across various concurrent API requests to \u201ccount\u201d your users\u2019 limits.Hasura Cloud offers rate limiting for unauthenticated users (based on IP) or parameterized by end-user session variables. Rate limiting rules can set a maximum number of requests per minute based on the parameters above, and can set query depth limits also. Over time, we will add support for dynamic query cost based limiting as well!GraphQL Query captureHasura cloud captures queries (optionally stripping out session and query variables since they might be sensitive) and makes it available to you for 2 main uses:Regression testingBootstrapping and managing an Allow ListYou can export the set of GraphQL queries and mutations that have been captured by Hasura in a given time window and then use it for all sorts of things.Regression testing: You can run a test in the Hasura console or your favourite test runner against your staging environment as you make changes! This way, you\u2019ll know if any changes you make to Hasura will affect GraphQL queries you\u2019re seeing in production.Allow-list management: You can also use the set of captured queries to create or update an allow list. For example, your staging project on Hasura cloud can capture queries from your application testing suite and use that to help you build an allow list.Network securityHasura connects to databases and services running on systems owned by you. For example, you can connect to Heroku Postgres, or RDS, or cloud SQL, or your own Postgres running on your VM.While all communication between Hasura and your services can be encrypted TLS/HTTPS you might further benefit from ensuring that your database and your services only accept connections from Hasura anyway.You can do this with:IP whitelistingVPC peering / private linkWe\u2019ve also started the process of getting our SOC2, ISO 27001 and HIPAA-readiness certifications so that it\u2019s easier for you to start using Hasura Cloud in an enterprise environment. If you\u2019d like to more, please get in touch!Try it out here: cloud.hasura.io "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using JSonnet for programmable Hasura metadata Description: In this post, we will look at how to programmatically generate or manipulate the Hasura metadata using the Jsonnet data templating language. Author: Phil Freeman"}, {"role": "assistant", "content": "The graphql-engine configuration (or \"metadata\") describes everything related to your tables, relationships and permissions that is needed to get Hasura running against your Postgres database, except for the Postgres instance itself. For that reason, it can be useful to export the configuration for backup, which is supported in the Hasura CLI, and also via the console's settings page.\nHowever, while the exported configuration is useful for this backup and restore workflow, it is not stored in a very human-readable format, which makes it less useful for direct or programmatic manipulation.\nWhy would we want to programmatically generate or manipulate the Hasura configuration? For some more advanced use cases, it might be beneficial to generate the configuration from another data source. It might also be useful to reduce duplication in the configuration file in the interests of maintainability, especially if the configuration might change a lot over time. I'll also discuss some more possible use cases at the end of the article.\nSetup\nIn this post, I'm going to work with the \"multiple roles\" example from the Hasura documentation, and show how we can refactor the exported configuration into a more readable form using the Jsonnet templating language.\nJsonnet is a \"data templating language\" which can be used to generate configuration files. This is preferable to using JSON directly, since JSON does not provide any programming capabilities of its own. Notably though, Jsonnet is not a full-fledged programming language, and its expressions cannot have side-effects, so it is a good fit for this sort of problem.\nI'd like to point out that this is hardly the only valid approach to this problem. If you find this approach useful, you might like to consider other similar data templating languages such as Cue or Dhall, but it should also be possible to use your favorite programming language to generate a config programmatically, with different trade offs.\nIf you would like to follow along, you can recreate the database schema using the following script:\nCREATE TABLE users (\n    id INT PRIMARY KEY,\n    name TEXT,\n    profile JSONB,\n    registered_at TIMESTAMP\n);\n\nCREATE TABLE articles (\n    id integer PRIMARY KEY NOT NULL,\n    title text,\n    author_id integer REFERENCES users (id),\n    is_reviewed boolean DEFAULT false,\n    review_comment text,\n    is_published boolean DEFAULT false,\n    editor_rating integer\n);\n\nCREATE TABLE public.editors (\n    editor_id integer PRIMARY KEY NOT NULL REFERENCES users (id)\n);\n\nCREATE TABLE public.reviewers (\n    id integer PRIMARY KEY NOT NULL,\n    article_id integer REFERENCES articles (id),\n    reviewer_id integer REFERENCES users (id)\n);\n\nYou will also need to make sure the jsonnet executable is available on your path.\nExporting the configuration\nIf we export the Hasura configuration directly from the console (click the settings icon in the top right, or navigate to /console/settings), we receive a large amount of JSON:\nRaw metadata\nThis file is slightly complicated, even for such a small example database, but it is easy to discover its structure: at the top level, we see a list of tables, each of which describes its own set of object and array relationships to other tables, and permissions for each role and operation. There is a lot of repetition which we can try to reduce by refactoring in small steps using Jsonnet.\nExample 1: Deduplicating constants\nThe simplest refactoring we can make is to hoist out repeated constants to the top level. For example, one string which appears many times is the name of the Hasura user ID custom header, X-Hasura-User-Id, so let's declare that once as a Jsonnet local:\n-- The HTTP header used to specify the user ID\nlocal XHasuraUserId = \"X-Hasura-User-Id\";\n\nNow we can refer to the constant using its given name in these places.\nNotice another small but important benefit of Jsonnet: we can add comments to our configuration file, something we could not do with JSON. It's hard to overstate how useful this simple feature can be for writing a clear, readable configuration.\nIf we save this modified JSON file with a .jsonnet extension, we can pass it to the jsonnet executable and compare the output to the original JSON file to make sure we haven't changed the configuration:\njsonnet input.jsonnet | diff - original.json \n\nExample 2: More deduplication\nWe can also hoist out objects and arrays which are duplicated across our configuration file. For example, the following two filter expressions are used more than once in our permissions, so we can move them out and define them once:\nlocal authorFilter =\n  {\n    \"author_id\": {\n      \"_eq\": XHasuraUserId\n    }\n  };\n  \nlocal reviewerFilter = \n  {\n    \"reviewers\": {\n      \"reviewer_id\": {\n        \"_eq\": XHasuraUserId\n      }\n    }\n  };\n\nNote that we can make use of constants like XHasuraUserId that we have already defined. In this way, we're building up a library of constants, so that our configuration file can be understood in small steps.\nAgain, after this change, we can run our modified file through the jsonnet executable to make sure we haven't modified the original configuration.\nExample 3: Using standard library functions\nFor the next change, we can see that we mention the list of columns in the articles table multiple times, so again, let's move that out:\nlocal allColumns = \n  [\n    \"author_id\",\n    \"editor_rating\",\n    \"id\",\n    \"is_published\",\n    \"is_reviewed\",\n    \"review_comment\",\n    \"title\"\n  ];\n\nHowever, two roles have access to all columns except for one. It would be repetitive to list out the columns again, just for the sake of one column being removed from the set. Instead, let's express the set of allowed columns by subtracting the smaller set of disallowed columns from the complete set, using a standard library function:\nlocal allColumnsExceptEditorRating = \n  std.setDiff(allColumns,\n  [\n    \"editor_rating\"\n  ]);\n\nThis more clearly expresses our intent: the set of allowed columns is not arbitrary, but the full set minus just one disallowed column.\nExample 4: using functions\nSo far, we have only used constants, but Jsonnet also allows us to reduce duplication using functions, by adding arguments to our locals. As a simple example, we notice that the object and array relationships are very verbose compared to their actual information content, so we can deduplicate them by extracting the common structure into a pair of functions:\nlocal objectRel(name, foreignKey) = \n  {\n    \"name\": name,\n    \"using\": {\n      \"foreign_key_constraint_on\": foreignKey\n    }\n  };\n  \nlocal arrayRel(schema, name, column) =\n  {\n    \"name\": name,\n    \"using\": {\n      \"foreign_key_constraint_on\": {\n        \"column\": column,\n        \"table\": {\n          \"schema\": schema,\n          \"name\": name\n        }\n      }\n    }\n  };\n\nNow, we can call these functions using syntax familiar to Javascript developers:\n{\n  ...,\n  \"object_relationships\": [\n    objectRel(\"user\", \"author_id\")\n  ],\n  \"array_relationships\": [\n    arrayRel(\"public\", \"reviewers\", \"article_id\")\n  ],\n  ...\n}\n\nWe are getting closer to a simple DSL (domain-specific language) for describing our configuration in Jsonnet. With a few functions like these, we can drastically reduce the amount of duplication, and succinctly express the configuration of the server.\nThe resulting Jsonnet file is considerably more readable than the original JSON.\nMore ideas\nIn this post, we've used Jsonnet to tidy up an existing server configuration, which can be useful for maintenance purposes if the configuration is likely to change a lot over time. However, there are many ways in which a programmable configuration file can reduce complexity, and even support features which are not directly supported by graphql-engine:\n\ngraphql-engine supports flat roles, but not hierarchical roles. We can flatten the hierarchy to emulate hierarchical roles, but the resulting configuration could easily become very complicated. With a configuration-driven workflow based on something like Jsonnet, we can perform that flattening in the configuration language. This allows us to compactly specify the hierarchical roles using a small set of functions, but hand off a set of flattened roles to graphql-engine in the form of the expanded configuration file.\nSimilarly, graphql-engine only supports one role per user. There has been a proposal to add support for multiple roles, but any such proposal has to decide how to merge overlapping permissions between roles. Instead, we could delegate such a decision to a function in the configuration language. Additive permissions could become just one option, expressed as a reusable function in a permissions DSL.\nPermissions are currently attached directly to tables and views, but it can be useful to add a layer of indirection, attaching permissions to other sorts of resources, and then mapping those resources onto tables. Using a configuration language, we wouldn't need to build support for this into graphql-engine directly, and instead we could let the configuration language perform the mapping from resource-based permissions to table-based permissions.\nPermissions rules such as \"if a user can write X, then they should be able to read X as well\" can be easily expressed as reusable functions in the configuration language.\nAny sort of duplication of tables in the configuration (for example, for isolation purposes, or for sharding) could be easily deduplicated using functions.\n\nWe'll have more to say on the subject of programmatic configuration soon, but in the meantime, if you are using Jsonnet or another configuration language with Hasura, please let us know!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Adding Relay support to Hasura Description: Relay is a JavaScript framework for declaratively fetching and managing GraphQL data. Hasura now has Relay support and we'd love your feedback! Author: Sezgi Ulucam"}, {"role": "assistant", "content": "\ud83d\ude80Hasura now has Relay support and we'd love your feedback! Get an auto-generated Relay backend on top of your database.Relay is a JavaScript framework for declaratively fetching and managing GraphQL data. It's exceptionally good at:Minimizing developer errorsMaximizing performanceIn both cases, the Relay compiler plays a starring role. During build time, it composes your GraphQL fragments into optimized and efficient batches to reduce round-trips to the server.These fragments are colocated with their views, so that each component describes exactly what data it needs. This declarative approach has several benefits:It's hard to over-fetch data (which would hurt performance), or under-fetch data (which might cause errors).Components can only access data they've asked for. This data masking prevents implicit data dependency bugs.Components only re-render when the exact data they're using is updated, preventing unnecessary re-renders.While it's busy composing fragments together, the compiler also applies transforms to your queries to remove redundancies and shorten query strings. This improves performance due to smaller upload bytes.To get even shorter strings, you can enable persisted queries while running the compiler. This will convert your query strings into md5 hashes. Not only does this boost performance, but it lets you allow-list queries on the server, improving security.Relay also provides type safety. The compiler automatically generates Flow (or TypeScript) types, which you can import into your component for type checking during build time.To learn more about these and other Relay features, like local state management, passing arguments to fragments, and fetching data as early as possible, check out the Relay docs.Relay & HasuraTo enable all these client-side benefits, Relay has a particular server specification. If you're not using Hasura, you have to implement these manually. The server must provide:A mechanism for refetching an object: The convention is a Node interface with a globally unique id field, as well as a root field called node, which allows fetching data by this id. This is great for performance on the client side, but kind of hard to implement on the server side, since you have to make sure \u00a0id's are globally unique, objects can be re-fetched via their id's, etc.A description of how to page through connections: Connections are Relay's way of standardizing pagination. They allow us to communicate more info between the client and the server, such as cursors and page info, so that we can paginate in a predictable pattern. On the client side, Relay saves you a ton of work by keeping track of the details of pagination and merging results automatically. But on the server side, you still need to implement the spec.By using Hasura, you save a ton of work on the server side as well, because you get an auto-generated Relay backend, which sets up the server spec for you automatically.To check it out, find the \"Relay API\" toggle on your Hasura console to get your Relay endpoint.If you have any questions or feedback, please join us on Discord or tweet us your thoughts. Happy exploring!For a more detailed breakdown of Relay and its benefits, check out our deep-dive on Relay."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Remote Joins: A GraphQL API to join across your database and other data-sources Description: Hasura Remote Joins lets developers treat data across different data sources as if it were one database without having to modify or affect the existing data source. Author: Tanmai Gopal"}, {"role": "assistant", "content": "We're happy to announce the release of Remote Joins today. Data Federation using Hasura Remote Joins is now available in v1.3.0 stable release.With Remote Joins, developers can treat data across different data sources as if it were one database without having to modify or affect the existing data sourceAs developers and teams move to using more and more 3rd party APIs, multiple databases & microservices, data and \"the source of truth\" is getting spread across these data-sources. Application developers have a hard time getting access to precise slices of data and a lot of dev cycles are spent in bringing all that data together securely. Remote Joins: A GraphQL API to join across your database and other data-sourcesRemote Joins in Hasura extend the concept of joining data across tables, to being able to join data across tables and remote data sources. Once you create relationships between types from your database and types created from APIs, you can then \u201cjoin\u201d them by running GraphQL queries.Remote joins can join across the database and APIs. These APIs can be custom GraphQL servers you write, 3rd party SaaS APIs, or even other Hasura instances.Of course, because Hasura is meant to be a GraphQL server that you can expose directly to your apps, Hasura also handles security and authorization while providing remote joins.Use Case #1: Customer data joined with Account/Billing/Payment information in StripeIf you have a payment method on your app that uses Stripe, a lot of account and billing information about your customer resides with Stripe. For example, the customer's saved cards, account balance, billing/invoice history.Customer data joined with account, billing, payment-transaction data in StripeOn your app, you'd typically want to show users their saved cards and invoice history. You'd write code to fetch the right data from the Stripe API for the right users/invoices and return that as an API response to your app. This is what this setup looks like on Hasura:Step 1: Setup OneGraph to connect to your Stripe accountSetup OneGraph to connect to your Stripe accountStep 2: Add OneGraph as a remote schema to HasuraAdd OneGraph as a remote schema to HasuraStep 3: Setup a remote relationship from your customer table to the Stripe resolver using the stripe_customer_id as the \"join\" keyYour table should have the stripe_customer_id columnSetup a remote relationship from your customer table to the Stripe resolver using the stripe_customer_id as the \"join\" keyStep 4: Make GraphQL queries to fetch customer and stripe information in one shot!Make GraphQL queries to fetch customer and stripe information in one shot!Use-case #2: User data joined with profile information in Auth0When you use an external authentication provider (or even an authentication/user management microservice that your team doesn't own directly), there will be some information that is only provided by the auth service. For example, last_login, verified_email, login_count and so on. In your database, you would typically have a user table that has an auth0_id. To fetch auth specific information, you would need to use this id and the Auth0 API. User data joined with account/profile/auth information in Auth0Here's how Hasura helps you automate that process:Step 1: Write a serverless function that wraps the Auth0 profile API in a GraphQL query resolver. Write a serverless function that wraps the Auth0 profile API in a GraphQL query resolver.\u00a0Step 2: Connect this as a remote schema in HasuraConnect this as a remote schema in HasuraStep 3: Go to the User table on Hasura and add a \"remote relationship\" to the schema you added aboveThe auth0_id identifier is the \"join\" key from your databaseAdd a \"remote relationship\" to the auth0 schemaStep 4: Make a GraphQL query to query data from the user table and the Auth0 profile information simultaneouslyMake a GraphQL query to query data from the user table and the Auth0 profile information simultaneouslyUse-case #3: Joining data across 2 separate databasesJoining data across 2 databasesThe title says it all ?. And here's how this works with Hasura!Step 1: Setup Hasura to point to Database 1 and Database 2 with the right access control rules so that your GraphQL API is secureThe \"remote\" Hasura + PostgresThe \"primary\" Hasura + PostgresStep 2: On the Hasura of your \"primary\" database, add the second Hasura as a remote schemaOn the Hasura of your \"primary\" database, add the second Hasura as a remote schemaStep 3: Set up remote relationships from any table in your \"primary\" database to \u00a0resolvers in the other database.Set up remote relationships from any table in your \"primary\" database to resolvers in the other databaseStep 4: Run a GraphQL query that joins data across 2 databases!A GraphQL query that joins data across 2 databases!Note: Hasura Remote Joins is a type of federation, similar to Apollo Federation. The key difference is that Apollo Federation aims to help organisations with multiple GraphQL teams \u201cfederate\u201d a global GraphQL schema. Hasura remote joins help you \u201cjoin\u201d across multiple data-sources (or GraphQL & REST microservices) so that you get a unified GraphQL API with data federation. This unified GraphQL API might be required for a team that has multiple sources of data for their application or might be a unified GraphQL API across various teams maintained by a central data API team in the organisation. Try it out!Get started with Remote Joins on Hasura Cloud today!Try it out for yourself and let us know what you think! We \u2764\ufe0f your feedback. Hit us up on our GitHub or on Discord."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: A deep-dive into Relay, the friendly & opinionated GraphQL client Description: A comprehensive guide to building performant, type-safe apps with Relay, from GraphQL fragments and query best practices, to the Relay compiler and the server spec. Author: Sezgi Ulucam"}, {"role": "assistant", "content": "\ud83d\ude80Hasura now has Relay support and we'd love your feedback! Get an auto-generated Relay backend on top of your database.A relay network is a broad class of network topology commonly used in wireless networks, where the source and destination are interconnected by means of some nodes. - WikipediaRelay is a JavaScript framework for declaratively fetching and managing GraphQL data. \"Sounds approachable enough,\" you say? Not so fast. Relay is an amazing friend once you win over its heart, but it doesn't bond with just anybody. There are strict conventions you must follow in order to become worthy in its eyes.The most impressive fact about Relay (besides having eyes) is that no matter how hard you try, IT WON'T LET YOU FAIL. With its strict conventions, it removes opportunities for developer errors at every turn. By standardizing everything, it gives you no choice but to write performant and type-safe apps.\ud83d\udca1 Facebook originally created GraphQL and Relay in the context of trying to improve UX for very slow network connections.\ud83d\udca1 The examples in this article are based on the experimental release of Relay Hooks, a new  Relay API that supports React Concurrent Mode and Suspense.TLDR: Is Relay for me?\u2705 Yes, if you'd like an opinionated, robust framework that will keep your app sturdy and performant as it increases in size and complexity.\u274c No, if you'd like freedom to follow your own path. Go with a less opinionated, more flexible GraphQL client like Apollo.Anatomy of Relay\nColocation\n\nFragments\nData masking\n\n\nRelay compiler\n\nAutomatic type generation\nTransforms\nPersisted queries\n\n\nQuery best practices\n\nwith useLazyLoadQuery\nwith usePreloadedQuery and Suspense\n\n\n\nError handlingFragment @arguments\nRelay's GraphQL server specification\n\nGlobally unique ids and the Node interface\nPagination through connections\nMutation structure\n\n\nIt's all well and good to have opinions, but how does Relay enforce them?Relay is made up of three libraries:Relay compiler: The \u2b50 of the show. Analyzes any GraphQL inside your code during build time, and validates, transforms, and optimizes it for runtime.Relay runtime: A core runtime that is React-agnostic.React/Relay: A React integration layer.You see, if there's anything wrong with your code, the Relay compiler won't pull any punches. It will tell you without hesitation that it simply refuses to build it. Your poor broken code won't even get to run locally, let alone in production.What are these conventions you speak of?Relay not only has a strict GraphQL server specification (which we'll explore in a bit), but also many conventions on the client side. The most famous one of these is colocation, where data definitions (what fields we need on a GraphQL type), and view definitions (how to display this data) live happily together inside each component.But how do we split the data definitions into separate components?Do you mean, how do we split them into fragments? Why, with GraphQL fragments, of course! Everything in Relay revolves around fragments. A fragment is a selection of fields on a GraphQL type.fragment AlbumFragment on Album {\n  name\n  genre\n  tracks {\n    name\n  }\n}\nFragments help you write DRY queries.{\n  artist(name: \"The Muppets\") {\n    id\n    albums {\n      image_url\n      ...AlbumFragment\n    }\n    related_artists {\n      name\n      albums {\n        release_date\n        ...AlbumFragment\n      }\n    }\n  }\n}\nIn Relay, fragments allow you to colocate data and view definitions.function AlbumDetail(props) {\n  const data = useFragment(graphql`\n    fragment AlbumDetail_album on Album {\n      genre      \n      label\n    }\n  `, props.album);\n\n  return (\n    <Genre>{data.genre}</Genre>\n    <Label>{data.label}</Label>\n  )\n}\nRelay composes fragments from multiple components into optimized and efficient batches to reduce round-trips to the server.With this structure:It's hard to over-fetch or under-fetch data, because each component declares exactly the data it needs. Relay defaults to the performant and defensive pattern.Components can only access data they've asked for. This data masking prevents implicit data dependency bugs: each component must declare its own data requirements without relying on others.Components only re-render when the exact data they're using is updated, preventing unnecessary re-renders.In short, colocating data and view definitions makes your components modular, easier to refactor, more performant, and less error-prone.By the way, did you notice the fragment name, AlbumDetail_album? Relay requires globally unique fragment names. The convention is <module_name>_<property_name>. The property_name part avoids naming collisions when you define multiple fragments in a single module.Also notice the useFragment hook. It takes a fragment definition and fragment reference (props.album), and returns the corresponding data. The fragment reference is the object that contains the fragment in the root query, passed into the component as a prop from the parent component.A fragment reference is like a pointer to a specific instance of a type that we want to read data from. - Relay docsThe root query is necessary, because fragments can't be fetched on their own. They must be included (\"rooted\") in a query inside a parent component. That said, the parent can also be a fragment component, as long as the root exists somewhere up the tree. We'll see the root query in a little bit.Sounds tight. But can you really be defensive without strong typing?Indeed, you can't! That's why Relay is thoroughly type-safe. Automatic type generation is only one of the cool things that the Relay compiler does. When you run the compiler, it takes any declared fragments in your code, and generates Flow types from them (or TypeScript, if you choose). You can then declare the type for your component's props thusly:import type {AlbumDetail_album$key} from './__generated__/AlbumDetail_album.graphql';\n\ntype Props = {| // Flow exact object type: no additional properties allowed\n  album: AlbumDetail_album$key,\n|};\n\nfunction AlbumDetail(props: Props) {\n  const data = useFragment(graphql`\n    fragment AlbumDetail_album on Album {\n      genre      \n      label\n    }\n  `, props.album);\n\n  return (\n    <Genre>{data.genre}</Genre>\n    <Label>{data.label}</Label>\n  )\n}\nOf note here:The $key suffix denotes a type for the fragment reference, while the $data suffix denotes a type for the shape of the data.Because our fragment reference (props.album) is properly typed, the returning data will be automatically Flow typed: {| genre: ?string, label: ?string} |}In addition to validation (checking queries for errors and injecting type information), the Relay compiler also applies a bunch of transforms to optimize your queries by removing redundancies. This reduces query payload size, which leads to... Hey, what was that one thing that Relay was always working towards?... Oh right, better performance! \ud83d\udc83\ud83d\udd7a Here's a neat REPL that demos some of these transforms.Relay also supports persisted queries with the --persist-output flag. When enabled, it converts your GraphQL operations into md5 hashes. Not only does this reduce query payload size even more, but your server can now allow-list queries, meaning clients are restricted to a specific set of queries, improving security.Back to the root query... What does it look like?Speaking of being obsessed with performance, Relay offers different ways to fetch and render queries. Let's first look at the less efficient, but more direct way, using the \u00a0useLazyLoadQuery hook.import React from \"react\";\nimport { graphql, useLazyLoadQuery} from \"react-relay/hooks\";\nimport type { AlbumQuery } from './__generated__/AlbumQuery.graphql';\nimport AlbumDetail from './AlbumDetail';\n\nfunction AlbumRoot() {\n  const data = useLazyLoadQuery<AlbumQuery>(\n    graphql`\n      query AlbumQuery($id: ID!) {\n        album(id: $id) {\n          name\n\n          # Include child fragment:\n          ...AlbumDetail_album\n        }\n      }\n    `,\n    {id: '4'},\n  );\n\n  return (\n    <>\n      <h1>{data.album?.name}</h1>\n      {/* Render child component, passing the fragment reference: */}\n      <AlbumDetail album={data.album} />\n    </>\n  );\n}\n\nexport default AlbumRoot;\nWe call useLazyLoadQuery inside our component (i.e. after our component loads \u2014 note this for later), passing in the query and variables, both checked by Flow against the auto-generated AlbumQuery type. We include the child fragment AlbumDetail_album in the query. The returned data is also automatically Flow typed, just like we saw with fragments earlier. Finally, we pass the fragment reference data.album into the child component AlbumDetail.The data obtained as a result of useLazyLoadQuery also serves as the fragment reference for any child fragments included in that query. - Relay docsNote that if we were at the very root of our app, we'd wrap everything with a RelayEnvironmentProvider, passing via the context an environment object, which would be configured with a network (fetch function with our GraphQL endpoint) and store (the local Relay cache). See examples here and here.What if I wanted to be less lazy?If lazy loading queries is not your jam, Relay offers you a way to fetch data more proactively. With usePreloadedQuery, we can start fetching data as soon as there's a user interaction like hovering over or clicking a link. And with Suspense, we can have a fallback UI to show while the data is loading.import React, { Suspense } from \"react\";\nimport {\n  preloadQuery,\n  usePreloadedQuery,\n} from \"react-relay/hooks\";\nimport type { AlbumQuery } from './__generated__/AlbumQuery.graphql';\nimport RelayEnvironment from './RelayEnvironment';\nimport AlbumDetail from './AlbumDetail'; \n\n// Immediately load the query on user interaction. For example,\n// we can include this in our routing configuration, preloading\n// data as we transition to new routes, e.g. path: '/album/:id'\n\n// See example here: \n// https://github.com/relayjs/relay-examples/blob/master/issue-tracker/src/routes.js\n\nconst preloadedQuery = preloadQuery(RelayEnvironment, AlbumQuery, {id: '4'});\n\nfunction AlbumRoot(props) {\n\n  // Define what data the component needs, with `usePreloadedQuery`.\n  // We're not fetching data here; we already fetched it\n  // with `preloadQuery` above.\n  // The result is passed in via props.\n  \n  const data = usePreloadedQuery<AlbumQuery>(\n    graphql`\n      query AlbumQuery($id: ID!) {\n        album(id: $id) {\n          name\n          ...AlbumDetail_album\n        }\n      }\n    `, props.preloadedQuery);\n\n  return (\n    <Suspense fallback={\"Loading...\"}>\n      <h1>{data.album?.name}</h1>\n      <AlbumDetail album={data.album} />\n    </Suspense>\n  );\n}\n\nexport default AlbumRoot;\nWe start fetching data as soon as possible with preloadQuery, even before the component loads. Then, inside the component, we render the data as it becomes available, with usePreloadedQuery. You can wrap any part of your component with a Suspense block to have granular control over what is rendered while you wait for data. This way, you can show some data without waiting for all of it to be ready.Suspense will render the provided fallback until all its descendants become \"ready\" (i.e. until all of the promises thrown inside its subtree of descendants resolve). - Relay docsWhat if there's an error?In case of an error, you can wrap your components in an ErrorBoundary that you define. See examples here and here. Error boundaries were introduced in React 16 as components that catch JavaScript errors anywhere in their child component tree, log those errors, and display a fallback UI.Note that unlike Apollo Client, which gives you error handling options, Relay ignores GraphQL errors unless:the fetch function provided to the Relay Network throws or returns an Errorthe top-level data field wasn't returned in the responseTo access GraphQL errors in your UI, the recommended practice is to model errors in your GraphQL schema.type Error {\n  # User-friendly message\n  message: String!\n}\n\ntype Album {\n  image: Result | Error\n}\nSee this article for an excellent breakdown of how you might model errors this way. And for more on nulls and errors in GraphQL, check out this cheatsheet.How can I access variables in fragments?Let's say you have an album image that's available in various sizes. In Relay, you can declare local variables for fragments using the @arguments and @argumentDefinitions directives.# Declare fragment that accepts arguments\nfragment AlbumDetail_album on Album\n  @argumentDefinitions(scale: { type: \"Float!\", defaultValue: 2 }) {\n  image(scale: $scale) {\n    uri\n  }\n}\n\n# Include fragment and pass in arguments\nquery AlbumQuery($id: ID!) {\n  album(id: $id) {\n    name\n    ...AlbumDetail_album @arguments(scale: 1)\n  }\n}\u261d\ufe0f That's a pretty nice superpower! \ud83e\uddb9\u200d\u2640\ufe0fI want to write a Relay-compatible GraphQL serverRelay makes three core assumptions about a GraphQL server.1. A mechanism for refetching an object\"The server must provide an interface called Node. That interface must include exactly one field, called id that returns a non-null ID. This id should be a globally unique identifier for this object, and given just this id, the server should be able to refetch the object.\" - GraphQL Docsinterface Node {\n  id: ID!\n}\nThe server must provide a root field called node that returns the Node interface. This root field must take exactly one argument, a non-null ID named id. - GraphQL Docstype Album implements Node {\n  id: ID!\n  name: String!\n}\n\n{\n  node(id: \"4\") {\n    id\n    ... on Album {\n      name\n    }\n  }\n}\nUsing the Node interface and globally unique ids, Relay is able to accomplish amazing feats of performance when it comes to fetching data from the cache or the network. For more on this, check out this excellent breakdown by Gabriel Nordeborn and Sean Grove.2. A description of how to page through connectionsRelay uses the GraphQL Cursor Connections Specification to standardize pagination. This saves a ton of work on the client side when it comes to managing cursors, merging results, keeping track of loading state, etc, in that you don't need to manually do any of that work. As long as you follow the standard, Relay takes care of the plumbing for you behind the scenes.In the query, the connection model provides a standard mechanism for slicing and paginating the result set. In the response, the connection model provides a standard way of providing cursors, and a way of telling the client when more results are available. - Relay specBy introducing layers of indirection such as \"edges\" and \"connections,\" Relay is able to communicate more information between the client and the server in a predictable way.{\n  artist {\n    name\n    albums(first: 2) {\n      totalCount\n      edges {\n        node {\n          name\n        }\n        cursor\n      }\n      pageInfo {\n        endCursor\n        hasNextPage\n      }\n    }\n  }\n}\nThe connection has info on itself such as totalCount, and pageInfo such as endCursor and hasNextPage. The edge has info on the node itself, such as its cursor.For more on pagination in Relay and how it automates everything for you, check out this article in the same series by Gabriel Nordeborn and Sean Grove.3. Structure around mutations to make them predictable*Mutation arguments must be wrapped in an input type object.By convention, mutations are named as verbs, their inputs are the name with \"Input\" appended at the end, and they return an object that is the name with \"Payload\" appended. - Relay docs*Although this convention is in the Relay docs, it's not enforced.input AddAlbumInput {\n  artistId: ID!\n  albumName: String!\n}\n\ntype AddAlbumPayload {\n  artist: Artist\n  album: Album\n}\n\nmutation AddAlbumMutation($input: AddAlbumInput!) {\n  addAlbum(input: $input) {\n    album {\n      id\n      name\n    }\n    artist {\n      name\n    }\n  }\n}\ud83d\ude80With Hasura's Relay support, you get an auto-generated Relay backend, which sets up the above spec for you automatically.ConclusionThat's it for our whirlwind tour of Relay! We covered:How Relay gives you no choice but to build performant, type-safe apps by following strict conventionsHow everything in Relay revolves around fragments & colocationHow the Relay compiler watches your back with automatic type generation and query validation, and saves you upload bytes with transforms and persisted queriesQuery best practices for even MOAR performant appsHow to handle loading and error statesHow to pass arguments to fragmentsHow to write a Relay-compatible GraphQL serverThat's A LOT of new concepts to grok! Cheers for making it to the end \ud83c\udf7bWhen I asked my friends about their experience with Relay, these were their pros & cons:Dev FavoritesLocal state managementStrong typing everywhereExcellent toolingData dependency colocationDev Complaints\"Fragment drilling\" for deep trees is gnarly (similar to \"prop drilling\")Testing is confusing, and generation of mock data is weirdNeeds a lot of toolingHard to debug GraphQLWhat about you? What has been your experience with Relay? Share with us on Twitter or in the comments below!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a Full-Stack Yelp Clone with React, GraphQL & Hasura Description: In this video series, we'll build a mini Yelp clone where instead of reviewing restaurants, users review planets from the classic sci-fi series \"Dune\". Author: Sezgi Ulucam"}, {"role": "assistant", "content": "In this video series, we'll build a mini Yelp clone where instead of reviewing restaurants, users review planets from the classic sci-fi series \"Dune\". (Why? Because there's a new Dune movie coming out... but back to the main point.)\ud83d\udc49\ud83c\udffd Here's the written tutorial.\ud83d\udc49\ud83c\udffd Here's the repo if you'd like to code along.Part 1: SearchWe start off with a search feature so users can search for planets.Deploy to HerokuCreate planets tableCreate React appSet up Apollo ClientStyle list with EmotionSearch form & statePart 2: Live reviewsOur users will be visiting these planets, and writing reviews about their experience. We add routing to display reviews for each planet, and subscriptions so they update live.Create reviews tableTrack relationshipsSet up routingSet up subscriptionsPart 3: Business logicWe implement a way to submit reviews through our UI, and (by special request from the Bene Gesserit themselves) some business logic to make sure the word \"fear\" doesn't appear in reviews.Add input formTest review mutationCreate actionAdd business logicOrder reviewsAdd review mutationWe did it! \ud83c\udf89"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Nulls in GraphQL: Cheatsheet Description: What nulls mean in GraphQL queries and responses, pros and cons of nullability, and different approaches to error handling by Apollo and Relay. Author: Sezgi Ulucam"}, {"role": "assistant", "content": "Nullability in GraphQL is a controversial topic. Some say constant null checks are a nuisance, while others err on the side of paranoia. I say, UFO's are real. But, that's not the point. In this post, you'll hear both sides of the discussion, find a cheatsheet you can refer to later, and read about different approaches to nulls and errors by Apollo and Relay.Nullability and error handling in GraphQLSending nulls in GraphQL queries\nReceiving nulls in GraphQL responses\n\nReceiving null for a nullable field\nReceiving null for nullable parent of a non-nullable field\n\n\nFinding the true meaning of nullAnatomy of GraphQL errors\nPros & cons\n\nBenefits of nullable types\nBenefits of Non-Null types\nSchema design and nulls\n\n\n\nDifferent approaches by GraphQL clients\n\nError handling in Apollo\nError handling in Relay\n\n\nNullability and error handling in GraphQLSo what's all the ruckus about? It all starts with GraphQL's unconventional approach to defaults and error handling.In other typed languages, like TypeScript, Flow, Rust, or Haskell, all types are non-nullable by default, and you opt into nullability. But in GraphQL, all types are nullable by default, and you opt into non-nullability. In fact, Non-Null is a type that wraps your original type when you opt in.TypeScript:interface User {\n  name?: string // opting into optional name\n  name: string | null // opting into nullable name\n}GraphQL:type User {\n  name: String! // opting into non-nullable name\n}As if that's not enough fodder for DRAMZ, GraphQL breaks with the REST convention of relying on HTTP status codes for error handling. Even if there's an error in the GraphQL layer while executing your request, the response is still \u00a0200 OK \ud83d\ude32Then how does the client know about errors? This is where nulls become relevant. The server returns a root field called errors, from which the client can extract them, and a root field called data that has all the requested fields. To top it off, any fields with errors have the value null.All of this to say: all types in GraphQL are nullable by default in case of an error. GraphQL doesn't let a small hiccup get in the way of our data fetching: when there's an error with a field, we still get partial data.We'll debate the pros and cons in a bit, including different approaches by Apollo and Relay. But first, the promised cheatsheet...Nulls in the queryA GraphQL query can have fields and inputs, with or without variables.Fields are always optional, whether nullable or not.type User {\n  name: String! // non-null\n  age: Int // nullable\n}{\n  user {\n    name // optional\n    age // optional\n  }\n}In inputs, such as field arguments, nullable types are always optional and non\u2010null types are always required.type User {\n  // nullable filter input, non-null size input\n  imageUrl(filter: String, size: String!): String\n}{\n  user {\n    imageUrl // not valid\n    imageUrl(size: null)  // not valid\n    imageUrl(size: 'med') // valid\n    imageUrl(filter: null, size: 'med') // valid\n    imageUrl(filter: 'juno', size: 'med') // valid\n  }\n}In addition, a variable of a nullable type cannot be provided to a non\u2010null argument.query user($imageSize: String) { // nullable variable\n  imageUrl(size: $imageSize) // field with non-null argument\n}Nulls in the responseIf you get a null value for a nullable field from the server, there are four possibilities:The value is actually null or absent, e.g. user doesn't existThe value is hidden, e.g. user has blocked you (permission error)There was another error fetching the valueAn error or null value has bubbled up from a Non-Null childWait, what was that about bubbling up?The GraphQL spec says that a null result on a Non-Null type bubbles up to the next nullable parent. If this bubbling never stops because everything is of Non-Null type, then the root data field is null.Let's say name is nullable, but the resolver returns an error while fetching it:data: { \n  user: { \n    name: null,\n    age: 25,\n    // other fields on user\n }\n},\nerrors: [\n  { \n    message: \"Name could not be fetched\",\n    // ...\n  }\n]If name is non-nullable and user is nullable, this is what happens instead:data: { \n  user: null\n},\nerrors: [\n  { \n    message: \"Name could not be fetched\",\n    // ...\n  }\n]When fields are nullable, as in the first example, you still get partial data: even if you don't have the name, you can still show the age and other fields. When you mark fields as Non-Null, as in the second example, you forfeit your right to partial data.How can I know the true meaning of null?The four possibilities of null leave us with three questions:Do we have an actual absent value or an error?If it's an error, which field is it on?What kind of error is it?And the answers lie in the errors list returned in the response.If an error is thrown while resolving a field, it should be treated as though the field returned null, and an error must be added to the errors list in the response. - GraphQL SpecAnatomy of the errors listThe errors list must include message, locations, and path entries, with an optional extensions entry.\"errors\": [\n  {\n    // error message from the resolver\n    \"message\": \"Name for user with ID 42 could not be fetched.\",\n    \n    // map to the field in the GraphQL document\n    \"locations\": [ { \"line\": 3, \"column\": 5 } ],\n    \n    // path to the field\n    \"path\": [ \"user\", 0, \"name\" ],\n    \n    // optional map with additional info\n    \"extensions\": {\n      code: PERMISSION_ERROR,\n      weatherOutside: 'weather'\n    }\n  }\n]The path entry answers our first two questions: whether the null result is intentional or due to an error, and what field it's on. The extensions entry answers our third question by allowing us to specify the type of error, and anything else we can possibly divine: the time of day when it happened, the weather outside, etc.Paul Rudd & Jason Segel in \"Forgetting Sarah Marshall\"In the case of a \u00a0Non-Null field, the path entry still specifies the correct source of the error, even when the data returned points to the parent as the troublemaker.data: { \n  user: null\n},\nerrors: [\n  { \n    message: \"Name could not be fetched\",\n    path: [ \"user\", 0, \"name\" ]\n    // ...\n  },\n  // ...\n]Pros & consNow that we've got the cheatsheet down, let's talk about the pros and cons of nullable vs Non-Null types.Benefits of nullable typesWe've already covered one major benefit:When the HTTP status code is 200 OK, we're able to get partial data from the server, despite errors on specific fields. But we still need a way to tell if something went wrong, which we can achieve with a null value on the erroneous field, along with an errors list (Btw, we'll discuss another solution to this \u2013 returning an Error type instead of null \u2013 in the Relay section below).Other benefits:Privacy when you want to obfuscate the reasons for null. Maybe you don't want the client to know whether you got a null on user because the user has blocked you or because the user simply doesn't exist. If you're a ghoster, this is the way to go.If you're serving different clients from a single schema, nullable fields are more reliable, and easier to evolve. For example, if you remove a Non-Null field from the schema, a client that you have no control ever may break when it receives null for that field.Coding defensively on the client side. Null check all the things!if (user && user.location) return user.location.city;Benefits of Non-Null typesIf you find null checks cumbersome, \u00a0Non-Null is the type for you. Why?You get guaranteed values. If you know that location is non-nullable, you can just do this:if (user) return user.location.city;If location returns null it will bubble up to user, and user.location.city will never execute. location is guaranteed to never be null, so you never need a null check on it.You can combine Non-Null types with query type generation to make your code even more predictable. For example, with TypeScript, the generated code for a users query can be:type GetUsersQuery = {\n  users: Array<{\n    __typename: \"User\",\n    name: string // if Non-Null in schema\n    name: string | null // if nullable in schema\n  }\n};Easy error detection. If you get a null value for a non-nullable field, you know it's because of an error, since it can't be a legitimate absent value.So how should I design my schema?Though this is a hotly debated topic, the general recommendation is to make everything nullable except when it doesn't make sense, e.g. getUserById(id: ID!), or the value is guaranteed to exist, e.g. a primary key like id. But you can weigh the pros and cons above, and decide for yourself! (If you do, please let me know in the comments).Different approaches by GraphQL clientsDepending on what GraphQL client you're using, you may be able to pick your strategy on a per-query basis, or come up with a different solution to error handling altogether.Apollo: Everything is possibleWhen it comes to error handling, Apollo gives you options. Three options, in fact.none (default): Treat GraphQL errors like network errors, ignore dataignore: Get data, ignore errorsall: Get both data & errorsThe all option matches the GraphQL spec, and will allow you to get partial data in case of an error. You can set the errorPolicy on a per-query basis, like so:const { loading, error, data } = useQuery(MY_QUERY, { errorPolicy: 'all' });Relay: All is good all the timeRelay is all about living the good life. A little GraphQL error here and there isn't enough to mess with Relay's zen thing.That's right. Relay ignores GraphQL errors unless:the fetch function provided to the Relay Network throws or returns an Errorthe top-level data field isn't returned in the responseThe Relay docs recommend modeling errors in your schema rather than returning null.type Error {\n  message: String!\n}\n\ntype User {\n  name: String | Error\n}For a detailed breakdown of how you might model this in your schema, check out this excellent blog post by Laurin Quast.ConclusionI hope this article has helped you master nulls in GraphQL. We covered:GraphQL's approach to nullability and error handlingWhat nulls mean in GraphQL queries and responsesPros & cons of nullable and Non-Null typesDifferent approaches to nulls and error handling by Apollo and RelayWhere do you stand in this discussion? What has been your experience with nullability? Let us know on Twitter or in the comments below!Further ReadingUsing nullability in GraphQL, by Sashko StubailoNullability in GraphQL, by Grant NorwoodWhen To Use GraphQL Non-Null Fields, by Caleb MeredithHandling GraphQL errors like a champ with unions and interfaces, by Laurin QuastGraphQL Best Practices: NullabilityGraphQL SpecHandling errors with ApolloRelay: Accessing errors in GraphQL Response"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Actions: Add custom business logic to Hasura Description: Hasura makes it easy to integrate business logic into your GraphQL API with Actions.  Author: Tanmai Gopal"}, {"role": "assistant", "content": "Actions make it easy for you to bring your new or existing business logic to Hasura. In this post, I'll introduce to you what actions are and why they're designed the way they are! Why Actions?With actions you can add any kind of business logic to your GraphQL API easily. Hasura handles CRUD, your actions handle everything else!Actions works really well with Hasura so you can choose which bits you want Hasura to take care of, and which bits you want to take care of. It\u2019s a spectrum that you can slide along back and forth as your application evolves.Just like the way Hasura works with your database, our aim with Actions is to help our users land up with best practices from the get go!\n\n\n\n\n\n\n\n\nScalable architecture\nA scalable pattern for adding complex domain logic\n\n\nNo lock-in\nNot locked in to any language/framework/runtime to implement your business logic\n\n\nPerformance best practices\nEven though the business logic is written by you, Hasura still handles all the GraphQL bits. Blazing fast as usual. Hasura ensures that your business logic automatically falls into the pit of best possible performance.\n\n\nLeverage Hasura security\nHasura has a very powerful authorization system. This system can be reused from your business logic in Actions easily.\n\n\nGreat with serverless\nHasura actions are a great fit with serverless because each action can be backed by an independent function\n\n\n\nHow do Actions work?Hasura Actions: The ease of writing REST, and the joy of consuming GraphQLStep 1: You specify the query or mutation GraphQL contract at HasuraStep 2: Hasura validates and delegates the incoming GraphQL request to your REST API (aka Action handler)Step 3: Your REST API Does Stuff\u2122 and returns minimal outputStep 4: Hasura enriches the output with the rest of the data graph according to what the users want in the final GraphQL response. Because Hasura joins the data together, you get all of the benefits of Hasura\u2019s blazing fast GraphQL execution.Try out Actions for yourself, setup a GraphQL project using Hasura Cloud and read the Actions docs to get started.To see all the different things that you can do with actions, check out this youtube playlist we put together in April in a series titled #ActionPackedApril.An example of using Hasura actions with NextJS API routes!Scalable architectureMost APIs are composed of a mix of CRUD and non-CRUD APIs. A scalable pattern for non-CRUD designs is a pattern based on a \u201ccommand & query\u201d architecture.When we think about CRUD, we think about models and we think about reading or writing to them with a few API endpoints (or GraphQL fields).When we think about a command and query design, you think about running a \u201ccommand\u201d which is an action or a workflow that makes sense for a user in your domain. The handler for this command then runs logic to process the request and then proceeds to update the views of the models. The final response to the request that the user made, is then a \u201cquery\u201d of those updated models.A command & query architecture for GraphQL with HasuraWhy is this architecture beneficial?It allows the authors of each command to deal primarily with the business logic associated with the \u201cdomain action\u201d. Models that have CRUD might get very complicated and can start encapsulating logic that is not relevant to the model at all. For example, insert on X should send an email.Usually, setting up the plumbing required to set up a CQS/CQRS based architecture with microservices takes a little bit of work. With Hasura, that architecture is available out of the box and it is tremendously easy to distribute the work across a team!Furthermore, from a security point of view, it becomes easy to think about access control.At a domain entity level: Who all have the ability to access this model or actionAt a user level: What models and actions does this user have access toHasura's security / permissions system is API driven, so you can easily extract both pieces of information above with just an API call!No vendor lock-inWe designed Hasura to work on new or existing databases. As you use Hasura, the data models that you end up with, are very naturally the data models that you would have put in place as a good database user. This means that if you want to \u201ceject\u201d out of Hasura tomorrow, your data is still yours, ready to be used by whatever application you want.With Actions, we extend that philosophy to business logic. GraphQL today is a still a young and maturing ecosystem. GraphQL frameworks are either vendor backed or made by independent developers without a large contributor community. Furthermore, most of the GraphQL work is restricted to the JavaScript ecosystem.However, REST does not suffer from these problems. The tooling for building, maintaining and operating REST APIs is mature & quite phenomenal. The runtime environments for REST have also evolved rapidly over the last few years. Serverless runtimes are getting increasingly sophisticated and can run functions which are presented as lightweight REST endpoints. GraphQL servers, however, are not suited to serverless deployments. GraphQL servers in production environments need to maintain a fair bit of state to efficiently validate, cache, rate-limit incoming requests. GraphQL subscriptions, are quite complex to maintain and near impossible in serverless environments.With Hasura, our aim has been to help users continue using their favorite technologies and tools (just like they own their own data and database) to add business logic in a way that would largely be the same even if they weren\u2019t using Hasura! With Hasura actions, your business logic can be any combination of your favourite technologies:Bring your favourite language x framework x runtime x deployment for Hasura ActionsIf you want to move away from Hasura then you can completely opt-out and build your own GraphQL server, or even migrate away from GraphQL entirely without having to redo all the work you\u2019ve done in your domain! Your data, your data models, your business logic will all carry forward.Performance best practicesHasura helps you fall into the pit of performance success as you use actions. :)For example: The best practice for a GraphQL mutation is to delegate the input information to a function that can process it and then give read-after-write semantics, have the GraphQL layer generate the right response for the mutation (depending on what the user requested).Hasura actions do exactly that, but in a way friendly to a microservices / serverless architecture. Your REST API performs business logic and returns just the right data or the right references to the data. Hasura joins those references with the rest of the data graph efficiently and generates the GraphQL response that your client deserves.If you\u2019re building your own GraphQL server, you have to worry about N+1 query problems, validating//caching query plans and other performance optimisations that Hasura handles for you. These problems become especially harder to address in a serverless environment. However, with Hasura\u2019s Actions architecture, serverless REST endpoints are a perfect fit!Leverage Hasura securityHasura\u2019s GraphQL security spans a few things:Authorization rules (declared at a model level)Allow-listingRate-limitingAuditingAs you build your business logic and your REST API handlers, you need not worry about any of these. The design of Hasura actions makes it very easy to leverage and reuse authorization rules that are specified at the Hasura level too!For example, you can specify Hasura authorization rules that correspond to end-users. From your Actions endpoint, you can invoke the Hasura GraphQL APIs as your end-user role. This makes it easy to write \u201cbackend\u201d code that executes with the same privileges as \u201cfrontend\u201d code, but with additional steps or validations that were not possible to run on the frontend. Your \u201cmodel\u201d level security stays in one place.Pairs well with Serverless \ud83c\udf77Hasura Actions are backed by REST APIs, These APIs could be served by a service or a serverless function or anything else that can serve a REST API!Serverless is an especially great fit both from an architecture / mental model point of view, and \u00a0also from a performance & scalability point of view.Each Hasura action can be backed by a serverless function (exposing a REST endpoint). This serverless function can be developed and deployed independently.This is a neat separation of concerns, because Hasura exposes a unified coherent GraphQL API to the consumer, regardless of how the serverless functions are built and run.Hasura also exposes a set of GraphQL endpoints on the database which can be used by the business logic to read and write data to the database. This is great for serverless, because connecting to the database directly is challenging. The business logic you write now also falls into the pit of success for both performance and scaling, because a) GraphQL/database performance is guaranteed by Hasura b) scalability is guaranteed by the cloud-vendor running the serverless functions. \ud83d\udd25Related Architectures & PatternsAn architecture with Actions that drive business logic is very similar to the CQS/CQRS and the 3factor architecture (read more about building 3factor apps with event-driven programming) pattern.Image from https://martinfowler.com/bliki/CQRS.htmlTry it out!If you're not running Hasura, setup a GraphQL project using Hasura Cloud and read the Actions docs to get started.If you're already running Hasura, make sure you're on version 1.2 or above and you're good to go!Additional Reading: Creating a GraphQL API for Notion with Hasura Actions"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a customizable e-commerce backend with Hasura Description: An overview of building customizable, scalable, & secure backends for e-commerce apps with Hasura with all fundamental features like auth, search, inventory validations, add to cart, order placement and payments. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Building an e-commerce app often requires a solid backend with data integrity and high performance for low latency browsing. There are plenty of APIs to worry about and manage in the whole process. Cross platform development makes it tricky with different data requirements in different platforms (i.e web, mobile)In this post, we will dive into building a robust e-commerce backend with all fundamental features like auth, search, inventory validations, add to cart, order placement and payments while also maintaining security.We want to be writing the least amount of code possible without compromising the \u00a0ability to scale when required and maintaining data integrity. Our choice of technologies are very simple and powerful:Database: PostgreSQL (Solid relational database to store and access data)APIs and Hasura Cloud: Hasura Cloud offers hosted GraphQL Engine projects with extra features for reliability and security and Hasura GraphQL APIs for the most boring CRUD stuff and robust AuthorizationCustom Logic: A simple Node.js server for auth, cart and order placements. (You can replace this with the language/framework of choice)Now let's jump into the database schema.Data modelling for e-commerceWe are using PostgreSQL for datastore. We have a bunch of tables with the right relationships setup. A simplified version of the data model will look something like this:PostgreSQL data model for e-commerce appThe primary user workflow on the frontend would be:Browse products -> Add product to cart -> Place order -> Make Payment.Throw in signup/login anytime before placing an order to complete the workflow.Alright, now let's break down the core components of an e-commerce app:Products listing with searchProduct pageUser authenticationCart page and Add to CartCoupon code validationsPlace order and make paymentSend email to customersSetting up a Cloud projectFollow the getting started docs to have an instance of Hasura up and running in no time.Data fetching APIsLet's look at the most commonly used API in the app - fetching product(s) from the database. Mix in all those fancy filters, pagination and you are looking at multiple endpoints or conditional fetching already.Here's what the simplest of query to fetch products looks like with Hasura GraphQL.GraphQL query for fetching productsThe above query is fetching 10 products (and their metadata like stock availability). Now you can expand this query to have as much information you want in your product listing page.Filtering and sortingObviously you would want to give users the ability to filter and sort the products on the UI. In the query above, you can add more arguments to achieve the same.GraphQL query with input argumentsWe have now added where and order_by arguments. If you have used SQL before, this would be very familiar territory :) You can use the same query to fetch one product at a time based on product_id in the filter. That would take care of the product detail page. So easy isn't it?Product Search APISearching across the product table can be implemented using Postgres functions. We can include what columns in the table we need to match the search term against. For example, you can write a function that looks like:Postgres function to search productsThis lets you check for similarity of the given search term relative to name, description and category. Obviously you can include other product metadata to search for. Note that the similarity function and <% operators are available with the pg_trm extension. Read more about how to use pg_trm.Now with Hasura, you can query the postgres function like:GraphQL query for postgres functionRead more about Postgres Search here.With the right usage of input bounce effect, you can build a solid search functionality using built-in Postgres features. But in case you are worried about latency and do not want the primary database to be hit for search operations, you can also make use of external services like Algolia with drop-in frontend code integrations. Let us know in the comments of other similar SaaS APIs for search :)Fetch cart itemsAnother primary data fetching use case is to fetch cart items. The important thing to note here is that, you should be able to filter out cart items that belongs only to the user who is browsing. This is a common permission system use case.The query for fetching cart items would look like:GraphQL query for fetching cart itemsNote that we haven't applied any user filter to restrict the items that belong to the current logged in user. This is done automatically using Hasura's permission system. We can assign the customers of the app a role called user and apply check constraints declaratively like the following.Hasura's role based permission systemHere we can filter the user_id column by making use of session variables that comes from the token.User AuthenticationNext up - Authentication. The app revolves around users and their ability to login and purchase products. We have a users table in the database. The data of customers signing up can go into this table. We need a way to authenticate the user and manage sessions. With Hasura, it is very easy to set up JWT based authentication.The backend to set up login and generate JWT tokens can be written in Node.js. Here's what the flow for a login with JWT looks like.JWT flow - Source: Auth0Now the above REST API call to our custom Node.js server can be queried via GraphQL on the same endpoint that Hasura provides.Hasura Actions can be used to extend the auto-generated GraphQL types. Actions are a way to extend Hasura\u2019s schema with custom business logic using custom queries and mutations.Custom mutation for signup would look very simple:GraphQL mutation for signupThe server generates JWT token using a custom secret shared with Hasura and returns the token back to the client after inserting the user into the database.On the other hand, you can also make use of Auth providers like Auth0, Firebase, Cognito which works well with Hasura's Auth system.Checkout the Hasura action - Github | YouTubeCustom Business LogicIn this section, we will be adding custom business logic for various use-cases using Hasura Actions and Postgres's built-in functions.We got away with the auto-generated GraphQL queries for most of the data fetching. But there are some complex data validations and custom logic which cannot be automated or very specific to the app.For example, in an e-commerce app users should be allowed to Add to cart only if there is inventory available. Similarly there is some validation required for coupon codes/discounts during order placement. Finally payments are completely external with providers like Stripe which requires validation APIs to be written on the backend.Let's look at them case by case:Add to Cart with Inventory ValidationAdd to cart with Inventory Validation using Hasura ActionsOn the frontend, Add to Cart flow is probably the first place you might want to enforce Authentication. Here's the built-in mutation that Hasura gives you for the cart table.GraphQL mutation for add to cartNow before adding to cart, you might want to perform validations; like check for stock availability. You can override the above Hasura mutation using a Derived Action that clones the above mutation type and lets you write custom code over this. With Action Codegen, the amount of code you will write will be very less and very focussed on the custom logic.The new custom mutation would look like:Custom GraphQL mutation to create cartAgain note that, permissions can be applied declaratively to ensure the cart item is tagged to the right user and cannot be manipulated.Check out the Hasura Action - Github | YouTubeCoupon code validationThis is the step before order placement. You could provide customers with the ability to apply coupon / discount codes before making the payment.Typically on the frontend, this would be a query to the server to check if the coupon exists and if so, what are the discount values based on various conditions.The following custom query can be exposed to the frontend:Custom GraphQL query to validate couponCheck out the Hasura Action - Github | YouTubePlace order and make payment with StripeOrder placement is the final workflow for a customer. The customer places the order by initiating the payment intent on Stripe. This step is when the banking details are entered by the customer.Stripe payment APIJust before doing this, the frontend needs a client secret to initiate the payment with Stripe. By now, the frontend knows the order details including the currency and the address. So the frontend can trigger the following mutation with server side code written in Node.js.GraphQL mutation for order placementNote that, the Node.js server gets access to all the session variables automatically to apply the right permissions. No extra code is required to conditionally check or verify if the right user is making the mutation.Check out the Hasura Action - Github | YouTubeRealtime Payment StatusOnce the payment is triggered from the frontend, it will be processed asynchronously on the backend. For example with Stripe, the frontend initiates a payment intent and server validates the payment in the backend async. Now once the validation finishes, the Node.js function can update the payment status into a table like order_payment.On the frontend, GraphQL subscriptions can be used to track the payment status in realtime. The subscription is very straightforward:GraphQL subscription for order payment statusAs soon as a row is inserted into the table, the frontend will get notified with the new data that can be shown to the user.What about performance and scaling?Hasura is pretty lightweight and is highly performant. The reason being that, Hasura compiles your GraphQL query to SQL with the right permissions applied and skips a lot of processing by leveraging Postgres's built-in features like JSON aggregations and Prepared statements. Read up more on Blazing fast GraphQL Execution.Hasura has been benchmarked for upto 1 million live subscriptions and scales really well. Since you have full control over the underlying Postgres database, you will be able to optimise and fine tune by adding the right indexes and so on.SummaryHasura gives you most of the APIs out of the box. Combined with a robust permission system, you are actually just writing backend code pertaining to your business logic.There are more e-commerce features like recommendation engine, support, tracking etc which we aren't covering in this post. But the takeaway is that, we have all the fundamental blocks to build anything on top or extend this very quickly with less amount of code.The bottomline is that, for most of your data fetching requirements, be it realtime or just normal dynamic data, the out-of-the-box GraphQL APIs from Hasura is sufficient with a comforting factor that you can always quickly extend and override the GraphQL types and bring in your own API.Additional Reading: Creating a GraphQL API for Notion with Hasura Actions"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: CouchDB style sync and conflict resolution on Postgres with Hasura Description: In this post we will look at integrating PouchDB's versioning and conflict management system with Hasura. Author: Gautam BT"}, {"role": "assistant", "content": "We\u2019ve been talking about offline-first with Hasura and RxDB (essentially Postgres and PouchDB underneath). This post continues to dive deeper into the topic. It is a discussion and guide to implementing CouchDB style conflict resolution with Postgres (central backend database) and PouchDB (frontend app user database). Here's what we are going to talk about:\nWhat is conflict resolution?\nDoes my app need conflict resolution?\nConflict resolution with PouchDB explained\nBringing easy replication and conflict management to pouchdb (frontend) and Postgres (backend) with RxDB and Hasura\n\nSetting up Hasura\nClient side setup\nImplementing conflict resolution\n\nUsing views\nUsing postgres triggers\n\n\n\n\nCustom Conflict Resolution strategies with Hasura\n\nCustom conflict resolution on the server\nCustom conflict resolution on the client\n\n\nConclusion\n\nWhat is conflict resolution?Let us take a Trello board as an example. Let's say you've changed the assignee on a Trello card while offline. In the meanwhile your colleague edits the description of the same card. When you come back online you would want to see both the changes. Now suppose both of you changed the description at the same time, what should happen in this case? One option is to simply take the last write - that is override the earlier change with the new one. Another is to notify the user and let them update the card with a merged field (like git!).This aspect of taking multiple simultaneous changes (which may be conflicting), and merging them into one change is called conflict resolution.What kind of apps can you build once you have good replication and conflict resolution capabilities?Replication and conflict resolution infrastructure is painful to build into the frontend and backend of an application. But once it is setup, some important use-cases become viable! In fact, for certain kinds of applications replication (and hence conflict resolution) are critical to the app's functionality!Realtime: Changes made by the users on different devices are synced with each otherCollaborative: Different users simultaneously work on the same dataOffline-first: The same user can work with their data even while the app is not connected to the central databaseExamples: Trello, Email clients like Gmail, Superhuman, Google docs, Facebook, Twitter etc.Hasura makes it super easy to add high-performance, secure, realtime capabilities to your existing Postgres based application. There is no need to deploy additional backend infrastructure for supporting these use-cases! In the next few sections we'll learn how you can use PouchDB/RxDB on the frontend and pair it with Hasura to build powerful apps with great user-experience.Conflict resolution with PouchDB explainedVersion management with PouchDBPouchDB - which RxDB uses underneath - comes with a powerful versioning and conflict management mechanism. Every document in PouchDB has a version field associated with it. Version fields are of the form <depth>-<object-hash> for example 2-c1592ce7b31cc26e91d2f2029c57e621. Here depth indicates the depth in the revision tree. Object hash is a randomly generated string.A sneak peek into PouchDB revisionsPouchDB exposes APIs to fetch the revision history of a document. We can query the revision history this way:todos.pouch.get(todo.id, {\n    revs: true\n})This will return a document containing a _revisions field:{\n  \"id\": \"559da26d-ad0f-42bc-a172-1821641bf2bb\",\n  \"_rev\": \"4-95162faab173d1e748952179e0db1a53\",\n  \"_revisions\": {\n    \"ids\": [\n      \"95162faab173d1e748952179e0db1a53\",\n      \"94162faab173d1e748952179e0db1a53\",\n      \"9055e63d99db056a95b61936f0185c8c\",\n      \"de71900ec14567088bed5914b2439896\"\n    ],\n    \"start\": 4\n  }\n}Here ids contains hierarchy of revisions of revisions (including the current) and start contains the \"prefix number\" for the current revision. Every time a new revision is added start is incremented and a new hash is added to the start of the ids array.When a document is synced to a remote server, _revisions and _rev fields need to be included. This way all clients eventually have the complete version history. This happens automatically when PouchDB is set up to sync with CouchDB. The above pull request enables this when syncing via GraphQL as well.Note that all clients do not necessarily have all the revisions, but all of them will eventually have the latest versions and the history of the revision ids for these versions.Conflict resolutionA conflict will be detected if two revisions have the same parent or more simply if any two revisions have the same depth. When a conflict is detected, CouchDB & PouchDB will use the same algorithm to auto pick a winner:Select revisions with the highest depth field that are not marked as deletedIf there is only 1 such field, treat it as the winnerIf there are more than 1, sort the revision fields in descending order and pick the first.A note about deletion: PouchDB & CouchDB never delete revisions or documents instead a new revision is created with a _deleted flag set to true. So in step 1 of the above algorithm any chains that end with a revision marked as deleted are ignored.One nice feature of this algorithm is that there is no coordination required between clients or the client and the server to resolve a conflict. There is no additional marker required to mark a version as winning either. Each client and the server independently pick the winner. But the winner will be the same revision because they use the same deterministic algorithm. Even if one of the clients has some revisions missing, eventually when those revisions are synced, the same revision gets picked as the winner.  Implementing custom conflict resolution strategies But what if we want an alternate conflict resolution strategy? For example \"merge by fields\" - If two conflicting revisions have modified different keys of the object we want to auto merge by creating a revision with both the keys. The recommended way of doing this in PouchDB is to:Create this new revision on any of the chainsAdd a revision with _deleted set to true to each of the other chains The merged revision will now automatically be the winning revision according to the above algorithm. We can do custom resolution either on the server or the client. When the revisions get synced all clients and the server will see the merged revision as the winning revision.Conflict Resolution with Hasura and RxDBTo implement the above conflict resolution strategy, we will need Hasura to also store the revision history and for RxDB to sync revisions while replicating using GraphQL. Setting up HasuraContinuing with the Todo app example from the previous post. We will have to update the schema for the Todos table as follows:todo (\n  id: text primary key,\n  userId: text,\n  text: text,\n  createdAt: timestamp,\n  isCompleted: boolean,\n  deleted: boolean,\n  updatedAt: boolean,\n  _revisions: jsonb,\n  _rev: text primary key,\n  _parent_rev: text,\n  _depth: integer,\n)Note the additional fields:_rev represents the revision of the record._parent_rev represents the parent revision of the record_depth is the depth of the record in the revision tree_revisions contains the complete history of revisions of the record.The primary key for the table is (id, _rev).Strictly speaking we only need the _revisions field since the other information can be derived from it. But having the other fields readily available makes conflict detection & resolution easier.Client side setupWe need to set syncRevisions to true while setting up replication\n    async setupGraphQLReplication(auth) {\n        const replicationState = this.db.todos.syncGraphQL({\n            url: syncURL,\n            headers: {\n                'Authorization': `Bearer ${auth.idToken}`\n            },\n            push: {\n                batchSize,\n                queryBuilder: pushQueryBuilder\n            },\n            pull: {\n                queryBuilder: pullQueryBuilder(auth.userId)\n            },\n\n            live: true,\n         \n            liveInterval: 1000 * 60 * 10,\n            deletedFlag: 'deleted',\n            syncRevisions: true,\n        });\n\n       ...\n    }We also need to add a text field last_pulled_rev to RxDB schema. This field is used internally by the plugin to avoid pushing revisions fetched from the server back to the server.const todoSchema = {\n    ...\n    'properties': {\n        ...\n        'last_pulled_rev': {\n            'type': 'string'\n        }\n    },\n    ...\n};Finally, we need to change the pull & push query builders to sync revision related informationPull Query Builderconst pullQueryBuilder = (userId) => {\n    return (doc) => {\n        if (!doc) {\n            doc = {\n                id: '',\n                updatedAt: new Date(0).toUTCString()\n            };\n        }\n\n        const query = `{\n            todos(\n                where: {\n                    _or: [\n                        {updatedAt: {_gt: \"${doc.updatedAt}\"}},\n                        {\n                            updatedAt: {_eq: \"${doc.updatedAt}\"},\n                            id: {_gt: \"${doc.id}\"}\n                        }\n                    ],\n                    userId: {_eq: \"${userId}\"} \n                },\n                limit: ${batchSize},\n                order_by: [{updatedAt: asc}, {id: asc}]\n            ) {\n                id\n                text\n                isCompleted\n                deleted\n                createdAt\n                updatedAt\n                userId\n                _rev\n                _revisions\n            }\n        }`;\n        return {\n            query,\n            variables: {}\n        };\n    };\n};We now fetch the _rev & _revisions fields. The upgraded plugin will use these fields to create local PouchDB revisions.Push Query Builder\nconst pushQueryBuilder = doc => {\n    const query = `\n        mutation InsertTodo($todo: [todos_insert_input!]!) {\n            insert_todos(objects: $todo){\n                returning {\n                  id\n                }\n            }\n       }\n    `;\n\n    const depth = doc._revisions.start;\n    const parent_rev = depth == 1 ? null : `${depth - 1}-${doc._revisions.ids[1]}`\n\n    const todo = Object.assign({}, doc, {\n        _depth: depth,\n        _parent_rev: parent_rev\n    })\n\n    delete todo['updatedAt']\n\n    const variables = {\n        todo: todo\n    };\n\n    return {\n        query,\n        variables\n    };\n};With the upgraded plugin, the input parameter doc now contains _rev and _revisions fields. We pass on to Hasura in the GraphQL query. We add fields _depth, _parent_rev to doc before doing so.Earlier we were using an upsert to insert or update a todo record on Hasura. Now since each version ends up being a new record we use the plain old insert mutation instead.Implementing conflict resolutionIf two different clients now make conflicting changes then both the revisions will be synced and present in Hasura. Both the clients will also eventually receive the other revision. Because PouchDB's conflict resolution strategy is deterministic both the clients will then pick the same version as the \"winning revision\".How can we find this winning revision on the server? We will have to implement the same algorithm in SQL. Implementing CouchDB's conflict resolution algorithm on PostgresStep 1: Finding leaf nodes not marked as deletedTo do this we need to ignore any versions that have a child revision and any versions that are marked as deleted:    SELECT\n        id,\n        _rev,\n        _depth\n    FROM\n        todos\n    WHERE\n        NOT EXISTS (\n            SELECT\n                id\n            FROM\n                todos AS t\n            WHERE\n                todos.id = t.id\n                AND t._parent_rev = todos._rev)\n            AND deleted = FALSEStep 2: Finding the chain with the maximum depthAssuming we have the results from the above query in a table (or view or a with clause) called leaves we can find the chain with maximum depth is straight forward:    SELECT\n        id,\n        MAX(_depth) AS max_depth\n    FROM\n        leaves\n    GROUP BY\n        idStep 3: Finding winning revisions among revisions with equal max depthAgain assuming the results from the above query are in a table (or \u00a0a view or a with clause) called max_depths we can find the winning revision as follows:    SELECT\n        leaves.id,\n        MAX(leaves._rev) AS _rev\n    FROM\n        leaves\n        JOIN max_depths ON leaves.id = max_depths.id\n            AND leaves._depth = max_depths.max_depth\n    GROUP BY\n        leaves.idCreating a view with winning revisionsPutting together the above three queries we can create a view that shows us the winning revisions as follows:CREATE OR REPLACE VIEW todos_current_revisions AS\nWITH leaves AS (\n    SELECT\n        id,\n        _rev,\n        _depth\n    FROM\n        todos\n    WHERE\n        NOT EXISTS (\n            SELECT\n                id\n            FROM\n                todos AS t\n            WHERE\n                todos.id = t.id\n                AND t._parent_rev = todos._rev)\n            AND deleted = FALSE\n),\nmax_depths AS (\n    SELECT\n        id,\n        MAX(_depth) AS max_depth\n    FROM\n        leaves\n    GROUP BY\n        id\n),\nwinning_revisions AS (\n    SELECT\n        leaves.id,\n        MAX(leaves._rev) AS _rev\n    FROM\n        leaves\n        JOIN max_depths ON leaves.id = max_depths.id\n            AND leaves._depth = max_depths.max_depth\n    GROUP BY\n        (leaves.id))\nSELECT\n    todos.*\nFROM\n    todos\n    JOIN winning_revisions ON todos._rev = winning_revisions._rev;\nSince Hasura can track views and allows querying them via GraphQL, the winning revisions can now be exposed to other clients and services.Whenever you query the view, Postgres will simply replace the view with the query in the view definition and run the resulting query. If you query the view frequently this might end up leading to a lot of wasted CPU cycles. We can optimize this by using Postgres triggers and storing the winning revisions in a different table.Using Postgres triggers to calculate winning revisionsStep 1: Create a new table todos_current_revisionsThe schema will be the same as that of the \u00a0todos table. The primary key will, however, be the id column instead of (id, _rev)Step 2: Create Postgres triggerWe can write the query for the trigger by starting with the view query. Since the trigger function will run for one row at a time, we can simplify the query:CREATE OR REPLACE FUNCTION calculate_winning_revision ()\n    RETURNS TRIGGER\n    AS $BODY$\nBEGIN\n    INSERT INTO todos_current_revisions WITH leaves AS (\n        SELECT\n            id,\n            _rev,\n            _depth\n        FROM\n            todos\n        WHERE\n            NOT EXISTS (\n                SELECT\n                    id\n                FROM\n                    todos AS t\n                WHERE\n                    t.id = NEW.id\n                    AND t._parent_rev = todos._rev)\n                AND deleted = FALSE\n                AND id = NEW.id\n        ),\n        max_depths AS (\n            SELECT\n                MAX(_depth) AS max_depth\n            FROM\n                leaves\n        ),\n        winning_revisions AS (\n            SELECT\n                MAX(leaves._rev) AS _rev\n            FROM\n                leaves\n                JOIN max_depths ON leaves._depth = max_depths.max_depth\n        )\n        SELECT\n            todos.*\n        FROM\n            todos\n            JOIN winning_revisions ON todos._rev = winning_revisions._rev\n    ON CONFLICT ON CONSTRAINT todos_winning_revisions_pkey\n        DO UPDATE SET\n            _rev = EXCLUDED._rev,\n            _revisions = EXCLUDED._revisions,\n            _parent_rev = EXCLUDED._parent_rev,\n            _depth = EXCLUDED._depth,\n            text = EXCLUDED.text,\n            \"updatedAt\" = EXCLUDED.\"updatedAt\",\n            deleted = EXCLUDED.deleted,\n            \"userId\" = EXCLUDED.\"userId\",\n            \"createdAt\" = EXCLUDED.\"createdAt\",\n            \"isCompleted\" = EXCLUDED.\"isCompleted\";\n    RETURN NEW;\nEND;\n$BODY$\nLANGUAGE plpgsql;\n\nCREATE TRIGGER trigger_insert_todos\n    AFTER INSERT ON todos\n    FOR EACH ROW\n    EXECUTE PROCEDURE calculate_winning_revision ()\nThat's it! We can now query the winning versions both on the server and the client.Custom conflict resolutionNow let us look at implementing custom conflict resolution with Hasura & RxDB.Custom conflict resolution on the server sideLet us say we want to merge the todos by fields. How do we go about doing this? The gist below shows us this:That SQL looks like a lot but the only part that deals with the actual merge strategy is this:CREATE OR REPLACE FUNCTION merge_revisions (item1 jsonb, item2 jsonb)\n    RETURNS jsonb\n    AS $$\nBEGIN\n    IF NOT item1 ? 'id' THEN\n        RETURN item2;\n    ELSE\n        RETURN item1 || (item2 -> 'diff');\n    END IF;\nEND;\n$$\nLANGUAGE plpgsql;\n\nCREATE OR REPLACE AGGREGATE agg_merge_revisions (jsonb) (\n    INITCOND = '{}',\n    STYPE = jsonb,\n    SFUNC = merge_revisions\n);Here we declare a custom Postgres aggregate function agg_merge_revisions to merge elements. The way this works is similar to a 'reduce' function: Postgres will initialize the aggregate value to '{}', then run the merge_revisions function with the current aggregate and the next element to be merged. So if we had 3 conflicting versions to be merged the result would be:merge_revisions(merge_revisions(merge_revisions('{}', v1), v2), v3)If we want to implement another strategy we will need to change the merge_revisions function. For example, if we want to implement the 'last write wins' strategy:CREATE OR REPLACE FUNCTION merge_revisions (item1 jsonb, item2 jsonb)\n    RETURNS jsonb\n    AS $$\nBEGIN\n    IF NOT (item1 ? 'id') THEN\n        RETURN item2;\n    ELSE\n        IF (item2 -> 'updatedAt') > (item1 -> 'updatedAt') THEN\n            RETURN item2\n        ELSE\n            RETURN item1\n        END IF;\n    END IF;\nEND;\n$$\nLANGUAGE plpgsql;\nThe insert query in the above gist can be run in a post insert trigger to auto-merge conflicts whenever they occur.Note: Above we have used SQL to implement custom conflict resolution. An alternate approach is to use a write an Action:Create a custom mutation to handle the insert instead of the default auto-generated insert mutation. In the action handler create the new revision of the record. We can use the Hasura insert mutation for this. Fetch all the revisions for the object using a list queryDetect any conflicts by traversing the revision tree.Write back the merged version.This approach will appeal to you if you would prefer writing this logic in a language other than SQL. Another approach is to create a SQL view to show the conflicting revisions and implement the remaining logic in the action handler. This will simplify step 4. above since we can now simply query the view for detecting conflicts.Custom conflict resolution on the client sideThere are scenarios where you need user intervention to be able to resolve a conflict. For example, if we were building something like the Trello app and two users modified the description of the same task, you might want to show the user both the versions and let them create a merged version. In these scenarios we will need to resolve the conflict on the client-side.Client-side conflict resolution is simpler to implement because PouchDB already exposes API's to query conflicting revisions. If we look at the todos RxDB collection from the previous post, here is how we can fetch the conflicting versions:todos.pouch.get(todo.id, {\n    conflicts: true\n})The above query would populate the conflicting revisions in the _conflicts field in the result. We can then present these to the user for resolution.ConclusionPouchDB comes with a flexible and powerful construct for versioning and conflict management solution. This post showed us how to use these constructs with Hasura/Postgres. In this post we have focused on doing this using plpgsql. We will be doing a follow up post showing how to do this with Actions so that you can use the language of your choice on the backend!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Authorization rules for a multi-tenant system - Google cloud platform Description: In this post we will be looking at implementing authorization of a hierarchical role based access control system in a multi tenant setup. Author: Gautam BT"}, {"role": "assistant", "content": "We had earlier covered writing permission rules for few popular use cases. In this post we will look at writing permission rules for a multi-tenant system. \u00a0Authorization in a multi-tenant system usually means two things:Each user needs to only have access to resources from that tenant.A user's role depends on the tenant they are trying to accessIf your use case does not require per tenant roles (i.e. does not require point 2. above), then have a look at this gist for a simpler implementation. We will loosely base this post on predefined roles in Google cloud platform's (GCP) role-based access control system. GCP's role-based access control system is an interesting use case because it is not only multi-tenant, the roles within each tenant are also hierarchical.GCP roles overviewIn GCP, every resource type has roles associated with it. For example: access to Google cloud storage buckets is controlled using storage_admin, storage_editor, storage_viewer roles. Similarly roles compute_admin, compute_editor, compute_viewer, etc control access to the compute engines.Roles can be per resource as well. For eg: Each bucket has a storage_admin, storage_editor, storage_viewer role associated with it. So a storage_viewer for a given bucket can access only that bucket but a someone with a global storage_viewer role can view any bucket.Both global roles and per resource roles are hierarchical i.e a Storage Admin can do anything a Storage Editor can do, etc.Finally, every role is per project so a user can be storage_admin in one project but be storage_editor in another.Setup Database Schema We will assume the following schema:We have \u00a0a projects table and a users table to keep track of projects and users in the system. roles table has the list of all roles along with hierarchy information. user_project_roles maps users to their roles in a project and user_bucket_roles maps users to their roles for a given bucket.We will also assume that we've created a one-to-one relationship user_bucket_roles from storage_buckets to user_bucket_roles. Note that keeping with the common convention, the relationship and the target table have the same name.Note: While GCP has many resources, in the above schema we have only included the storage_buckets table. We will be looking at how to write permission rules for this table. Permission rules for other tables would be similar. Creating a Storage Bucket in practice will probably involve coordinating between multiple services, allocating space, etc. The goal for this post is to only explain how to implement a system similar to GCP's role-based access control system. As such, we will conveniently ignore these complexities. Flattening hierarchical roles Similar to Example 3 in the previous post we will flatten user_project_roles and user_bucket_roles into flattened_user_project_roles and flattened_user_bucket_roles using the gist below. This allows us to not worry about the hierarchy in the roles. For example if a user is assigned the storage_admin role in the user_project_rules table, flattened_user_project_roles will have 3 rows assigning them: storage_admin, storage_editor and storage_viewer. Permission rules Since we have modeled the roles in the database, we will use a single role called user in Hasura to define permissions rules. We will assume that the session variable X-Hasura-User-Id contains the user_id and X-Hasura-Project-Id contains the project that the user is trying to access. Permission rules on storage_buckets \nSelect permissions:A user can view a storage_bucket if they have the storage_viewer role on the given bucket or they have the global storage_viewer role.We can implement the first rule with the following permission rule:{\n  \"_and\": [\n    {\n      \"_exists\": {\n        \"_table\": {\n          \"schema\": \"public\",\n          \"name\": \"flattened_user_project_roles\"\n        },\n        \"_where\": {\n          \"_and\": [\n            {\n              \"user_id\": {\n                \"_eq\": \"X-Hasura-User-Id\"\n              }\n            },\n            {\n              \"role_id\": {\n                \"_eq\": \"storage_viewer\"\n              }\n            },\n            {\n              \"project_id\": {\n                \"_eq\": \"X-Hasura-Project-ID\"\n              }\n            }\n          ]\n        }\n      }\n    },\n    {\n      \"project_id\": {\n        \"_eq\": \"X-Hasura-Project-ID\"\n      }\n    }\n  ]\n}The above JSON uses the _exists operator to check if there is a row in the flattened_user_project_roles with the given role, project_id and user_id.The second rule can be implemented as:{\n  \"user_bucket_roles\": {\n    \"_and\": [\n      {\n        \"user_id\": {\n          \"_eq\": \"X-Hasura-User-Id\"\n        }\n      },\n      {\n        \"role_id\": {\n          \"_eq\": \"storage_viewer\"\n        }\n      }\n    ]\n  }\n}In the above rule user_bucket_roles is a many-to-many relationship between storage_buckets and user_bucket_roles. Hasura evaluates the rule by fetching user_bucket_roles for the current row and validating that user_id and role_id are equal to the given values.We can now put the two in an _or clause for the final rule:{\n  \"_or\": [\n    {\n      \"_and\": [\n        {\n          \"_exists\": {\n            \"_table\": {\n              \"schema\": \"public\",\n              \"name\": \"flattened_user_project_roles\"\n            },\n            \"_where\": {\n              \"_and\": [\n                {\n                  \"user_id\": {\n                    \"_eq\": \"X-Hasura-User-Id\"\n                  }\n                },\n                {\n                  \"role_id\": {\n                    \"_eq\": \"storage_viewer\"\n                  }\n                },\n                {\n                  \"project_id\": {\n                    \"_eq\": \"X-Hasura-Project-ID\"\n                  }\n                }\n              ]\n            }\n          }\n        },\n        {\n          \"project_id\": {\n            \"_eq\": \"X-Hasura-Project-ID\"\n          }\n        }\n      ]\n    },\n    {\n      \"user_bucket_roles\": {\n        \"_and\": [\n          {\n            \"user_id\": {\n              \"_eq\": \"X-Hasura-User-Id\"\n            }\n          },\n          {\n            \"role_id\": {\n              \"_eq\": \"storage_viewer\"\n            }\n          }\n        ]\n      }\n    }\n  ]\n}Update permissions:Update permissions would look the same as the select permissions except we would be using the role_id storage_editor instead of storage_viewerWe also we need to prevent the user from updating the project_id column of a bucket:Delete permissions:Delete permissions would again look the same as the select permissions except we would be using the role_id storage_admin instead of storage_viewerInsert permissions:Only a user with a storage_admin role should be able to create a bucket:{\n  \"_exists\": {\n    \"_table\": {\n      \"schema\": \"public\",\n      \"name\": \"flattened_user_project_roles\"\n    },\n    \"_where\": {\n      \"_and\": [\n        {\n          \"user_id\": {\n            \"_eq\": \"X-Hasura-User-Id\"\n          }\n        },\n        {\n          \"role_id\": {\n            \"_eq\": \"storage_admin\"\n          }\n        },\n        {\n          \"project_id\": {\n            \"_eq\": \"X-Hasura-Project-ID\"\n          }\n        }\n      ]\n    }\n  }\n}We also want the project_id of the created bucket to be X-Hasura-Project-Id. We can do this using column presets: Permission rules on user_project_roles and user_bucket_roles We also need rules that allow certain users to assign or remove the roles of other users. We can have roles role_admin & role_viewer for being able to edit and view user roles respectively. Permission rules on user_project_roles and user_bucket_roles would then look similar to the insert permission rule on storage_buckets with the role name changed.ConclusionIn this post we've seen how to implement permission rules for a full-fledged hierarchical multi-tenant system. Postgres Views and Hasura's Permission DSL make a rather powerful combination!If you are using Hasura and need help with authorization, or want to share some interesting use cases you have implemented, ping us on Discord or tweet to us at @HasuraHQ!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Adding custom business logic to Hasura using Dark Description: Hasura has recently implemented a way to create custom mutations called Actions.\nWant to handle complex business logic? Actions are the way to go!\n\nTo create a new action, you need to provide a definition, handler (REST\nendpoint), and specify the kind \u2013 sync or async. When you run the custom GraphQL\nmutation, Hasura makes a POST request to the specified handler with the mutation\narguments and the session variables. If you want to know more about the\nmachinery behind it, check out the docs\n[https Author: Aleksandra"}, {"role": "assistant", "content": "Hasura has recently implemented a way to create custom mutations called Actions. Want to handle complex business logic? Actions are the way to go!To create a new action, you need to provide a definition, handler (REST endpoint), and specify the kind \u2013 sync or async. When you run the custom GraphQL mutation, Hasura makes a POST request to the specified handler with the mutation arguments and the session variables. If you want to know more about the machinery behind it, check out the docs or our article introducing actions.In this article, we're going to explore how Hasura and Dark work together by creating a custom mutation and implementing a handler for it using Dark. And all of that will be done without leaving the browser!What is Dark?Developing services is a pretty complicated job. Before you actually start writing code, you need to decide on stuff like hosting, CI/CD pipeline, language, and then you need to stitch it all together. Another problematic thing around it is deployment. Making a commit, creating PR, running CI, actual deploy \u2014 it's a lot. So what Dark aspires to do is to take this whole complexity away from us, and leave developers to only worry about writing code.As being a setupless solution, Dark consists of language, editor, runtime, and infrastructure, so that you don't need to spend time figuring all of that on your own. What's more, ease of writing deployless backends is one of Dark's primary concepts, so while your writing your code, every change is instantly deployed to the cloud!The language itself is described as a statically-typed functional/imperative hybrid, based loosely on ML. The dark compiler was written in OCaml, and syntax wise, you may spot a resemblance between these two languages.With Dark, you write your code in a structured editor that makes sure you won't write syntactically incorrect code. There's no parser included, which means no syntax errors \u2014 with every keystroke, you modify the AST directly.Creating an actionOnline mafia gameDo you know the Mafia game? It's an old-timey party game in which the objective is for the mafia to kill off civilians until they are the majority, or the civilians to kill off the entire mafia. As the rules can be easily extended, players could be assigned with many different roles.Yet, for our example, let's take only three: mafioso, civilian, and a doctor. There are also some constraints regarding roles:There can be only one doctor.The ratio between mafia players and all players should be around 1/3.The minimum number of mafia players is 2.For example, for eight players, there should be one doctor, two mafiosos, and five civilians.When a new player enters a game, I want to assign him a role. However, how do I make sure the above constraints aren't violated? How do I know what characters are still available?In real life, someone probably would need to go through all the cards and choose the characters based on the players count, and then deal the cards to the players.But what about an online version of the game? There is no game master among the players and I can't have information about other players' roles on the frontend, because it's secret. That's when actions come into play! I'm going to create a custom mutation that performs the following logic:Fetch all taken roles from the DB, along with the number of players in the game.Based on already present characters, check which are still available.From a set of available roles, choose a random one.Insert a new user into the database.Hasura custom mutation flowNew Hasura ActionThe first thing to do is to define the mutation and all the required types. The definitions below mean:The name of the custom mutation is CreateNewPlayer.It accepts two arguments: a string nickname and game_id of type CreateNewPlayerUuid, which is a custom scalar.The mutation returns CreateNewPlayerOutput, which consists of newly created player info.New Action definitionNext, I need to provide an HTTP endpoint to which Hasura will make POST requests whenever CreateNewPlayer mutation is called. In my case, I'll put a link to my Dark canvass with a dummy route (yet to be created) called new_player.HTTP handler created with DarkAs the last step, I'm saying that the kind of the mutation is Synchronous, which means that Hasura would keep the request open till receiving a response from the handler.Kind of the ActionCreating REST endpoint in DarkIf you want to create an HTTP handler, you probably follow these four steps:Agree upon request body parameters.Write an implementation.Make endpoint accessible.Test it by sending a request.Dark took a different approach. One of the core concepts of Dark is Trace-Driven-Development. It allows you to develop your backends from incoming requests. In other words, you can send a request to the endpoint that doesn't even exist. Then, based on the received request, you can implement your handler. Let's see it in action!I'm going to call my newly created mutation. It will result in an error because I haven't implemented it yet in Dark. But I can go to my Dark canvass, check the 404 section and see that Dark captured the request that Hasura for CreateNewPlayer action.404 sectionNow, by clicking + button, I can convert nonexistent HTTP endpoint into a real handler. The handler is automatically hosted for me. I can also check what exactly was sent to Dark in a request body. ImplementationI will skip some implementation details related to determining a new role. You can find screenshots with code here.As the first step, I'm making a request to Hasura to extract the information I need \u2013 all the roles that are already taken and the number of players. As you can see in the screenshot below, there are already three civilians and one doctor.Fetch all needed data from HasuraThe next step is to randomly choose a role from all available roles.Available rolesNew RoleAs I have a new role for the player, now I can make a call to Hasura and insert a new player to the database.Call to Hasura and handler returning dataAll the implementation is done, so now I can start using CreateNewPlayer mutation \ud83c\udf89Call mutation from Hasura ConsoleConnecting Action with the graphThe custom mutation I just created returns information about the newly created player. The next thing I'd need is information about the game and other players. But I don't want to make another call to Hasura. Instead, I want to get all the additional info by calling only CreateNewPlayer mutation.In order to obtain that, I'm going to modify the definition, so that it also returns gameId and create a new relationship between the Action and game table. I also need to return gameId from the handler.Add gameId to type definitionNew relationshipReturn gameId from the handlerThanks to the new relationship, I can now fetch all the data about the game I want! I can fetch info about the players associated with the same game as well.MutationResultSummaryWe explored how to add custom business logic to Hasura in a few steps, learned how to take advantage of the Trace Driven Development, and how quickly we can get a rest endpoint up and running with Dark. Moreover, we added a custom mutation with Hasura Actions, and took things to the next level by creating an relationship and connecting it to the graph!Additional Reading: Creating a GraphQL API for Notion with Hasura Actions"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Effect of Intel's power management on web servers Description: While benchmarking Hasura, we noticed Hasura performed better when there was more load on the system. In this blog post we cover basics of processor power management & talk about how we see 50% improvement in latencies by tweaking power management settings. Author: Gautam BT"}, {"role": "assistant", "content": "While benchmarking some tweaks to Hasura we noticed some strange behavior. We were measuring latencies for a simple GraphQL request being served by Hasura. Surprisingly Hasura was performing better when there was more load on the system than otherwise.At 100 requests per second the processor load was low enough that the processor going to sleep was negatively impacting the latency measurements. However, when a browser was open on the same machine, latency measurements improved!The explanation for this is quite intriguing and is documented in detail in this post by Brandon. This blog post explains that post from a layman\u2019s perspective.BackgroundModern Intel processors have extremely sophisticated power management that modifies the clock frequency and powers up and down subsystems dynamically and constantly. This can happen several times a second. p-states and c-statesPower management happens both when the processor is idle and otherwise. Idle states of a processor are called C-states and are denoted as C0, C1, C2... C0 is the operating state whereas C1, C2, C3.. are states where some or more of the subsystems are shutdown. Generally the higher the C-state the lower the power consumption but also the longer it takes for the processor to move back to the operating state C0.We can look at the idle states available on my laptop using the cpupower command:$ cpupower idle-info\nCPUidle driver: intel_idle\nCPUidle governor: menu\nanalyzing CPU 0:\n\nNumber of idle states: 9\nAvailable idle states: POLL C1 C1E C3 C6 C7s C8 C9 C10\nPOLL:\nFlags/Description: CPUIDLE CORE POLL IDLE\nLatency: 0\nUsage: 5845\nDuration: 3482887\nC1:\nFlags/Description: MWAIT 0x00\nLatency: 2\nUsage: 155904\nDuration: 40263250\nC1E:\nFlags/Description: MWAIT 0x01\nLatency: 10\nUsage: 505874\nDuration: 163384145\nC3:\nFlags/Description: MWAIT 0x10\nLatency: 70\nUsage: 405130\nDuration: 115592556\nC6:\nFlags/Description: MWAIT 0x20\nLatency: 85\nUsage: 2407019\nDuration: 1503034108\nC7s:\nFlags/Description: MWAIT 0x33\nLatency: 124\nUsage: 657\nDuration: 674710\nC8:\nFlags/Description: MWAIT 0x40\nLatency: 200\nUsage: 1694254\nDuration: 2067037470\nC9:\nFlags/Description: MWAIT 0x50\nLatency: 480\nUsage: 38\nDuration: 65699\nC10:\nFlags/Description: MWAIT 0x60\nLatency: 890\nUsage: 44032\nDuration: 79069478\nWe can see that my processor has these idle states available: POLL, C1, C1E, C3, C6, C7s, C8, C9, C10. We can also see a Latency field that tells us the maximum time (in \u00b5s) the processor will take to go from that state to the operating C0 state. It is possible to enable/disable these states using the cpupower idle-set command. Similarly when a processor is not idle it will be in one of several P-states denoted by P0, P1, P2, P3. The higher the p-state, the lower the voltage and operating frequency of the processor. This means CPU bound programs will in general execute faster in a lower P-state than a higher p-state. We cannot explicitly set the processor to a particular state. However we can set a CPUFreq governor using the cpupower frequency-set -g <governor> command.On my laptop I can see the available governors with:$ cpupower frequency-info\nanalyzing CPU 0:\n  driver: intel_pstate\n  CPUs which run at the same hardware frequency: 0\n  CPUs which need to have their frequency coordinated by software: 0\n  maximum transition latency:  Cannot determine or is not supported.\n  hardware limits: 400 MHz - 4.00 GHz\n  available cpufreq governors: performance powersave\n  current policy: frequency should be within 400 MHz and 4.00 GHz.\n                  The governor \"powersave\" may decide which speed to use\n                  within this range.\n  current CPU frequency: Unable to call hardware\n  current CPU frequency: 1.60 GHz (asserted by call to kernel)\n  boost state support:\n    Supported: yes\n    Active: yes\nWe can see that I can set the governor to performance or powersave and the current governor is powersave.This article explains the various P-states and C-states that intel processors implement in greater detail.We can also measure the time spent by the processor in various states using the turbostat command:\n$ sudo turbostat --quiet sleep 5\n5.004098 sec\nCore\tCPU\tAvg_MHz\tBusy%\tBzy_MHz\tTSC_MHz\tIRQ\tSMI\tC1\tC1E\tC3\tC6\tC7s\tC8\tC9\tC10\tC1%\tC1E%\tC3%\tC6%\tC7s%\tC8%\tC9%\tC10%\tCPU%c1\tCPU%c3\tCPU%c6\tCPU%c7\tCoreTmp\tPkgTmp\tGFX%rc6\tGFXMHz\tTotl%C0\tAny%C0\tGFX%C0\tCPUGFX%\tPkg%pc2\tPkg%pc3\tPkg%pc6\tPkg%pc7\tPkg%pc8\tPkg%pc9\tPk%pc10\tPkgWatt\tCorWatt\tGFXWatt\tRAMWatt\tPKG_%\tRAM_%\n-\t-\t6\t0.35\t1640\t1991\t2009\t0\t1\t54\t11\t78\t0\t1555\t0\t523\t0.00\t0.01\t0.00\t0.18\t0.00\t27.94\t0.00\t71.50\t0.95\t0.00\t0.30\t98.39\t54\t55\t99.96\t300\t2.76\t1.83 0.00\t0.00\t20.55\t0.12\t0.09\t0.15\t75.31\t0.00\t0.00\t0.76\t0.09\t0.00\t0.32\t0.00\t0.00\n0\t0\t12\t0.77\t1625\t1991\t730\t0\t0\t2\t5\t48\t0\t603\t0\t90\t0.00\t0.00\t0.01\t0.98\t0.00\t47.68\t0.00\t50.57\t1.25\t0.00\t0.90\t97.07\t54\t55\t99.96\t300\t2.76\t1.83\t0.00\t0.00\t20.56\t0.12\t0.09\t0.15\t75.31\t0.00\t0.00\t0.76\t0.09\t0.00\t0.32\t0.00\t0.00\n0\t4\t4\t0.25\t1650\t1991\t154\t0\t0\t0\t0\t4\t0\t125\t0\t70\t0.00\t0.00\t0.00\t0.07\t0.00\t22.48\t0.00\t77.17\t1.77\n1\t1\t2\t0.15\t1689\t1991\t95\t0\t0\t8\t0\t3\t0\t66\t0\t32\t0.00\t0.01\t0.00\t0.05\t0.00\t9.76\t0.00\t90.01\t0.86\t0.00\t0.08\t98.91\t54\n1\t5\t6\t0.35\t1626\t1991\t222\t0\t0\t0\t0\t4\t0\t129\t0\t115\t0.00\t0.00\t0.00\t0.06\t0.00\t18.13\t0.00\t81.44\t0.67\n2\t2\t5\t0.31\t1672\t1991\t187\t0\t0\t1\t0\t6\t0\t163\t0\t53\t0.00\t0.00\t0.00\t0.10\t0.00\t39.79\t0.00\t59.78\t1.02\t0.01\t0.13\t98.53\t54\n2\t6\t8\t0.50\t1623\t1991\t299\t0\t0\t1\t6\t5\t0\t246\t0\t61\t0.00\t0.00\t0.01\t0.08\t0.00\t46.01\t0.00\t53.39\t0.84\n3\t3\t4\t0.27\t1642\t1991\t180\t0\t0\t5\t0\t5\t0\t139\t0\t43\t0.00\t0.01\t0.00\t0.07\t0.00\t29.59\t0.00\t70.05\t0.58\t0.00\t0.10\t99.06\t54\n3\t7\t3\t0.19\t1664\t1992\t142\t0\t1\t37\t0\t3\t0\t84\t0\t59\t0.00\t0.05\t0.00\t0.05\t0.00\t10.11\t0.00\t89.58\t0.66\nIn the above output we can see that the processor spends < 0.5% in C0 state (Busy%) because we are executing the sleep command and there is not much else happening on the machine. The bulk of the time is spent in C7 state (CPU%c7). This man page explains the various fields that turbostat will output.Processor states while Hasura serves a GraphQL RequestHow does all of this affect Hasura's performance you ask? Let us look at power management states of the machine while Hasura executes a simple GraphQL query. The below measurements were made on my laptop running Intel i7-8550U (1.8GHz turbo boost to 4GHz). Both Hasura and Postgres were running on the same machine and a simple GraphQL query fetching about 5 rows was used. I've used wrk2 - launched using turbostat - to run a constant load of 100RPS.Here's the output from the wrk2 command:\nRunning 1m test @ http://localhost:8181/v1/graphql\n  1 threads and 10 connections\n  Thread calibration: mean lat.: 2.489ms, rate sampling interval: 10ms\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     2.49ms  521.70us  12.94ms   75.29%\n    Req/Sec   105.63    102.66   363.00     37.27%\n  Latency Distribution (HdrHistogram - Recorded Latency)\n 50.000%    2.52ms\n 75.000%    2.80ms\n 90.000%    3.03ms\n 99.000%    3.53ms\n 99.900%    5.90ms\n 99.990%   12.94ms\n 99.999%   12.94ms\n100.000%   12.94ms\nHere's the output from turbostat:\n60.011521 sec\nCore\tCPU\tAvg_MHz\tBusy%\tBzy_MHz\tTSC_MHz\tIRQ\tSMI\tC1\tC1E\tC3\tC6\tC7s\tC8\tC9\tC10\tC1%\tC1E%\tC3%\tC6%\tC7s%\tC8%\tC9%\tC10%\tCPU%c1\tCPU%c3\tCPU%c6\tCPU%c7\tCoreTmp\tPkgTmp\tGFX%rc6\tGFXMHz\tTotl%C0\tAny%C0\tGFX%C0\tCPUGFX%\tPkg%pc2\tPkg%pc3\tPkg%pc6\tPkg%pc7\tPkg%pc8\tPkg%pc9\tPk%pc10\tPkgWatt\tCorWatt\tGFXWatt\tRAMWatt\tPKG_%\tRAM_%\n-\t-\t50\t3.00\t1669\t1992\t70009\t0\t261\t1971\t1526\t11017\t34\t71007\t4\t17698\t0.01\t0.04\t0.06\t1.62\t0.02\t49.45\t0.00\t45.85\t5.20\t0.07\t2.63\t89.10\t39\t40\t100.00\t300\t24.38\t20.60\t0.00\t0.00\t25.92\t1.42\t0.42\t0.11\t47.07\t0.00\t0.00\t1.15\t0.46\t0.00\t0.43\t0.00\t0.00\n0\t0\t50\t3.08\t1629\t1992\t8822\t0\t32\t210\t185\t1225\t8\t9399\t2\t2005\t0.00\t0.03\t0.05\t1.39\t0.03\t51.97\t0.00\t43.49\t5.23\t0.07\t2.56\t89.07\t39\t40\t100.00\t300\t24.38\t20.60\t0.00\t0.00\t25.92\t1.42\t0.42\t0.11\t47.07\t0.00\t0.00\t1.15\t0.46\t0.00\t0.43\t0.00\t0.00\n0\t4\t50\t3.07\t1615\t1992\t9119\t0\t49\t259\t216\t1523\t5\t9173\t0\t2120\t0.00\t0.04\t0.06\t1.81\t0.03\t48.68\t0.00\t46.35\t5.24\n1\t1\t60\t3.29\t1807\t1992\t8405\t0\t20\t171\t191\t1201\t0\t8406\t0\t2056\t0.01\t0.02\t0.06\t1.35\t0.00\t49.63\t0.00\t45.67\t4.86\t0.07\t2.42\t89.36\t39\n1\t5\t45\t2.75\t1653\t1992\t8734\t0\t39\t385\t191\t1384\t8\t8161\t0\t2498\t0.00\t0.07\t0.05\t1.64\t0.02\t44.75\t0.00\t50.75\t5.40\n2\t2\t52\t3.15\t1644\t1992\t9181\t0\t29\t269\t184\t1436\t3\t9256\t0\t2008\t0.00\t0.07\t0.06\t1.70\t0.02\t50.97\t0.00\t44.07\t5.19\t0.07\t2.49\t89.10\t39\n2\t6\t53\t2.99\t1756\t1992\t7914\t0\t25\t265\t183\t1138\t2\t8432\t0\t2248\t0.01\t0.03\t0.05\t1.34\t0.01\t49.80\t0.00\t45.80\t5.35\n3\t3\t45\t2.80\t1616\t1992\t8336\t0\t25\t209\t150\t1401\t6\t8112\t0\t2663\t0.00\t0.05\t0.05\t1.66\t0.02\t44.79\t0.00\t50.68\t5.20\t0.07\t3.06\t88.88\t39\n3\t7\t46\t2.85\t1617\t1992\t9498\t0\t42\t203\t226\t1709\t2\t10068\t2\t2100\t0.01\t0.03\t0.06\t2.12\t0.00\t55.00\t0.02\t39.95\t5.14\n\nWe can see the processor was busy i.e in C-state 0 less than 3.5% of the time, while it spent about 90% of the time in the deep C-7 state, which is slow to wake and do useful work.Essentially at 100 RPS Hasura takes so little of the processor time that the processor spends most of the time in deep sleep \ud83e\udd13 That in turn increases the latency of requests because it takes time for the processor to wake up and serve the request.Fine tuning processor states\"performance\" p-state governorHow does using the performance governor improve the situation? Switching it to performance mode with...$ sudo cpupower frequency-set -g performance\n...reduces latency as the CPU is more ready to ramp up and do work when it comes:wrk2 output:\nRunning 1m test @ http://localhost:8181/v1/graphql\n  1 threads and 10 connections\n  Thread calibration: mean lat.: 2.002ms, rate sampling interval: 10ms\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     1.94ms  579.10us   9.26ms   68.44%\n    Req/Sec   105.33    100.88   333.00     43.93%\n  Latency Distribution (HdrHistogram - Recorded Latency)\n 50.000%    1.90ms\n 75.000%    2.34ms\n 90.000%    2.62ms\n 99.000%    3.21ms\n 99.900%    5.98ms\n 99.990%    9.27ms\n 99.999%    9.27ms\n100.000%    9.27ms\nWe see better latencies at pretty much every percentile. turbostat output:\n60.006307 sec\nCore\tCPU\tAvg_MHz\tBusy%\tBzy_MHz\tTSC_MHz\tIRQ\tSMI\tC1\tC1E\tC3\tC6\tC7s\tC8\tC9\tC10\tC1%\tC1E%\tC3%\tC6%\tC7s%\tC8%\tC9%\tC10%\tCPU%c1\tCPU%c3\tCPU%c6\tCPU%c7\tCoreTmp\tPkgTmp\tGFX%rc6\tGFXMHz\tTotl%C0\tAny%C0\tGFX%C0\tCPUGFX%\tPkg%pc2\tPkg%pc3\tPkg%pc6\tPkg%pc7\tPkg%pc8\tPkg%pc9\tPk%pc10\tPkgWatt\tCorWatt\tGFXWatt\tRAMWatt\tPKG_%\tRAM_%\n-\t-\t67\t1.72\t3930\t1992\t67347\t0\t227\t1648\t1301\t9135\t32\t72585\t2\t17162\t0.01\t0.04\t0.08\t1.62\t0.01\t50.84\t0.00\t45.66\t8.01\t0.10\t2.23\t87.95\t49\t49\t100.01\t300\t13.73\t11.58\t0.00\t0.00\t19.86\t1.41\t0.35\t0.04\t52.64\t0.00\t0.00\t2.49\t1.69\t0.00\t0.64\t0.00\t0.00\n0\t0\t62\t1.58\t3923\t1992\t9298\t0\t34\t167\t175\t1277\t3\t9688\t0\t1967\t0.02\t0.05\t0.08\t1.92\t0.01\t54.27\t0.00\t42.06\t7.76\t0.07\t2.17\t88.42\t49\t49\t100.01\t300\t13.73\t11.58\t0.00\t0.00\t19.86\t1.41\t0.35\t0.04\t52.64\t0.00\t0.00\t2.49\t1.69\t0.00\t0.64\t0.00\t0.00\n0\t4\t64\t1.64\t3924\t1992\t7662\t0\t36\t196\t144\t917\t2\t8251\t0\t2133\t0.00\t0.02\t0.06\t1.19\t0.01\t47.09\t0.00\t49.97\t7.69\n1\t1\t71\t1.81\t3932\t1992\t8996\t0\t26\t232\t196\t1253\t4\t9553\t1\t2035\t0.00\t0.05\t0.09\t1.73\t0.01\t53.37\t0.00\t42.92\t8.42\t0.08\t2.25\t87.44\t46\n1\t5\t68\t1.74\t3933\t1992\t8754\t0\t35\t224\t136\t1132\t5\t9662\t0\t2293\t0.02\t0.04\t0.06\t1.59\t0.01\t53.98\t0.00\t42.54\t8.48\n2\t2\t70\t1.78\t3937\t1992\t7611\t0\t20\t200\t149\t1063\t3\t8247\t0\t2260\t0.00\t0.02\t0.08\t1.53\t0.00\t47.71\t0.00\t48.85\t7.35\t0.08\t1.78\t89.01\t47\n2\t6\t60\t1.54\t3918\t1992\t7920\t0\t25\t210\t134\t780\t2\t8559\t1\t1993\t0.00\t0.04\t0.06\t1.05\t0.01\t51.94\t0.00\t45.33\t7.58\n3\t3\t74\t1.88\t3937\t1992\t8414\t0\t25\t259\t164\t1262\t3\t9352\t0\t2064\t0.00\t0.10\t0.07\t1.78\t0.02\t49.07\t0.00\t47.05\t8.32\t0.16\t2.72\t86.91\t48\n3\t7\t69\t1.76\t3930\t1992\t8692\t0\t26\t160\t203\t1451\t10\t9273\t0\t2417\t0.00\t0.03\t0.17\t2.16\t0.01\t49.29\t0.00\t46.56\t8.45\nNotice Bzy_MHz is now close to the maximum advertised frequency of 4GHz (with turboboost).Prohibit idle statesWe can keep the processor from entering deep sleep states with:$ sudo cpupower idle-set -D2\nThis will disable any idle state with a latency >= 2\u00b5s which on my laptop is all idle states.Here's the results from wrk2:\nRunning 1m test @ http://localhost:8181/v1/graphql\n  1 threads and 10 connections\n  Thread calibration: mean lat.: 0.995ms, rate sampling interval: 10ms\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     1.13ms  353.31us   8.95ms   79.16%\n    Req/Sec   108.45    104.68   333.00     19.19%\n  Latency Distribution (HdrHistogram - Recorded Latency)\n 50.000%    1.20ms\n 75.000%    1.32ms\n 90.000%    1.41ms\n 99.000%    1.79ms\n 99.900%    4.33ms\n 99.990%    8.96ms\n 99.999%    8.96ms\n100.000%    8.96ms\nAgain we can see an improvement at almost every percentile.Turbostat output:\n60.005077 sec\nCore\tCPU\tAvg_MHz\tBusy%\tBzy_MHz\tTSC_MHz\tIRQ\tSMI\tC1\tC1E\tC3\tC6\tC7s\tC8\tC9\tC10\tC1%\tC1E%\tC3%\tC6%\tC7s%\tC8%\tC9%\tC10%\tCPU%c1\tCPU%c3\tCPU%c6\tCPU%c7\tCoreTmp\tPkgTmp\tGFX%rc6\tGFXMHz\tTotl%C0\tAny%C0\tGFX%C0\tCPUGFX%\tPkg%pc2\tPkg%pc3\tPkg%pc6\tPkg%pc7\tPkg%pc8\tPkg%pc9\tPk%pc10\tPkgWatt\tCorWatt\tGFXWatt\tRAMWatt\tPKG_%\tRAM_%\n-\t-\t3685\t99.76\t3693\t1992\t68091\t16\t0\t0\t0\t0\t0\t0\t0\t0\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.24\t0.00\t0.00\t0.00\t85\t84\t99.92\t300\t398.38\t99.59\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t19.73\t18.80\t0.00\t0.35\t0.00\t0.00\n0\t0\t3685\t99.76\t3693\t1992\t9292\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.24\t0.00\t0.00\t0.00\t81\t84\t99.92\t300\t398.38\t99.59\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t19.73\t18.80\t0.00\t0.35\t0.00\t0.00\n0\t4\t3685\t99.76\t3693\t1992\t7130\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.24\n1\t1\t3685\t99.76\t3693\t1992\t9178\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.24\t0.00\t0.00\t0.00\t80\n1\t5\t3685\t99.76\t3693\t1992\t9541\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.24\n2\t2\t3685\t99.76\t3693\t1992\t7925\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.24\t0.00\t0.00\t0.00\t85\n2\t6\t3685\t99.76\t3693\t1992\t8344\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.24\n3\t3\t3685\t99.76\t3693\t1992\t9789\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.24\t0.00\t0.00\t0.00\t80\n3\t7\t3685\t99.76\t3693\t1992\t6892\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.24\nAs expected the Busy% is now close to the 100%. ConclusionIn this article we have seen that processor power management has significant effect on response times with Hasura. Using the performance governor and disabling idle states improves the average latency by almost 50%. A couple of caveats to be aware of though:With wrk2's, measured latencies are [only] accurate to a +/- ~1 ms granularity, due to OS sleep time behavior.Disabling idle states/using the performance governor has the effect of also consuming more power. In particular disabling all idle states means that the CPU is running at peak frequency continuously. You might want to experiment with disabling some of the deeper C-states and leaving the shallower ones on.We have also not tried these in production and these are not official recommendations. However, if you do try these techniques out, let us know of your experience on Discord or tweet to us at @HasuraHQ"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Decreasing latency noise and maximizing performance during end-to-end benchmarking Description: While benchmarking some tweaks to Hasura we noticed some confusing and misleading/inconsistent results which led us down a bit of a rabbit hole. This is a summary of what we've learned so far, what is left to try, and what we've tried which didn't seem to have much effect.  Author: Brandon"}, {"role": "assistant", "content": "I was taken by surprise to find that benchmarking graphql-engine at 100 RPS was such a light load that power management was skewing my measurements dramatically (and worse, misleading me when e.g. having the browser open (i.e. more load) actually caused a dramatic performance improvement, causing me to think I'd made a performance breakthrough!).While benchmarking some tweaks to graphql-engine we noticed some confusing and misleading/inconsistent results which led us down a bit of a rabbit hole. This is a summary of what we've learned so far, what is left to try, and what we've tried which didn't seem to have much effect. We're starting to explore two interrelated things:understanding how to maximize performance in a running graphql-engine generally; this is a type of noise reduction as well for the purposes of benchmarking, but is also obviously beneficial for best-practices to share with users or to help them measure performanceto understand how to run graphql-engine such that latency is consistent (but possibly much slower), so that we can be confident that a change we've made was actually beneficial or notSome of the results here probably apply to production deployments of graphql-engine (and other services) as well; but we don't have concrete recommendations there yet. If you have success with any of these techniques in production, let us know!Things that were very effectiveBackground: modern intel processors have extremely sophisticated power management that modifies the clock frequency and powers up and down subsystems dynamically and constantly (many times per second). There are knobs on modern linux for some amount of control over how all this behaves: the intel_idle driver allows some control over c-states (processor idle states), while intel_pstate deals with p-states. See the references at the bottom for more.I was taken by surprise to find that benchmarking graphql-engine at 100 RPS was such a light load that power management was skewing my measurements dramatically (and worse, misleading me when e.g. having the browser open (i.e. more load) actually caused a dramatic performance improvement, causing me to think I'd made a performance breakthrough!).We can use \u00a0turbostat (from the cpupower package) to look at power management states of the machine during an execution of some load, with:$ turbostat --interval 0.1 sleep 120For my laptop (with Core i7-3667U), without tweaks, running the 100 RPS benchmark:\n    Core     CPU Avg_MHz   Busy% Bzy_MHz TSC_MHz     IRQ     SMI  CPU%c1  CPU%c3  CPU%c6  CPU%c7 CoreTmp  PkgTmp GFX%rc6  GFXMHz Pkg%pc2 Pkg%pc3 Pkg%pc6 Pkg%pc7 PkgWatt CorWatt GFXWatt\n       -       -     110    9.44    1167    2494       0       0   14.44    4.30    0.01   71.81      57      57    0.00       0    4.58    1.66    3.58   35.50    4.14    1.16    0.31\n       0       0     113    9.80    1161    2494       0       0   13.99    3.98    0.00   72.23      56      57    0.00       0    4.58    1.66    3.58   35.50    4.14    1.16    0.31\n       0       1     103    8.92    1160    2494       0       0   14.87\n       1       2     113    9.85    1151    2494       0       0   14.12    4.62    0.02   71.40      57\n       1       3     110    9.18    1196    2494       0       0   14.79\n\n        Thread Stats   Avg      Stdev     Max   +/- Stdev\n          Latency     2.44ms  833.69us  13.82ms   78.94%\n          Req/Sec    26.36     67.81   400.00     85.22%\n        6004 requests in 1.00m, 2.35MB read\n...we can see the processor was busy (in C-state 0) less than 10% of the time, while it spent over 70% of the time in the deep C-7 state, which is slow to wake and do useful work.\"performance\" p-state governorintel_pstate offers two \"governors\" or modes. Switching it to \"performance\", with...$ sudo cpupower frequency-set -g performance\n...significantly reduces latency as the CPU is (waves hands) more ready to ramp up and do work when it comes:\n    Core     CPU Avg_MHz   Busy% Bzy_MHz TSC_MHz     IRQ     SMI  CPU%c1  CPU%c3  CPU%c6  CPU%c7 CoreTmp  PkgTmp GFX%rc6  GFXMHz Pkg%pc2 Pkg%pc3 Pkg%pc6 Pkg%pc7 PkgWatt CorWatt GFXWatt\n       -       -      99    4.49    2206    2494       0       0   13.02    5.00    0.01   77.47      55      55    0.00       0    5.35    2.06    3.97   48.43    4.40    1.67    0.13\n       0       0     104    5.01    2073    2494       0       0   12.72    5.05    0.02   77.20      54      55    0.00       0    5.35    2.06    3.97   48.43    4.40    1.67    0.13\n       0       1      93    3.96    2361    2494       0       0   13.77\n       1       2     103    4.99    2071    2494       0       0   12.31    4.95    0.00   77.74      55\n       1       3      95    4.00    2386    2494       0       0   13.30\n\n        Thread Stats   Avg      Stdev     Max   +/- Stdev\n          Latency     1.52ms    1.14ms  33.86ms   97.87%\n          Req/Sec    26.48     67.00   333.00     85.14%\n        6004 requests in 1.00m, 2.35MB read\n Notice Bzy_MHz is close to the advertised clock speed here.Prohibit deep sleep statesFinally we can keep the processor from entering deep sleep states with:$ sudo cpupower idle-set -D10  \n(Note: you can re-enable all idle states with : sudo cpupower idle-set -E)\n    Core     CPU Avg_MHz   Busy% Bzy_MHz TSC_MHz     IRQ     SMI  CPU%c1  CPU%c3  CPU%c6  CPU%c7 CoreTmp  PkgTmp GFX%rc6  GFXMHz Pkg%pc2 Pkg%pc3 Pkg%pc6 Pkg%pc7 PkgWatt CorWatt GFXWatt\n       -       -      68    2.28    3000    2494       0       0   97.72    0.00    0.00    0.00      64      64    0.00       0    0.00    0.00    0.00    0.00    8.74    5.86    0.11\n       0       0      71    2.36    3000    2494       0       0   97.64    0.00    0.00    0.00      60      64    0.00       0    0.00    0.00    0.00    0.00    8.74    5.86    0.11\n       0       1      67    2.25    3000    2494       0       0   97.75\n       1       2      73    2.43    3000    2494       0       0   97.57    0.00    0.00    0.00      64\n       1       3      63    2.09    3000    2494       0       0   97.91\n\n        Thread Stats   Avg      Stdev     Max   +/- Stdev\n          Latency     1.25ms  404.96us   6.43ms   68.94%\n          Req/Sec    25.66     63.97   333.00     84.95%\n        6004 requests in 1.00m, 2.35MB read\nHere we see a smaller but significant reduction in latency. We can see that we're spending nearly all our time in the shallower C1 state, ready to wake up quickly.We can graph latency improvements from these two changes. Here we're measuring from instrumented graphql-engine:FYI Here are all AWS instances under $3/hr that allow setting both cstate/pstate:                       CPUS      ECUs    RAM\n  $1.591   c4.8xlarge    36      132      60 GiB\n  $1.872   h1.8xlarge    32      99      128 GiB\n  $2.00    m4.10xlarge   40      124.5   160 GiB\n  $2.128   r4.8xlarge    32      99      244 GiB\n  $2.496   i3.8xlarge    32      99      244 GiB\nThings that seemed to have little or dubious effect ...but might when e.g. graphql-engine is carefully isolated on CPUs:echo 0 | sudo tee /proc/sys/kernel/randomize_va_space+RTS -qa -qm and/or pinning the parent haskell process with taskset -cp $(ps -F <pid> | awk '{print $7}' | tail -n1) <pid>I'm still trying to understand what's happening here, but launching a new OS thread (at ~5 us latency) is unlikely to be a bottleneck, however maybe it's associated with some other type of blockingecho never | sudo tee /sys/kernel/mm/transparent_hugepage/enabled : this resulted in 15% regression for small return payloads, and 15% improvement for large ones. Worth revisiting.Things that are likely to be effective but not exploredcarefully isolating graphql-engine on particular CPUs. See also link dump below.Copied from LLVM benchmarking docs:Use https://github.com/lpechacek/cpuset to reserve cpus for just theprogram you are benchmarking. If using perf, leave at least 2 coresso that perf runs in one and your program in another::cset shield -c N1,N2 -k onThis will move all threads out of N1 and N2. The -k on meansthat even kernel threads are moved out.Disable the SMT pair of the cpus you will use for the benchmark. Thepair of cpu N can be found in/sys/devices/system/cpu/cpuN/topology/thread_siblings_list anddisabled with::cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | \\\nawk -F, '{print $2}' | \\\nsort -n | \\\nuniq | \\\n( while read X ; do echo $X ; echo 0 | sudo tee /sys/devices/system/cpu/cpu$X/online ; done )\nRun the program with::cset shield --exec -- perf stat -r 10 <cmd>\nThis will run the command after -- in the isolated cpus. Theparticular perf command runs the <cmd> 10 times and reportsstatistics....and not practical for actual deployment but potential for more stable (and slower) latenciesdisable turboboost: echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turboconsider using tmpfs for our benchmarking postgres instance so we never touch diskThings that seemed to be a waste of timefiddling with /sys/devices/system/cpu/cpu*/cpufreq/scaling_min_freq and .../intel_pstate/min_perf_pct. It's not clear if this did anything. It certainly didn't produce a fixed CPU frequency. \u00a0From https://www.kernel.org/doc/Documentation/cpu-freq/intel-pstate.txt :For contemporary Intel processors, the frequency is controlled by the processor itself and the P-State exposed to software is related to performance levels. \u00a0The idea that frequency can be set to a single frequency is fictional for Intel Core processors. Even if the scaling driver selects a single P-State, the actual frequency the processor will run at is selected by the processor itself.References dumpControlling placement of processes and isolating them: https://webcache.googleusercontent.com/search?q=cache:SmEke2ayDOAJ:https://access.redhat.com/solutions/2884991+&cd=1&hl=en&ct=clnk&gl=ussolid info on p/c-states https://metebalci.com/blog/a-minimum-complete-tutorial-of-cpu-power-management-c-states-and-p-states/Kernel pstate driver docs: https://www.kernel.org/doc/html/v4.12/admin-guide/pm/intel_pstate.htmland https://www.kernel.org/doc/Documentation/cpu-freq/intel-pstate.txtrestricting to c0 doesn't play nice with HT !:https://www.codeblueprint.co.uk/2017/03/06/intel_idle-max_cstate-considered-harmful.htmlhttps://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/535130controlling cstates : https://wiki.ntb.ch/infoportal/_media/embedded_systems/ethercat/controlling_processor_c-state_usage_in_linux_v1.1_nov2013.pdfconstellation of kernel cargo cult crankery:https://www.codethink.co.uk/articles/2018/configuring-linux-to-stabilise-latency/https://gitlab.com/CodethinkLabs/determinism/wikis/Kernel Tuningshttp://epickrram.blogspot.com/2015/09/reducing-system-jitter.htmlhttp://epickrram.blogspot.com/2015/11/reducing-system-jitter-part-2.htmlFurther reading:We have explored this topic further in this blogpost: \u00a0https://hasura.io/blog/effect-of-intels-power-management-on-webservers/"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL & Hasura events happening in April Description: A list of all the GraphQL & Hasura events happening in April.  Author: Hasura"}, {"role": "assistant", "content": "Over the next few weeks, we have over 15 online events where we will be talking about all things GraphQL & Hasura. Whether you\u2019re new to GraphQL, looking to add GraphQL to a legacy stack or just want to know all the new features coming soon to Hasura there\u2019s an event for you.Here\u2019s a short guide to help you choose an event that\u2019s right for you!Hasura usersThe Hasura Community call (29th April)Developers using GraphQL in large orgs/ EnterpriseHasura demos to help you migrate from legacy stacks to GraphQL using Hasura (14th & 16th April)GraphQL Fireside Chats: Platform and data centric use cases with GraphQL with special guests, Sasha Solomon, Eve Porcello, and Alex Banks (14th April)GraphQL Fireside Chats: \u00a0Performance and monitoring tips/tricks/caveats with GraphQL with special guests, Mark Stuart and Jon Wong (21st April)GraphQL Fireside Chats: Authorization and security patterns for GraphQL with special guests, German Frigerio and Sean Grove (28th April)GraphQL Fireside Chats: Event driven applications with GraphQL & serverless with special guests, Simona Cotin and Christian Nwamba (12th May)Developers who are new to GraphQLRealtime GraphQL and serverless made easy for React with Hasura \u00a0(BudapestJS on 16th April)Zero to Production with Fullstack GraphQL and React (React Summit Remote Edition Workshop on 17th April)GraphQL devs looking to up their gameHow I Went from Being Skeptical about Relay to Falling in Love with it (React Summit Remote Edition 17th April)Building an offline-first React app with RxDB & Hasura (ReactJS Bangalore 18th April)Integrate business logic into your GraphQL API with Hasura Actions (GraphQL Wroclaw 28th April)We are organizing more events as you read this. Keep an eye on Twitter for announcements.Looking forward to seeing you there!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Game on! Building a Unity Leaderboard with GraphQL & Hasura Description: In this three-part series, we build a Unity leaderboard with Hasura!  Follow along as Baris Tumerkan, a Unity dev who is learning GraphQL, and Sezgi Ulucam, a GraphQL dev with no Unity experience, team up to solve this challenge together. Author: Sezgi Ulucam"}, {"role": "assistant", "content": "\ud83d\udcf9 In this three-part video series, we build a Unity leaderboard with Hasura!Follow along as Baris Tumerkan, a Unity dev who is learning GraphQL, and Sezgi Ulucam, a GraphQL dev with no Unity experience, team up to solve this challenge together \ud83d\udcaaSneak preview of Unity leaderboard built with HasuraWe both learned a lot from this project (Sez about Unity, and Baris about GraphQL). The best parts were:Discovering how easy it is to set up queries, mutations, and subscriptions with this Unity GraphQL client by Tade Ajiboye. Import the package, plug in your Hasura endpoint, and you\u2019re good to go!Building and testing out our GraphQL operations in the Hasura console.Running into bugs in our code and crushing them along the way \u2013 so you don't have to \ud83d\ude0eSeeing our project start to come together \u2013 especially the live scores!Using Hasura Actions to customize business logic for high scores.Naming our imaginary players after pets... who were named after foods. Because why not? \ud83d\udc39\ud83c\udf69If you'd like to code along at home, check out the repo here.Part 1: Queries & MutationsIn this video, we:\ud83c\udfae Set up our Postgres database with Hasura\ud83c\udfae Hook up our GraphQL client\ud83c\udfae Set up GraphQL queries and mutations for our leaderboardPart 2: UI & SubscriptionsIn this video, we:\ud83c\udfae Display top scores on our leaderboard UI\ud83c\udfae Show live scores on our leaderboard with GraphQL subscriptionsPart 3: High Scores with Hasura ActionsIn this video, we implement business logic for high scores using Hasura Actions! With this feature, scores are only updated when they increase.\n\n\nNote that there's a simpler way to implement high scores, via Check Constraints under the \"Modify\" tab.\nBut when you need more complex business logic, Actions are the way to go!\n\nWe hope you had fun watching and coding along, and that you were inspired to build your own Unity projects using Hasura. And if you do, please share in the comments. We'd love to hear about them!Additional Reading: Creating a GraphQL API for Notion with Hasura Actions"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing GraphQL Fireside Chats: A Special Series of Online Events Description: A series of live streamed panel discussions between prominent members from the GraphQL community. Join them as they discuss key topics in the GraphQL ecosystem. Author: Hasura"}, {"role": "assistant", "content": "Photo by L\u0101sma Artmane on UnsplashWe are thrilled to be partnering with This Dot for a special series of GraphQL online events!Starting on April 14th, we will be co-hosting weekly \u201cGraphQL Fireside Chats\u201d, where each week we\u2019ll focus on a different GraphQL related topic. Join the panel of special guests for a roundtable discussion and lively Q&A!The purpose of these events is for the GraphQL community to come together and discuss some key topics in the GraphQL ecosystem and facilitate cross collaboration amongst the community. We are thrilled to be able to connect with fellow GraphQL enthusiasts!Check out the various topics we\u2019ll be discussing below:April 14, 2020: Platform and data centric use cases with GraphQL with special guests, Sasha Solomon, Eve Porcello, and Alex Banks.April 21, 2020: Performance and monitoring tips/tricks/caveats with GraphQL with special guests, Mark Stuart and Jon Wong.April 28, 2020: Authorization and security patterns for GraphQL with special guests, German Frigerio and Sean Grove.May 12, 2020: Event driven applications with GraphQL & serverless with special guests, Simona Cotin and Christian Nwamba.If you would like to attend the live stream and interact with the speakers during the Q&A, sign up for each event. You'll receive a live link the day of!Looking forward to seeing you there! "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a Realtime Geolocation App with Hasura GraphQL and PostGIS Description: In this post we will look at building a data-heavy realtime geo-location app with PostGIS and Hasura GraphQL to plot nearby restaurants in a map. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Location-based apps are used across industries and have changed the way users interact with the app. For developers though, building a realtime geolocation app is complex on multiple fronts. Some of the complexities involved are:Managing a large dataset in the databaseEase of API consumptionRealtime frontend renderingIn this post, we will look at building a data-heavy real-time geo-location app with PostGIS and Hasura GraphQL to plot nearby restaurants on a map. At the end, you will realize that Postgres can handle required datasets, GraphQL gives a neat API layer to consume on the client and makes it easy for frontend rendering.To set up the backend, we will be running a Postgres database, seeding some location data from OpenStreetMap and creating a Postgres function to query nearby data. On the frontend, we will be running a React app with Apollo Client integration to query for location data using Hasura GraphQL API.Set up Hasura & Postgres with PostGISAlright, let's start with the backend setup. Step 1: Clone the sample appgit clone https://github.com/karthikvt26/postgis-hasura-map\ncd postgis-hasura-mapStep 2: Run GraphQL Engine locallycd hasura\ndocker-compose up -dYou can verify if Hasura is running by heading to http://localhost:8080/console and you should be able to see the console UI.Step 3: Install Hasura CLIHead to Hasura CLI docs to setup the CLI on your machine which we will use in the next step for applying migrations.Step 4: Configure Hasura Endpoint in config.yamlendpoint: http://localhost:8080Now we are done setting up Hasura, Postgres with PostGIS extension and configured Hasura CLI as well.Seeding Location DataWe already have location data exported from OpenStreetMap for Bangalore and nearby localities for the same. We can quickly get started by applying the migrations for the same.hasura migrate applyThis will create the necessary tables, seed data and metadata for Hasura. Additionally this also creates a Postgres function which we will talk about a little later.In case you are interested to export a data set for a different city, then follow the steps below. Else skip to Fetching Nearby Data: Postgres Function section below :)Head to overpass turbo and update the query to the following:node\n  [amenity=restaurant]\n  ({{bbox}});\nout;Now select the region of choice to plot the restaurant data and click on Run at the top. Once you have the locations plotted on the map, click on Export and choose raw OSM data.osm2pgsqlWe need a way to import this raw data into Postgres. We will make use of a module called osm2pgsql which reads this raw data and imports into Postgres. Head over to its github repo for installation instructions.Once you are done setting this up, execute the following command to do the migration:osm2pgsql --create -d postgres -H <postgres_host> -P 5432 -U postgres restaurants_data.osmReplace <postgres_host> with the right IP (Note that you are running Postgres in docker and hence the host value is different for different platforms).Once the import is done, head over to the Hasura Console at http://localhost:8080/console -> Data tab and Track the table.Fetching Nearby Data: Postgres FunctionNow that we have a dataset ready in Postgres, we need a way to fetch the list of restaurants nearby. The frontend that renders the location map gives the lat/long co-ordinates as input. We will make use of PostGIS, a spatial database extender for Postgres to query nearby data directly in Postgres.Our dataset is available on planet_osm_point table. We will make use of the following PostGIS's functions:ST_TRANSFORM - changes the coordinates of a geometry from one spatial reference system to anotherST_DWITHIN - Returns true if the geometries are within the specified distance of one another.ST_X and ST_Y - Returns the x and y co-ordinates respectively.We will create a Postgres function which makes use of these PostGIS functions:CREATE OR REPLACE FUNCTION public.get_nearby_restaurants(lat double precision, long double precision, bound integer)\n RETURNS SETOF nearby_restaurant\n LANGUAGE sql\n STABLE\nAS $function$\n    SELECT name, ST_X(ST_Transform(way, 4326)) as long, ST_Y(ST_Transform(way, 4326)) as lat \n        FROM planet_osm_point\n        WHERE ST_DWITHIN(way, ST_TRANSFORM(ST_SetSRID(ST_Point(long, lat),4326), 3857), bound);\n$function$\nThis function accepts lat, long and a bound range arguments to calculate the locations nearby that bound. The input for these comes from the GraphQL query made from the React app. We will look at this in the next section.React App IntegrationThe react app can be started with the Hasura endpoint env.REACT_APP_HASURA_ENDPOINT=localhost:8080 npm startIf you had successfully applied the data migration, your app should look something similar to this:Alright, now let's see how this is done.We need to setup a GraphQL client for React. We chose Apollo Client for this example.import ApolloClient from 'apollo-boost';\n\nconst httpUrl = `http://${ process.env.REACT_APP_HASURA_ENDPOINT }`;\n\nconst client = new ApolloClient({\n  uri: `${httpUrl}/v1/graphql`,\n});\n\nexport {\n  client\n};\nThe GraphQL query to fetch the restaurants is pretty straightforward.query fetchNearbyRestaurants (\n  $bound: Int!,\n  $lat: float8!,\n  $long: float8!\n) {\n  get_nearby_restaurants (args: {\n    bound: $bound,\n    lat: $lat,\n    long: $long,\n  }) {\n    name\n    lat\n    long\n  }\n}You can notice that, get_nearby_restaurants is the Postgres function and name, lat and long are the fields returned by the function. We are passing in the necessary variables for the function arguments.To render the Map on React, we are using React Leaflet - https://react-leaflet.js.org.SummaryWe looked at the capabilities of Postgres via functions and it's extensibility via PostGIS. Hasura being a layer over Postgres, leverages all the power of the database and gives a neat GraphQL API for the client to consume.This app uses OpenStreetMap, although you can make use of Google Places API too. You can read more about integrating with Google Places API with Hasura GraphQL."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Fetch your GraphQL data automagically: Building a movie app with Hasura & gqless Description: gqless, a next-generation GraphQL client inspired by Relay & Apollo Client, uses ES6 Proxies to know when properties are accessed, and convert the paths into GraphQL queries. In this video, Sam and Sez build a movie app together, using gqless and Hasura. Author: Sezgi Ulucam"}, {"role": "assistant", "content": "If you\u2019re into laziness and magic (who isn\u2019t?), here\u2019s a frontend tidbit that will blow your mind:ES6 Proxies let you assign a custom handler for fundamental operations like property lookup. gqless (by Sam Denty), a next-generation GraphQL client inspired by Relay & Apollo Client, uses this to know when properties are accessed, and convert the paths into GraphQL queries.If you feel, as I do, that this might change your world forever, here\u2019s a demo video of Sam and me building a movie app. Follow along with the tutorial below to join in the fun! You can find the demo app here on GitHub.\n\n\n\n\nStep 1: Hasura setupCreate a project on Hasura Cloud. Click on the \"Deploy to Hasura\" button to signup for a free account and create a new project.\nFrom the Data tab on the Hasura console, create two tables: movies and characters.\n    \nColumns for movies: id (uuid), name (text), image_url (text)    \nColumns for characters: id (uuid), name (text), image_url (text), movie_id (uuid)\nFor the two id columns, enter the function gen_random_uuid() in the default value field.\n\n\nCreate a one-to-many relationship between movies and characters by:\n    \nAdding a foreign key constraint from the movie_id field of the characters table to the id field of the movies table\nClicking \u201cTrack All\u201d in the Data tab\n\n\nFrom the \u201cInsert Row\u201d tab on each table, add your favorite movies and characters as rows! Hint: you can also do this with mutations in the GraphiQL tab.That\u2019s all you need to get going with Hasura!Step 2: App setupAssuming you have npm installed, from your Terminal, run:npx create-react-app moviesThen cd movies and install dependencies:npm install gqless @gqless/react typescript\nnpm install @gqless/cli --save-devOpen your app in your favorite text editor, preferably VS Code so you can autocomplete all the things\u2014even GraphQL fields \ud83d\ude2e, as we\u2019ll see in a second.Next, follow the instructions here to set up codegen and generate all the types from your schema. For the url, use your Hasura GraphQL endpoint, found in the GraphiQL tab of the Hasura console.After you run the generate command, you should see a new folder in your app where you specified the outputDir.We\u2019re all set up for gqless magic!Step 3: Basic appCreate two new files in your src folder.index.tsx:import React from \"react\";\nimport reactDOM from \"react-dom\";\nimport { App } from \"./App\";\n\nreactDOM.render(<App />, document.getElementById(\"root\"));App.tsx:import React from \"react\";\n\nexport const App = () => {\n return <div>movies!</div>;\n}From your Terminal, run npm start. Your app should load at http://localhost:3000.Step 4: Fun with gqlessLet's make a list of movies, using gqless to run the query.In App.tsx, update imports:import React, { Suspense } from \"react\";\nimport { query } from \"./movies_api\";\nimport { graphql } from \"@gqless/react\";We'll use React's Suspense to specify a loading state. query will return an ES6 Proxy, which will be automatically typed from our schema. And we'll wrap our component in gqless's graphql so it automatically updates when the data is fetched.Now add a new Movies component:const Movies = graphql(() => {\n  return (\n    <div>\n      {query.movies.map(movie => {\n        return <div key={movie.id}>{movie.name}</div>;\n      })}\n    </div>\n  );\n});\n\nexport const App = () => {\n  return (\n    <Suspense fallback=\"loading\">\n      <Movies />\n    </Suspense>\n  );\n};See what happened there? You didn't have to write out the GraphQL query separately. All you need is query.movies, and gqless takes care of the query for you behind the scenes.And here's the kicker: in VS Code, when you type movie., you can autocomplete the GraphQL fields available on the object! \ud83d\udc83Step 5: Listing charactersLet's list the characters for each movie. Update the types import:import { query, movies } from \"./movies_api\";We'll use the movies type in our new Movie component:const Movies = graphql(() => {\n  return (\n    <div>\n      {query.movies.map(movie => {\n        return <Movie key={movie.id} movie={movie}></Movie>;\n      })}\n    </div>\n  );\n});\n\nconst Movie = graphql(({ movie }: { movie: movies }) => {\n  return (\n    <div>\n      <h3>{movie.name}</h3>\n      {movie.characters.map(character => {\n        return <div key={character.id}>{character.name}</div>;\n      })}\n    </div>\n  );\n});You should now see the characters listed under each movie title.Step 6: Movie linksWe want each movie to link to its own page. We'll use React Router for navigation, and emotion for styling. In your Terminal, run:npm install react-router react-router-dom @types/react-router @types/react-router-dom @emotion/styled @emotion/coreAnd add imports to App.tsx:import { BrowserRouter, Link, Switch, Route } from \"react-router-dom\";\nimport styled from '@emotion/styled';Update the App component:export const App = () => {\n  return (\n    <BrowserRouter>\n      <Suspense fallback=\"loading\">\n        <Switch>\n          <Route path=\"/movie/:id\">\n            {({ match }) => {\n              const movie = query.movies_by_pk({ id: match?.params.id! });\n              return movie && <Movie movie={movie}></Movie>;\n            }}\n          </Route>\n          <Movies />\n        </Switch>\n      </Suspense>\n    </BrowserRouter>\n  );\n};We add a route for viewing a movie by id, in which we query for the movie, passing the id from the route as a variable. We then pass the returned movie to the Movie component.Let's also update our Movie component, so that each movie links to its respective route:const Movie = graphql(({ movie }: { movie: movies }) => {\n  return (\n    <div>\n      <h3>\n        <Link to={`/movie/${movie.id}`}>\n          {movie.name}\n        </Link>\n      </h3>\n      {movie.characters.map(character => {\n        return <div key={character.id}>{character.name}</div>;\n      })}\n    </div>\n  );\n});You should now be able to click on each movie to navigate to its route.Step 7: Extensions!You may have noticed that when you click on a movie, it says \"Loading\" even though we've already fetched the same data for the Movies component. To fix this, we can use gqless's extensions to customize the cache. First, we'll set keyed values to make sure movies are keyed by their id in the cache. Then, we'll set up cached redirects, which let you first search for an existing value in the cache before fetching from the network.In /movies_api/extensions, open index.ts, and add the following:import { GET_KEY, REDIRECT, RedirectHelpers } from \"gqless\";\n\n// Keyed values\n// object name `movies` must match type name from our schema\nexport const movies = {\n  [GET_KEY]: (movie: any) => movie.id\n  // now the movie should be keyed by its `id` in the cache\n};\n\n// Cached redirects\n// the names `query_root` and `movies_by_pk` are based on our schema\nexport const query_root = {\n  movies_by_pk: {\n    // we check the cache before fetching from the network\n    [REDIRECT](args: any, { getByKey }: RedirectHelpers) {\n      return getByKey(args.id);\n    }\n  }\n};Now if you reload the page and click on each movie, you should no longer see the \"Loading\" text.Step 8: List charactersFirst, quick clean up: We don't actually need to list the characters on the index page. So let's remove those and just show a link to each movie:const StyledMovies = styled.div`\n  display: flex;\n  flex-direction: column;\n`;\n\nconst Movies = graphql(() => {\n  return (\n    <StyledMovies>\n      {query.movies.map(movie => {\n        return (\n          <Link key={movie.id} to={`/movie/${movie.id}`}>\n            {movie.name}\n          </Link>\n        );\n      })}\n    </StyledMovies>\n  );\n});Now, update type imports:import { query, movies, characters } from \"./movies_api\";And add new components for the character list:const StyledCharacter = styled.div`\n  display: flex;\n  align-items: center;\n`;\n\nconst CharacterImg = styled.img`\n  width: 100px;\n  height: 100px;\n  object-fit: cover;\n  border-radius: 50%;\n  margin: 10px;\n`;\n\nconst Characters = graphql(({ movie }: { movie: movies }) => {\n  return (\n    <div>\n      {movie.characters.map(character => {\n        return <Character key={character.id} character={character} />;\n      })}\n    </div>\n  );\n});\n\nconst Character = graphql(({ character }: { character: characters }) => {\n  return (\n    <StyledCharacter>\n      <CharacterImg className=\"charImg\" src={character.image_url} />\n      {character.name}\n    </StyledCharacter>\n  );\n});Finally, we render Characters inside the Movie component, wrapped in a Suspense fallback so that the movie name loads first, and we see \"Loading\" text while the characters load:const Movie = graphql(({ movie }: { movie: movies }) => {\n  return (\n    <div>\n      <h3>\n        <Link to={`/movie/${movie.id}`}>{movie.name}</Link>\n      </h3>\n      <Suspense fallback=\"loading\">\n        <Characters movie={movie} />\n      </Suspense>\n    </div>\n  );\n});ConclusionThat wraps it up for our demo app! Here's a summary of what we learned:How to set up array relationships with Hasura,How to set up gqless using Hasura's GraphQL endpoint,\nMany of gqless's awesome features, including:\n    \nAutomagic data fetching,\nType generation,\nExtensions for caching behavior.\n\n\nNow that you have these newfound superpowers, what will you build next? Please share in the comments! And if you'd like to venture further into the world of fluent GraphQL clients, check out our recent blog post: Fluent GraphQL clients: how to write queries like a boss."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Why we chose TypeScript for the Hasura Console Description: At Hasura, we wanted to introduce a statically typed frontend language for quite some time now. We discussed which one we should choose and evaluated options such as PureScript, TypeScript, ReasonML, and Elm. Author: Aleksandra"}, {"role": "assistant", "content": "At Hasura, we wanted to introduce a statically typed frontend language for quite some time now. We discussed which one we should choose and evaluated options such as PureScript, TypeScript, ReasonML, and Elm. Following aspects were most crucial to us:Hasura Console is a pretty large codebase for a small team, so we can't rewrite it all to the new language. We need something that works well with the existing JavaScript code. A language that we can inject into our codebase and gradually migrate.We use React extensively. Thus we need something that goes with React well and improves React components development.We want to enhance developer experience for the external contributors and us. By adopting a statically typed programming language, we aim to make things easier, and the development process way faster. We don't want to introduce high adoption cost nor force developers to learn a new and completely different language.After all of the discussions, we decided to choose TypeScript. In this article, I'm going to tell you how we made the decision and why we wanted a statically typed language in the first place. I will also walk you through all of the four languages by a brief overview of each of them.Why do we want static typing?Introducing a new language always comes with a cost. Developers need to invest time into setup, integration and then adoption. The team may also be less productive for a while until everyone becomes fluent in a new language. All of this will take a more or less significant amount of time depending on the chosen language, but it always does take time. That's why every team needs to consider all the benefits of the statically typed language adoption, decide whether they outgrow the cost and think through why they need a new language. Answers to this question may be different among the teams and among the particular developers as well. For us, the following reasons were most important:A lot of errors may be caught in an early phase of development and be fixed immediately, rather than lurking in the code to be discovered much later.Refactoring becomes more straightforward with the statically typed language. Developers gain more confidence thanks to the compiler or type-checker, notifying them about the type errors.Types serve as excellent documentation. It's easier for new developers to dive into some fragments of the codebase and start working with them without any broader knowledge about the codebase.Safety that comes with static typing can be a huge productivity boost.PureScriptPureScript has been around since 2013 and is maintained by the community. Inspired by Haskell and ML, it is a purely functional programming language that provides many features to help with code correctness. Some of these features are immutability, pattern matching, strong type inference, and a powerful type system. It also has a great FFI (Foreign-Function Interface) that allows us to call JavaScript functions from within PureScript.Why PureScript?PureScript has a decent ecosystem. There are bindings for many JavaScript libraries, and even when there's no support for some library, PureScript FFI makes it easy to include them yourself.PureScript has a great type system and provides features such as typeclasses, higher kinded types, row polymorphism, higher-rank types, and many more.It's a purely functional language, so if you're a fan of functional programming, PureScript is a great choice. The style of programming in PureScript empowers you to maximize functional purity, strictly limiting state and side effects.Main PureScript target is JavaScript but it can compile to other languages as well. You can write full-stack applications in PureScript.How to setup PureScript in the Console codebase?1. Install all the required dependencies and initialise new PureScript project with spago:yarn global add purescript spago   \nyarn add -D purs-loader     \n\nspago init   \nspago install purescript-react-basic\n\nspago init command will create a new files:packages.dhall: this file is meant to contain the totality of the packages available to your project.spago.dhall: project configuration \u2014 among others the list of your dependencies, the source paths that will be used to build.2. Update webpack configuration by adding loader for the PureScript files and handling .purs extension.Adding purs-loader to the webpack configHandling .purs extension3. Now, we're ready to start writing code in PureScript! Below is the example of a simple button component written in PureScript:Button component in PureScriptElmElm is a purely functional programming language designed in 2012. Elm uses abstractions called flags, ports, and custom elements to communicate with JavaScript. The Elm Architecture pattern makes it easy to develop frontend applications. The three concepts that are the core of The Elm Architecture:Model \u2014 the state of your app,View \u2014 a function to turn your state into HTML,Update \u2014 a way to update your state based on messages.The current implementation of the Elm compiler targets HTML, CSS, and JavaScript.Why Elm?Elm has a strong type system and great type inference.It promises no runtime exceptions. It uses type inference to detect corner cases and world-class compiler messages help a user with debugging.Elm has great performance. Comparing it to React and Vue it seems to produce slightly-smaller bundle sizes and faster render times.Beginner-friendly syntax makes it easy and fun to use. At the same time, it's a very powerful language that embraces all the good parts of functional programming.How to setup Elm in the Console codebase?1. Install dependencies. react-elm-components allows to use Elm components inside React.yarn add -D react-elm-components elm-webpack-loader\n\n2. We also need to add elm.json file with Elm project configuration.3. Update webpack configuration.4. Example component in Elm:TypeScriptTypescript is a typed superset of JavaScript developed and maintained by Microsoft. It adds optional static typing to the JavaScript world and its adoption can bring you more robust software at super low cost. Since it's a superset then any valid JavaScript is a valid TypeScript, so basically you can just change the extension from .js to .ts and, et voila, you have a valid TypeScipt file. From there, you can incrementally add type checking where you think it\u2019s necessary. It's important to notice that it's not a completely new langauge \u2014 it's just JavaScript with additional features, and most JavaScript pitfalls.TypeScript transpiles to JavaScript with help of TypeScript Compiler (tsc) written in TypeScript.Why TypeScript?TypeScript brings optional static typing, meaning that you can write type annotations, but you don't have to. Whatever you feel like. It also makes it easier for JavaScript developers to dive into TS.TypeScript has high compatibility with JavaScript, meaning that every JS library is going to work in TypeScript code and vice versa.There are many ways to adopt TypeScript \u2014 you can add type checking with // @ts-check or write declaration files (d.ts) to have TypeScript benefits without writing TypeScript.TypeScript is designed with gradual adoption in mind.Zero configuration support in many modern IDEs. For example VS Code or WebStorm have TypeScript support working out of the box.How?1. Install dependencies:yarn add -D typescript @babel/preset-typescript fork-ts-checker-webpack-plugin\n\n2. Update .babelrc file.3. Update webpack config.4. Example component in TypeScript:ReasonMLReasonML is a syntax extension for OCaml \u2014 the statically typed functional language with object-oriented features developed in the late 1990s. Reason was created by Facebook and provides the same features as OCaml does, but its syntax is more similar to JavaScript. The intention behind this is to make adoption by JavaScript programmers easier.Reason doesn't directly compile to JavaScript. .re files are transformed into the OCaml AST by OCaml preprocessor for Reason (refmt). It is then processed by the BuckleScript compiler called bsc, which produces JavaScript files.Why ReasonML?Reason has a rock solid type system and strong type inference.Reason is immutable and functional by default, but it supports mutations and side-effects.The syntax is similar to JavaScript.Reason supports React with ReasonReact and JSX syntax. In fact, first prototypes of React were done in SML \u2014 another dialect of ML. Also, React and Reason share the same creator.JavaScript package managers work with Reason out of the box. You still can use npm and \u00a0yarn.How?1. Install dependencies:npm install --save-dev bs-platform reason-react\n\n2. Add bsconfig.json:3. Update scripts:4. Example component:ComparisonDynamically typed languages are great for prototyping; they can give us a lot of flexibility, which results in significant development speed. Statically typed languages, on the other hand, provide more control, increase program correctness, but they also may decrease the speed of adding new code.However, they make working with the existing code easier. That's why we need to decide what can make us most productive.We need to determine where we want to be in the diagram below. What is more important for us? Development speed or correctness and control?Development speed vs ControlThe languages we were discussing are different from each other in many regards. We can't say that one is superior and the other is significantly worse. What we can do is compare them with the aspects essential for us and our project. JavaScript interoperabilityTypeScript is a superset of JavaScript, so it works almost out of the box with JavaScript. You can call the JS code from the TS file and vice versa. The only thing that you need to do is to find or provide type definitions for the JavaScript modules.ReasonML and PureScript have pretty similar JavaScript interop. Both BuckelScript and PureScript compilers produce a readable and performant JavaScript code that you can use in any JavaScript file. If you want to call external JavaScript functions in ReasonML or PureScript code, you need to provide type annotations for them. They also both require that the boundary between them and the existing JavaScript code is explicitly defined.Elm provides the ability to interoperate with JavaScript through ports and web components, which are deliberately quite limited, which leaves Elm behind its competitors when it comes to JavaScript interop experience.IDE supportFrom my point of view as a VSCode user and my experience with those languages, TypeScript is a winner here. VSCode is written in TypeScript, and it has first-class support for this language. There are plugins for various editors available for all of these languages, but only one of those languages has builtin support in popular opensource IDE. In my opinion, an IDE purpose-built for a language will always provide a better developer experience (see RubyMine, PyCharm, etc.).Type safetyLanguages that we were discussing fall into two groups. TypeScript is a gradually typed programming language, which means its type system allows both statically typed and dynamically typed expressions. As the name suggests, it will enable us to introduce static typing to the existing dynamically typed codebase gradually. TypeScript's type system is unsound, which means that there's no guarantee that static type predictions are accurate at runtime. Here are some examples of type unsoundness in TypeScript:const push3 = (arr: Array<string | number>): void => { \n  arr.push(3); \n}\n\nconst strings: Array<string> = ['foo', 'bar'];\npush3(strings);\n\nconst s = strings[2];\nconsole.log(s.toLowerCase()) // runtime error\n\nconst cat = dog as Cat; // runtime error\n\ntype Foo = {  \n  bar?: {    \n    x: number;  \n  }\n}\n\nconst foo: Foo = {};\nconst x = foo.bar!.x; // runtime error\n\nTypeScript gives you tools to work around the type system, so unless you're careful, you can't always trust it to have your back. Type soundness is one of the most significant advantages of having a proper static type system as we have in TypeScript alternatives.PureScript, Elm, and ReasonML are in the ML family, so they come with a sound and robust type system. If the program is well-typed, then the type system ensures that it's free from certain misbehaviours. They are entirely different programming languages that support JavaScript as a compile target, and as a consequence migration from JavaScript code requires more effort as in case of TypeScript. SummaryAfter evaluating all the possibilities, we decided to go with TypeScript. We think that this choice would improve our productivity with minimal adoption cost. Yes, this language has fewer features than its alternatives, and notably, it brings less type safety. Type inference is much weaker, and it's still JavaScript, which means it brings most of the JavaScripts snares with it. Yet, despite its drawbacks, TypeScript satisfies our needs \u2014 with minimal effort, we can improve our codebase. Both setup cost and time for onboarding developers are small, and it doesn't introduce a lot of new things to the codebase. It's just our old JavaScript with types.We're going to adopt TypeScript slowly, the new things we will write in TypeScript and the existing JavaScript code will be gradually migrated. Do you want to know how it went? Stay tuned! We hope to publish How did we adopt TypeScript in the Console piece soon!Here Aleksandra explains how we used to Typescript to build multiple database support for Hasura 2.0."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Authorization System through Examples Description: Authorization systems are about rules like, \u201cCan user U perform action A [on resource R]?\u201d. In this post, we implement real-world authorization rules using Hasura's JSON-based DSL. Author: Gautam BT"}, {"role": "assistant", "content": "Authorization systems are about being able to specify rules that will allow the system to answer, \u201cCan user U perform action A [on resource R]?\u201d. In this post, we look at implementing some real-world authorization rules using Hasura's JSON-based DSL. But first, a quick recap of Hasura's authorization system.Authorization with Hasura: RecapEvery request to Hasura executes against a set of session variables. These variables are expected to be set by the authentication system. While arbitrary variables can be set and used in defining authorization rules, two variables are of particular interest in the context of authorization:X-Hasura-User-Id: This variable usually denotes the user executing the request.X-Hasura-Role: This variable denotes the role with which the user is executing the current request. Hasura has a built-in notion of a role and will explicitly look for this variable to infer the role.We can create new roles\u2014which are just arbitrary names\u2014from the Hasura console. Once a role is created, we can set permissions for select, insert, update, and delete for each table in our schema. A permission rule is a JSON object that looks something like this:{\"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}The above rule is equivalent to saying: allow this request to execute on the current row if the value of the user_id column equals the value of the session variable X-Hasura-User-Id.The JSON here is essentially a domain language for expressing authorization rules. It is possible to express pretty complex rules: Any columns in the table can be used in place of user_idSeveral operators ( _gt, _lt, _in, etc) can be used in place of _eq operatorIt is possible to traverse relationships (we will see how to do this shortly)Operators _and and _or can be used to chain rulesIt is even possible to query tables not related to the current object as part of the rule execution (again, more on this shortly).All of this is documented in the official documentation. Let us now look at how to apply these in the real world.Example 1: Access control for a payroll management system\nSimple role-based access controlConsider a simple HRMS system with the roles HR, Employee, Manager, and Director. We want to implement the following authorization rules:Only HR should be able to edit payrollsEmployees should be able to see their own payrollsManagers should be able to view all payrolls, but not edit themThe database schema and relationships for this use case would look like:Once the above schema and relationships have been created in Hasura, we can create the custom roles HR, Employee, and Manager. We then need to create the following permission rules on the payroll table for each of these roles:\nHR:\n\nSelect: Without any checks\nInsert: Without any checks\nUpdate: Without any checks\nDelete: Without any checks\n\n\nEmployee:\n\nSelect: { \"employee_id\": {\"_eq\": \"X-Hasura-User-Id\"} }\nInsert: Denied\nUpdate: Denied\nDelete: Denied\n\n\nManager:\n\nSelect: Without any checks\nInsert: Denied\nUpdate: Denied\nDelete: Denied\n\n\n\nNow suppose we want a manager to only be able to read or update their reportees payrolls (but not payrolls of other employees). Assuming relationships \"employee\" from payroll to employee (using employee_id), and \"manager\" from employee to employee (using manager_id), we need to change the select and update rules for the role manager on the payrolls table to be:{ \n    \"employee\": {\n    \t\"manager_id\": {\"_eq\": \"X-Hasura-User-Id\"}\n    }\n}The above rule tells Hasura to fetch the employee associated with the payroll record and match their manager_id with the current user id. Hasura can traverse arbitrarily nested relationships, making this a powerful construct. In addition, we need to set the column update permissions so that only the salary field can be updated:Example 2: Per-document roles in Google Docs\nPer-resource rolesLets us a look at a more complicated example: a Google Docs clone where we have documents, users, and roles per document. Each document can have owners, editors, and viewers with these rules:Owners can read, update and delete a documentEditors can read and update a documentViewers can only read a documentOther users can't read, update or delete a documentIn this case, we can use a single Hasura role user and directly model the per document role in the schema. The database schema and relationships would look something like this:After creating the above schema and relationships in Hasura, we can configure the following permission rules on the documents table for the Hasura role user:Select permissions:{\n    \"_or\": [\n        {\"owners\": {\"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}},\n        {\"editors\": {\"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}},\n        {\"viewers\": {\"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}}\n    ]\n}\n This rule illustrates the use of the _or operator. The rule will apply if any of the three conditions in the array are matched.Update permissions:{\n    \"_or\": [\n        {\"owners\": {\"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}},\n        {\"editors\": {\"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}}\n    ]\n}\n Delete permissions:{\"owners\": {\"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}}}Insert permissions:Any user can insert a document. However, whenever a document is created, we need to make sure that the owners table is also populated along with the (user_id, document_id) tuple so that selects, updates and deletes don't fail. This can be done by setting up a Postgres event trigger:Note: In this example, there are two role-based systems in action: Hasura's built-in role-based access control system, and the per-document role-based access control system implemented using the above database schema and permission rules. As far as Hasura's system is concerned, all requests are being executed against the user role. Example 3: Hierarchical roles in an organization\nHierarchical rolesIn a hierarchical role-based access control system, roles are arranged into a hierarchy and a user with a role will have access to anything that the role or a child role has access to. Example: Consider an organization with the following role hierarchy: employee > lead > manager > director > department_head. A user who is higher in the hierarchy should be able to do any action that someone lower in the hierarchy can do. For example, a manager should be able to do anything a lead or an employee can do. Another way of looking at this is that the manager is assigned multiple roles [manager, lead, engineer], and they have access to a resource if at least one of these roles has access to it.For this example, we will again use a single Hasura role user and directly model the roles in the schema:We will also assume there is a documents table that only managers should have access to.For matching a user against all applicable roles in the hierarchy, we need to recursively traverse the role table to fetch all the roles. While Hasura's DSL allows us to traverse relationships, we cannot do so recursively. So we have to figure out a way to flatten the role hierarchy. Assume we have magically created the following view:flattened_user_roles (user_id, role_id): This will contain the result of traversing the role hierarchy and creating all possible (user_id, role_id) combinations. We can now write the permission rule for selects on the documents table:{\n    \"_exists\": {\n        \"_table\": {\n           \"table\": \"flattened_user_roles\",\n           \"schema\": \"public\"\n        },\n        \"_where\": {\n            \"_and\": [\n                { \"user_id\": {\"_eq\": \"X-Hasura-User-Id\"} },\n                { \"role_id\": {\"_eq\": \"manager\"} }\n            ]\n        }\n    }\n}The above rule illustrates the use of the _exists operator. _exists allows us to write rules that query tables unrelated to the row being accessed. The rule succeeds if the conditions in the _where clause yield at least one row.How do we generate flattened_user_roles though? Postgres' WITH RECURSIVE to the rescue! In the above query, we create a view that generates a flattened version of the hierarchical user_roles table. Exercise: How would you implement a scenario with per-resource role-based access control that is also hierarchical? Hint: Instead of using _exists above, traverse the relationship from the object.Note: This example also shows how to model scenarios where one user can have multiple roles. Hasura's built-in role-based access control system currently allows a request to only be executed against a single role. However, by modeling the roles directly in the database, we can implement scenarios where one user has multiple roles. We are also working on adding multiple role support  directly to Hasura.Example 4: Attribute-based access control for a CMS\nAttribute-based access controlAttribute-based access control is about being able to write authorization rules based on the attributes of a resource.Example: Consider a university with multiple departments, and each department has documents and users. We want users to be able to read documents that belong to their department. The schema in this case would look something like:Assuming there is a department relationship defined from document to department, and a users relationship defined from department to users, the select permissions on the documents table would be:{\n    \"department\": {\n    \t\"users\" : {\"id\": {\"_eq\": \"X-Hasura-User-Id\"}}\n    }\n}ConclusionIn this post, we looked at implementing authorization rules for several real-world use cases. Along the way, we learned the syntax for writing some pretty complex rules. The example under \"Hierarchical roles in an organization\" illustrated using a view to handle more complex scenarios. If you would like to learn more:The learn course has a tutorial on building permission rules for a Slack clone. The documentation explains everything in great detail.This blog post explains how to build authorization rules for a Hacker News clone.If you are using Hasura and need help with authorization, or want to share some interesting use cases you have implemented, ping us on Discord or tweet to us at @HasuraHQ!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Five years of GraphQL \ud83c\udf89 Description: 2020 will mark five years since GraphQL was released\n[https://engineering.fb.com/core-data/graphql-a-data-query-language/]!\n\nTo celebrate, we asked some of the usual suspects \ud83d\ude0e in the community, including\nLee Byron, co-creator of GraphQL [https://hasura.io/graphql/], to reflect on\ntheir journey, and where GraphQL is headed next.\n\n\n\nTakeaways\n * GraphQL is more than just a tool: it changes how teams communicate. It\n   empowers developers to request for the data they want, reducing their\n   depen Author: Shraddha"}, {"role": "assistant", "content": "2020 will mark five years since GraphQL was released!To celebrate, we asked some of the usual suspects \ud83d\ude0e in the community, including Lee Byron, co-creator of GraphQL, to reflect on their journey, and where GraphQL is headed next.TakeawaysGraphQL is more than just a tool: it changes how teams communicate. It empowers developers to request for the data they want, reducing their dependency and communication overhead with API teams. It thrives on community and collaboration.GraphiQL rocks: it\u2019s the feature people would miss the most if it went away.\ud83d\udd77\ud83d\udd78The most popular mascot choice for GraphQL is spiders. Honorable mention: \u201cHumans... Humans are also nice animals.\u201d - Uri Goldshtein \nWe got a variety of answers for what to change about GraphQL:\n        \nan approach to GraphQL that works well with microservices; less coupled to a monolithic design,\na more opinionated stance on authorization, especially with the introspection schema,\nmake it easy for beginners to get started,\nthe ability to handle errors in the query.\nSee video for more!\n\n\nAmong the hardest parts of using GraphQL are the need for a heavy client-side library, initial adoption, and building a good GraphQL server that leverages modern serverless and cloud native trends.The most popular way to convince someone to migrate to GraphQL is to show them GraphiQL and get them started on the frontend. Don\u2019t forget to tell them it\u2019s sunny all the time, they can wear flip-flops, and the food is great!What about you? How would you convince someone today to migrate to GraphQL? And how would you answer the other questions? Please share in the comments! And if you\u2019d like to tweet us a video of your answers, we won\u2019t stop you \ud83d\ude08The QuestionsWhat have you learned in the last five years of GraphQL that you\u2019d use in the next five years?2. Describe GraphQL in five words.3. What feature of GraphQL would you really miss if it were to go away?4. Which animal or plant would be your GraphQL mascot?5. What would you want to change about GraphQL?6. How did you and GraphQL meet?7. What stack did you migrate to GraphQL from?8. What do you think is the hardest part of using GraphQL?9. What is the biggest challenge you faced when adopting GraphQL?10. How would you convince someone today to migrate to GraphQL?\ud83c\udfa5 Starring (in order of appearance)Alex Banks & Eve Porcello, Moon HighwayJon Wong, CourseraUri Goldshtein, The GuildIvan Goncharov, APIs.guruSasha Solomon, TwitterLee Byron, Co-creator of GraphQLShruti Kapoor, PaypalTanmai Gopal, HasuraManish Jain, Dgraph Labs"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura FAQ: What are the best practices? (Part 1) Description: Have you been wondering about best practices with Hasura? Here\u2019s a quick rundown of some of our recent FAQ\u2019s. Author: Sezgi Ulucam"}, {"role": "assistant", "content": "Have you been wondering about best practices with Hasura? Here\u2019s a quick rundown of some of our recent FAQ\u2019s.What's the best practice for managing business logic?Actions are the new hotness for business logic. Check out:DocsDemoOther approaches for custom business logic:Remote schemas & remote joins: When you have an existing or external GraphQL server.Event triggers: When your logic only needs to run after a data change event in the database.Scheduled triggers: When your logic runs on an interval (cron) or a scheduled time in the future.Postgres functions: When your logic is data-intensive and fastest/easiest when implemented as a Postgres function.Our blog post on all of the aboveIs there a way to prevent GraphQL queries from becoming a pile of strings?Sounds like you\u2019re looking for a fluent GraphQL client. We wrote about this recently: Fluent GraphQL clients: how to write queries like a bossThese clients give you the ability to write queries as objects instead of strings. Some of them also integrate with TypeScript and eliminate duplicate data definitions.What's the recommended practice for dev/staging environments and persisted data?Hasura migrations make managing environments easy. Check out:DocsDemo: Migrating a GraphQL + Postgres app from Staging to ProductionI want to have my own custom signup flow. What\u2019s the best way to achieve this?Actions allow you to run business logic before executing a mutation. With an action, you can call an HTTP webhook (a REST endpoint or a serverless function) that talks to your auth provider, sign up the user with the provider, and save the user to your database afterwards. And here\u2019s the kicker: Hasura provides a codegen that generates the handler code for you.Also, check out this guide on creating a GraphQL API for Notion with Hasura Actions.I don't see Actions on my console. How do I go about enabling it?Update your Hasura GraphQL engine to the latest stable version.That\u2019s it for now. Do you have any burning questions regarding best practices with Hasura? Ask us in the comments! \ud83d\udc47\ud83d\udc47\ud83d\udc47"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Pro: Regression testing for your GraphQL API Description: Hasura Pro offers a comprehensive testing framework that lets you validate your GraphQL schema or test its integrity manually and programmatically to give you early feedback on potential regressions in your GraphQL API. Author: Hasura"}, {"role": "assistant", "content": "Hasura Pro has a comprehensive testing framework that lets you validate your GraphQL schema or test its integrity manually and/or programmatically. This helps you get early feedback on potential regressions in your GraphQL API.Hasura Pro regression testing demoGraphQL engine and API TestingTesting a Hasura-based GraphQL API needs to factor in the following aspects of Hasura\u2019s architecture:Hasura generates a different GraphQL schema for each role defined in its built-in authorization system.A large part of GraphQL engine is essentially a compiler that converts GraphQL requests into performant SQL.Based on this, here\u2019s how you could think about some of the traditional API test types:Unit tests: Simple API unit tests are completely unnecessary with Hasura because you are essentially testing Hasura\u2019s compiler :) Load tests: While GraphQL engine is performant and scalable, you should load test your GraphQL API to identify any bottlenecks in your database.Regression tests: Arguably the only kind of tests you need with GraphQL engine - regression tests ensure continued support for operations required by your frontend apps or users i.e. validating changes to the GraphQL schema (schema integrity) against these operations to ensure that there are no breaking changes or regressions in your GraphQL API. Changes to the underlying Postgres schema and/or the Hasura configuration could potentially lead to unwanted regressions in your schema. Therefore, schema validation is crucial to get early feedback when iterating on new features. In the next section, we\u2019ll take a look at how to configure a Regression test suite and test changes in your local or dev instance.Regression testing in Hasura ProHasura Pro comes with a comprehensive test bench that lets you seamlessly compile a test suite on each instance and execute them on any GraphQL engine instance. Here\u2019s a reference development workflow that is enabled by Hasura Pro:1) Build your database schema and configure Hasura as required by your frontend apps or public GraphQL API.2) Deploy changes to production after testing them.3) Create a regression suite on production. 4) Iterate on your GraphQL schema to support new features or edits \u00a0 \u00a0 a) Test changes in your dev instance against the production instance\u2019s regression test suite. Fix any issues highlighted by the tests or plan to communicate regressions to affected stakeholders.5) [Coming Soon] Run all changes through a CI/CD pipeline \u00a0 \u00a0b) Run Regression tests programmatically for all changes in the team6) Promote changes to prodCreating a test suiteEach Hasura Pro instance can be configured with a separate test suite. Hasura Pro captures an immutable list of operations received by an instance for you to select the operations you want to add to its test suite. Ideally, you want to create a regression test suite on an instance which has received requests with operations you\u2019d like to continue supporting or ensure are not \u201cbroken\u201d - your production instance or a shared Q.A instance which receives operations in your app or, if you have a public GraphQL API, \u00a0those from your consumers. \u00a0Here\u2019s an example of operations from a production instance being added to its tests suite:Running a test suiteA good development workflow would require that tests be run as follows:As early as possible in the workflow for catching regressions.Independently for all changes without exclusively relying on an individual developer to do so manually. A test suite configured on a Hasura Pro instance can be run on the same instance or any other Hasura Pro instance registered to your team or organisation, including local ones. This is how we recommend that you incorporate regression tests into your GraphQL engine workflows: Test manually with schema and metadata migrationsLet\u2019s say you\u2019re a developer iterating on a feature and ,as part of your work, need to modify your Postgres schema or the Hasura configuration. It is likely that you are doing so by running the console via the Hasura CLI to generate migrations that you can version control. Before committing your changes in git, you should run tests to get an early warning for potential regressions. Your team may want to designate the test suite from your production instance (or a suitable alternative) as the default suite to be used for this, and you can choose to run this test suite on your local or development instance. For example, let's say the column 'title' (in a typical authors and articles schema) has been modified as part of a feature iteration. Assuming the operation from the previous example is part of the test suite on production, here's how the feedback on this change looks like:As you can see, one of the tests fails because it expects a field, title, to be part of the type articles - which is something our proposed change just modified and removed support for. [Coming Soon] Run tests programmatically in your CI/CD pipelineWe are working on a feature where the Hasura CLI exposes commands for you to programmatically trigger execution of a test suite in your automated testing setup, typically in CI scripts. Running these tests programmatically ensures that your team catches potential breaking changes even if individual developers don't manually test for this. Keep watching this space!Getting started with Regression testingRegression testing is part of the Hasura Pro feature set - if you are interested in checking it out, set up a chat with us! "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura 2020: The next phase of our journey Description: We just announced our fundraise and I thought I\u2019d take this opportunity to put together some notes on what 2020 has in store for Hasura. Author: Tanmai Gopal"}, {"role": "assistant", "content": "We just announced our fundraise (TechCrunch, BusinessWire) and I thought I\u2019d take this opportunity to put together some notes on what 2020 has in store for Hasura.Our entire team is super excited with what we\u2019ve been able to achieve in the last few months. Hasura started as something we built ourselves to ease data access and accelerate development. Over time, our user base picked Hasura up and built a passionate, positive and extremely helpful community around Hasura, and that has become the bedrock on which Hasura has matured.We\u2019ve seen the Hasura GraphQL engine get used to build all kinds of applications including, mission critical & tier-1 applications, from fast growing startups to Fortune 100 users. We\u2019ve seen how Hasura has helped fill a key gap in modern application and cloud-native development by handling the plumbing around data-access and eventing that seemed absent in today\u2019s fast-moving app landscape. Most importantly, Hasura delivers this value over a GraphQL API. GraphQL is key to Hasura\u2019s offering, since GraphQL puts the consumers of data and APIs front and center. And these consumers are most often the developers who are responsible for building applications & services that will ultimately deliver value to the business (aka ship product!).With this fundraise, with our new partners (Sandeep at Vertex Ventures US, SAP.iO Fund) and angels (including James, founder Firebase & Amod ex-CTO Flipkart, Founder Udaan), we aim to do 4 things:Serve the needs of our production and enterprise users better than ever beforeDouble down on the GraphQL, security and Postgres features that the Hasura community wants and deliver them faster than ever beforeBring the Hasura experience to other databases beyond the Postgres ecosystem and work with database vendorsInvest in our users and the Hasura community, who\u2019ve helped make Hasura what it is todayIf you\u2019re using Postgres (or a Postgres flavour) at work today and are thinking about GraphQL to accelerate developer productivity, please do reach out and talk to us! And of course, if you\u2019re using Hasura already in your team/org, I\u2019d love to hear about what you\u2019ve been building and welcome any input you have for us!I\u2019m at @tanmaigo on twitter and tanmaig@hasura.io. Don\u2019t hesitate to reach out! ?"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Level up your serverless game with a GraphQL data-as-a-service layer Description: In this post, we talk about data access challenges with serverless, a shift in our thinking that provides an awesome solution to these challenges, and how Hasura fits into all of this awesomeness. Author: Sezgi Ulucam"}, {"role": "assistant", "content": "Based on true events, and on Tanmai Gopal\u2019s Byteconf talkYou may think that cloud computing isn\u2019t really in a cloud, so why should \"serverless\" really mean without servers?I applaud your astute observation skills. In fact, there are servers in serverless. They\u2019re just more dynamic and hip. Serverless is like getting a pay-as-you-go phone. Instead of paying a contractual fixed amount, you only pay for what you use. Your cloud provider dynamically allocates machine resources based on demand. All of this makes for the kind of elastic and devil-may-care lifestyle we\u2019ve all come to love and cherish about the cloud.But, like any good thing, there are problems sometimes. In this post, we\u2019re going to talk about these problems, a shift in our thinking that provides an awesome solution to these problems, and how Hasura fits into all of this awesomeness. Ready?Yes but. Serverless? Give me an exampleOk. Here\u2019s a \u201cHello Kitty\u201d serverless function for you:$ cat index.js\nexports.function = (req, res) => {  \n  res.json({\"message\": \"Hello Kitty\"});\n}\n\n$ serverless functions deploy hello_kitty\nhttps://serverless.functions.net/hello_kitty\n\n$ curl https://serverless.functions.net/hello_kitty\n{\"message\": \"Hello Kitty\"}See what we did there? We wrote pure business logic. We didn\u2019t worry about ops, deploys, provisioning a server, scaling... none of that stuff that burdens ordinary mortals. We just wrote code, and deployed. And it scaled \u201cfor free\u201d (you can\u2019t see that part above, but trust me, it did). How did all this happen? Because we\u2019re just that cool? Well yes, but also because serverless is stateless. When we run a serverless function, we have no access to the file system or the disk, where we\u2019d normally persist state. We can\u2019t even persist state across separate invocations of the same function: they\u2019re all independent.This is what allows the cloud vendor to guarantee that each function will always work independently. It gives us the ability to scale on demand.Sounds too good to be trueYou\u2019re right, and here\u2019s the catch: when you have so much freedom and independence, data access becomes a problem. Where you\u2019d normally use standard rules and optimizations, you\u2019re suddenly left in the cold, because there\u2019s no central server component, no mastermind to orchestrate the chaos.Here\u2019s what you miss out on:1. Pooling connectionsIn a \u201cserverful\u201d scenario, the database handles long-living, persistent, and relatively few concurrent connections. The API backend, on the other hand, handles tens to hundreds of thousands of concurrent, flaky clients which frequently disconnect from it.To tame this free-for-all, the backend maintains a persistent connection pool to the database, and multiplexes incoming requests into individual connections from this pool. This way, you avoid the overhead of creating new connections.In serverless land, this is not an option. As we know, serverless functions can\u2019t hold state for the life of them. So how are they to hold a connection pool that lasts across invocations? Indeed, every invocation may result in a new connection to the database. And when the serverless function finishes execution, that connection is gone. If you\u2019ve ever been ghosted, you know how that feels.2. Optimizing performance with prepared statementsIn a scenario where multiple users are making the same database query, but with different variables, serverful architectures typically cache prepared statements, so that subsequent requests only need to send a variable.In serverless land, this is hard to do, because\u2026 you guessed it: serverless functions can\u2019t share state across invocations. Because they all have independent connections to the database, they\u2019re oblivious to the jolly good time they could be having with prepared statements.3. Endpoint discovery / load-balancing / high availabilityIn a serverful setup, the backend maps requests to appropriate database instances to accommodate load-balancing and high availability configurations. With serverless, this is not straightforward, because splitting load requires knowing where the last request was routed. That\u2019s going to be challenging for what we know now to be stateless, carefree, happy-go-lucky serverless functions.4. SecurityIn a serverful setup, it\u2019s easier to manage security, because only backends inside the same VPC or network boundary can access the database. With serverless, you might want to provision serverless functions in different VPC\u2019s, maybe closer to your users than your database. In this case, configuring security is trickier, because requests come from across regions or vendors.So what to do in the face of these challenges? What\u2019s the missing piece of the puzzle?Serverless dev is like frontend devGiven what we\u2019ve said about serverless so far, you may have noticed that it resembles frontend development more than backend:Backend servers query databases. Frontend applications make API calls to HTTP services.Backend servers maintain long-living connections to upstream services like databases and event queues. Frontend applications make stateless connections to HTTP services that can handle frequent disconnects and spiky or massive loads.Backend servers maintain state across multiple requests, e.g. for load balancing. Instances of a frontend application don\u2019t share state.Backend servers usually run in the same security and trusted context as the database. Frontend applications run on an untrusted user device.Indeed, serverless and frontend are like twins. So why not have serverless functions connect to HTTP services, just like frontend applications do? Instead of thinking about accessing the database directly, we need to think about accessing data via a service that sits between the database and our serverless functions.This data-as-a-service:can connect over stateless HTTP connections,is scalable,handles stateful logic like load balancing and high availability,provides a secure boundary to access data or run business logic,is the missing mastermind we\u2019ve been looking for.Mystery solved! But what does all this have to do with GraphQL?You got your data, you got your service. But you still need an API for your data-as-a-service. And GraphQL is a great API for data! Here\u2019s why:GraphQL returns JSON data, which is what modern frontend clients expect.GraphQL is flexible: it allows you to build a server that can arbitrarily fetch data from the right place in the right way.You can optimize to your heart\u2019s content when building a GraphQL API: implement efficient fetching, data transformation, authorization, etc.Because this is an API boundary, you can enforce security, rate-limiting, allow-listing, etc. Your serverless developers are spared from these concerns.With GraphQL, you can build not just a data API, but a \u201cdata as a service\u201d that implements everything you expect from a service, like scalability and security.\n\n\nThe journey of a GraphQL query with HasuraWhen you use Hasura for your data-as-a-service layer, your query\u2019s journey looks like this:Your query is parsed into an internal data structure (AST).The authorization engine annotates the query based on authorization rules.The planner applies any user given transformation rules and compiles the query into SQL. It also starts fetching data from any other services if required.The Postgres adapter inside Hasura takes the compiled SQL query and makes the query to the underlying database, while taking care of connection pooling, load balancing, prepared statements, etc.A response is sent back to the client.\n\n\nHasura + serverless = \ud83e\udd84\ud83c\udf08\ud83e\udd84\ud83c\udf08Hasura pairs really well with serverless, because of these features:GraphQL types: The Hasura query engine maps database concepts like tables, views, functions, etc from database types into GraphQL types automatically, so that they\u2019re serializable in JSON.Security: Hasura uses a fine-grained authorization engine that exposes only the right tables, functions, views and rows to the GraphQL client. This allows you to enforce security when querying from a serverless function.Transactions: Hasura supports transactions over websockets. This means serverless functions can operate on multiple entities in the database in one sweeping transaction.Performance: Hasura\u2019s compiler-based approach results in extremely high performance. Hasura handles a few thousand queries per second in just a few megabytes of RAM. Hasura\u2019s performance is usually within 1% of Postgres\u2019 query performance. This means minimal overhead is added for querying the database from your serverless functions.So if you ever find yourself wrestling with data access in a serverless world, remember to think of your data more as a service than a database. Use GraphQL for a unified data API for frontend and serverless app development. Let your data-as-a-service layer worry about database primitives, scalability, and security. And if you don\u2019t want to write it from scratch, have Hasura do it for you \ud83d\udc83"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: A Design Guide for Building Offline First Apps Description: Building Offline first apps is hard! We discuss 4 approaches that will help you build robust offline first apps with a fantastic user experience. Author: Gautam BT"}, {"role": "assistant", "content": "Last month we've published a guide to building an offline first app using RxDB + Hasura. In this article we will be diving into some of the engineering challenges that make designing robust offline first applications with good user-experience hard, and explore some architectures. In the process we will also look at various existing approaches.\nIntroduction\nArchitecture\nConflict Resolution\n\nApproach 1: Conflicts don't matter\nApproach 2: Version those objects\nApproach 3: Use conflict free replicated data types\nApproach 4: Do what super human does\n\n\nMigrations\nThings that can't be offline\nSummary\n\nIntroduction: Why Offline first?Mobile networks are frequently flaky. In addition people use mobile & web apps on the move. If your app did not have the ability to work well offline, every time your user took a subway or a plane or an Uber your app will potentially lose connectivity leading to a frustrating user experience.Why is this a hard problem?Making an app offline first requires you to design for offline first both on the front end and back end. For example:You have to worry about when is it safe to update your application codeYour data schema will have to take conflict management into account (More on this later).Where conflict resolution requires user intervention, your app UX will have to be modified.You might need a mechanism for migrating data on the front end apps.The app UX needs to handle scenarios where something cannot be completely offline (search for example).Offline first is an application development paradigm where developers ensure that the functionality of an app is unaffected by intermittent lack of a network connection. In addition offline first usually implies the ability to sync data between multiple devices.ArchitectureFor an app to be offline first we need to make sure that both code and the data required for it to function are not dependent on the presence of network.Making code available offlineFor mobile applications there is nothing you need to do to make code available offline. Web applications will have to use service workers to achieve the same. Using service worker however is not trivial. In particular, you will have to worry about when is it safe to update your code, also since web application users are typically not used to sites working offline, you will want to show a banner to notify users of the same. This page has a good discussion on this topic.Making data available offlineA simple solution to making data available offline is to have a local database that you read and write to. The database can then be replicated to and from a server whenever network is available. If your app can be used from multiple devices, it is possible for the user to make conflicting changes on different devices. We will look at how to handle this in the next sectionConflict ResolutionApproach 1: Conflict's don't matterThe simplest way to handle conflicts is to assume that they don't matter and users will simply correct the data later. This approach also known as last-write-wins surprisingly works for a number of apps. In particular Firebase & Trello follow this approach.Assuming you do want to detect and resolve conflicts, there are two broad approaches: Versioning your objects & Transmitting changes.Approach 2: Version your objectsConsider a medium like app that shows the number of likes a post has received. Assume the initial number of likes was 100 and two Devices D1 & D2 are modifying this as follows:D1 & D2 sync data from the server and display number of likes as 100 to the user. D1 & D2 then go offlineUser clicks like on both D1 & D2.D1 & D2 will display number of likes as 101.D1 & D2 go online and sync data to the server.In this situation the correct value of number of likes is 102. One way to achieve this is to do what git does - version your changes:Every time a change is made a new version is created. In addition the parent version for a given version is also kept track. A conflict can be identified by the fact that two versions have the same parent. Similar to git we will need \"strategies\" to merge the two diverging versions. In our case because we know the value stored is a counter, we can auto merge by setting the final value to be sum of (Value at current version - Value at parent version) on each of the branches. Two generic auto merge strategies are:Last write wins: Here we will simply take the last revision to reach the server as the final valueMerge by fields: If the two devices modified different fields of the object, it might be possible to auto merge by taking the new fields from both objects - This is similar to how git auto merges non conflicting changes.In the above approach merging is done by the server and clients simply fetch the merged value. However if you need user intervention to resolve a conflict you need to do the merge on the client. This will require all clients to store the version history of the document as well. A problem with this is that clients can independently merge the document creating new multiple merge revisions. Clients need to handle this (For example by examining the history and ignoring one of the merge versions).PouchDB & CouchDB handle this by using a hash of the document contents as the object version. So if two different devices resolve the conflict the exact same way they will end up with the exact same version id.How do we delete an object?A simple way to delete objects is to have a \"deleted\" flag in the object. Merge function can then decide what to do if a deleted object has been modified. On the client, any object with the deleted flag set can be immediately purged as there will be no more updates to it. On the server ideally we would want to know that all clients have deleted the object before purging the object. The not so ideal but practical alternative is to periodically purge old objects.Who uses this currently?PouchDB & CouchDB follow the above model. In this setup, versions are stored both on the client and the server. When there is a conflict a deterministic algorithm auto picks a winning version. However it is also possible to manually fix conflicts by deleting unwanted revisions and creating a new revision with the merged value if required. This is explained in more detail here.AWS Datastore + AppSync also follows a model similar to what is described here. Versions in this case are monotonically incrementing integers set by the sever. This also means all merging is done by the server - You can configure AppSync with different merge strategies. Implementing server side conflict resolution with HasuraConflict resolution can be implemented using Actions. Instead of relying on Hasura's auto generated mutations, we can write a custom mutation (this can be a serverless functions) that handles the insert after checking for conflict resolution. To achieve this:Change your schema to add \"version\" column (integer) to each entity that requires conflict resolution. Make sure the primary key contains the version column in it: (id, version) for eg.Define a custom action to handle the insert. This mutation would take parent_version_id as a parameterIn the action handler, check if the latest version > parent_version_id. If so detect a conflict, implement conflict resolution and write a new versionThe above will have to be done in a transaction, to ensure correctness when there are concurrent requests.Clients can use subscriptions to observe for changes. Approach 3: Use Conflict free replicated data types (CRDT)Instead of storing number of likes we can store the operations \"increment\" & \"decrement\". Both D1 & D2 will create a new operation \"increment\" and after step 4, both of them will have two new increment operations. To show the number of likes you simply sum up all operations. We can implement common data structures using this pattern as follows:Objects: To implement an object, we use operations setKey and removeKey. The object would again be a result of applying all the operations. With this approach if two different devices set the same key, the last operation to be transmitted wins.Lists: To implement a list, we can use operations addElement and removeElement. The list would be the result of applying all the operations.Nested data structures: If you model your data using relationships, nested data structures are not a problem since each object will have a unique id. If you are using a document store, you can use some way to specify the path of the object to apply the operation (JsonPath for example)For this to work we will need to make sure that:All operations are transmitted eventually (No data loss)There are no duplicate transmissions - this can be achieved by having a UUID associated with each operation and de-duplicating using that.For certain data types the order of operations will also need to be maintained - this can be achieved by time stamping all operations and applying them in the timestamped order.What we have described here are known as Conflict Free Replicated Data Types (CRDT). While CRDT's are designed to function without there being a central authority, they can also be used with a central server. For those interested, this video has a good explanation of CRDT's.How can this be done with Hasura?Design your schema to store the operations involved. Use Hasuras auto generated insert mutations to store the operations. Use subscriptions on these tables to sync operations to clientsYou can in addition use event triggers on insert to calculate and store aggregates on the serverApproach 4: Ship changes to server, store aggregates on serverKnowing there is a central server allows us to simplify the implementation:We store only aggregate data on the server and apply operations to the aggregate data as they are received.On the client we store only un-transmitted operations. Once all operations have been transmitted simply re-sync the aggregate from the server. The UI will show the result of applying all pending operations to the last synced aggregateSuperhuman follows what's described above.How can this be done with Hasura?Define a custom mutation using actions for handling the operations being shipped by the clientIn the action handler apply the operations on the latest version of the object to generate a new version of the objectUse subscriptions on clients to observe for changesMigrationsSimplest way to handle migrations is to nuke the client db whenever there is a schema change and let it sync from the server. This is not a great idea if the amount of data to be synced is large. In that case you will need to have migrations run on the device as well. If you are doing this you will need to have a schema version and make sure that any syncing happens only on the latest schema versions i.e devices that haven't run the migrations should not be able to pull or push data.Online only things in an Offline first appNot everything can be offline. Your application UX needs to handle this. For example:Authentication: Your application needs to allow the user to continue usage even if the authentication token (JWT/Session id) is expired. Once the device is back online, the user needs to be reauthenticated so that data sync can continue in the background. A related point to keep in mind is that client side storage for web applications is per domain and not per user. Since the same user can be having multiple accounts, a separate db needs to be created per user.Search: Your application's UX needs to notify the user that search is unavailable or that the results being shown are only from what is available locally. Attachments: While it is possible to sync any attachments involved for offline use, you might choose to not to do so to save bandwidth or it is possible that the attachment has not yet synced. Applications need to gracefully handle this scenario.SummaryOffline first is not new. There are several different frameworks & libraries to build an offline first app: Firebase, AWS DataStore + AppSync, RxDB + Hasura, PouchDB + CouchDB, Gun, Kinto, Replicache, Redux Offline, etc. However building offline first apps is still challenging and requires you to design for it. In this post we've looked how to go about doing this.Here are a bunch of good reads and videos on this topic:Superhuman: Architecting a web app to \u201cjust work\u201d offlineTrello: Airplane Mode: Enabling Trello Mobile OfflineCouchDB Conflict management guidePouchDB Conflict management guideAWS Datastore: Data driven mobile and web appsCRDTs and the Quest for Distributed ConsistencyHow are you making your app offline first? Ping us on Discord or tweet to us at @HasuraHQ"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Fluent GraphQL clients: how to write queries like a boss Description: Wouldn\u2019t it be nice if we could write GraphQL queries as objects instead of strings? It turns out we can, thanks to fluent GraphQL clients, with strong typing and autocompletion to boot! Author: Sezgi Ulucam"}, {"role": "assistant", "content": "\u201cI'm writing an appeal to the faculty senate so that I can move on from string theory,\u201d says Sheldon in The Big Bang Theory.If you work with GraphQL, you may not have a beef with string theory, but you may still feel the pain of string query. We all have days when we want to move on from strings.(image from gotql readme)Wouldn\u2019t it be nice if we could write GraphQL queries as objects instead of strings? It turns out we can do exactly that, with fluent GraphQL clients!Fluent APIs aim to make code more readable via method chaining by returning this or self from each method. This will look familiar thanks to jQuery:$(\"#iReturnThis\").html(\"I also return this\").addClass(\"me-three\");Fluent GraphQL clients allow you to write your query as an object, which they then convert to a good old string query behind the scenes. In addition to freeing you from strings, they offer:strong typing,a single source of truth for type definitions,autocomplete all the things!Let\u2019s see some examples. We\u2019ve created a companion repo for you to reference as you read along: Awesome Fluent GraphQLgotqlgotql works like a transpiler, with a built-in HTTPRequest Client (Got), allowing you to write your GraphQL queries as objects instead of strings | Example repographql-zeusgraphql-zeus creates an autocomplete client library for JavaScript or TypeScript | Example repogqlessWith gqless, your queries are auto-generated at runtime! Using ES6 Proxies, gqless converts object property lookups into GraphQL queries. | Example repographql-typed-clientgraphql-typed-client generates a strongly typed client library for any GraphQL endpoint. It allows writing queries as plain JS objects, with optional chain syntax. | Example repotyped-graphqlifytyped-graphqlify lets you build typed GraphQL queries in TypeScript, and avoid redundancy of type definitions. | Example repographql-codegen-hasuragraphql-codegen-hasura is a collection of plugins for graphql-code-generator, with strongly typed wrappers for all the Hasura GraphQL methods. | Demo @ graphql-codegen-hasuragraphql-bindinggraphql-binding allows you to invoke a binding function which constructs a GraphQL request.mst-sqlmst-gql allows type reuse between GraphQL and MobX-state-tree, and generates a query builder for chaining.Hasura + Fluent GraphQL ClientsAs a Hasura user, this means you get even more superpowers than you already have. If you\u2019re building an app with TypeScript, for instance, fluent GraphQL clients will give you object queries, type definitions, autocompletion, and all of the other goodies we talked about. And if you\u2019re building custom business logic for Hasura using actions or remote schemas, you get a convenient data API to work with!Have fun playing around with these and choosing your favorites \ud83d\udc83If we missed anything, add it to our Awesome Fluent GraphQL list, and add your examples to the repo as well!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Speak at a conference or meetup about Hasura. We\u2019ll help! Description: If you\u2019re speaking at a conference or a meetup about Hasura, we can help you prep, send you swag to give away, and also help with the logistics  Reach out to us. Author: Shraddha"}, {"role": "assistant", "content": "TL;DR If you\u2019re speaking at a conference or a meetup about Hasura, we can help you prep, send you swag to give away, and also help with the logistics \u00a0Reach out to us by filling up this form. A lot of our users have been giving talks about Hasura at meetups and conferences around the world. Some of them have reached out to us for assistance, and we thought it would be a good idea to compile all the ways we can help in one place.So here are all the ways in which we can pitch in with your talk,if you would like us to, ofcourse :)Submitting your talk idea, the CFP ProcessWe\u2019d be happy to give you feedback on your talk proposal. Fill up the google form below and tell us about your abstract or proposal, and we can give you some pointers and tips. Our engineers have spoken at several meetups and conferences around the world, and we\u2019ll try to make sure one of them sends you relevant feedback.Resources to help you prepareOnce your talk has been accepted, we can help with some slides , speaker notes and videos from previous talks that can help you prepare.We can also schedule calls with someone in our developer relations team if you would like to practise.SwagWe\u2019d love to ship you some special speaker swag as well as stickers to give away. This usually takes 2-3 weeks, so this will depend on the time we have before the talk!Publicizing your talkWe\u2019d be happy to help spread the word via social media, our newsletter and by posting the event on our page.LogisticsIf your proposal has been accepted at a conference, and you need travel assistance, let us know the details in the form below, and we can help.How you can reach outIf you would like us to help in any of the ways mentioned above (or in some other way that we have not mentioned), just fill the google form here and we\u2019ll get back to youSome of the other engagements that you can reach out to us about:Starting a Hasura User Group in your cityGiving a Hasura workshopSponsoring a GraphQL meetupPaid writing opportunities via out Technical Writer ProgramIf you have any other ideas, do reach out to us at hello@hasura.io#COMMUNITY#CONFERENCE#CFP#MEETUPS"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a UI for PostgreSQL Database Migrations Description: In this post we look back at the journey of building a UI for generating database migrations with a focus on ease of use, developer experience, productivity and the design choices we made to build this system. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRIn this post we look back at the journey of building a UI for generating database migrations for PostgreSQL with a focus on ease of use, developer experience, productivity and the design choices we made to build this system.Migrations are hard!Managing database schema is hard. Most of the tooling requires you to hand write SQL statements or scaffold boilerplate code for any DDL updates. There are lots of dependencies like the frontend apps connected to it, different versions, schema constraints, caching and syncing with different deployment systems etc. As your application grows, you will end up with a chain of migration files that become unmanageable by hand. Things could break or go wrong in multiple layers.Surprisingly there were no major UI interfaces that deals with migrations for SQL databases like PostgreSQL to ease some of the pain.MotivationWhen you create your data schema in Hasura console, metadata implicitly gets created to track different postgres tables, relationships, permissions and the GraphQL schema to be generated. This metadata layer allows Hasura to support PostgreSQL fully, whether it is an existing or a new database.However the schema and therefore the metadata are likely to change over time. The standard practice for dealing with this is to write migrations, test them out on staging and then run them in production. With Hasura, we went a step ahead and auto generated these migrations right from the console. Once generated you can check in these migrations in version control and run them on your production instances.Hasura Console and a CLI to manage file generation was the combination that forms a core experience of our product with regard to database migrations. This post explains the design choices we've made to make this experience seamless.Constraints and ChallengesIn building the UI to manage migrations, we kept some constraints to make the migration workflow better. We wanted to solve the hard problems of ease of use in generating the migrations and making incremental changes as smooth as possible. We wanted to have thePower of phyMyAdmin like admin tools with a simpler UI.Familiartiy of up and down migration systems like rails.Flexibility to manage migrations automatically via UI and manually via CLI.Ease of use for someone who is not very familiar with PostgreSQL.FeaturesA good DX was on the forefront to support database migrations via UI. Ideally it shouldn\u2019t add more complexity to the way you would have managed migrations otherwise using a different tool (note rails, django, alembic etc). We wanted to keep developer experience smooth, with the following features in mind.Manage via UIDevelopers should be able to use a web interface to manage postgres schema (schema, tables, views, columns, foreign keys, indexes, relationships etc) easily.Manage via Editor for flexibilityThe ability to generate / update migration files directly using text editors of choice (ie, vim, vscode etc) and apply the migrations. This would be quite useful in case anybody wants to make quick changes.Migrate easily between dev/staging/productionThe generated migration files should be used to easily replicate the state of database in different instances, typically in dev or staging instances for testing and applying to production.RollbackThe migrations generated from the UI should allow for rollback from one state to another. It should have an up/down action for each schema change allowing for rollback.Error Handling with feedbackDisplaying actionable error messages when things go wrong so that users can understand better and address them easily.Dependency ManagementAgain an extension of error handling; The UI should warn about modifying schema which has dependencies.Import/ExportA simple UI that lets users upload metadata that can be applied directly. This is particularly useful when there\u2019s some metadata file available to quickly replicate the state of Hasura.Version control / collaborationDevelopers working in a team would like to replicate database state at any point of time locally. The goal is to make it easier to work in teams with a git like version control enabled for a hasura project.Evaluating options for data persistenceThe web interface alone is not enough to complete the migration workflow. In fact the UI just facilitates developers for a quick and neat way to manage the application schema. What is more important is the persistence of the migration state. The migration state has to be stored somewhere external. Typical options for such persistence use cases are:Database (PostgreSQL in Hasura's case)Browser\u2019s local storageFile system of the end userCloud storageThe console UI can make a simple API call to the server to store/retrieve migration state from the database. Storing the entire migration state in the database would result in a single point of failure and features like version control, collaboration or replication of state would become difficult.Using Browser\u2019s local storage would result in temporary persistence and again suffers from lack of version control and sharing of schema state.The next choice was using the File system. Here the browser doesn\u2019t have privileged access to write files into the system on a given directory. We were convinced that a command line tool that has read/write access to the file system was the natural choice to persist migration state, the Hasura CLI.Under the HoodThe fundamental architecture of the console revolves around React components and http requests back and forth via the CLI.The CLI serves the Hasura Console with the basic HTML packaged inbuilt and the React SPA mounted on load.ArchitectureConsole UI: Behind the ScenesThe console UI is built on React aided with few other libraries to enhance the UX. All the schema modification operations from the UI goes via Hasura CLI in simple HTTP POST requests which the CLI intercepts and generates the migration files for.Each DML action via the UI needs to generate a request body that captures both the database state and the metadata state in a transaction. Hasura GraphQL has a bulk endpoint which is executed in a transaction for applying both the DML and metadata related changes. The UI templates an SQL for both up and down migrations for each action.File generationAs mentioned earlier in the architecture, the Hasura CLI does the file generation part; generating up and down migration files.The files had to also comply with the naming pattern for keeping it distinct and useful enough to hand write them manually when required.The format of the file naming looks like the following:prefix/version + separator + operation + .yaml/.sqlIn the UI, you can manually name your migration file for raw SQL statements.Tracking DML statements in SQLHasura console gives you the freedom of executing SQL statements directly. Although this gives the freedom of using psql like control to your database, we need to also track the DML statements out of them to create appropriate migrations automatically.The UI gives the power of raw SQL control with automatic tracking of the DML statements.In the above example, we are creating a table account. Note the Track this checkbox; this lets the UI to automatically create the request instructing Hasura to track this table in its internal metadata system. Obviously this is a simple case of an SQL statement. We used a regex match to group and parse the required table/view/function name that is being created.It looks something like this:/create\\s*(?:|or\\s*replace)\\s*(view|table|function)\\s*((\\\"?\\w+\\\"?)\\.(\\\"?\\w+\\\"?)|(\\\"?\\w+\\\"?))/; This identifies DML statements and extracts the right identifier (like the table/view/function names).Error NotificationsGetting human readable error notifications was a key challenge. It gives something actionable to the user managing the schema.Whenever DML statements are executed, postgres will raise errors on cases when there is a relation which already exists or if there are dependencies and so on.Parsing of postgres errors into a human readable error notification was one of the key components of this UI.Metadata syncCollaborating easily in teams is an important feature. It is super easy since the migration files are in version control and everybody can have their own instance.When there are multiple instances of Hasura running against a postgres database, it is easy to apply changes to one instance and reload the metadata across all instances. This can also be done via the UI of each of those instances. Comparison with Rails Active RecordNot to forget, we were inspired by the Rails migration system during our initial building phase. Let's walk through the differences though.Files structureIn Rails, there will be 1 file per migration, which will have a definition of up and down methods, where as with Hasura, there can be upto 4 files [up.sql,up.yaml] and [down.sql, down.yaml]. Here we are separating out the methods into different files as well as having new files for Hasura metadata. The key difference in files is that Hasura lets you write SQL in .sql files normally and use them for migrations instead of scaffolding boilerplate code to do this in Rails in Ruby code.RollbackSimilar to applying the migrations where the up method is run, for rollbacks, rails will run the down method and hasura will run [down.yaml, down.sql] to rollback both the schema and metadata changes accordingly. Hasura migration system ensures that both Postgres schema and its own metadata are rolled back and kept in sync.Try it outIn case you haven\u2019t used our migration system but using PostgreSQL, you can give this a try for a UI based migration workflow. Broadly these are the steps you need to get started.Setup HasuraSetup CLIOpen Hasura console via CLIStart tinkering with the UI to manage schema!SummaryThere are no UI based migration tools that solves all the problems we tried to solve during this process. We at Hasura are obsessed to make these experiences accessible and friendly for all developers building their apps with PostgreSQL. In a production app, typically all changes to the database are recorded as migrations and possibly stored in version control so that teams could collaborate and work on replicating schema state. Though UI for database migrations improves developer productivity, it is not a one solution fits all system. We encourage power users to also fallback to hand writing migrations for custom requirements which Hasura lets you control fully. You can totally skip the UI!We set out to build a UI to improve productivity and ease of use and we are just getting started!Good ReadsYour migrations are bad and you should feel badBetter database migrations with Postgres"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing Hasura Pro Description: Hasura Pro is an enterprise ready version of the Hasura GraphQL Engine that offers improved reliability and security controls.  Author: Hasura"}, {"role": "assistant", "content": "An enterprise ready version of the Hasura GraphQL Engine that offers improved reliability and security controls and enterprise-grade SLAs for your Hasura deploymentWe launched the Hasura GraphQL Engine as an open source project in July 2018. Since then, it has seen widespread adoption across some of the world\u2019s most exciting organizations, from fast growing startups such as Matternet, Cherre and Credimi, to Fortune 500 enterprises such as Jeppesen (a Boeing Company) and BBVA, to even nonprofits (ProPublica Illinois) and government departments (US Department of the Interior). The Hasura GraphQL Engine currently sits at over 14,000 stars on Github, and was one of the fastest growing projects of 2019 as per the 2019 JavaScript Rising Stars report.Hasura based architectures have helped enterprises set up a modern data access layer for their new or existing digital transformation initiatives, and have saved hours of developer time. According to one of our users running Hasura in production, a Principal Architect at a Fortune 500 Healthcare Organization, \u201cHasura has helped reduce app development time from months and years to just weeks.\u201dAt Hasura, we have been working closely with our users from the very beginning on the product roadmap, via our Discord community and via support engagements with our larger enterprise customers. We heard a common refrain, particularly among our enterprise users - they wished that more advanced security and reliability features came out of the box. Their IT and Ops teams mandated certain requirements for them to run Hasura in Production so that they would not have to set it up and maintain themselves. Hasura Pro came out of these discussions.?Introducing Hasura Pro?An enterprise-ready version for teams to ensure secure & reliable data access.Hasura Pro comes with:\u272a Monitoring/Analytics\u272a Rate-limiting\u272a Regression Testing\u272a Allow-listing workflowsChat with us to learn more \u2192 https://t.co/B2cB5GXF9t\u2014 Hasura (@HasuraHQ) January 16, 2020\n\nTweet annoucing Hasura ProWhat is Hasura Pro?Hasura Pro is an enterprise-ready version of Hasura that offers advanced reliability and security features out of the box for your Hasura deployment. Hasura Pro is available under a commercial license, and is bundled with enterprise-grade SLAs and guarantees.Here\u2019s a look at what you get out of the box with Hasura Pro:Monitoring & Analytics: Hasura Pro includes advanced monitoring and analytics capabilities to provide complete observability into your GraphQL stack. Hasura Pro analytics also integrate with external logging and APM tools, and provide API access to Hasura generated metrics.Combined with the Analyze feature that the Hasura GraphQL Engine already comes with, Hasura Pro can help you find and fix slow queries by adding the correct indices.Finding and fixing slow queries with Hasura ProHasura Pro also reports errors as and when they occur. You can then inspect the query to get more details on the error, and how to fix them.Realtime Error Reporting with Hasura ProAll Hasura Pro generated metrics are available via an API, and are ready for integration with your favorite logging and APM tools - be it Datadog, Prometheus or Stackdriver. We also provide several APM integrations out of the box.Rate Limiting: Hasura Pro lets you enforce session based rate limits to prevent abuse / DoS attacks on your GraphQL API.Rate-limiting with session-based controls with Hasura ProBecause the configuration is user identity and session aware, Hasura Pro also lets you manage Quality of Service at the user level. Rate limits can be set at a global level or at a role level, and can be enforced based on the number of requests per minute/hour or query depth.Regression testing: Sometimes when you make schema or configuration changes, you can accidentally break your API. Hasura Pro lets you create regression test suites consisting of operations on your production traffic. Everytime you make a change on dev/staging, you can then run the test suite to see if your latest updates cause or would have caused breaking changes on production.Creating a regression suite with Hasura ProAllow-listing: Your GraphQL API could be subject to scraping and abuse by unwanted agents. To prevent this, the Hasura GraphQL Engine comes with an allow-listing feature so that you can allow only the GraphQL operations used by your app. Hasura Pro improves on the allow-listing workflow so that you can automatically creating allow-lists based on your production traffic, and providing simple workflows to manage your allow-lists across environments.Creating allow-lists to prevent unauthorized queries with Hasura ProHasura Pro also includes a warn mode where it will log unknown operations in production without preventing their execution.Team management and Collaboration: We have seen that the Hasura console serves as a collaboration hub for teams using Hasura. Hasura Pro further improves this workflow by providing you the ability to create member specific logins with granular privileges, such as running GraphQL queries, viewing metrics, viewing data/only part of the data, etc.Setting up granular permissions for your Hasura Console with Hasura ProSSO integrations for team management are also available on request.Deployment and support options: Hasura Pro processing can either be done via a central server (all logs and metadata are sent to a central server for processing), or the processing can be performed on your own infrastructure.Hasura Pro also comes with a variety of support options - enterprise grade SLAs (upto 15 minutes), priority development support and a dedicated account manager.Getting StartedHasura Pro is already being used in production across our enterprise users, and is battle-tested.Hasura Pro has been co-created with our users, and that will continue to be the case - if you are a Hasura Pro customer, your feedback will make it into the short/medium term product roadmap.If you are interested in using Hasura Pro, setup a chat with us!PS: We have several new features coming up on this front, as well as self-serve workflows. Watch this space!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The pros and cons of updatable views in PostgreSQL Description: In this post, I will cover the following:\nWhat are updatable views, \nHow Hasura used updatable views, \nKey problems with updatable views (from a Hasura point of view) and\nMoving away from updatable views Author: Phil Freeman"}, {"role": "assistant", "content": "I recently started looking at a class of bug reports in which non-admin users of Hasura were unable to use certain newer features from Postgres. For example, if the schema contains a table with a GENERATED AS IDENTITY column (a more modern, SQL-compliant equivalent of a SERIAL typed column), then admins would be able to insert data into the table, but non-admin users would not. A similar issue arises with generated columns in the Postgres 12 release.The root cause turned out to be the way we were using \u201cupdatable views\u201d to enforce Hasura permissions, a relatively recent addition to Postgres. So, what are updatable views, how were they related to the problem here and how did we fix it?In this post, I will cover the following:What are updatable viewsHow Hasura used updatable viewsKey problems with updatable views (from a Hasura point of view)Moving away from updatable viewsI hope that blogpost serves as an introduction to updatable views and its caveats for advanced Postgres users!Updatable viewsViews are a great tool for controlling data access in SELECT queries. This can allow us to separate the data representation in the table itself from the API provided to users.As a working example, we can create a table of articles, but a view which only returns the articles which as set as public:CREATE TABLE articles\n  ( id integer NOT NULL PRIMARY KEY\n  , title text NOT NULL\n  , description text\n  , public bool NOT NULL DEFAULT 'false'\n  );\n\nCREATE VIEW public_articles AS \n  SELECT * FROM articles\n  WHERE public;\nBut what about when we need to insert or update data?Postgres 9.1 introduced INSTEAD OF triggers on views, otherwise known as updatable views, to solve this problem. When creating the view, we can specify a trigger function to run on attempted insert operations on the view. Instead of simply failing as usual, Postgres will delegate to the trigger to decide how to insert the data. This way, we can preserve any invariants in our data representation while providing a complete but well-encapsulated API.For example, perhaps we should only be able to insert articles marked as public into our public_articles view:CREATE OR REPLACE FUNCTION public_article_on_insert() RETURNS trigger LANGUAGE plpgsql AS $$\t\n  BEGIN\t\n    IF NEW.public THEN\t\n      INSERT INTO articles (id, title, description, public) VALUES \n        ( NEW.id\n        , NEW.title\n        , NEW.description\n        , NEW.public\n        );\n      RETURN NEW;\n    ELSE\n      RAISE check_violation using message = 'article must be public'; \t\n    END IF;\t\n  END $$;\n  \nCREATE TRIGGER public_article_on_insert_trigger \n  INSTEAD OF INSERT ON public_articles \n  FOR EACH ROW EXECUTE FUNCTION public_article_on_insert(); \nUpdatable views for permission checking in HasuraIn Hasura, we previously used updatable views to implement permission checks for non-admin users.The na\u00efve approach to check Hasura permissions would be to add a trigger on the target table and rollback the current transaction if the desired constraint did not hold. However, we don\u2019t want to add triggers to our users\u2019 schemas in general, or any changes at all all in fact. So instead, we would create an updatable view inside the Hasura schema (hdb_catalog) and use a trigger there to check the constraint. Clever! And this avoided polluting the user's schema.Caveat 1: Updatable views don't have default column valuesIn the case of tables with generated or default values, we needed to do a little more work. To illustrate the problem, consider changing the definition of the table to make the id column GENERATED ALWAYS:CREATE TABLE articles\n  ( id integer NOT NULL GENERATED ALWAYS AS IDENTITY PRIMARY KEY\n  , title text NOT NULL\n  , description text\n  , public bool NOT NULL DEFAULT 'false'\n  );\nOur existing insert trigger won't work, even if we try to use DEFAULT for the id:> INSERT INTO public_articles (id, title, public) values (DEFAULT, 'The pros and cons of updatable views', true);\n\nERROR:  cannot insert into column \"id\"\nDETAIL:  Column \"id\" is an identity column defined as GENERATED ALWAYS.\nBut we can make it work by omitting the explicit id, and using the default value instead inside the trigger itself:CREATE OR REPLACE FUNCTION public_article_on_insert() RETURNS trigger LANGUAGE plpgsql AS $$\t\n  BEGIN\t\n    IF NEW.public THEN\t\n      INSERT INTO articles (title, description, public) VALUES \n        ( NEW.title\n        , NEW.description\n        , NEW.public\n        );\n      RETURN NEW;\n    ELSE\n      RAISE check_violation using message = 'article must be public'; \t\n    END IF;\t\n  END $$;\nHowever, the Hasura compiler has to handle not just one specific table, but any table the user provides. The question then becomes: How will Hasura distinguish this sort of case based on the metadata available. We need to fetch the default values from the table metadata and modify the data on insert if necessary.Using the Postgres metadata available in the information_schema tables, we could gather the necessary data to do this and simply join it with the inserted row in the new trigger.However, there is another problem. The Postgres metadata does not track default values for certain types of columns, such as columns with GENERATED AS IDENTITY types or computed columns. And this is the root cause of the problem in the introduction. We are unable to provide default values if we use the updatable view trick for these sorts of columns!Caveat 2: ON CONFLICT does not make sense on updatable viewsViews don't have unique constraints (or any constraints, for that matter). ON CONFLICT clauses require unique constraints during an INSERT operation, and hence these can no longer work with updatable views.We used to handle upsert operations using this strategy, by employing ON CONFLICT clauses in our INSERT statements, but again, it requires a little more work.If we add a uniqueness constraint to our table, for example:CREATE TABLE articles\n  ( id integer NOT NULL GENERATED ALWAYS AS IDENTITY PRIMARY KEY\n  , title text NOT NULL UNIQUE\n  , description text\n  , public bool NOT NULL DEFAULT 'false'\n  );\nthen we may wish to support upsert operations based on violations of that uniqueness constraint. We could modify our hand-written trigger as follows:CREATE OR REPLACE FUNCTION public_article_on_insert() RETURNS trigger LANGUAGE plpgsql AS $$\t\n  BEGIN\t\n    IF NEW.public THEN\t\n      INSERT INTO articles (title, description, public) VALUES \n        ( NEW.title\n        , NEW.description\n        , NEW.public\n        ) ON CONFLICT ON CONSTRAINT articles_title_key DO \n          UPDATE SET \n            public = NEW.public, \n            description = NEW.description;\n      RETURN NEW;\n    ELSE\n      RAISE check_violation using message = 'article must be public'; \t\n    END IF;\t\n  END $$;\nThis is simple enough in the case of a single hand-written trigger function, but again, consider that Hasura must handle any such combination of ON CONFLICT clauses generically for any table and for any role.The implementation in Hasura would generate one view per user role, each with its own insert trigger, and then pass the ON CONFLICT portions of the INSERT statement to the trigger indirectly through a transaction-scoped runtime parameter.This became a problem, because the SQL generation logic inside Hasura became very complicated and also required runtime parameters to be passed around. This became a key blocker for supporting Postgres flavours like Citus, YugaByte, Cockroach!Solution: Cutting out the complexityThis is all getting a bit complex! The updatable view technique was a nice way to work around the need for a trigger on the original table, but the need to provide special case handling for default values and the use of global variables both seem like indications that something is not quite right with the design here. Let\u2019s simplify things!Instead of checking permissions inside a trigger, we could have the insert CTE rollback its own transaction in the case of a permission failure.To do this, we need a helper function which can raise an exception inside an expression context:CREATE OR REPLACE FUNCTION hdb_catalog.check_violation(msg text) RETURNS bool AS \n$$\n  BEGIN\n    RAISE check_violation USING message=msg;\n  END;\n$$ LANGUAGE plpgsql;\nWith this simple function, we can simply modify the RETURNING clause of the insert CTE to add a CASE expression which will check the condition after the insert tentatively completes, and throws an exception if necessary. For example:INSERT INTO articles (title, public) VALUES\n    ('The pros and cons of updatable views', 'true')\n  RETURNING \n    *, \n    CASE \n      WHEN public \n      THEN NULL \n      ELSE hdb_catalog.check_violation('article must be public')\n    END;\nIt\u2019s important that the function call is evaluated lazily inside the CASE expression here, or we would always throw an exception!By making this change, we can simplify the code quite a bit - no more updatable views or triggers, and the same code path can be used for all users. Admins simply have a condition expression which always evaluates to true, granting access in every case.ConclusionSo, as we\u2019ve seen, updatable views can be a powerful tool in specific cases where we want to encapsulate a data representation behind a different API. However, in Hasura\u2019s case, where we need to deal with any possible schema at all, it\u2019s not quite the best fit, and it was simpler to use more basic tools at the cost of a little more code generation."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Blazing Fast GraphQL Execution with Query Caching & Postgres Prepared Statements Description: In this post, we will look at important performance considerations for building apps at scale and how we leveraged PostgreSQL query caching and prepared statements to improve performance. Author: Praveen Durairaju"}, {"role": "assistant", "content": "Hasura GraphQL Engine is fast and there are different dimensions to it; latency, throughput, concurrency and so on. In this post, we will look at important performance considerations for building apps at scale and how we leveraged PostgreSQL query caching and prepared statements to improve performance.Performance MetricsHigh ThroughputHasura can process a large number of queries (1000 q/sec) in a tiny footprint of just 50MB RAM and importantly with low latency.Concurrency & RealtimeHasura supports a massive amount of concurrency, particularly useful for real time applications. For example, we have tested this out by benchmarking our subscriptions; 1 million subscriptions dispatching unique updates to 1 million connected clients every second. Here\u2019s a little more on the subscriptions benchmark architectureLarge queries & resultsDespite being a layer on top of Postgres, for fairly large queries and (or) large results, Hasura has been able to match performance within 1% of native Postgres. We will look into detail how this was made possible below.Hasura GraphQL query processing architectureThere are multiple stages to processing the incoming GraphQL query. Hasura does query parsing, then validates it against the Authorization engine with corresponding session variables. Then there is a Planner, which understands how to convert the GraphQL query to a SQL query.Hasura Data WrapperThis forms the Data Wrapper interface which will act as the base for transforming a GraphQL query into SQL, NoSQL or any other query interface as long as there is type information. We will look at how a GraphQL query is processed in the Data Wrapper.Compiling GraphQL to SQLTypically when you think of GraphQL servers processing a query, you think of the number of resolvers involved in fetching the data for the query. This approach can lead to multiple hits to the database with obvious constraints associated with it. Batching with data loader can improve the situation by reducing the number of calls.Internally Hasura parses a GraphQL query, gets an internal representation of the GraphQL AST. This is then converted to a SQL AST and with necessary transformations and variables the final SQL is formed.GraphQL Parser -> GraphQL AST -> SQL AST -> SQLThis compiler based approach allows Hasura to form a single SQL query for a GraphQL query of any depth.PostgreSQL Prepared StatementsThe SQL Statement (that was compiled in the step above) must be parsed. But imagine repeating the same or very similar requests frequently and Postgres needing to parse it, consuming time that could have been spent somewhere else. This is where Postgres `prepare` statements come in, where parsing can be skipped. Only the planning and execution will happen. The prepare statements are session specific and work only for that session.This is how Postgres handles a SQL statement normally.SQL query -> Plan, Optimise, Execute.In prepared statements scenario,(SQL query, name) -> ExecutePostgreSQL Prepared StatementsConsider this example:PREPARE fetchArticles AS\nSELECT id, title, content FROM article WHERE id = $1;and you execute this using the followingEXECUTE fetchArticles(\u2018f1e8aa91\u2019);The id parameter can change or the same could be repeated as long as it\u2019s in the same postgres session.GraphQL Authorization with prepareHowever, it's not enough to just translate GraphQL to SQL! When queried by users or apps or clients, a GraphQL API should make sure that the data access is secure. This is the most common form of \"business logic\" that is embedded in a hand-written GraphQL server. Hasura automates this by providing developers with a fine-grained declarative auth DSL for every Postgres entity.In the context of Authorization, Hasura supports adding session variables in headers (x-hasura-*). In many cases you would want to restrict data fetching based on the user who is logged in. Ex: fetch articles written by the currently logged in user. With Hasura, you define a permission policy that implies these conditions. When the GraphQL Query is made, the session variables corresponding to the permission policy are injected into the where clause arguments.PREPARE fetchArticlesByAuthor AS\nSELECT id, title, content FROM article WHERE id= $1 AND author_id = $2;\nNow the above statement can be executed like below:EXECUTE fetchArticlesByAuthor(\u2018f1e8aa91\u2019, 1);Note that the author id coming from the session variable has been passed in as an argument to the WHERE clause. The value of the argument could change depending on the user who is logged in. But with prepared statements, the parsing of the query is avoided by the database. Obviously the above example is the simplest of cases. But Hasura\u2019s authorization system has to support the following use cases:Auth per graphql type, using filtering conditions or property of the sessionThe filtering conditions can traverse relationships to arbitrary depths.Multiple roles for the same querySchema visibility (field level access)This we believe increases performance vastly and our benchmarks do represent the same. Read more about Tweaking GraphQL Performance using Postgres Explain Command for optimising the generated SQL. Some quick fixes like adding the right indexes would boost the performance a lot.PostgreSQL JSON AggregationsEfficiently generating the SQL is one part of the optimisation. But now how do we parse the response from the database which is a flat table into a neat JSON that the client can understand?Doing transformations of SQL results into client readable JSON would mean double processing since the database already did one level of processing to generate a response and now before sending it back to the client, there\u2019s another round of data transformation. The larger the data, the more time it is to send it back to the client.This is where JSON aggregations come in. This is where you can ask Postgres to send the response back as a JSON that doesn\u2019t need any manual transformations before returning it to the client.JSON aggregations PostgreSQLHasura doesn\u2019t do any of the processing apart from generating the prepared statement with json aggregation. This typically results in a performance improvement of 5x to 8x as opposed to doing manual data transformations to JSON.Hasura's Query CachingIf you look at the Data Wrapper layer in the Architecture above, you can see that every GraphQL query is processed in three steps:parsingvalidationplanningThese steps are inexpensive, but they do take time.Hasura maintains an internal cache to improve this process. When a GraphQL query plan is created, the query string and variable values are stored in an internal cache, paired with the prepared SQL. The next time the same query is received, parsing and validation of the GraphQL query can be skipped, and the prepared statement can be executed directly.Currently, only queries and subscriptions are cached\u2014not mutations\u2014but most queries can be cached. Simple queries that do not contain variables are trivially cacheable, but using variables allows Hasura to create a parameterized query plan that can be reused even if variable values change. This is possible as long as query variables only contain scalar values, but more complex variables may defeat the cache, as different variable values may require different SQL to be generated (since they may change which filters are used in a boolean expression, for example).By default, cached query plans are retained until the next schema change. Optionally, the --query-plan-cache-size option can be used to set a maximum number of plans that can be simultaneously cached, which may reduce memory usage if an application makes dynamically-generated queries, oversaturating the plan cache. If this option is set, plans are evicted from the cache as needed using a LRU eviction policy.Query caching eliminates the parsing/validation for GraphQL queries while prepared statements eliminate the same for PostgreSQL queries. This allows Hasura to be very performant, since queries that hit the cache essentially only pay for the execution cost of the resulting SQL query, nothing more.ConclusionHasura's architecture ensures that 2 \"caches\" are automatically hit so that performance is high :)GraphQL query plans get cached at Hasura: This means that the code for SQL generation, adding authorization rules etc doesn't need to run repeatedly.SQL query plans get cached by Postgres with prepared statements: \u00a0Given a SQL query, Postgres needs to parse, validate and plan it's own execution for actually fetching data. A prepared statement executes significantly faster, because the entire execution plan is already \"prepared\" and cached!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: What is Hasura? Description: The Hasura GraphQL Engine is a blazing-fast GraphQL server that gives you instant, realtime GraphQL APIs over Postgres, with webhook\u2026\n Author: Hasura"}, {"role": "assistant", "content": "The Hasura GraphQL Engine is a blazing-fast GraphQL server that gives you instant, realtime GraphQL APIs over Postgres, with webhook triggers on database events, and remote schemas for business logic.Hasura helps you build GraphQL apps backed by Postgres or incrementally move to GraphQL for existing applications using Postgres.The Hasura GraphQL Engine is open-source. You can check out the complete repo here.FeaturesHasura comes with a lot of features that help you adopt GraphQL and a 3factor architecture (read more about 3factor & event-driven programming) quickly:Make powerful queries: Built-in filtering, pagination, pattern search, bulk insert, update, delete mutationsRealtime: Convert any GraphQL query to a live query by using subscriptionsMerge remote schemas: Access custom GraphQL schemas for business logic via a single GraphQL Engine endpoint. Read more.Trigger webhooks or serverless functions: On Postgres insert/update/delete events (read more)Works with existing, live databases: Point it to an existing Postgres database to instantly get a ready-to-use GraphQL APIFine-grained access control: Dynamic access control that integrates with your auth system (eg: auth0, firebase-auth)High-performance & low-footprint: ~15MB docker image; ~50MB RAM @ 1000 req/s; multi-core awareAdmin UI & Migrations: Admin UI & Rails-inspired schema migrationsPostgres \u2764\ufe0f: Supports Postgres types (PostGIS/geo-location, etc.), turns views to graphs, trigger stored functions or procedures with mutationsYou can check out all the Hasura features on our website and docs.Architecture and performanceThe Hasura GraphQL Engine fronts a Postgres database instance and can accept GraphQL requests from your client apps. It can be configured to work with your existing auth system and can handle access control using field-level rules with dynamic variables from your auth system.You can also merge remote GraphQL schemas and provide a unified GraphQL API.How Hasura works in your\u00a0stackWhile designing Hasura, we had two objectives:Hasura should be able to run anywhere, from your local machine to the Heroku free tier to a server on Amazon, Google or Microsoft.You should be able to extract maximum performance/dollar.Consequently, we optimised Hasura for low memory footprint & latency. On the Heroku free tier, it consumes ~50MB of RAM even while serving more than ~1000 requests/per second.If you\u2019re interested in the architecture decisions involved in building this, you can read about it here.Getting StartedOur aim with the Hasura GraphQL engine is to help you setup a GraphQL server with the least amount of friction, so that you can start building applications without any setup.With that in mind, here\u2019s how you can deploy Hasura to Heroku in just 30 seconds.You can also use the Docker container to run it anywhere.Update: We now have a one-click deploy to DigitalOcean. Other cloud providers will be added soon!And that\u2019s it! Once you have deployed Hasura, (on Heroku or using Docker), you are now ready to start making GraphQL queries, as realtime GraphQL APIs are automatically generated for your databases. Go ahead, try it out\u00a0:)In case you wish to use Hasura as a self-hosted BaaS, we have a tutorial featuring Hasura integrations with an infra provider and an auth provider. Check it out here!More tutorials, integrations and examples are available here.Getting SupportIn case you are ever stuck, you can reach out to us on Discord (we are extremely active) or by creating a Github issue. You can also follow our Twitter to keep up with updates on Hasura and the company.Since our launch, our users have built some extremely cool stuff using Hasura\u200a\u2014\u200afrom a data platform for real estate to one of the most popular ReasonML tools.We would love more such stories! If you do try Hasura out or use Hasura to build something awesome, let us know via the Discord\u200a\u2014\u200ait would make our day!\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: What to expect from GraphQL Asia 2020 ?? Description: A 2 day conference featuring in-depth talks about GraphQL by engineers from across the globe. Author: Hasura"}, {"role": "assistant", "content": "GraphQL Asia is almost here & we\u2019re super excited!Here is a quick look at what you can expect from the second edition of Asia\u2019s largest GraphQL conference.DatesGraphQL Asia will be taking place \u00a0from 20th to 22nd Feb\u201920 in Bengaluru, India.It kicks off with a dedicated day of GraphQL workshops on 20th at The Terrace at Hasura. The conference will run on 21st & 22nd at the Leela Palace hotel.TalksEngineers from all over the world will be coming down to Bangalore to share their experiences, best practices and insights about working with GraphQL.Just selecting these talks was incredibly exciting! \u00a0We can\u2019t wait to hear them live. Here are the speakers and topics we have lined up:The Future of GraphQLA sneak peek at the future of GraphQL by Robert Zhu (AWS)GraphQL in OrganizationsHow we improved PayPal checkout conversions using GraphQL by Vishakha SinghAuthorization for GraphQL @ Intuit \u00a0by Vijayan Srinivasan & Bala DuttBuilding Flipkart Ads Platform by Abinash Mohapatra3 Year GraphQL in Production at Product Hunt by Radoslav Stankov Taming schemas at scale: A modular approach to GraphQL by Gago Frigerio (Coursera) GraphQL as an API Gateway for E-commerce at Scale with Tooling by Yogesh Desai (Tokopedia) GraphQL at Expedia (Vrbo) by Monika JaiswalGet Better at GraphQLHidden gems of GraphQL query by Ivan Goncharov (APIs.guru) Be lazy and get promoted with great GraphQL documentation by Sean Grove (OneGraph) Better IoT with Serverless GraphQL by Sarjeel Yusuf (Atlassian)Potential of GraphQL Directives by Kiran Abburi (Neostack)Lecture SeriesGraphQL as an API by Shipra Gupta (Adobe)Becoming a \u201cSecret\u201d Agent: Securing Your GraphQL Backend with JWTs by Sam Julien (Auth0)How to convince your boss to use GraphQL by Lachlan Young (Tanda)How to build efficient GraphQL servers by Sauradyuti Coondu ()Build a GraphQL Client for React in 25 minutes by Vilva Athiban P B (Omio)AutoFIT, a framework to automatically inject failures per request invocation to test fault tolerant/resilient GraphQL APIs by Rajat Khare & Arun Lingala (Intuit)You can read more about the talks here. WorkshopsGraphQL Asia will also let you get your hands dirty with four workshops ranging from beginner to advanced, that introduce you to GraphQL and tools to get you productive quickly. Your conference ticket gives you access to the workshops as well.We will be announcing the workshops soon. Keep an eye out on the GraphQL Asia Twitter handle for detailsConference passesYou can buy a pass for the workshops & conference here.We have group discounts for large groups, so get your company to send in a large team:10% off for 5 tickets of more15% off for 10 groups of 10 or more20% off for groups of 20 or moreDiscounts get automatically applied when you choose your number of tickets.You can reach out to hello@graphql.asia if you would like make a group purchase via your organization or if you have any other questions.We can\u2019t wait to meet you in Bangalore ???! See you soon!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building an offline first web app with RxDB & Hasura Description: In this post we will look at how to make a web app offline first using Hasura GraphQL, RxDB and Postgres. Author: Gautam BT"}, {"role": "assistant", "content": "In this post we will look at how to make a web app offline first using a client side database (PouchDB) that is synced securely with Postgres. We'll be using two open-source tools, RxDB and Hasura to help set up the sync.An offline-first todo app syncing data between a client-side db and a central Postgres database.Contents\nOverview\nOffline first Todo App\n\nDemo\nBoilerplate setup\nOffline mode for assets\nHasura Setup\nRxDB Setup\nMaking the app work client-side / offline only\nSetting up GraphQL replication\n\nPull Query\nPush Query\nTrigger replication on change\nAuthorization\n\n\n\n\nLimitations\n\nCompaction\nConflict Resolution\nEventual consistency\n\n\nConclusion\n\nOverviewTo make an app truly offline first, we primarily need to do two things:Any code and assets used should be available offlineAny data changes should be made locally first and then synced to the cloud.We will briefly look at 1) and spend more time on 2).For achieving 2) we make all changes locally first using RxDB and then sync to Postgres using Hasura in the background. If you are an existing Postgres user, this approach is appealing as it allows you to continue using Postgres while implementing an offline first design on the front end. Making changes locally first also has the additional advantage of removing network latency on user operations. (Related: Optimistic UI updates).RxDB uses PouchDB under the hood to provide a NoSQL database on the client. It then provides an ability to replicate this data with either CouchDB or a GraphQL server. An offline-first Todo AppDemoTry it out here: https://rxdb-hasura-demo.netlify.com/ Try out the sync:Login to the app from two different browsers. Create some todos in one and watch as they get synced to the other (Note: This might not work in a private window on firefox due to this bug). Try out the offline-first:On \u00a0browser 1: Simulate offline mode using the \"Work offline\" option under File menu on firefox or devtools on chrome. On browser 1: Create/edit/delete todos in offline mode. You'll see that nothing has changed on browser 2.On browser 1: Go back online. You'll see that browser 2 gets synced!You can also go offline and refresh the page and see that data is persisted offline too.A demo showing realtime sync, offline-first usage & offline-online syncBoilerplate SetupTo build this app locally, head to this github repo (note the boilerplate git tag). The boilerplate consists of a simple react todo app along with Authentication using Auth0. If you try running the app at this point, you will be able to login, and see an empty todo list, but you won\u2019t be able to add any todos. That is because the functionality required to add a todo to a store, fetch todos from the store, etc has been left unimplemented in the boilerplate. Part 1: Offline mode for our web appTo make our app work offline, we need to get the web assets to work offline. Create React App already has the required boilerplate for making an app offline using service workers. We need to change serviceWorker.unregister() to serviceWorker.register() in index.js. Once you do this, refresh the page, go offline and refresh again. You will see that the page loads despite there being no network connection!One thing to keep in mind here is the design of the Authentication mechanism. We are using JWTs issued by Auth0 for authentication. While refreshing these tokens, we need to make sure that the user is online, otherwise the user will see a network error from Auth0. renewSession() {\n    const interval = setInterval(() => {\n      const shouldRenewSession = this.isLoggedIn && (!this.idToken || this.isExpired());\n      if (window.navigator.onLine && shouldRenewSession) {\n        this.auth0.checkSession({}, (err, authResult) => {\n          if (authResult && authResult.accessToken && authResult.idToken) {\n            this.setSession(authResult);\n          } else if (err) {\n            this.logout();\n            console.log(err);\n            alert(`Could not get a new token (${err.error}: ${err.error_description}).`);\n          }\n        });\n      }\n    }, 5000);\n  }Part 2: Setting up the backend (Postgres + Hasura + Auth)We will use a simple schema with two tables on Hasura:todo (\n  id: text primary key unique, \n  userId: text, \n  text: text, \n  createdAt: timestamp, \n  isCompleted: boolean, \n  deleted: boolean, \n  updatedAt: boolean\n)\n\nusers (\n  auth0_id: text primary key unique,\n  name: text\n)Hasura is integrated with Auth0 for illustrative purposes. If you are new to Hasura & Auth0, this tutorial has a good overview.Once this setup is done, you'll have a working secure GraphQL API for a todo backend app. For now, we're not going to use it and proceed instead with making our web app work completely offline.Part 3: Setting up the client side db (RxDB)? The overall RxDB setup is in this commit. Setting up the client db schema:We only need one collection \u2018todos\u2019 with fields (id, userId, text, isCompleted, createdAt, updatedAt, userId) \u00a0as defined in src/components/schema.js for this application. Read more about setting up RxDB in your web app and a basic schema here: https://rxdb.info/install.htmlOnce you're done with this, you'll be able to play around with RxDB from the devtools console in your browser.> window.db.todos.insert({\n    id: \"mytodo1\",\n    text: 'Todo 1',\n    isCompleted: false,\n    createdAt: new Date().toISOString(),\n    updatedAt: new Date().toISOString(),\n    userId: localStorage.getItem(\"userId\")\n}).then((doc) => console.log(doc.toJSON()))\n---------------------------------------------------------------------------\n{\n    id: \"mytodo1\",\n    text: \"Todo 1\",\n    isCompleted: false,\n    createdAt: \"2019-12-31T16:31:24.541Z\",\n    updatedAt: \"2019-12-31T16:31:24.541Z\",\n    ...\n}\n\nPart 4: Making the app work \"client-side only\"Making the app functional is fairly straight forward and requires wiring the up the client side db calls with the right UI actions. For example adding a todo can be achieved as shown in this commit:--- a/src/components/Todo/TodoInput.js\n+++ b/src/components/Todo/TodoInput.js\n\n   addTodo(text) {\n+    this.props.db.todos.insert({\n+      id: uuidv4(),\n+      text: text,\n+      isCompleted: false,\n+      createdAt: new Date().toISOString(),\n+      userId: this.props.auth.userId\n+    });\n   }\nSimilarly we can:Implement loading existing todos as shown in this commit.Implement marking a todo as completed as shown in this commit.Implement deleting a todo as shown in this commit.Implement clear completed functionality as shown in this commit.Note that RxDB has an issue that prevents us from updating a document if the GraphQL replication plugin is enabled. At the time of writing this post the issue has been fixed but not yet released. We have worked around this issue in the commit above.At this point we can create, edit & delete todos client-side only and everything will work entirely offline too!However since replication is not yet setup, if you say open the app in a different browser and login, you will not see the todo items you created in the first browser window.Part 5: Setting up GraphQL replication to sync data from the app to PostgresReplication using GraphQL for RxDB is described here (Note that this feature was released in November and is still in beta). Put briefly, RxDB expects two GraphQL queries, one for pulling changes that have happened on the server and one for pushing changes that have happened on the client. Conflicts are assumed to be taken care of by the server (more on this later).? The code to setup replication can be seen in this commit.5a) Pull QueryThe pull query is as follows:const pullQueryBuilder = (userId) => {\n    return (doc) => {\n        if (!doc) {\n            doc = {\n                id: '',\n                updatedAt: new Date(0).toUTCString()\n            };\n        }\n\n        const query = `{\n            todos(\n                where: {\n                    _or: [\n                        {updatedAt: {_gt: \"${doc.updatedAt}\"}},\n                        {\n                            updatedAt: {_eq: \"${doc.updatedAt}\"},\n                            id: {_gt: \"${doc.id}\"}\n                        }\n                    ],\n                    userId: {_eq: \"${userId}\"} \n                },\n                limit: ${batchSize},\n                order_by: [{updatedAt: asc}, {id: asc}]\n            ) {\n                id\n                text\n                isCompleted\n                deleted\n                createdAt\n                updatedAt\n                userId\n            }\n        }`;\n        return {\n            query,\n            variables: {}\n        };\n    };\n};Essentially RxDB fetches documents in batches by using the updatedAt timestamp for the last synced document. If there are multiple documents with the same updatedAt timestamp and the current fetch retrieves only some of them, then the next fetch will use the id of the last fetched document to fetch the remaining documents. To make sure that we do not miss any updates we need the server to set the updatedAt whenever it receives the document. To see why, suppose we have two client A & B both offline. Consider the following sequence of events:A performs an update t1.B performs an update at t2A performs an update at t3 (t1 < t2 < t3). A goes online and syncs the changes at t1 & t3. updatedAt will now be set to t3.B goes online and syncs the documentNow A will not fetch the change at t2 because it queries only for changes that have happened after t3. Having the server create timestamps will make sure that B's update has a timestamp > t3.Luckily an updatedAt column that will auto update on every change is part of Hasura\u2019s frequently used columns list:5b) Push QueryFor the push query we essentially need an upsert. Luckily Hasura provides exactly that. ?const pushQueryBuilder = doc => {\n    const query = `\n        mutation InsertTodo($todo: [todos_insert_input!]!) {\n            insert_todos(\n                objects: $todo,\n                on_conflict: {\n                    constraint: todos_pkey,\n                    update_columns: [text, isCompleted, deleted, updatedAt]\n                }){\n                returning {\n                  id\n                }\n              }\n       }\n    `;\n    const variables = {\n        todo: doc\n    };\n\n    return {\n        query,\n        variables\n    };\n};5c) Trigger replication on changesTo trigger replication as soon as a change happens on the backend, we use a GraphQL subscription:  async setupGraphQLReplication(auth) {\n        const replicationState = this.db.todos.syncGraphQL({\n           url: syncURL,\n           headers: {\n               'Authorization': `Bearer ${auth.idToken}`\n           },\n           push: {\n               batchSize,\n               queryBuilder: pushQueryBuilder\n           },\n           pull: {\n               queryBuilder: pullQueryBuilder(auth.userId)\n           },\n           live: true,\n           /**\n            * Because the websocket is used to inform the client\n            * when something has changed,\n            * we can set the liveIntervall to a high value\n            */\n           liveInterval: 1000 * 60 * 10, // 10 minutes\n           deletedFlag: 'deleted'\n       });\n   \n       replicationState.error$.subscribe(err => {\n           console.error('replication error:');\n           console.dir(err);\n       });\n\n       return replicationState;\n    }5d) Authorization for secure replicationWe can't \"replicate\" data directly to/fro the client and the backend! This is because we shouldn't let clients get access to data that their users don't have read and/or write access to.Assuming you've setup the right permission rules on Hasura for securing the GraphQL API, all you need to do is pass the JWT to Hasura to make sure the data replicated back and forth is the right data for the logged in user!We will pass the id token we receive from Auth0 to Hasura while making the GraphQL queries to pull/push/subscribe. In addition whenever we refresh the token, we will restart replication to make sure that the user is seeing the right data based on their latest access rules. We do this by encapsulating the replication as well as the subscription in a GraphQLReplicator class:export class GraphQLReplicator {\n    constructor(db) {\n        this.db = db;\n        this.replicationState = null;\n        this.subscriptionClient = null;      \n    }\n\n    async restart(auth) {\n        if(this.replicationState) {\n            this.replicationState.cancel()\n        }\n\n        if(this.subscriptionClient) {\n            this.subscriptionClient.close()\n        }\n\n        this.replicationState = await this.setupGraphQLReplication(auth)\n        this.subscriptionClient = this.setupGraphQLSubscription(auth, this.replicationState)\n    }\n\n    async setupGraphQLReplication(auth) {\n        const replicationState = this.db.todos.syncGraphQL({\n           url: syncURL,\n           headers: {\n               'Authorization': `Bearer ${auth.idToken}`\n           },\n           push: {\n               batchSize,\n               queryBuilder: pushQueryBuilder\n           },\n           pull: {\n               queryBuilder: pullQueryBuilder(auth.userId)\n           },\n           live: true,\n           /**\n            * Because the websocket is used to inform the client\n            * when something has changed,\n            * we can set the liveIntervall to a high value\n            */\n           liveInterval: 1000 * 60 * 10, // 10 minutes\n           deletedFlag: 'deleted'\n       });\n   \n       replicationState.error$.subscribe(err => {\n           console.error('replication error:');\n           console.dir(err);\n       });\n\n       return replicationState;\n    }\n   \n    setupGraphQLSubscription(auth, replicationState) {\n        ...\n    }    \n}LimitationsCompactionBy default, PouchDB will store upto 1000 revisions of a document. The problem is amplified by the fact that each change causes two versions of the document to be created - one when the change is made and the other when the change is re-synced from the server. To avoid this, while creating/initialising RxDB set the revs_limit parameter to a smaller value.Further when we call \"remove\" on a document, it is not actually purged from disk, instead a flag is set marking the item as deleted. If your documents are large in size or are frequently updated this might become an issue.A simple workaround to these compaction issues (that might work in some cases) is to destroy the entire database periodically and let it re-sync from the server.Conflict ResolutionWhen we have clients from multiple devices making changes we will potentially have conflicting changes. We can divide the kind of conflicts we will have as follows:Two (or more) clients edit the same key of a documentA client deletes a document while other client(s) edit some fields in it.For the simple todo app it might be okay to adopt a conflict resolution strategy of last write wins. A more sophisticated strategy would be to keep track of multiple versions and alert the user in case of a conflict. Pouchdb has an existing conflict resolution mechanism, however this data is not exposed during GraphQL replication by RxDB. Currently the only way out would be to build a similar system on top of RxDB.Eventual ConsistencyEven if individual documents are consistent, it is possible to get into a scenario where the data as a whole is inconsistent. For example, let us consider a case where we have two entities with a many to many relation. With RxDB we would model it as three collections A, B and AtoB. Now consider the following sequence of events:client X creates document a, b & atob of types A, B and atobdocuments a & atob sync to the serverdocument disconnects (note that b has not yet synced to the server)Client Y fetches documents a & atob and will see an inconsistency when if tries to fetch the associated document of type b.Clients have to explicitly deal with issues like the above, where related data might not be present. ConclusionIn this post we have looked at how to create a simple offline first todo app using Hasura as a GraphQL backend and RxDB as the store on the front end. For more on this, you can read our follow on post where we talk about some of the engineering challenges that make offline first hard and explore some architectures:A Design Guide for Building Offline First AppsHere are some other good reads on similar offline-first topics:Architecting a web app to \u201cjust work\u201d offline \u00a0Airplane Mode: Enabling Trello Mobile Offline"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Handling GraphQL Errors with Hasura & React Description: A quick guide to common errors that occur while working with GraphQL API & Hasura and how to handle them while building APIs with Hasura. Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written and published as part of the Hasura Technical Writer ProgramTable of Contents\nIntroduction\nErrors: REST Vs GraphQL\nAn introduction to GraphQL Errors\n\nSpecial case GraphQL errors with Hasura\n\n\nCustom error handling with React \n\n404 resource not found error\nNetwork Errors / Server Errors\n\n\nGraphQL Errors\n\nHasura GraphQL API\nHandling errors at top level\nCustom logic on certain errors\nHandling Errors at component level\n\n\nError policies (apollo client)\nSummary\n\nIntroductionUnlike REST APIs, GraphQL API responses do not contain numerical codes by default. The GraphQL spec leaves it up to GraphQL tools to show / not-show GraphQL errors. This makes it important for people working with GraphQL to understand the errors and how these errors are handled by their GraphQL tool of choice. In this article, I will cover: A quick introduction to common errors experienced in GraphQL APIsHow to handle GraphQL errors while building APIs with HasuraBuilding custom error pages on a client side React appErrors: REST Vs GraphQLREST API\u2019s use various API response codes which are returned with every API \u00a0request to tell the users/developers what happened to their request. \u00a0This is kind of obvious to someone working with REST, but GraphQL doesn\u2019t work that way. GraphQL responses do not contain numerical codes by default, and in case of an error, return an errors array with description of what went wrong. See the sample errors array below:\"errors\": [{\n      \"message\": \"Name for character with ID 1002 could not be fetched.\",\n      \"locations\": [ { \"line\": 6, \"column\": 7 } ],\n      \"path\": [ \"hero\", \"heroFriends\", 1, \"name\" ]\n  }]The GraphQL spec generally discourages adding properties to error objects, but does allow for it by nesting those entries in an extensions object.GraphQL services may provide an additional entry to errors with key extensions. This entry, if set, must have a map as its value. This entry is \u00a0reserved for implementers to add additional information to errors however they see fit, and there are no additional restrictions on its \u00a0contents. (docs).This extensions object is used by GraphQL servers (including Hasura) to add \u00a0additional properties to the errors object. For example, sample errors array returned by Hasura looks like this:\u201cerrors\u201d: [{\n \u201cextensions\u201d: {\n   \u201cpath\u201d: \u201c$.selectionSet.post.selectionSet.name\u201d,\n   \u201ccode\u201d: \u201cvalidation-failed\u201d\n },\n   \u201cmessage\u201d: \u201cfield \\\u201dname\\\u201d not found in type: \u2018post\u2019\u201d\n }]Hasura returns a extensions.code \u00a0object which can be used to classify errors and show appropriate response on the client side. In this post, I will use this object to show custom error-pages on a React client. But first let\u2019s learn \u00a0about the common GraphQL errors and how they are handled in Hasura.An introduction to GraphQL ErrorsGraphQL Errors fall into the following categories : Server errors: These \u00a0include errors like 5xx HTTP codes and 1xxx WebSocket codes. Whenever \u00a0server error occurs, server is generally aware that it is on error or is \u00a0incapable of performing the request. Server errors may also occur due \u00a0to closing of websocket connection between client and server, which may \u00a0happen due to various reasons (see CloseEvent for different types of 1xxx error codes). No data is returned in this case as GraphQL endpoint is not reached.Client errors: \u00a0These include errors like malformed headers sent by client, \u00a0unauthorized client, request timeout, rate-limited api, request resource \u00a0deleted, etc. All the client errors return 4xx HTTP codes. Same with \u00a0server errors, no data is returned.Error in parse/validation phase of query: \u00a0These include errors while parsing the GraphQL query. For example, if \u00a0client sends malformed GraphQL request, i.e. syntax error. Or if the \u00a0query does not pass GraphQL internal validation, i.e. client sent inputs \u00a0that failed GraphQL type checking. In both these cases, no partial data \u00a0can be returned. In case of validation error, errors \u00a0array is returned showing what went wrong, while queries with syntax \u00a0errors are usually not sent to GraphQL endpoint and are caught at the \u00a0client side.Errors thrown within the resolvers: \u00a0Resolver errors may occur due to lots of reasons, depending on the \u00a0implementation of resolver functions. For example, errors like poorly \u00a0written database queries, or errors thrown on purpose like restricting \u00a0users from particular countries to access some data. Most importantly, \u00a0these kind of errors can return partial data/fields that are resolved \u00a0successfully alongside an error message.Some of these errors do not apply for Hasura GraphQL Engine. \u00a0For example, resolver errors (unless you are writing custom resolvers, \u00a0in which case you have to take care of the code being error-free). Special case GraphQL errors with HasuraThere are 2 cases in which Hasura itself will throw errors: Directly altering tables / views: If the tables/views tracked by the GraphQL engine are directly altered using psql or any other PostgreSQL client, Hasura will throw errors. To troubleshooting those errors, see hasura docs.Partial Data: Hasura enforces query completeness - a query that returns incomplete data will fail. Partial data is returned only if the query/mutation deals with remote schema, depending on the resolvers written by developers.Now let\u2019s jump into implementation of error pages.Custom error handling with React For implementing error pages, I will use the code from a hackernews-clone app I made as boilerplate. You can easily follow along and add error pages in your app \u00a0accordingly. The final code is hosted here.404 resource not found errorLet\u2019s first start by simply adding a 404 resource not found error page, which is shown when user goes to any unspecified route. This can be simply achieved using routing alone. In App.js, we have to make the following changes:\nNotice \u00a0that you just have to add a wild card Route with and asterisk(\u2018*\u2019) in \u00a0the end, which matches if any other routes does not match.Now we can create the NotFound component as : We get a 404 error whenever user enters an unspecified route/url:Any unspecified route is matched with asterisk(*) and 404 page is shownNetwork Errors / Server ErrorsNetwork Errors are errors that are thrown outside of your resolvers. If networkError is present in your response, it means your entire query was rejected, \u00a0and therefore no data was returned. Any error during the link execution \u00a0or server response is network error.For example, the client failed to connect to your GraphQL endpoint, or some error occurred within your request middleware. The best way to catch network errors is to do it on top-level using \u00a0the apollo-link-error library. apollo-link-error can be used to catch and handle server errors, network errors, and GraphQL errors. apollo-link-error can also be used to do some custom logic when a GraphQL or Network error occurs. Now let\u2019s implement network-error page using apollo-link-error. In App.js, we have to make the following changes:Note that in line 8, we have intentionally changed the GraphQL endpoint uri to replicate a network error. We defined onError which catches both graphQLErrors and networkErrors and allows us to do custom logic when error occurs. Every time networkError occurs, the if statement in line 18 is executed, and we redirect the user to a network-error page using react-router history prop (see line 20). In most simple terms, history object stores session history, which is used by react-router to navigate to different paths. We push the path network-error on history object, and we have defined the path in routes (line 32). Thus, when the if statement executes, the user is automatically redirected to /network-error url.See this thread to better understand react-router and history object. Note that we are using withRouter in App.js to get access to the history object through props.We will now create NetworkError component as:We get a network error, whenever the client cannot connect to the server:Whenever a network error occurs, user is redirected to /network-errorGraphQL ErrorsHasura provides various API\u2019s, but our react client will make requests to the GraphQL API.Hasura GraphQL APIAll GraphQL requests for queries, subscriptions and mutations are made to the Hasura GraphQL API. All requests are POST requests to the /v1/graphql endpoint.The /v1/graphql endpoint returns HTTP 200 status codes for all responses. Any error that is thrown by the Hasura GraphQL API, will fall under GraphQL Errors. Hasura GraphQL API throws errors, returning an errors array having errors[i].extensions.code field with pre-defined codes. These codes can be used to classify errors and do custom logic accordingly.Note: Hasura GraphQL API errors-codes are not documented currently, see this open issue for more information. Apollo-link-error and apollo/react-hooks make GraphQL error handling easy for us. By default, we want our app to \u00a0display global error pages (for example, a page with message \u201coops \u00a0something went wrong\u201d) whenever we encounter some query-validation errors or data-exception errors. But we also want the flexibility to be able to handle an error in a specific component if we wanted to.For \u00a0example, if a user was trying to upvote an already upvoted post, we \u00a0want to display an error message in-context with some notification bar, \u00a0rather than flash over to an error page.Handling errors at top levelTop level errors can be handled using apollo-link-error library. For example, if we are trying to query a field which is not present, a validation-failed error would be returned by Hasura GraphQL API. Or trying to mutate a field with string value but the field accepts an integer, data-exception error will be thrown.Example error responses returned by Hasura GraphQL API:{\n \u201cerrors\u201d: [{\n    \u201cextensions\u201d: {\n    \u201cpath\u201d: \u201c$.selectionSet.dogs.selectionSet.name\u201d,\n    \u201ccode\u201d: \u201cvalidation-failed\u201d\n   },\n   \u201cmessage\u201d: \u201cfield \\\u201dname\\\u201d not found in type: \u2018dogs\u2019\u201d\n }]\n}{\n  \"errors\": [{\n      \"extensions\": {\n        \"path\": \"$.selectionSet.insert_dogs.args.objects\",\n        \"code\": \"data-exception\"\n      },\n      \"message\": \"invalid input syntax for integer: \\\"a\\\"\"\n    }]\n}These \u00a0are errors for which developer is at fault, and the end-users will \u00a0probably not understand what went wrong, if shown the above error \u00a0messages. In other words, these error messages are meant to help \u00a0developers. In such cases, it\u2019s a good idea to use top-level error pages \u00a0which shows a \u201csomething went wrong\u201d message. We will implement the \u00a0same using apollo-link-error. In App.js, we have to make the following changes:Every time a graphQLError occurs, if block in line 7 gets executed, which triggers the switch case with extensions.code as the switch expression, thus we can map error-codes to logic we want to perform. Note that I haven\u2019t put a break statement after data-exception (line 10) as I want to show same error-page on both data-exception and validation-failed errors. We redirect end-user to /something-went-wrong route in case of these errors.We will now create SomethingWentWrong component as:On validation-failed error, we get a \u201csomething went wrong\u201d page:Custom logic on certain errorsWe can also do some custom logic in case of certain error rather than redirecting to error-pages. For example, if the error occurs while validating JWT (jwt\u2019s are used for authentication), or if the JWT has expired, we can write custom logic to refetch the JWT, and send back the api request. Errors array:{\n  \"errors\": [{\n        \"extensions\": {\n        \"path\": \"$\",\n        \"code\": \"invalid-jwt\"\n      },\n      \"message\": \"Could not verify JWT: JWSError (JSONDecodeError  \\\"protected header contains invalid JSON\\\")\"\n  }]\n}{\n  \"errors\": [{\n        \"extensions\": {\n        \"path\": \"$\",\n        \"code\": \"invalid-jwt\"\n      },\n      \"message\": \"Could not verify JWT: JWTExpired\"\n  }]\n}We will now write custom logic to handle these errors. In App.js, we will make the following changes:If the error-code is invalid-jwt, we refetch the JWT and try the API request again with new authorization header. Here is a diagram of how the request flow looks like now:Handling Errors at component levelErrors can also be handled at component level, using functions provided by apollo-react-hooks. \u00a0There may be many reasons why we would want to handle errors at \u00a0component level, for example, you may want to do some component-level \u00a0logic, or display notifications if some particular error happens.Here, \u00a0we will be handling unique key constraint violation error, which is \u00a0preventing a user to upvote an already upvoted post. Error array \u00a0returned by Hasura GraphQL API:{\n  \u201cerrors\u201d:[{\n      \u201cextensions\u201d: {\n      \u201cpath\u201d:\u201d$.selectionSet.insert_point.args.objects\u201d,\n      \u201dcode\u201d:\u201dconstraint-violation\u201d\n    },\n    \u201dmessage\u201d:\u201dUniqueness violation. duplicate key value violates unique constraint \\\u201dpoint_user_id_post_id_key\\\u201d\u201d\n  }]\n}We have a post component which is using apollo/react-hooks function useMutation to mutate data on the server. When the above error is thrown, we catch the error and check for error-code.We can access errors array returned by Hasura using error.graphQLErrors. Note that the errors array may contain more than one error, so we are iterating over the array to check if the error code constraint-violation is present. If a match is found, we show a toast notification with error message.I am using react-toastify to show error notifications. Now, whenever user tries to upvote a post already upvoted by him/her, error notification pops up:Whenever user tries to upvote already upvoted post, error notification is shownError policies (apollo client)At \u00a0last, if you are writing custom resolvers and using remote schemas with \u00a0Hasura, your queries/mutation may return partial data with errors \u00a0depending on implementation of the resolvers. In that case, apollo errorPolicy may come handy.You can simply set errorPolicy on each request like so:const { loading, error, data } = useQuery(MY_QUERY, { errorPolicy: 'all' });Now, if server returns partial data and error, both data and error can be recorded and shown to user. Check out this link to know more about errorPolicy.SummaryYou now know how to handle errors while building GraphQL APIs using the Hasura GraphQL Engine. If you have any comments, suggestions or questions - feel free to let me know below. References:The Definitive Guide to Handling GraphQL ErrorsFull Stack Error Handling with GraphQL and ApolloError handlingAbout the authorAbhijeet Singh is a developer who works across a range of topics including fullstack Development, Android, Deep \u00a0Learning, Machine Learning and NLP. He actively takes part in \u00a0competitive programming contests and has interest \u00a0in solving \u00a0algorithmic problems. He is a startup enthusiast and plays \u00a0table tennis \u00a0and guitar in spare time."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Full Text Search with Hasura GraphQL API and Postgres Description: In this post, you will look at how to implement full text search with Hasura GraphQL API, leveraging some of the Postgres features and it's support for search. Author: Praveen Durairaju"}, {"role": "assistant", "content": "In this post, you will look at how to implement full text search with Hasura GraphQL API, leveraging some of the Postgres features and it's support for search.There are different ways to approach full text search with Postgres. The requirements for search could range from searching a single column, multiple columns or across tables, which can potentially be achieved using views. I have taken the chinook database schema with sample data to demonstrate the examples. You can apply the above sql to your Postgres instance connected to Hasura. If you don't have Hasura running already, you can start by getting a Hasura project running on Hasura Cloud. Follow the docs to get started.LIKE and ILIKE operatorThe first easy approach is to use the LIKE or ILIKE operator in a where clause. Both these operators are used for pattern matching in Postgres. The difference is that ILIKE does case insensitive pattern matching.There are a couple of important selectors associated with this. The % selector and _ selector.% sign is used to pattern match any sequence of zero or more characters._ sign is used to match any single character.Now, let's look at the use cases one by one. Simple search on one columnConsider the following query:query {\n  tracks(where: {name: {_ilike: \"%wall%\"}}) {\n    id\n    name\n    album {\n      id\n      title\n    }\n  }\n}The above query returns the list of tracks which contains the substring wall in the column name. The response looks like the following:\n\n\nid\nname\n\n\n\n\n2\nBalls to the Wall\n\n\n147\nBlood In The Wall\n\n\n151\nBehind The Wall Of Sleep\n\n\n1780\nIt's A Bitter Pill To Swallow\n\n\n2538\nWall Of Denial\n\n\n3373\nFour Walled World\n\n\nRemember this uses the where clause with ilike usage as %wall%. Let's look at the different usage of the ilike operator.%wall% - Find all occurences of track name containing the substring wallwall% - Find all occurences of track name starting with wall%wall - Find all occurences of track name ending with wall_wall_ - Find all occurences of track name with a 6 character word having wallYou can also combine both these selectors to apply a match.Searching on multiple columnsSearching multiple columns can be done via simple boolean expressions in the where clause. We can specify that the term wall should be either matched with field name or the album title using the _or operator.{\n  tracks(where: {_or: [{name: {_ilike: \"%wall%\"}}, {album: {title: {_ilike: \"%wall%\"}}}]}) {\n    id\n    name\n  }\n}\nNote that we are making a pattern match across a relationship called album. We can apply the same for multiple columns too.Built-in Full Text SearchNow consider the following SQL with tsvector and tsqueryselect id, name from tracks where to_tsvector(name) @@ to_tsquery('wall')This will return a response like the one below:\n\n\nid\nname\n\n\n\n\n2\nBalls to the Wall\n\n\n147\nBlood In The Wall\n\n\n151\nBehind The Wall Of Sleep\n\n\n2538\nWall Of Denial\n\n\n3373\nFour Walled World\n\n\nThis is built-in Postgres full text search that returns documents matching a search query of stemmed words. But this doesn't account for mis-spelling.Fuzzy SearchIn the above examples, notice that the results do not have any order with respect to matching the name. There is no ranking for this search to give more relevant results. This query also scans through all rows in the table to find the results, making it extremely slow.Let's say you want to fuzzy match the search terms to column data to accommodate typos or similar results.We will make use of the pg_trgm extension in Postgres to implement this. This module provides functions and operators to determine similarity of alphanumeric text.Let's get started by creating the extension:CREATE EXTENSION pg_trgm;We need to also setup indexCREATE INDEX tracks_name_gin_idx ON tracks\nUSING GIN ((name) gin_trgm_ops);Let's try executing the following SQL query to see what results we get. Note that the search term we use here is wals, a typo of wall.SELECT id, name \nFROM   tracks \nWHERE  'wals' <% ( name ) \nORDER  BY Similarity('wals', ( name )) DESC \nLIMIT  5; The result will look like:\n\n\nid\nname\n\n\n\n\n601\nWalkin'\n\n\n2941\nWalk On\n\n\n2868\nWalkabout\n\n\n2\nBalls to the Wall\n\n\n10\nEvil Walks\n\n\nSearch using Postgres FunctionsThe above SQL cannot be directly queried using GraphQL. Hence, we create a Postgres function to implement fuzzy search.CREATE FUNCTION search_tracks(search text) \nreturns setof tracks AS $$ \nSELECT   * \nFROM     tracks \nWHERE    search <% ( NAME ) \nORDER BY similarity(search, ( NAME )) DESC limit 5; \n\n$$ language sql stable;Once this function is created and tracked, you can make the following GraphQL query:query {\n  search_tracks(args: {search: \"wall\"}) {\n    id\n    name\n  }\n}This above input argument args -> search can even have typo terms which would give back similar results.Access Control for SearchThere might be a use case to allow this search functionality to only logged in users of the app. This restriction can be applied using Hasura's role based permission system. Table permissionsFor example, under tracks table, we can apply the following select permission for role user.Function permissionsOnce the permission for the table is defined, it will be applied to the function which uses it underneath for querying. Read more on different example on our Docs on Fuzzy Match Search Functions.This is a series of posts demonstrating the possibilities of Searching with Hasura GraphQL. In the next post, we will take a look at how to integrate ZomboDB extension to perform search with Elasticsearch for more advanced use cases."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Cursor and Offset Pagination Techniques with Hasura GraphQL Description: Learn about different pagination techniques and types of pagination components that are typically used in an app and how you can use Hasura GraphQL queries effectively for each use case. Author: Praveen Durairaju"}, {"role": "assistant", "content": "In this post, we will look at different pagination techniques and types of pagination components that are typically used in an app and how you can use Hasura GraphQL queries effectively for each use case.Offset PaginationOffset based pagination is a popular technique where the client makes a request with parameters for limit (number of results) and offset (no of records to skip).Let's take a simple example. Assume that you want to fetch 10 products from the list of all products, starting with the 21st one. This can be translated to the following query:query {\n  products(limit: 10, offset: 20, order_by: {name: asc}) {\n    id\n    name\n  }\n}You will typically make the above query when the client is trying to navigate to the 3rd page of the results. This type of pagination works best if the data is mostly static and reasonably small. It gives a good experience to users for quickly navigating to specific pages.However it has a couple of issues that needs to be addressed.Performance When the dataset is really large (millions of records) and you are trying to fetch the farthest of the results, it doesn't scale nicely. Consider the following query:query {\n  products(limit: 10, offset: 10000000, order_by: {name: asc}) {\n    id\n    name\n    category\n  }\n}The query is trying to fetch 10 products with an offset of 10 million, sorted by name. You might assume that it returns only 10 products and hence the query must be performant. But it isn't.For the above query, the database still has to read till the offset (10 million), get the number of results in limit (10) and finally discard the results before the offset making it less performant than originally assumed.However do note that this becomes a bottleneck only if you are frequently dealing with a huge offset with a dataset size in millions of records.Missing / DuplicatesIf data is frequently updated, there is a possibility for duplicate / missing items. Consider the following query:query {\n  products(limit: 10, offset: 0, order_by: {name: asc}) {\n    id\n    name\n  }\n}This will give an initial list of 10 products sorted by their name. Assume somebody has inserted a few more products starting with \"A\". And now the client wants to fetch the next set of items and changes the offset to be 10.query {\n  products(limit: 10, offset: 10, order_by: {name: asc}) {\n    id\n    name\n  }\n}But the above query will result in products being duplicate in the response since new products were added which could have potentially appeared in the first query, but it didn't because it was added after the first query was made by the client. This could lead to poor UX on the frontend with either missing or duplicate items.Cursor / Keyset PaginationCursors work by returning a pointer to a specific record in the dataset. It does so with some assumptions. The cursor must be unique and sequential. This ensures that duplicate records are not fetched again, always reading data after a specific row rather than relying on the position of the record.Common examples of cursors include id \u00a0(auto-incrementing integer/big integer), created_at timestamp which should be unique and sequential. Consider the following query with a where clause.query {\n  products(limit: 10, where: {id: {_gt: 10}}) {\n    id\n    name\n  }\n}Here we are fetching products where the value of id is greater than 10. This will always skip the previously fetched results which would have been ids 1 to 10, ensuring no duplicate results. id is acting as the cursor column here; unique and sequential. Similarly we can apply this for timestamps provided they are always unique, since its already sequential.Cursor based pagination is more performant than offsets because we can leverage database indexes on the columns in the where clause that are being used as cursors.Relay spec for PaginationRelay is a framework for building data driven React apps. Relay's support for pagination relies on certain assumptions about the GraphQL server. Relay provides a standard mechanism to slice data and paginate the results. Hasura now supports the relay spec and you can get started here. After you have Hasura running, you should be able to make a query similar to the one below.query {\n  products(first: 10, after: \"cursor\") {\n    edges {\n      cursor\n      node {\n        id\n        name\n      }\n    }\n    pageInfo {\n      hasNextPage\n    }\n  }\n}Here, slicing is done using the first argument. This is similar to the limit which we had applied in the earlier queries. Pagination is done using the after argument, which will pass a cursor expecting results to be after that cursor. It also asks for hasNextPage which tells the client whether there are more results, since there is no concept of total number of pages in Cursor pagination.Pagination TypesWe have seen the two techniques (offset and cursor). Now let's see the different pagination types. Numbered PaginationConsider the following pagination component, the popular Google search pagination.If you want to build the kind of numbered pagination that Google has, you have to use an offset based technique.Now adding to the above, let's say there is also a direct link to First and Last pages of the results. How would you represent this in a query?\u00ab First    \u2039 Previous    Current    Next \u203a    Last \u00bbWith cursors, you never know what the last page looks like. This kind of pagination falls in offset where you fetch the count of total results and switch offsets based on what the client has clicked. For example, say you have 100 products and in each page you will display 10 of them. In your client, you will make a query to fetch the total count and also fetch the nodes for the given condition.query {\n  products_aggregate {\n    aggregate {\n      count\n    }\n  }\n  products(limit: 10, offset: 10, order_by: {name: asc}) {\n    id\n    name\n  }\n}\nNow assuming the user clicked on Last \u00bb, the above query would transform into: query {\n  products_aggregate {\n    aggregate {\n      count\n    }\n  }\n  products(limit: 10, offset: 90, order_by: {name: asc}) {\n    id\n    name\n  }\n}\nWe calculate the offset to be 90 since we had the total count 100 in the previous query and we have a limit of 10 for each query.// 100 / 10 = 10\npages = count / limit; \n\n// (10 * 10) - 10 = 90\noffset = (limit * pages) - limitInfinite Scroll / Load moreThe landing page of Hasura Blog has an infinite scroll. It initially loads up a few blog posts (say 20) and keeps loading more on user scroll. The infinite scroll will last till all the data is loaded. This is generally better suited for cursor based pagination. query {\n  blogs(limit: 20, where: {id: {_gte: 20}}, order_by: {id: desc}) {\n    id\n    title\n    description\n    cover_image\n  }\n}As the user keeps scrolling, we change the value of the cursor in the where clause. So the where clause now becomes where: {id: {_gte: 40}}.If in case your id column is a uuid then it doesn't qualify to be a cursor since it is not sequential. There you would end up using offset based pagination or some other unique and sequential column like a timestamp column.Limit in nested objectWith Hasura, you can also limit the number of results in a nested object. We have been fetching just the products in the above queries. Now let's say we also want to fetch the images of products, but we only want the recent 5 images.This can be done the following way:query {\n  products(limit: 10, where: {id: {_gt: 10}}) {\n    id\n    name\n    category\n    images(limit: 5, order_by: {created_at: desc}) {\n      id\n      url\n      alt\n    }\n  }\n}We apply the limit argument and an order_by argument to fetch the latest 5 images for each product. Here's docs on the same.Choosing the right approach for pagination would depend on the use case and whether the data is frequently updated or not.Good ReadsIn addition to this post, these are other good references i recommend going over.Evolving API Pagination at SlackPagination 101 - Frontend focussedHasura Pagination docs"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Why Saigar chose GraphQL & Hasura to build a crowdsourced intelligence platform Description: Why Saigar chose GraphQL & Hasura to build a crowdsourced intelligence platform Author: Hasura"}, {"role": "assistant", "content": "Every year thousands of families are struck with tragedy as their loved ones go missing. The recent explosion in the volume of publicly available data has given rise to a new category of intelligence tools to help deal with this problem - commonly referred to as OSINT (Open Source Intelligence). OSINT refers to the collection, processing and analysis of publicly available data that has been determined to be of intelligence value. Sources of OSINT include the surface, deep and dark web, social media, news media, academic sources, and government records. A typical use case would be to leverage these sources of information to crowdsource new leads on active missing persons cases. Saigar Technologies has built Saigar CE, the world\u2019s first dedicated OSINT collection platform for crowdsourced intelligence initiatives. It contains tools to crowdsource OSINT from multiple sources and combine them together in a structured format for sharing with intelligence stakeholders such as law enforcement. Since Saigar CE\u2019s launch in June 2019, the platform has been used to facilitate 6 crowdsourced OSINT events at various industry conferences such as \u200bDEFCON\u200b\u200b and \u200bBSidesLV\u200b, and other virtual crowdsourced OSINT events.The platform The Saigar CE platform empowers users to work collaboratively across the entire OSINT process. The typical OSINT lifecycle looks like this:Intelligence data is collected, tagged and submitted.Next, the intelligence is processed .i.e. it is archived as accepted or rejected. It can also be modified with new intel as and when data comes in.Case creation, user management and general administration of crowdsourced initiatives can be done by \"Admin\" users. These admins can also export intel packages for the stakeholders involved in the case.Saigar CE also provides a system for the different users involved in each case to get feedback so that they can continuously improve their performance.Saigar\u2019s requirements Saigar\u2019s primary KPIs were:Feature velocity: As they were building a product for a new market, they would have to iterate on the platform quickly based on community feedback and push those changes quickly for future events. Out-of-the-box support for realtime data\u200b: The nature of these events calls for realtime data. Intelligence Administrators need to process submissions as they come in and contestants need to receive feedback on their submissions in a timely manner. \u00a0 \u00a0 \u00a0 \u00a0Security: User accounts on the platform have different privilege levels and should only have the ability to view and modify data for their access level \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Scalable from day 1: They wanted a backend that was scalable and performant by default. They wanted to focus on providing the maximum value for their customers i.e. building powerful features, and did not want to spend time on backend implementation and worrying about performance issues. Evaluating the Stack - Choosing GraphQL With several years experience in fullstack development working with various products, the Saigar Team knew right from the start that they wanted to use GraphQL. However, \u00a0they wanted to avoid using a solution that would require them to build custom JWT middleware to validate Auth0 JWT Tokens and then handle RBAC for each GraphQL API call. They also wanted to avoid having to write custom resolvers for what are essentially CRUD operations, increasing development time and efforts.Evaluating and using Hasura After their initial trial, the team felt that Hasura suited their needs perfectly! It enabled them to create the schema and interact with the data quickly. They did not have to worry about backend implementation, including deploying, scaling, and securing the backend server. The platform required Role Based Access control rather cumbersome to do with a custom GraphQL server implementation, but was incredibly easy to do with Hasura. Not having to worry about concerns such as the RBAC implementation and the security layer on the backend had a major impact.In total, from the time they started development to their first launch, it took them 4 months with very limited dev resources!Going ForwardSaigar will be exhibiting Saigar CE at more industry related events, putting it into the hands of potential users to get their thoughts and feedback. For more information and to follow their progress, you can check out their website, and follow them on Twitter. This article is based on interviews with Peter Vicherek, the Co-Founder & CTO of Saigar Technologies. Over his career he has worked with multiple startups to develop and bring new products to market. \u00a0Peter is also the author of multiple open source security tools and has spoken at numerous cyber security conferences, including BSides Toronto and DEFCON Toronto. \u00a0With a passion for crowdsourcing technology, Peter enjoys working in the space to tackle complex social problems. Follow Peter on Twitter here. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Customising and Aliasing GraphQL Fields with Hasura Description: Learn how to customise the names of root fields and the column fields in Hasura's GraphQL schema and how it differs from GraphQL Aliases on the client side. Author: Praveen Durairaju"}, {"role": "assistant", "content": "It is important to distinguish what it means to customise names of fields at the server side and GraphQL aliasing at the client side. Hasura has support for customising names of root fields and column fields.Try it out on Hasura Cloud and read the docs to get started!Client Side Aliasing with GraphQL AliasesGraphQL Aliases let you rename the result of a field to anything you want. Consider the table users with fields id, full_name, email_addr, created_at and updated_at. Let's look at the anatomy of a simple query to understand this better. For example:query getUsers {\n  users {\n    id\n    full_name\n    email_addr\n  }\n}There are 3 parts to the above query: operation type \u00a0- queryoperation name - getUsersselection set - id, full_name, email_addrThe response structure of the above query would look like:{\n  \"data\": {\n    \"users\": [\n      {\n        \"id\": \"3ea1352a-6654-444d-9357-62816ccbd2b3\",\n        \"full_name\": \"Praveen\",\n        \"email_addr\": \"email@mydomain.com\"\n      }\n    ]\n  }\n}Now let's say, you want to map the response fields to UI elements directly and not deal with any transformations on the frontend. Ideally you would like the response from server to come in that format to achieve this. This can be done with GraphQL Aliases. You can alias the selection set to names of your choice.We most likely want to replace verbose or vague fields with simple and readable fields for the UI.Now the query can be written like below and the response format will match the keys you intended to be.# Query\nquery {\n  users {\n    id\n    name: full_name\n    email: email_addr\n  }\n}\n\n# Response\n{\n  \"data\": {\n    \"users\": [\n      {\n        \"id\": \"3ea1352a-6654-444d-9357-62816ccbd2b3\",\n        \"name\": \"Praveen\",\n        \"email\": \"email@mydomain.com\"\n      }\n    ]\n  }\n}Of course the above example is basic. But in certain cases, Aliases are necessary for the query to work. For example, if you have more than one top level field in the same query with different arguments, then you need to alias it. For example:# Query with different arguments\n\nquery {\n  users(order_by: {created_at: desc}) {\n    id\n    name: full_name\n    email: email_addr\n  }\n  users(order_by: {created_at: asc}) {\n    id\n    name: full_name\n    email: email_addr\n  }\n}\n\n# Response is an error\n\n{\n  \"errors\": [\n    {\n      \"extensions\": {\n        \"path\": \"$.selectionSet\",\n        \"code\": \"validation-failed\"\n      },\n      \"message\": \"cannot merge fields with different arguments under the same alias: \\\"users\\\"\"\n    }\n  ]\n}The above query doesn't work because we are using different arguments for the same top level field called users. (Note the different order_by object). Now for this to work, we need to make use of Aliases. The above query can be rewritten in the following format with alias for atleast one or both for it to work.query {\n  usersByDesc: users(order_by: {created_at: desc}) {\n    id\n    name: full_name\n    email: email_addr\n  }\n  usersByAsc: users(order_by: {created_at: asc}) {\n    id\n    name: full_name\n    email: email_addr\n  }\n}GraphQL Aliases are dynamic, written on the client making the request. But as you can see, this is a workaround for use cases where you want the selection set to be named more readable. It is not prudent to write this for every query from every client making this request for modifying trivial fields.Server Side Customisation with HasuraHasura lets you customise the field names at the server side. The tables and the columns created in Postgres can be customised to have a different name that can be used while querying. For example, the default root field names generated for a table named users are:Queries/Subscriptions:usersusers_by_pkusers_aggregateMutations:insert_usersdelete_usersupdate_usersThe above root fields and the names of the column fields can now be customized. In Console, you can head to the Modify tab of the corresponding table under Data. For example, we can now change the field name of email_addr to email so that clients can now query without aliasing on the frontend.So the following query should work after customising both the email_addr and full_name fields with Hasura.query {\n  users {\n    id\n    name\n    email\n  }\n}Do note that, once you customise the fields to have a different name, you can no longer make the query with the original field names that you used before the customisation. Please ensure that your client apps are in sync with this change as once you begin using custom names, requests with the original field names will result in an error.Hasura now lets you customize the names of the root fields & column fields in your GraphQL schema!So you can use ?camelCase, ? snake_case or your own special convention to name your them.? See it in action \u2935\ufe0f #GraphQL pic.twitter.com/XdgientrVT\u2014 Hasura (@HasuraHQ) November 17, 2019\n\nNow you can use your favorite naming conventions like camelCase or snake_case without worrying about the underlying postgres schema."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: \"Planet scale\" workloads ? with Hasura Description: Hasura fits well with the larger Postgres open-source ecosystem. Here are some Hasura + Postgres combinations designed to scale out your workloads. Author: Tanmai Gopal"}, {"role": "assistant", "content": "Hasura fits well with Postgres, Postgres vendors and the larger Postgres open-source ecosystem.In this post, we list some specific Hasura + Postgres combinations meant designed to scale out your workloads:Planet-scale, geo-replicated Postgres: YugaByte + HasuraTime-series workloads: Timescale + HasuraServerless with Aurora & HasuraPlanet-scale, geo-replicated Postgres: YugaByte + HasuraEarlier this month, YugaByte announced support for Hasura. Instant, realtime GraphQL in a few seconds on a massively scalable database! Watch the talk at the Distributed SQL conf: https://vimeo.com/362360359YugaByte + Hasura: https://docs.yugabyte.com/latest/integrations/hasura/hasura-1/\nTime-series workloads with Timescale & HasuraTimescale is a Postgres flavour that makes dealing with time-series workloads easy. It supports massive ingestion rates and some neat Postgres functions that make it easy to work with time-series data. Hasura is a perfect pair and allows users to leverage the power of Timescale\u2019s custom time-series aggregation functions. Build secure realtime IoT dashboards to applications that scale, easily!Watch a demo: https://www.youtube.com/watch?v=tsY573yyGWAGet started: https://hasura.io/blog/using-timescaledb-with-hasura-graphql-d05f030c4b10/Serverless with Aurora & HasuraHasura works instantly with AWS Aurora Postgres and provides a neat way to add custom business logic in serverless functions with remote schemas, event-triggers (so that you can trigger serverless functions on database changes) and actions. Here\u2019s a full-fledged reference stack for Hasura on AWS, including using AWS Lambda for your business logic in serverless functions! https://github.com/hasura/hasura-aws-stackIn addition to above, Hasura works well with all major Postgres vendor offerings (Azure Postgres, GCP Cloud SQL, Heroku Postgres), including your own self-hosted Postgres of course. We\u2019ve also added native support for Postgres extensions like PostGIS to make geo-location querying easy!If you have questions about your favourite Postgres distribution or extensions hit us up on our discord or our github! "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Adding custom business logic with Hasura Description: Learn the different ways of customising and extending Hasura so that you can add your own business logic easily, with your favourite tools & frameworks. Author: Tanmai Gopal"}, {"role": "assistant", "content": "Hasura provides instant realtime GraphQL on Postgres along with an authorization engine. This makes it easy to setup a blazing-fast GraphQL API that is secure & scalable, including GraphQL subscriptions that work and scale out of the box!How would you add custom business logic though?Data driven logicBring your own GraphQL APIEvent-driven business logic (write-heavy workflows)This blog post covers the different ways of customising and extending Hasura so that you can add your own business logic easily, with your favourite tools & frameworks.You will notice that Hasura encourages opinions that allow you to leverage the benefits of cloud-native patterns (stateless microservices, serverless functions, event-driven) as you think about adding new or existing business logic to your application.Data-driven logicOften, some business logic is better (or easier) done in the database than in the application layer. For example, complex high-performance aggregations (say a continuous time-series aggregate) or transactional stored-procedures.Hasura makes it easy to use Postgres views (named-queries) and functions to add new types and fields to the GraphQL API. Hasura also makes it easy to customise and extend types and fields generated in the GraphQL API.Views & FunctionsComputed fieldsCustomising GraphQL schema and field names (you can use the metadata APIs or the Hasura console UI to do this easily)Hasura makes it easy to get the best out of your database, while enforcing and reusing a common authorisation system so that you don't have to write code for data-heavy tasks.Bring your own GraphQL service: Custom business logicHasura provides an easy way to add business logic using your own GraphQL API using \"remote schemas\".(This method is ideal when you need to bring types to your GraphQL schema that are not derived from the database that Hasura is already pointing to.)Domain specific business logic often reside in microservices that might speak to other databases, external SaaS APIs or require complex application layer processing and transformations. Remote SchemasRemote JoinsHasura can integrates them directly into a unified GraphQL API and also allows you to configure semantic relationships between the types exposed by different GraphQL service by configuring \"remote joins\".Event-driven business logic (write heavy workflows)As microservices and serverless functions proliferate the \u201cbackend stack\u201d, event-driven patterns are becoming the norm to deal with increasing complexity. Driving business logic with events allows for easier decoupling and fault-tolerance without compromising on scalability.Hasura Actions provide an easy way to create a custom GraphQL mutation specification and maps them to event handlers which can be microservices APIs or serverless functions (or even Postgres functions!). Hasura internally takes care of the event plumbing, including event capture, persistence and delivery.Read more about Hasura actions here!Try it out!Set up a free GraphQL backend with Hasura Cloud to try it out for yourself!Check out the docs to learn how to get started!Make sure you're on version 1.3 or above and you're good to go."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL & Microservices with Hasura Description: Hasura provides a simple way to bring the power of GraphQL to your microservices architecture. Author: Tanmai Gopal"}, {"role": "assistant", "content": "Hasura provides a simple way to bring the power of GraphQL to your microservices architecture.Hasura can connect to databases, upstream \u201cmid-tier\u201d microservices, external SaaS APIs and event-driven business logic to expose a unified GraphQL API to your consumers.Hasura is a GraphQL engine that takes in metadata configuration at runtime and allows developers to dynamically map upstream data-models and API resources into an interconnected graph.Unified GraphQL API across servicesHasura also provides a way to create custom GraphQL mutations which are internally converted to events and delivered to microservices (or serverless functions) in an event-driven way. This makes it possible to integrate and migrate to event-driven business logic seamlessly.Event-driven business logic with Hasura & GraphQLDecoupled microservice teamsWorkflows for microservice teams are not affected and microservice teams can remain independent and don\u2019t have to coordinate with each other. Hasura addresses challenges around dynamically building a stable GraphQL API & performance.Read more about how to add microservices to Hasura with Remote Schemas and how you can use the upcoming Remote Joins feature to join across them!Event-driven business logic for fault-tolerance and scalabilityHasura is designed to integrate with the emerging cloud-native patterns for building services. The future of business logic workflows in APIs or serverless functions is going to be event driven, and Hasura makes that easy.Read more about the upcoming Actions feature and how you can try it out!SecurityHasura provides an authorisation engine that allows for securing the unified GraphQL API, including a powerful role, attribute and rule based access control. Additionally, Hasura provides a way to \u201cmask\u201d portions of the schema for different end-user roles making it easy to configure different levels of schema visibility for your unified GraphQL API.Talk to us!Over the coming months, we are adding key capabilities to the Hasura GraphQL engine. We'd love to chat with you to share preview builds, talk to you about our roadmap and understand what challenges you're facing with GraphQL and microservices. Discord: discord.gg/hasuraGithub: github.com/hasura/graphql-engineSetup a call with the team: Schedule a chat"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Exploring Hasura as a Headless CMS Description: Hasura is a headless GraphQL API platform which can be used by writers and developers with the right tooling to build content heavy apps for multiple platforms. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRHasura is a GraphQL API platform which can be used by content writers and developers with the right tooling to build secure content heavy apps for multiple platforms.\nHeadless CMS: Quick Intro\nArchitecture\n\nMulti-platform\n\n\nFrontend Tooling\n\nReact Admin\nMarkdown Content\n\n\nExtensible via Integrations\n\nRemote Joins\n\n\nSecure APIs\n\nAuthentication\nAccess control\nAllow Lists\n\n\nOpen Source & Self Hosted\nQuirks\nSummary\n\nHeadless CMS: Quick IntroHeadless CMS is becoming popular among teams with content writers and developers because of the flexibility and the power it gives to both.In a headless CMS, the front-end system is not coupled with the backend and the developers can build rich experiences with any framework and any platform (web, mobile, IoT etc) of choice. The backend is API driven with structured data. Traditionally you will get a WYSIWYG / HTML editors which is coupled with the database and the CMS is used to edit and preview content live.Technically the headless CMS is just a data store with a delivery layer.ArchitectureHasura would be the content delivery layer via GraphQL, backed by Postgres.Multi-platformWith this architecture, a single GraphQL API gives structured data making it easy to build customised frontends specific to each platform (web, mobile, AR, VR).Hasura comes with a basic built-in CMS, the Hasura Console. It lets you manage data in Postgres. All tables in postgres schemas which are tracked by GraphQL Engine can be managed using this console. It is primitive and directly mapped to database fields. Here's how a simple data insertion looks like.Hasura Console Basic CMSIt is designed to handle basic data types. In case rich content types including media like images that writers use, then this may not suffice. In this post, we will look at various options that can be used by writers and developers independently for building a CMS with Hasura for advanced workflows.Frontend ToolingDevelopers can choose to build rich content types over Hasura GraphQL APIs as a foundation for their custom CMS. Let's look at the existing frontend tooling available which enables easy customisation.React Admin: Build your own CMSreact-admin is a frontend framework for building admin apps in React. It adapts to any backend via a Data Provider model where the APIs are written to match the format that react-admin is expecting. It comes with rich text editor, custom fields, custom layout and a bunch of built-in components to build an admin panel.Hasura as a data provider supports react-admin through this package ra-data-hasura which lets you build custom interfaces mapped to fields in postgres.This is perfect for react developers who wants to build their customised dashboard. Here's an example app of the integration between Hasura and react-admin.Markdown ContentContent authors interested in modifying content via text editor of choice in Markdown, can use static site generators like Gatsby which can source data from Markdown.This architecture allows for maintaining primary content (eg: blog) via Markdown while associated data (like comments) can come in from Hasura GraphQL via a unique identifier like id or slug. Here is an example of such a use case.Gatsby.js, a react framework to build web apps, sources data from different data sources. Markdown is one of them.The idea here is to maintain content in markdown, source it in Gatsby and merge with data from Hasura GraphQL to build the app. When gatsby builds run, we apply an upsert mutation to sync markdown content with the appropriate database fields.Here's a sample app demonstrating how this can be done with Gatsby + Hasura. This hooks into gatsby-node.js createPages method to query for markdown content and uses ApolloClient to make a mutation via GraphQL. You can choose to sync raw markdown body optionally along with the unique identifier like slug.TinaCMSEnter TinaCMS - an editing toolkit for frontend frameworks. This will hook into existing static site generators like Gatsby/Next/Gridsome by providing a layer on top of them (via sidebar UI) and manipulating the content source in realtime.You can hook up tina-cms plugin to Gatsby for realtime markdown preview.Currently works with Gatsby and Next with integrations with other frameworks coming soon.Extensible via IntegrationsHasura GraphQL can be extended with external APIs opening up integrations with services which provide a CMS layer.Remote JoinsRemote Joins in Hasura extend the concept of joining data across tables in postgres, to being able to join data across tables and remote data sources. Once you create relationships between types from your database and types created from APIs, you can then \u201cjoin\u201d them by running GraphQL queries.This lets a developer to integrate external CMS specific solutions like Contentful so that the content authoring team can independently work on adding rich media sources while developers can leverage Remote Joins to establish relationships between Contentful and Hasura APIs.Here's how you add Contentful APIs to Hasura via Remote Joins. There's also an example with Gatsby + Contentful to build a music playlist app where the media rich content like images/videos come from Contentful, letting authors manage it easily and the primary content comes from Hasura GraphQL sourced into Gatsby.There's also a Wordpress plugin called wp-graphql which can be added to Hasura as a remote schema and you can now start managing content through Wordpress while it is joined with primary data in Hasura via Remote Joins.Secure APIsHeadless CMS should not only be API first but also secure. In this section we will see how Hasura helps you configure auth to expose APIs securely to the right stakeholders.AuthenticationThe example that i mentioned above with Gatsby also comes with Auth0 integration. Hasura comes with support for Auth through JWT or webhook modes. Developers can integrate any authentication provider into Hasura with protected routes for content authors managing admin dashboards. Read more about configuring Auth here.Access ControlRestricting access to content based on roles is a critical feature for an Headless CMS. Hasura supports role based access control where you can define permission rules for each role. For example, the content authors from marketing team probably doesn't need access to read/write other sensitive backend fields.All of this can be configured quickly with the console UI abstracting away the complexity for both developers and authors.Allow ListsIf you are exposing the APIs to a team of content writers, you can restrict which queries are allowed to be sent by the client by defining Allow Lists so that Hasura executes only those. This is highly recommended in production instances of Hasura.On top of role based access control, these are the only queries which will work on the API.Open Source & Self HostedAnother key criteria for a Headless CMS is also to see if there's an option to self host with local dev environments for testing. Ideally you would like to own your data instead of hosting it on a cloud provider.Hasura is open-source, can be self-hosted anywhere and can connect to postgres instance that you can host and control. It comes in a tiny docker image (~30MB) that can be run locally or in the cloud. Here are some of the deployment guides.The easiest way to get started with Hasura is to deploy it on Heroku.\nThis gives you an Hasura instance connected to Postgres in under a minute. It's an easy way to play around with the APIs.QuirksWhen you are using a Headless CMS in general (not just Hasura), you won\u2019t be able to preview content updates instantly on different mediums as you do with traditional CMS.The workaround here for a web app is to update data -> build on change -> trigger serverless functions to rebuild. A typical example is to host the app on Netlify and trigger Netlify functions to achieve the same. For other platforms, it should work fine if the APIs are used directly to source content.There's also Gatsby Preview, which gives instant feedback on the content changed by re-building on demand in a staging like setup.The react-admin data provider mentioned above currently doesn't support Remote Joins.SummaryHeadless CMS as a solution is growing among businesses where content teams would like to make changes quickly via an usable interface and developers would like to work with tools and frameworks of their choice. Hasura GraphQL is an API first solution which can be leveraged with external tooling and APIs to empower content teams. Developers can rely on Hasura GraphQL for efficient querying, type system, a schema with granular access control rules and freedom to work with their favourite frameworks."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Approaches to add dynamic content to statically generated sites (JAMStack) Description: The different approaches to add dynamic content to statically generated sites with JAMStack. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRThe different approaches to add dynamic content to statically generated sites with JAMStack.\nJAMStack Constraints\nTypes of Content\n\nUser specific: Public / Auth protected\nFrequency of updates: Infrequent / Frequent\nInteractivity: Non-interactive / Interactive\n\n\nSource data during build phase\nStatic pages hydrated on client\nSeparate Static and Dynamic pages\nCaveats\nSummary\n\nJAMStack ConstraintsJAMStack is an emerging architecture for web apps which aims to decouple frontend and backend dev teams. Read more about it here. The architecture allows for a performance and operational benefit by imposing a few key constraints on how you build your UI.These are the key constraints:Pre-built markup/UI Markup is pre-built to ensure good performance and SEO. UI should be pre-built and deployed as HTML files instead of getting rendered on the client (like what vanilla SPAs do).No server side code that deals with UINo server side code implies that the UI developer deploys their code as an HTML/CSS bundle, focussing on rendering the frontend. They needn't do server-side templating / server-side rendering.As you can imagine, these two constraints give the UI team great ownership in being able to deploy and own their code (no server-side complexity). There is no server side templating like how it's typically done in Django/Rails application and they can assume content will be injected into HTML from a data source.JAMStack is great for static sites or static site generators, but the key question is, what about \"dynamic\" sites or apps?Types of ContentBefore we try out these approaches in-depth, let's walkthrough the different types of content to see which approach works for what. The content for a webpage comes from a data source, typically a database. This data could change frequently based on user interactions. There could be multiple data sources to get content from. For example, a blog can have content coming from a database and user information coming from an auth provider. Broadly content types can be classified to be one or more of the following:User specific (public or private)In case of static web pages, the content served would be the same for all users. But what if, depending on the user who is viewing the page the content served needs to be different? Users of the site can also generate personal content and how do you build all those static pages pre-deployment time? The answer is simple. You can't build the markup ahead for user specific content. This gives us the classification based on whether the content is publicly accessible or protected. Content on a webpage can be public, accessible to all or protected behind auth and will be different for different users.Frequency of updates (frequent / infrequent)Another classification of content would be to base it off on whether the content changes and if so, how often. If content changes frequently, then it needs a dynamic runtime to take over from the pre-built markup that was generated.Let's say there's page content that you want to change and you want to reflect the changes immediately to your users. With pre-built markup, this doesn't happen realtime and there is a delay with re-building and deploying. Content on a webpage can change frequently, realtime or doesn't change that often.Interactivity (interactive / non-interactive)Users interacting with the page in the form of search, comments, forms, e-commerce checkout, payments etc are dynamic and a pure static web page doesn't scale for these kind of interactions. And hence the final classification based on interactions. Content on a webpage can be either interactive or non interactive.Choosing the right approach for your dynamic contentLet's see how we can apply different approaches to handle different types of content and decide which ones are more suitable for a given use case. Go over the below flow to see which approach is applicable for your use case.We'll look at different approaches to handle dynamic content and how it can be implemented to make a JAMStack (pre-built markup) site dynamic.Approach #1: Source data during build phaseThe simplest way to make JAMStack Dynamic is to source dynamic content at build time. The content can come from any data source. Data sources can be markdown (suited for blogs/docs), JSON/YAML and APIs, including third party ones. The API(s) would be responsible for fetching content from a database or a datastore and thus makes it a powerful way to build sites with dynamic content.The data sources are compiled to HTML, typically using a static site generator. During the build phase, these generators traverse through every path of the app and compile flat HTML files out of it. It uses specified data sources to compile the HTML.ArchitectureWith git automation, git push should trigger an automated build which is then deployed to cloud providers. Add a CDN for caching and an app with decent traffic should run easily.What is this architecture best suited for? Let's say you have some data which is coming from the database (accessed using APIs) but is not updated frequently. With this architecture, you can cache content which is infrequently updated, and only trigger builds for the content that changes. Do note that the data still comes from a data source like a database/markdown during the build phase and hence any changes will reflect eventually.Trying it out with Gatsby and HasuraHasura fits into Dynamic JAMStack with its unified GraphQL API sourced into Gatsby. Here's an example of using Gatsby with Hasura with dynamic content. Hasura is being used as a Headless CMS in this example and the amount of backend code written is significantly reduced.Approach #2: Static pages hydrated on clientAs you would imagine, just having pre-built content may not suffice for many apps. In this approach, we will look at how to fetch data at both build and run time for any page. This is achieved using a process called hydration, that allows static markup to be sent to the client which further becomes a dynamic app with the ability to manipulate the DOM. The benefit is obvious; users see a fully rendered page and then it becomes interactive like an SPA.Now let's look at various possibilities of dynamic content on the client. You have a site that requires client side functionalities such as search, comments, handling forms and e-commerce actions. Do note that some of them could be coupled with Authentication/Authorization as well.To handle these client side interactions, what we need is a pre-rendered static site + a dynamic runtime, maintaining the performance benefits and major parts of SEO.ArchitectureWith the markup rendered on the client, we would like the client to take over to make it interactive. This should be possible with hydration. HydrationHydration is a concept where the client expects the generated markup on both client and server to be identical. It is a process during which the framework (like react/vue) takes over the static HTML sent by the server and turns it DOM that can be manipulated and react to client-side data changes. In short, this makes sure that the client is ready for dynamic content without reloading.\"All pages should work by default without JavaScript being enabled on the browser and client side rendering is just a progressive enhancement.\"Interactions -> Serverless FunctionsOn the client side, the user would typically do auth (login/logout), interactive actions (search, comments) etc. \u00a0The user interactions can be forwarded to serverless functions which can internally perform business logic to handle state of the app.User interactivity can be lazy loaded on the client. Consider the example of this blog. The Disqus comments are loaded on the client where as the article was pre-rendered during build phase.RealtimeFeed based UIs could require data to come in realtime. How do we update content on the UI realtime? Websockets FTW! A websocket connection can be setup to listen for notifications. Anytime new data comes in, the client is notified about the data update.If you have access to a Realtime GraphQL API, it should be as simple as setting up websockets on the app to connect and fetch live data. Obviously this API needs to support Auth, in case it's required. Here's a reference video tutorial for how it's done with Hasura.Asynchronous callsOn the client, you can make asynchronous calls to external APIs for fetching/mutating data for UI views which are not user blocking. The APIs can be serverless functions.Trying it out with Gatsby and HasuraHere's an example of using Gatsby with Hasura. Markup is hydrated on the client and some data is fetched only during runtime, assuming its dynamic. Approach #3: Separate Static and Dynamic PagesBy now, you would have realised that static data is important for improved page load time and time to interactive. But what about pages which cannot be pre-built at all? Imagine a user profile page or a search results page. The content of these pages cannot be determined during build phase.Some pages of the app are completely static, while some are completely dynamic. We can separate out the static and dynamic pages neatly, so that static pages get their performance boost, while dynamic pages are completely loaded on the client, behaving like an SPA.Let's consider the example of an e-commerce product page to understand this better. There are some routes which are primarily static and doesn't have dynamic interactions on the client.In an e-commerce site, pages can be pre-built for homepage, listing page, product page etc and can be rendered on the server. But user profile, search results, cart or payment pages cannot be pre-built. They are completely loaded on the client. Usually these pages are also behind Auth.Completely Static Pages -> Sourced during build phase. Ideal for performance and SEO. Completely Dynamic Pages -> Loaded on client side on demand. Interactive for users. Distributed the load to client. Protected routes are used to block access to users.In the above example, product details would be pre-rendered markup where as ADD TO CART would be dynamic interaction on the client and the Cart page itself would be protected by auth, being completely dynamic.CaveatsThe approaches mentioned above come with a few caveats. Although they are not blockers, it's good to know them to know where this works and where it doesn't.Completely Dynamic AppsWhat about apps which are completely protected behind auth? Hasura Console is a good example of that; In fact all admin dashboards are protected behind Auth. Content can be different for users who have logged in. Obviously these pages cannot be built during compile time.There is a way to handle this. This approach requires you to build your app as an SPA (Single Page Application) where everything is loaded on the client or SSR (Server Side Rendering) where the server compiles HTML on every request. I didn't mention this approach earlier because this doesn't have any pre-built markup and is not the scope of this post ?Updating ContentIf data in the database changes, the app will not show the current data till it's rebuilt. So the option is to either trigger a rebuild whenever there is a data change or take advantage of hydration to update data dynamically on the client once the page is loaded.What about large sites? Build time increases with fairly large sites. Assume a content site with over thousands of posts. Rebuilding all the pages would be very slow. Deployments need to be instant. So how does one perform quick deployments? Incremental builds is the way forward. Here\u2019s a good read from Netlify. The idea is to cache the older content which doesn't require changing and rebuild only the pages which have been added recently so that deployments are instant. This solution may not exist for all frameworks today.SummaryJAMStack is a neat architecture to highlight the benefits of static sites that we were building a few years ago (and continue to). The idea behind Dynamic JAMStack is to get pre-built markup to the client as much as possible and do progressive enhancement on the client via Hydration to open up a lot of possibilities. Coupled with modern frontend frameworks and headless CMS, these static sites can become truly dynamic bringing in a whole lot of benefits and flexibility to build powerful apps. This way we build sites for performance.Of course these approaches may not be suitable for all types of apps. There are still use cases which requires an app to be completely SPA or completely SSR for obvious reasons and they continue to work well in their current architecture.Here are some good reads: Beyond Static: Building Dynamic Apps with GatsbyIsomorphic rendering on the JAMStackJAMStack for Indie Hackers"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Dynamic JAMStack with Gatsby and Hasura GraphQL Description: JAMStack is a modern architecture for building web apps. In this article, we will look at how to extend a static site to become dynamic using Hasura GraphQL Author: Praveen Durairaju"}, {"role": "assistant", "content": "JAMStack is a modern architecture for building web apps. The developer workflow is vastly improved in comparison to the traditional CMS based experience. With a JAMStack powered web app, you get faster performance due to pre-built markup, the ability to deliver scalable apps via CDN, clean separation of concerns and security between the app UI and the data from the API and finally easy deployments with automated builds and git integration.But what better than making it even more extensible and powerful by making the site dynamic, powered by Hasura's GraphQL APIs. In this article we will look at how a dynamic app can be built with all the benefits of JAMStack using Hasura and GraphQL.Getting StartedJAMStack doesn\u2019t have any opinion on the tooling used as long as the output is a good old static site. If you are a frontend developer looking to onboard the JAMStack workflow, you might be wondering if your popular framework supports it. There are many popular static site generators around modern frontend frameworks like React and Vue.In this post, we will look at the most popular React static site generator - Gatsby.Static Sites with GatsbyLet\u2019s create a new gatsby site and build a simple static site to list down articles.Install the gatsby CLI - npm install -g gatsby-cliCreate a new site - gatsby new gatsby-siteStart development - gatsby developNavigate to http://localhost:8000 to see the app running.Source data during the build phase, rebuild on data changesStatic site generators source data during the build phase and generates an entirely static website. Now that you have the site running, change any file in src/pages to see the rebuild happening for a live change. This is of course doing a live reload of the changes in a development setup.Let\u2019s generate production build to output HTML files.Execute the following commands:gatsby build && gatsby serveto build and serve the static site. You should see the page looking something like this:Gatsby generates HTML in build phase. So what happens underneath when you run gatsby build ? Gatsby traverses through every path of your app and compiles flat HTML files out of it. It uses specified data sources to compile the HTML. The site you ran above has static data written directly in HTML. Any change to data in the HTML would be rebuilt the next time you run the build command.Great! So any static content becomes HTML that can be served by a simple HTTP server. But this is not enough for most apps. Isn't it? Typically you would need data coming from a database, content specific to users or restricted behind a paywall or even interactive actions on the page with data processing. The data requirements quickly become dynamic.Adding Dynamic DataIn the above static site, what if the article list is coming from a database instead of hand writing each one of them? HTML pages have to be built for as many articles the site has. Static site generators like Gatsby come with the ability to connect to external data sources. GraphQL can be one of them.Gatsby comes with a native GraphQL support and lets you add any GraphQL API as a data source plugin among a list of external data sources. GraphQL APIs can also be explored using GraphiQL that runs at http://localhost:8000/___graphqlHow Hasura fits into Dynamic JAMStackHasura is an open-source engine that gives you realtime GraphQL APIs on new or existing Postgres databases, with built-in support for stitching custom GraphQL APIs.Hasura fits into JAMStack by being the data source for Gatsby via its unified GraphQL API endpoint. It also supports Authorization (which is not the scope of this post)Follow the instructions in the docs to deploy Hasura and create the table users and articles required for the app. Note the Heroku URL for GraphQL Endpoint. You will be configuring this in the Gatsby app next to fetch data. Now let\u2019s add the Hasura data source to get content from Postgres into Gatsby.We use the gatsby plugin gatsby-source-graphql to connect to any external GraphQL API. Do note that all static site generators would have some way to connect to external API for fetching data.In your gatsby-config.js, the plugin configuration would look like:{\n     resolve: 'gatsby-source-graphql',\n     options: {\n       typeName: 'HASURA',\n       fieldName: 'hasura',\n       url: `${ process.env.HASURA_GRAPHQL_URL }`,\n     },\n   },Now head to http://localhost:8000/___graphql to explore the hasura source with articles and users.In the app, head to src/pages/index.js and add the following GraphQL Query to fetch data from Postgres.export const query = graphql`\n  query ArticleQuery {\n    hasura {\n      articles {\n        id\n        title\n      }\n    }\n  }\n`Now change the listing of articles to:const IndexPage = ({data}) => (\n  <Layout>\n    <SEO title=\"Home\" />\n    <ul>\n    \t{data.hasura.articles.map((article) => (\n\t      <Link key={article.id} to={\"/\"+article.id}><li>{article.title}</li></Link>\n    \t))}\n    </ul>\n  </Layout>\n)This should now fetch articles from Postgres database with its id and title.Awesome! So the articles now come from the database and sourced at build time.Here's the source repo for code reference.Separate Static and Dynamic DataNow let\u2019s consider the case where the listing page has articles which cannot be accessed without authorization. Only once the user logs in, articles which are private can be accessed by them. This is a typical use case for a dynamic app where content is different for logged in users. There\u2019s also an account page which can be accessed only by users who have logged in. Obviously these pages cannot be built during compile time. We have two components to this:Setting up Auth (for example: Auth0)Configuring permissions for usersSetting up AuthThe first part is to setup Authentication for our app. Head to docs to setup Auth0. You can replace it with any Auth provider or write your own custom Auth. Once the Auth provider is in place, the gatsby implementation is all about restricting content on specific routes based on whether a user is authenticated or not.Protected RoutesThe gatsby-source-graphql setup configured earlier would work during build phase. If we need to make authenticated requests on the client side, we need to setup react-apollo on the client and set the appropriate headers before making the request. Here is an example of how to fetch articles dynamically on the client side. import React from 'react';\nimport { useQuery } from '@apollo/react-hooks';\nimport gql from 'graphql-tag';\n\n// This query is executed at run time by Apollo.\nconst APOLLO_QUERY = gql`\n  {\n    articles {\n      id\n      title\n      user {\n        id\n        name\n      }\n    }\n  }\n`;\n\nexport default ({token}) => {\n  const { loading, error, data } = useQuery(APOLLO_QUERY, {context: {headers: {'Authorization': 'Bearer ' + token}}});\n\n  return (\n      <div>\n        {loading && <p>Loading...</p>}\n        {error && <p>Error: ${error.message}</p>}\n        {data && data.articles && data.articles.map((article) => {\n          return (\n            <div key={article.id}>{article.id} - {article.title}</div>\n          );\n        }\n        )}\n      </div>\n  );\n};\nIf you try to access the articles page without logging in, you should be redirected to the login page. Now refresh the page to do a login and see if the articles are visible.The second piece is to setup permissions for the users. Let's look at how that can be done using Hasura.Hasura Role Based Access ControlHasura has role based access control where data fetching can be limited to specified conditions based on what role the logged in user is. Now this can be used to restrict content that can be accessed by users of the app.In the above app, we have an articles listing page which is protected using authentication. But now, let's restrict what content can be seen by each logged in user.Before that, we will setup a select permission for role user restricting the user id to be equal to session variable x-hasura-user-id. (This will be set in the JWT token)Head to the Hasura console to setup permissions like below.The permissions have been set. Now try inserting some values for different users.Refresh the page and you should be able to see only your own articles. This is because of the filter applied in the permissions for the role user.In the above app, we started off with a simple static page which lists out the articles and proceeded to integrate Hasura as a data source for Gatsby. We were able to fetch articles from the database. Then we were able to restrict the content generated based on which user was logged in.Here's the source repo for reference.DeploymentThe dynamic app is now ready! Let\u2019s see how it can be deployed. As you know by now, despite sourcing content from a database it is a static site with HTML files for each page. It can be deployed on various providers. But for the deployment provider to be JAMStack compliant, it needs to support Git automation where the source code lives in git and git push should trigger an automated build and deploy latest changes on the cloud.Popular choices are Netlify, Github Pages and Heroku while Cloudflare is the popular choice for CDN. I would recommend using Netlify for deploying any JAMStack site.Netlify DeploymentThe deployment experience on Netlify is seamless. It comes with its own CDN as a bonus. Netlify supports Git automation. Just connect your git repo for automated builds on Netlify. Other notable options are Heroku, Zeit Now. Static sites can also be hosted on AWS S3 with a CDN like Cloudfront.Note that faster the compiler of static site generator, quicker and better the deployment experience.Other static site generatorsIn case you are looking for the static site generators around React and Vue, here are a few options:The frameworks based on React include react-static (Hasura Guide), apart from Gatsby.The frameworks based on Vue include Nuxt.js (Hasura Guide) and Gridsome (Hasura Guide). Some of the other popular static site generators include Jekyll, Hugo, Hexo, VuePress. They don\u2019t leverage the power of frameworks or data source integrations but are still solid choices to get started."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Contributor Days is back again and in-person! Description: Joins us live in San Francisco to talk about using GraphQL with Microservices! Author: Rajoshi"}, {"role": "assistant", "content": "Photo by Christian Joudrey on UnsplashWe are partnering with This Dot for another special edition of GraphQL Contributor Days! This event will take place in San Francisco, CA on November 1st, 2019 -- right after GraphQL Summit! We have been hosting this event quarterly with those working closely with GraphQL. The purpose of the event is for the GraphQL community to come together and discuss some key topics in the GraphQL ecosystem, and facilitate cross collaboration within the community. The theme for the quarter is \u201cGraphQL with Microservices\u201d. Some topics we\u2019ll be discussing:Architecture and team structureChallenges, Solutions & RoITooling: Opportunities for new tools, examples of in-house toolingIf you\u2019re in the city for GraphQL Summit and would like to attend this event, do request for an invite on the event page :)Our featured guests include\u2026Tracy Lee: Founder,\u2008This\u2008Dot\u2008&\u2008RxJS\u2008Core\u2008TeamRajoshi Ghosh: Co-Founder, HasuraTanmai Gopal: Founder, HasuraIvan Goncharov: GraphQL consultant, APIs.guruDan Schafer: Software Engineer, FacebookSasha Solomon: Sr. Software Engineer, TwitterEve Porcello: Co-Owner, Moon HighwayAlex Banks: Instructor, Moon HighwayTim Griesser: Senior Software Engineer,\u2008CypressShruti Kapoor: Software Engineer,\u2008PaypalJim Gust: Sr. Technical Product Manager, ExpediaDan Boerner: Distinguished Program Manager, Expedia GroupPatrick Arminio: Senior Software Engineer, VerveZhifan Li: Senior Software Engineer, IntuitChristian Nwamba: Senior Cloud Developer Advocate, MicrosoftAnd more to be announced! If you\u2019re interested in attending, sign up for an invitation here. Have a topic you\u2019d like us to talk about? Leave us a comment below, and let us know!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Joins Hacktoberfest 2019! Description: Contribute to open source Hasura projects and win limited-edition swag from DigitalOcean and Hasura Author: Harsha"}, {"role": "assistant", "content": "DigitalOcean is back for another round of Hacktoberfest! This time they\u2019ve teamed up with Dev.to for their giant, month long celebration of open source software. We\u2019re excited to announce that Hasura is taking part in Hacktoberfest for the 2nd year in a row! So what is Hacktoberfest?6 years ago, DigitalOcean started Hacktoberfest as a way to celebrate open source projects. They organized challenges that encouraged developers all over the world to contribute to any open source project they felt like. This has been a huge win for the entire open source community. Each year Hacktoberfest helps thousands of developers discover interesting projects to contribute to and helps thousands of open source projects (like us) get discovered! \u00a0It\u2019s rare to see such a large, successful company have such a massive grassroots level impact. They\u2019ve even got an independent category for projects focused on climate change this year! We want to give a huge shout out to DigitalOcean for everything they\u2019ve done and to Dev.to for partnering with them on this project.We are also very happy that we get to play a part in this! via GIPHYThe Hasura Hacktoberfest challenge We\u2019ve got our own challenge for Hacktoberfest 2019 :You can open a PR for any of our open source projects (mentioned below) and we will send you a sticker pack.If the PR you submit for any of the issues in our 2 repos tagged with the label Hacktoberfest gets merged, you\u2019ll win a limited edition (designs coming soon!) Hacktobefest t-shirt along with a sticker pack! Our open source projects We\u2019ve got 2 open source repos that you can contribute to as part of this year\u2019s challenge:The Hasura GraphQL Engine: The Hasura GraphQL Engine is a GraphQL server that gives you instant, realtime GraphQL APIs over Postgres, with webhook triggers on database events for asynchronous business logic.The GraphQL Tutorial Series: This is a community maintained tutorial series that introduces developers to the basics of GraphQL and will teach them how to build a realtime application in 2 hours.Contributing to any of these repos will count towards the Hacktoberfest challenge.Getting startedReady to dive in? Here\u2019s how to contribute:Make sure you register! The first thing you need to do is sign up for Hacktoberfest before 31st Oct. Check out our Contributing Guide: Jumping into a new project can be a little confusing at first. Our Contributing Guide will help you get oriented with the Hasura GraphQL Engine. We are working on a contributing guide for the GraphQL Tutorials and will announce it on Twitter as soon as it\u2019s out. Contributing to a Hasura Challenge issue: To qualify for the Hasura challenge, you need a submit a PR for a Hasura GraphQL Engine or GraphQL Tutorial issue tagged with the label Hacktoberfest. You can find a list of Hasura Challenge issues for the \u00a0Hasura GraphQL Engine here. We will add a list of Hasura Challenge issues for the GraphQL tutorials soon.We will keep adding issues to these lists. Keep an eye on our Twitter to know more. We want to help you win! This year, Hacktoberfest expects over 150,000 participants! With only the first 50,000 participant to complete the challenge being eligible for the limited-edition shirt, it\u2019s a race against the clock to win the challenge! We want to help as many contributors win the Hacktoberfest challenge as well as the Hasura challenge. If you have any doubts or questions, reach out to us on our Discord server!via GIPHYFollow us on Twitter for updates on new issues & other hacktoberfest announcements (like the new tshirt design :))Happy hacking! "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a serverless Nuxt app using GraphQL and Hasura Remote Joins Description: How to build a serverless Nuxt app using GraphQL, Hasura Remote Joins with Azure functions and Cloudinary for image transformations Author: Hasura"}, {"role": "assistant", "content": "This tutorial was written by Travis Reynolds and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.In this guide, we will build a simple Instagram-style Nuxt app, where users can upload images, like and also add comments to other people's images. Images will be stored on an API, and we will use Hasura Remote Joins to transform image URLs so we can query the exact-sized images we want from the database, instead of having to transform the URL client side with a dedicated library.This post is a part of our Remote Joins series. Remote Joins in Hasura allows you to join data across tables and remote data sources. Data Federation using Hasura Remote Joins is now available starting from the v1.3 beta release. Jump on our discord or comment on github and let us know what you think!PrerequisitesWe will be using a pre-built Nuxt app as the frontend, so some experience with VueJS and SSR frameworks will be helpful - although not necessary. For the backend, we will use:Hasura for GraphQL APIs & Remote JoinsHerokuAzure Functions to create a remote schema and events handler, Auth0 for authenticationCloudinary to host our images. Let's get started!Firstly, download the sample repository. In this, we have a couple of folders:Migrations - We will use these to setup the tables for our databaseFunctions - The Function app we will deploy to AzureUncover - The Nuxt frontend appSetting up the Database We will be using Heroku to deploy a Hasura instance. There is already a nice quick start guide on deploying to Heroku - Follow this, but we need to change one thing. Currently, the Remote Joins feature is in beta, so we will need to use the Docker image of the beta release. As of this writing, it is hasura/graphql-engine:v1.3.0. Make a note of this, then edit the Dockerfile FROM value to the PR version.FROM hasura/graphql-engine:v1.3.0We can now deploy this to Heroku - follow the rest of the quick start guide.Once Hasura is deployed, follow this next article on securing the Hasura console. Nice work! We can now visit our app at <app-name>.herokuapp.com/console. My app is called Uncover App, so I can reach the console at https://uncover-app.herokuapp.com/console. Go ahead and login with your admin secret (from the quick start guide above).The Hasura consoleWe need to add one more environment variable to Heroku: this will decide what role any unauthorized users will be given - so we can add permissions accordingly:HASURA_GRAPHQL_UNAUTHORIZED_ROLE = publicNow we need to apply the migrations to Hasura. As we are using the PR docker image, we will need to download the Hasura CLI for that version. Setup Hasura CLI referring to instructions from docs.I am using Linux, and I have downloaded the CLI to my uncover folder.Now we can apply these migrations to Hasura - simply run the following command, replacing your endpoint and admin secret:./cli-hasura-linux-amd64 migrate apply --endpoint <your-endpoint> --admin-secret <your-secret>This command will take a few seconds to run, and once it is done, you should be able to see all the tables needed for our app in the Hasura console.Setting Up Auth0Next we need to setup Auth0 to manage authentication - refer to the official guide to configure this. Add http://localhost:3000/login/callback as the Allowed Callback URLs and Allowed Web Origins.Make sure to note the created application ID and Domain - we will need these later.CloudinaryWe will be using Cloudinary to host our images, and to run transformations on them. Visit their website to create an account. Once you are logged in, make a note of your cloud name.We will need to create an upload preset to allow unsigned uploads from our app, so click on the settings icon in the navbar. Now select the Upload tab, and scroll down to the Upload presets: tab. Click Enable unsigned uploading, and make a note of the name of the preset that has just been created - it's mode will be Unsigned.We can now start setting up our functions app, which will run our 'remote' GraphQL server.I will be using Azure + VSCode for this, however you can use Zeit Now, AWS Lambda, etc.Azure SetupIf you already have an active account and subscription with Azure, you can skip these next steps.Visit https://azure.microsoft.com/en-gb/free, and click Start for free. Azure gives you a number of services for free with an active subscription, including 1 million function executions a month - which will be more than enough for our demo app.You can either create a new Microsoft account, or login to an existing one. I will create a new account for this. Just follow the few steps to add an email address, password, and email verification.At the next step, you will continue to setup your free account. Fill out the necessary details, then you will need to add a phone number, and a card for verification - Don't worry, you won't be charged at all for this demo app. You won\u2019t be charged unless you upgrade.Agree to the T&C's (after carefully reading them through, of course...) and click Sign up. On the next screen, Click Go to the Portal, and you should be greeted with a screen like this:We can leave this for now, we will be managing our Function app from VS Code.Functions CLITo run our function app locally, we will need to install the 'Core Tools' packages. Instruction for this can be found here.Once installed, open Code, and head to the extensions page. Search for Azure Functions ms-azuretools.vscode-azurefunctions and install this package. You will notice that another extension that this package requires is also installed, Azure Account.Once these have finished installing, press F1, and type Sign In. You should see an option called Azure: Sign In - navigate to this, and press enter. A browser window will open, asking you to authenticate with an Azure account. Choose the account we have just setup, and you will be redirected to a page confirming the sign in.Great, we are all set up! Now we can deploy our Function app.Open Code in a folder which will hold the function, and select the Azure icon in the left hand menu.As yet, we haven't added any subscriptions for the extension to use. Click Select Subscriptions, and choose a subscription to show. Mine was called Free Trial, so I selected that, and clicked OK.You will see your subscription shown in the side panel now. Right click on that, and select Create Function app in Azure. In the popup, name your Function app (note this will also be part of your URL), and select a location. Hit enter, and the extension will set up a complete new Function app for you, configuring all the resource groups/storage accounts etc necessary. You will have to wait a few minutes for the setup to complete, but when it is done, you can deploy your app.If you want to configure this manually, you can select the Create Function app in Azure (Advanced) create option.As the functions are built with Webpack, we need to run a build command before deploying. But before that, we need to set some ENVs. Create a new file called local.settings.json, and paste the following contents into it:{\n  \"IsEncrypted\": false,\n  \"Values\": {\n    \"CLOUDINARY_NAME\": \"<unsigned upload preset name>\"\n  }\n}Open a terminal, cd into our functions folder, and run the below commands:cd functions\nyarn # Or npm i\nyarn build # Or npm run buildNow you will see an index.js in our root graphql folder.Open the Azure sidebar again, and select the blue upload arrow in the Functions section. Once you have clicked this, a popup will appear, asking what folder we want to deploy. Select the functions folder, then select the Functions app you have just created.Click Deploy in the next confirmation box, and our app will start to deploy our functions.Once this process has finished, a notification will appear confirming a successful deploy. Make sure to click on View Output, and copy down the Http Trigger URL that is shown - this is the URL to our GraphQL server. Next, we need to upload our local settings (ENVs) to our Function app. Tap F1, and type in Azure Functions: Upload LocalSettings. Hit enter, and make sure to choose the local.settings.json from our functions folder. Select our function app, and press enter. Now the extension will add the local settings to the remote Function app.Remote JoinVisit your Hasura Console, and click on the Remote Schemas tab. Click Add, and type in a name for your remote schema - GraphQL will do. Then paste your Function app URL (https://<app-name>.azurewebsites.net/graphql) into the URL input. Now we can click Add Remote Schema, and we have joined our two GraphQL schemas!Next, click on the data tab. Browse to the images table, and select the Relationships tab. At the bottom of the page, there is a section called Remote Relationships. Here comes the fun part...Click Add, name the relationship transform_url, and select our GraphQL remote schema.Now we have to configure it. We want to use the transform_image_url resolver, so select that. Now we need to pass it our url to transform. To do so, find the url argument, and select it. Make sure the columns that appear are set to From Column and url - then click save.Here, we are telling Hasura that we want to use the url column from our images table as an argument for url on the remote schema.Nuxt FrontendNext step is to run our Nuxt app.Open the uncover directory in VS Code, and add a new .env file at the root of the project.You will need to add the below values:APP_URL=http://localhost:3000AUTH0_CLIENT_DOMAIN=<auth0-application-domain>\nAUTH0_CLIENT_ID=<auth0-application-id>\nAPOLLO_ENDPOINT=<hasura-endpoint (https://<app-name>.herokuapp.com/v1/graphql)>\nAPOLLO_WSS_ENDPOINT=<hasura-wss-endpoint (wss://<app-name>.herokuapp.com/v1/graphql)>\nCLOUDINARY_PRESET=<unsigned-upload-preset>\nCLOUDINARY_NAME=<account-name>Now we can run the app!cd uncover\nyarn # or npm i\nyarn develop # or npm run developYou can view the app on http://localhost:3000.The GraphQL query that is used to get data from the remote join looks something like this:query AllImages {\n  images {\n    id\n    created_at\n    title\n \tdescription\n    altText\n    placeholder: transform_url (width: 300, height: 200, quality: 20, gravity: \"center\", crop: \"fill\") # Remote Join Field\n    card: transform_url (width: 600, height: 400, gravity: \"center\", crop: \"fill\")  # Remote Join Field\n    full: transform_url (width: 1200, height: 800, gravity: \"center\", crop: \"fill\")  # Remote Join Field\n    user {\n      profile {\n        username\n        last_name\n        first_name\n      }\n    }\n  }\n}\nWe have a few different queries here. Firstly, we are querying for all images in the database. Then, we fetch data that we will use in our app - for example, the title, description, the user it belongs to, and their profile. Then we query the transform_url field multiple times.Firstly, we query it to get a placeholder image url - this is a small and low quality image that can be loaded quickly. It will be shown first as the higher quality image loads (For example, like Medium.com images). Then we query for the card image - this is a higher quality image that will be shown in the timeline. Finally, we have the full image. This will be used in the lightbox when a user clicks on an image.SummaryRemote Joins is very useful here, as we can query it multiple times under different aliases to get different transformed urls, using a serverless function to do the heavy-lifting. If we did not use a remote join, we would have firstly had to install the Image CDN specific client library, which are often very large - for instance the vue-cloudinary package will add ~22kb to our Nuxt app. Then when we had queried the image url, we would need to go through each image object, and use the client library to produce the transform_urls each time we view an image - this can take valuable load time.Instead, we immediately have all the information needed to start showing images.About the authorTravis Reynolds is a freelance frontend engineer from Portsmouth, UK, but he often tinkers around with NodeJS and GraphQL as well - he likes to learn new things that may help him in future projects. You can check out his infrequently-updated blog at travisreynolds.dev."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building an Instagram clone in React with GraphQL and Hasura - Part 2 Description: A detailed tutorial on how to build and Instagram clone using React.js, GraphQL and Hasura - Part 2. Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Abhijeet Singh and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.In part-1 of this series, we setup our backend and Auth0. In this part, we will setup our React app and connect it to our backend. React App SetupWe will firstly implement user authentication. We will be using JWT (JSON \u00a0web tokens) for authentication. Let\u2019s first create some basic header in our react app for showing login button.Replace the contents of styles/App.css file with this file. These styles will be used throughout our app so you don\u2019t have to worry about the styling. Also download this file and place it in your styles/ directory. We will use this to show various buttons within our app.Setup Apollo GraphQL ClientReplace contents of App.js to use Apollo GraphQL client as shown below. (See apollo github repository for more help)In line 13 change the uri to your GraphQL Endpoint on hasura, which you can find on hasura console (remember where you created tables). Here we have imported the header component which we will implement now.Create header component and use react-routes:We will use react-router to implement single-page application behaviour. Install react-router using:$ npm install react-router-domReact \u00a0Router, and dynamic, client-side routing, allows us to build a \u00a0single-page web application with navigation without the page refreshing \u00a0as the user navigates. React Router uses component structure to call \u00a0components, which display the appropriate information.By \u00a0preventing a page refresh, and using Router or Link, the flash of a \u00a0white screen or blank page is prevented. This is one increasingly common \u00a0way of having a more seamless user experience. (source)For using react-router in our app, we have to wrap whole app in BrowserRouter It is a context provider for routing, which provides several props necessary for routing (like match, location, history). See this if you are unfamiliar with context. Replace the contents of index.js:Next, we will create a Header component for navigation within app. Create a Header.js file in components directory. The contents of Header.js should be:Here we are creating a navbar similar to Instagram navbar. Later we will add some routes to it for navigation. That\u2019s it! We have successfully created a header navbar and used react-routes in our app.Right now your app should look like thisAuth0 JWT integration with React AppFollow along with Auth0-react-quickstart guide as reference to include Auth0 in react app. Configure Auth0 client by setting Allowed Callback URLs, Allowed Web Origins, Allowed Logout URLs to http://localhost:3000 and add the custom API if you haven\u2019t done already. Now install auth0-spa-js :$ npm install @auth0/auth0-spa-jsNow we will include react-auth0-wrapper in our app, which is a set of custom react-hooks that enable you to work with the Auth0 SDK. Create a new directory src/auth and add file react-auth0-wrapper.js populate it with code from here.Now add another file as auth/auth_config.json in src/auth . Populate auth_config.json with following code (change the values accordingly):Now we are ready to include login functionality in our react app. Basically, we will be including a login button in header. This button will lead to login through Auth0 with redirect to our localhost once login/signup is completed. At the same time login/signup data will be updated in our User table in hasura backend due to the Auth0 rules we added earlier. Once the login is done, we will get the accessToken in JWT format using functions provided by Auth0 SDK in App.js . This accessToken will then be used as a authorization header in apollo client queries to \u00a0backend, thus every query which goes to backend will have authorization \u00a0header.Firstly, change the contents of index.js to the following:Here, we are using the Auth0Provider which is a context provider for Auth0 client. Any children components will now have access to the Auth0 client.Having provided the Auth0 client to our app, we now replace the contents of components/Header.js file to the following:We are using useAuth0 hook (line 7) to make use of various functions provided by Auth0. isAuthenticated is used to check if user is logged in or not. loginWithRedirect is used to login and redirect after login to specified redirect-url. user object has information about the current logged in user.Here, \u00a0if the user is logged in, we will take user to user-profile, which we \u00a0will implement later. If the user is logged out, we will just show login \u00a0button.Now we will make changes in our App.js to include Auth0 functionality. Change the contents of App.js to the following:We are using useState hook(line 26) to set initial accessToken value to empty string. If the user is logged in, the token is fetched from the Auth0 SDK client using getTokenSilently() (line 35). Notice that this function returns a Promise and is asynchronous. This function attempts to return the current access token. If the token is invalid, the token is refreshed silently before being returned from the function. If thetry block successfully gets executed, accessToken value is set to the JWT access-token from Auth0 (line 36).The component re-renders when we get accessToken value. Thus after the async function has finished executing, we store the value of accessToken in state. The component re-renders and apollo-client gets the token value, thus re-rendering the whole ApolloProvider (context-provider) with new token value and the authentication header.Once we have accessToken, we will use this to make requests to our backend using apollo client. See apollo-docs for apollo authentication using headers. Basically, we are passing the accessToken as authorization header(line 52), in our apollo queries. This client is then used inside the ApolloProvider(context provider) to provide the child elements access to apollo client created here.Now, \u00a0you should be able to login in our app. Clear cache and login. You must \u00a0be asked to give access to your auth0 tenant by our hasura backend. \u00a0Give the access and you\u2019re good to go.Note : If you are facing errors, remember to not keep any dependency on react-apollo . @apollo/react-hooks must be used instead.Now our app looks like thisImplementing Feed and Likes(realtime updates of Likes)We will implement a list of posts (feed) and a like button. Create a new component components/Feed.js as:POSTS_LIST query(line 8) is being used to fetch details from Post table in our database. We are querying the id of the post.useQuery (line 18) is a custom apollo-client react hook. We get the query data in data object (line 18) which is then passed as a prop to the Post component, which we will implement now.Create a new component components/Post.js as:Here, we are getting the props passed by Feed.js component and using the id prop, we are getting the complete post data using POST_INFO query. We are then rendering the data with styling in return statement. We are using function timeDifferenceForDate (line 68) for converting post.created_at to instagram style time. Now we need to implement this function. We are \u00a0also importing Like component which takes care of like functionality, \u00a0which we will implement later. Create a new directory src/utils and create a new file TimeDifference.js as:It is just a utility function to convert the date-time data into our required format.Now we will implement the Like component. Create a new file components/Like.js as:Like components gets the post_id through props. Here we are writing two mutations and one query. FETCH_LIKES is used to fetch the number of likes from Post table. Also we are fetching whether the currently logged in user has already liked the post (line 15). LIKE_POST and DELETE_LIKE are used to insert a like in Like table and delete from Like table respectively.We are storing countLikes (number of likes) and liked (if the user like the post) in state variables. As the state changes, \u00a0the Like component re-renders which gives us updated view if user likes \u00a0the post. If the user likes the post, we are showing a red heart, \u00a0otherwise a white heart in UI. To implement this, we are checking value \u00a0of liked (line 104) and rendering \u00a0buttons accordingly. As the user likes the post, state changes (line \u00a0109), component re-renders, and like mutation occurs (line 108) which \u00a0records the like in database, and number of likes is increased (line \u00a0110). We have two mutations, submitting the like (line 58) and deleting the like(line 69). Both mutations uses refetchQueries argument (line 60) which is used to refetch the query FETCH_LIKES, thus updating the apollo cache with new values. This implements real-time likes.We now have all the components in place to implement post feed. We need to change App.js to include Feed.js. Make following changes in your App.js:Switch is a part of react-router which is used to match components with their \u00a0paths. Insert some random data (posts) from Hasura Console in Post table try the app.Now our app looks like thisTry liking posts, and see the real-time updates in likes, thanks to refetchQueries . We haven\u2019t yet implemented the user-profile, so the user profile links won\u2019t work. Next we will implement the same.Implementing User ProfileOur user profile will have instagram style UI with user information on top and grid of posts uploaded by user at bottom. We will implement profile in two components, one will take care of rendering the main UI and the \u00a0other will handle follow functionality.Create a new component components/Profile.js as: We \u00a0have three different queries, which will fetch all the basic data of \u00a0user to be displayed. Notice that we could have called all the queries \u00a0in one go, but while refetching the queries in case of follow mutation, \u00a0we will have to refetch all the data to update cache, but only follow \u00a0data would have changed. Thus we have made two separate queries for NUMBER_OF_FOLLOWERS(line 41) \u00a0and NUMBER_OF_FOLLOWING(line 31). We have exported these queries, thus while implementing Follow component, we will be able to import and refetch the queries. This will \u00a0become more clear once we start implementing follow functionality.We are getting user_id as props which will be used to query our backend database for user info, for the given user_id. The data is then rendered in return() . The props (user_id) here is being passed in form of url, and we are using props.match.params.id to get that prop. These props are provided by the react-router BrowserRouter context provider, which is included in our index.js file.Query USER_INFO is used to fetch data from \u00a0table User and Post. \u00a0In line 103, we are checking if currently displayed profile is same as \u00a0the user currently logged-in. In that case we will show a Logout button. \u00a0If the profile is of other users, we will show a Follow button instead. \u00a0isLoggedUser function is used to check this condition. Follow button is implemented in Follow component which we will implement next. Also we are using react-bootstrap rows to implement posts grid at the bottom of user profile, with three \u00a0items per row (line 147). Each post item in the grid is a clickable link \u00a0which leads to the respective post. Here, we are passing id as props through the url (to={\u201c/post/\u201d + post.id}) in line 148, which is accessed via props.match.params.id in the receiving component. This is a react-router way of passing prop. See this example for more details.Now, we will implement Follow component. Create a new file component/Follow.js as: This is identical to Like component. It is being fetched whether the currently logged in user follows the currently rendered profile using FETCH_FOLLWERS query. If data returned by FETCH_FOLLWERS is not empty, we will initially set followed state to true (line 115). Here, we are using a state followed(line 49) to check whether current user follows the displayed profile and a ref variable firstRun(line \u00a052) which checks if the component is being rendered for the first time, \u00a0which is useful as we want to do certain operations(line 112) on first \u00a0time rendering of the component only, like setting the state followed to true or false initially depending on data returned from query FETCH_FOLLWERS. Also we are using two mutations FOLLOW_USER and UNFOLLOW_USER which are inserting and deleting data from Follow table in our backend. Notice that both these mutations refetch three \u00a0queries (line 66) in order to update apollo cache with correct data \u00a0after the mutation. This automatically implements real-time data \u00a0updates, where as soon as the mutation is performed, the number of \u00a0followers of the displayed profile, and the number of following of the \u00a0logged-in user updates.Now, we will make the required changes in App.js. But firstly create a new file as components/SecuredRoute.js as:This \u00a0will help us to create some secured routes which can only be accessed \u00a0if the user is logged-in. We will use secured routes while routing. \u00a0Using secured route, if someone tries to access the url\u2019s without \u00a0logging-in, user will be redirected to login automatically.Now make the following changes in App.js :Now, you should be able to visit user profiles. Insert some sample data \u00a0from Hasura console, and see the user profiles and follow functionality. \u00a0See the real-time update in follow functionality.User profile looks like thisImplementing Submit Post functionalityCreate a new file components/Upload.js as: SUBMIT_POST mutation is used to make a entry in our database table Post. We are using react-bootstrap modal to show a popup box to enter values of url and caption. Currently, image uploading is not supported, as we are not implementing any storage service to store images.We have a form (line 48) which has two input fields for caption and url. We are using react state to store values of caption, url and error (if mutation is not successful). If the form is submitted, submitPost mutation is called which changes the data and refetchQueries updates data in apollo cache for queries POST_LIST and USER_INFO thus updating the feed and user profile respectively.Now we will do the required changes in App.js :If the user is authenticated, we will show a upload button which will open the following popup when clicked:Finally, \u00a0we have our app ready with upload post functionality. You can navigate \u00a0to user-profiles, create new posts and see real-time updates of \u00a0new-posts, likes and follows.You should now have a working Instagram clone. Incase you'd like to reference it, the final code for this app is hosted here. See live demo of the app here.Acknowledgements :TimeDifference function: https://github.com/howtographql/react-apolloFew Styles taken from : https://pusher.com/tutorials/instagram-clone-part-1About the author Abhijeet Singh is final year UG student in Computer \u00a0Science and Engineering from IIIT Kalyani. He has done work in Full \u00a0Stack Development, Android, Deep Learning, Machine Learning and NLP. He actively takes part in competitive programming contests and has interest \u00a0in solving algorithmic problems. He is a startup enthusiast and plays \u00a0table tennis and guitar in spare time."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building an Instagram clone in React with GraphQL and Hasura - Part I Description: A detailed tutorial on how to build and Instagram clone using ReactJS, GraphQL and Hasura - Part 1. Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Abhijeet Singh and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.IntroductionIn this tutorial I will show you how to use Hasura GraphQL Engine and React to build a clone of the Instagram web app. The final code is hosted here. See live demo of the app here.The app will have the following features:Users can create account (using Auth0 authentication)Authenticated users can upload new postsAuthenticated users can like postsAuthenticated users can follow user accountsDisplay posts in feedRealtime \u00a0updates when other users upvote a post, create a new post or follow \u00a0user profile (using react state management and updating apollo cache)Initial SetupLet's start by building the frontend of our app using React.To get started quickly, we will create a basic react app with no build configuration using create-react-app. Run the following commands in terminal:$ npx create-react-app instagram-clone\n$ cd instagram-clone\n$ npm startNote : npx is a npm package runner. The typical use is to download and run a package temporarily or for trials. create-react-app is an npm package that is expected to be run only once in a project\u2019s lifecycle. Hence, it is preferred to use npx to install and run it in a single step. (source)Server will start and you should be able to see a welcome to react screen (like below) on http://localhost:3000.To improve the project structure, let\u2019s create two directories, both inside the src folder. The first is called components and will hold all our React components. Second one styles, that is for all the CSS files you\u2019ll use.Move the required jsx and css files to appropriate directories and change the references to these files in both index.js and App.js accordingly. Your directory structure should look like this:\u251c\u2500\u2500 node_modules\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 public\n\u2502 \u251c\u2500\u2500 favicon.ico\n\u2502 \u251c\u2500\u2500 index.html\n\u2502 \u2514\u2500\u2500 manifest.json\n\u251c\u2500\u2500 src\n\u2502 \u251c\u2500\u2500 App.test.js\n\u2502 \u251c\u2500\u2500 components\n\u2502 \u2502 \u2514\u2500\u2500 App.js\n\u2502 \u251c\u2500\u2500 index.js\n\u2502 \u251c\u2500\u2500 logo.svg\n\u2502 \u251c\u2500\u2500 serviceWorker.js\n\u2502 \u2514\u2500\u2500 styles\n\u2502   \u251c\u2500\u2500 App.css\n\u2502   \u2514\u2500\u2500 index.css\n\u2514\u2500\u2500 README.mdI\u2019ll be using react-bootstrap package for the styling. Install the package using:$ npm install react-bootstrap bootstrapMake sure to add the CDN links to stylesheets in /public/index.html. Copy the links from here.Also install react-apollo GraphQL Client using the following command:$ npm install apollo-boost apollo-link-context @apollo/react-hooks @apollo/react-hoc graphqlApollo Client helps you consume a GraphQL API from the front-end.Next we will setup the backend using the Hasura GraphQL Engine and Postgres. See this guide to quickly get started with Hasura Cloud, which gives you a fully managed, production ready GraphQL API as a service. Follow the steps to create a free database with Heroku.Setting up the backendOur Postgres Database structure :type Post {\n    id - integer, primary key\n    caption - text\n    url - text\n    created_at - timestamp with time zone\n    user_id - text\n}\n \ntype User {\n    name - text\n    last_seen - timestamp with time zone\n    avatar - text\n    email - text\n    id - text, primary key\n}\n\ntype Like {\n    id - integer, primary key\n    user_id - text\n    post_id - integer\n}\n\ntype Follow {\n    id - integer, primary key\n     follower_id - text\n    following_id - text\n}Database StructureCreating tablesCreate the following tables : User tableuser tablePost table (id has type integer(autoincrement)):post tableAdd the foreign key constraint from user_id column to User.id . Set Update and Delete Violation as restrict.Like table (id has type integer(autoincrement)):like tableFollow table (id has type integer(autoincrement)):follow tableCreate an object relationship by \u00a0jumping to relationship section of tables. These relationships will help us to query data in a single query and also perform various kinds of \u00a0aggregate functions like getting number of posts for every user. See this link for creating relationships through foreign key in hasura. We just need the following relationships.user tablepost tableNow we will set up the permissions for the tables, so that only logged \u00a0in users can do certain things. Set up the permissions as shown below:post tableLike tableFollow tableSee access control basics if you are not much familiar with access control and how it works in hasura.To make it less complicated, I have given all the row level permissions as without any checks and column level as all columns and aggregation queries (used to count number of points for a post).Using Auth0 JWT for AuthenticationThe following figure quickly explains how JWT authentication works. Here, Application Server (our backend) needs to be configured to verify that the incoming JWT are created by the authentication server (Auth0).We are using Auth0 as authentication server. It will create a JWT and pass to our react app. Then, using apollo client, we will send the auth_token(access_token in our case) to our backend (application server), i.e. Hasura GraphQL \u00a0Engine. Hasura is needed to be configured to verify that incoming JWT is created by Auth0 authentication server.Let's get started with Auth0, using @apollo/react-hooks and @auth0/auth0-spa-js. We have to follow these steps:Auth0 JWT integration with Hasura : See this guide for Auth0 JWT Integration with Hasura and get the JWK (key for validation of incoming JWT). In Auth0 dashboard, set Allowed Callback URLs, Allowed Web Origins, Allowed Logout URLs to http://localhost:3000 and save changes.You may also need to disable OIDC Conformant in Auth0 > Applications > Your_app > Settings > Advanced Settings. \u00a0Once you have the key, next step is to activate the JWT mode in Hasura. \u00a0Go to your heroku dashboard, choose our app and go to settings. Here \u00a0you can see some Config Varswhich we will set-up now. Add the config vars as following:Here, HASURA_GRAPHQL_ADMIN_SECRET key for the JWT mode to work. Put the key in HASURA_GRAPHQL_JWT_SECRETenvironment variable. Once you have added this, the GraphQL endpoints can only be queried using Authorization header or X-Hasura-Admin-Secret header.Finally, set HASURA_GRAPHQL_UNAUTHORIZED_ROLE to anonymous because we do allow unauthenticated users to read some data.Unauthorized role: used when access-key is not sent in access-key only mode or \u201cAuthorization\u201d header is absent in JWT mode. Example: anonymous. Now whenever \u201cAuthorization\u201d header is absent, request\u2019s role will default to \u201canonymous\u201d.Next step is to add rules in our Auth0 app. We will add 2 more rules to Auth0 app as shown below:Here we are replacing idToken with accessToken . Basically, auth0 provides different types of tokens and the auth0-spa-js does not expose the idToken anymore, so we will use accessToken.Update: From version 1.2.0 of auth0-spa-js, the raw value of idToken is exposed using getIdTokenClaims method.Read more about tokens here.The new SPA SDK only uses Authorization Code Grant + PKCE (not Implicit Grant anymore) Thus we have to use accessTokens in place of idTokens.(See this thread).Add one more rule for keeping our postgres database and Auth0 users in sync:Change admin_secret and url accordingly. Now, whenever a new user signs up or a user logs in, our \u00a0hasura postgres database will be updated automatically to store the user \u00a0information in our users table. See this for more details.Note: Here we are just doing a post request with graphql mutations, and we have x-hasura-admin-secret in header, thus the incoming request have full access to mutate the database.We also need to create a custom API in Auth0 Dashboard > APIs which will point to our Hasura GraphQl API. Create a new api as shown below and give it any name. Change Identifier with your graphql endpoint.Now that we have got our backend and Auth0 setup, we can get started on building out the React app. Head over to the 2nd part of this series to learn how. About the author Abhijeet Singh is final year UG student in Computer \u00a0Science and Engineering from IIIT Kalyani. He has done work in Full \u00a0Stack Development, Android, Deep Learning, Machine Learning and NLP. He \u00a0actively takes part in competitive programming contests and has interest \u00a0in solving algorithmic problems. He is a startup enthusiast and plays \u00a0table tennis and guitar in spare time."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building scalable Flutter apps using GraphQL, Hasura and event-driven serverless, Part 3 - building the Flutter client Description: A guide to building scalable Flutter apps using GraphQL, Hasura and event-driven serverless, Part 3 -  Building the Flutter Client Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Can Ta\u015fp\u0131nar and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.IntroductionThis is the third part in the \"Building a quiz app with Hasura & Firebase\". In this final part, we will create the Flutter frontend. Part 1 covered how to deploy Hasura and how to model the relational data with permissions. Part 2, covered how Auth and Business Logic can be handled by combining Hasura and Firebase services.Also, we've written a guide to event-driven programming that may be useful to go through.To keep things simple, we are not going to cover all the Flutter code. Instead we are going to look into important parts. You can find the finished project on GitHub.Making the flutter frontendFollowing dependencies are needed in pubspec.yaml file. dependencies:\n  flutter:\n    sdk: flutter\n  graphql_flutter: ^1.0.0\n  firebase_auth: ^0.11.1+11\n  cloud_functions: ^0.4.1Flutter app needs a setup to connect your Firebase project. You can follow this guide. The App consists of 6 screens. MainLoginSignUpPlayResultScoresMainThis is the entry point of the app. First thing to do is to check if there is a signed in user. In case there is no user, app navigates to login screen. From there you can sign in or go to SignUp page to register with the custom Firebase function we have created earlier. After a successful sign up, the user is signed in and navigated to the AuthCheck again.Once there is a user logged in, MainApp widget is returned with user and token. In MainApp, we prepare GraphQL Client and wrap the MaterialApp with GraphQL Provider. Don\u2019t forget to change the links below with your GraphQL endpoint.We are now in Menu. Once the user hits play button, app is navigated to the Play page. In this page we have a Query widget that makes a query to our custom Postgres function to get the question that hasn\u2019t been answered by the user.Another important part in this page is the addAnswer method. Once the user taps on an answer option for the question, this method is called to insert to answer to theuser_answers \u00a0table.Once the insert operation is finished, user is navigated to the Result page. This page shows whether the answer is correct or incorrect and shows the correct one in either case. User can continue to answer other questions by tapping on next button. If user has answered all questions, app notifies the user. Apart from that, remember inserting an answer on user_answers table triggers the functions we have deployed earlier in the event section. If the answer is correct, user gets 10 points.Lastly, let\u2019s have a look on Scores page. In this page, we are getting the user score date real time thanks to the subscription feature. There is a subscription widget similar to the Query widget we have used in Play page earlier. That's it! We have created a simple Flutter quiz app powered by Hasura and Firebase. We have seen how Firebase services like Authentication and Cloud Functions can be used with Hasura to handle user authentication and business logic.About the AuthorCan Ta\u015fp\u0131nar is passionate and enthusiastic developer whose journey started with the question \u201cHow to make a video game?\u201dMoving on with mobile application development and Full Stack Web development. Enjoys working with modern and open source technologies."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building scalable Flutter apps using GraphQL, Hasura and event-driven serverless, Part 2 - Setting up Auth Description: A guide to building scalable Flutter apps using GraphQL, Hasura and event-driven serverless, Part 2-  Setting up Auth Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Can Ta\u015fp\u0131nar and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.IntroductionThis is the second part in the series on \"Building scalable Flutter apps using Hasura, GraphQL, and event-driven serverless\". This part will cover how Hasura can work with Firebase services to provide Auth and Business Logic.Part 1 covers how to deploy Hasura and how to model the relational data with permissions. Part 3 covers how to create the Flutter frontend.Also, we've written a guide to event-driven programming that may be useful to go through.AuthenticationSo far, we have used Hasura\u2019s permission system to restrict access to certain roles. Hasura checks for the X-Hasura-User-Id header to identify which user is making the request.However, right now anyone can send this header and act as some other user. To prevent that we will use Hasura in JWT mode. When in JWT mode, Hasura will expect an Authorization header with a token. Sending a direct header like X-Hasura-User-Id will not be allowed. We are going to store X-Hasura-User-Id and the user role in the Firebase token by using custom claims.FirebaseCreate a new Firebase project and head to Authentication > Sign-in method. Enable Email/Password in providers.For storing X-Hasura-User-Id and X-Hasura-User-Role in Firebase token, we will use the custom claims feature of Firebase Admin SDK. There are several ways to set the required claims for a user on register process. We are going to make use of Firebase Cloud Functions and create a custom sign-up function. With this function we will make sure the user will have the required claims when the sign-up process is completed. Initialize a Firebase project if you haven\u2019t already.$ npm install -g firebase-tools  // Install Firebase CLI\n$ firebase login                 // Login to your Firebase account\n$ firebase init                  // Initialize a Firebase projectChoose your project from the list and setup functions. Once the setup is completed change directory to functions folder.$ npm install \u2013save graphql-requestOpen index.js and add the following lines. Don\u2019t forget the replace <your-graphql-endpoint> and <your-admin-secret> with your own values.Now that we have a function for signing up and setting up necessary claims, lets continue with creating two more functions to sync Firebase users with Hasura. These functions will act as triggers and will insert or remove users depending on the operation on Firebase. However, in Firebase, outbound requests for projects on the free plan are not allowed. You will need a billing account for this to work.After adding these changes, go to the parent directory where you initialized your Firebase project and deploy with the following command.$ firebase deployThe Firebase side is done for now, let\u2019s continue with Hasura.HasuraOpen Heroku dashboard and click the Hasura app we have created. Head to Settings tab.We need to add some config variables for Hasura to switch to JWT mode and decode Firebase user tokens. Hit Reveal Config Vars and add following variables.HASURA_GRAPHQL_ADMIN_SECRET -> <your-password>\nHASURA_GRAPHQL_JWT_SECRET ->\n{\n    \"type\":\"RS256\",\n    \"jwk_url\":\"https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.\n    com\",\n    \"audience\": \"<your-firebase-app-id>\",\n    \"issuer\": \"https://securetoken.google.com/<your-firebase-project-id>\"\n}After setting these variables, Hasura console will be protected by the password you typed and Hasura will switch to JWT mode. When making requests to GraphQL API, we need to send the following header.\u201cAuthorization\u201d : Bearer <JWT TOKEN>Event TriggerIn our quiz app, users will get 10 points if they choose the correct answers. But trusting client to increment the score wouldn\u2019t be a good idea. Hasura has several ways to handle these kinds of cases where client can\u2019t be trusted, or cases where some additional business logic is required. In our case we are going to make use of event triggers. We are going to create an insert trigger on the table user_answers. Whenever a new answer is inserted to this table. In case the answer is correct, score column in the user table will be incremented.Let\u2019s first create the function to perform these operations. Go to your Firebase project folder and open up index.js. Add the following function.Go to the parent Firebase project directory and deploy.$ firebase deployNow that the function is ready, lets add it to Hasura. Head to the Events tab in Hasura Console. Create a new trigger with the settings below. For Webhook URL, you need to paste the URL for the function we have just created. To get the function URL, open up Firebase console and head to Functions.Now that we have the Hasura & Firebase working we can move on to building the client. In the final part we will to dive into Flutter to create the mobile app.About the AuthorCan Ta\u015fp\u0131nar is passionate and enthusiastic developer whose journey started with the question \u201cHow to make a video game?\u201dMoving on with mobile application development and Full Stack Web development. Enjoys working with modern and open source technologies."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building scalable Flutter apps using GraphQL, Hasura and event-driven serverless, Part 1 - Setting up Hasura Description: A guide to building scalable Flutter apps using GraphQL, Hasura and event-driven serverless, Part 1 -  Setting up Hasura Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Can Ta\u015fp\u0131nar and published as part of the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.IntroductionIn this blog post we are going to make a quiz app with Flutter powered by Hasura and Firebase. We are going to see how Hasura can work with Firebase services such as Authentication and Cloud Functions to handle user authentication and business logic. Combining these services together will provide us a flexible and scalable architecture. This will be a 3-part series. In this first part, we are going to deploy Hasura and we will see how to model the relational data with permissions. Part 2 will cover how to setup Auth with Firebase and Hasura. Part 3 will cover how to make the flutter frontend. Also, we've written a guide to event-driven programming that may be useful to go through.Hasura Deployment on HerokuDeploy Hasura less than a minute by using the Heroku button below.When it is done, you can access the Hasura Console by opening your Heroku app URL.Modelling DataLet\u2019s start with modelling our data first. We are going to define columns, relationships and permissions. Hasura makes it easy to create relationships by tracking foreign keys in tables and suggesting possible relationships. By setting permissions you can restrict access for rows and columns for users by their role.Head to the Data > Add TableHINT: You can use the \u201cFrequently used columns\u201d for these frequently used types:Table: usersid, email, name columns will be filled by the data coming from Firebase. Notice that id here is type of Text. We are going to talk more about that later on. score column will be incremented if the user submits the correct answer. created_at will be set to now when a new user is inserted to the table. Let\u2019s set id as the primary key.After creating table head to the Permissions tab. For now, there is only one role which is admin and it has full access on every operation below. Enter a new role called \u201cuser\u201d. For users table we are only going to give select permissions. A user should be able to see other\u2019s name and score. We are mostly going to deal with following rules when we are adjusting select permissions.Row select permissions to restrict access to rows.Column select permissions to restrict access for certain columnsWe don\u2019t need a custom check for Row select permissions. For the Column select permissions, check only name and score columns. This way we ensure that users can only see these fields. Table: questionsid is a field with type UUID and the primary key. Default function gen_random_uuid() will generate a unique UUID whenever a new object is inserted to the table. question is the question title and created_at will be used for ordering questions later. For permissions, every user can make a query to get the questions, so we have the following permission for the questions table.Table: question_answersid is again type of UUID and the primary key for the table. answer is the question answer. Since we are going to give multiple answer options for a question, we need to know which answer is the correct one. To do that we have the is_correct column with a type of Boolean. question_id is here to point out which question this answer belongs to. So, it will be a foreign key. Users should be able to make a query to get the possible answers of a question. Permissions for this table is as below.Table: user_answersid is type of UUID and primary key. user_id, question_id and answer_id are foreign keys to point out to other tables. Notice that user_id column is type of Text instead of UUID. This is because we defined the id field in users table as Text. This is because it will be filled by the value coming from Firebase Authentication and Firebase user id is a string. \u00a0 For the permission part, we are going to allow users to insert answers, but we need to make sure a user can only insert an answer if the request is coming from the same user. In other words, a user can\u2019t insert a new record on behalf of someone else. This is where Sessions Variables comes in. Hit with custom check box and give the following rule.Every request from user that comes to the GraphQL API, will include a X-Hasura-User-Id header. We are going to talk more on that in Authentication part. Last thing we need to do is to set to the user_id field automatically from the X-Hasura-User-Id header. For that we are going to make use of Column presets. This allow a column to be filled by a static value or a session variable in our case. At the end insert permissions for user_answers table should look like below.Now that we defined our tables, it is time to create relationships for GraphQL API. Head to the Data tab. Hasura tells you about the Untracked foreign-key relations. Hit Track All to expose these relations over GraphQL API.To sum things up, we defined the tables, gave permissions and tracked relationships. Hasura already prepared our API for us at this point. You can head to the GRAPHIQL tab and explore the API. There is one last thing to do before we move on. We need a way to query the questions that hasn\u2019t been answered by the user. To do that, we can create a PostgreSQL function with an argument. Hasura will again magically track this function and expose it over GraphQL API. Head to Data > SQL. In here, you can write raw SQL. It can be for creating triggers, views or a function in our case. Write the following code and don\u2019t forget to check Track this box. This box will tell Hasura to track this function and expose it over GraphQL API.CREATE\nOR REPLACE FUNCTION public.unanswered_questions(userid text) RETURNS SETOF questions LANGUAGE sql STABLE AS $ function $\nSELECT\n  *\nFROM\n  questions\nWHERE\n  id NOT IN (\n    SELECT\n      question_id\n    FROM\n      user_answers\n    WHERE\n      user_id = userID\n  ) $ function $\nAfter you click Run, the function will take its place below the tables on the sidebar.Now that we have deployed Hasura and modeled the data & relationships, we have can move on to the next step. In Part 2 we are going to prepare Auth and Business Logic by integrating Firebase services. Part 3 will cover how to to make the Flutter frontend.About the AuthorCan Ta\u015fp\u0131nar is passionate and enthusiastic developer whose journey started with the question \u201cHow to make a video game?\u201dMoving on with mobile application development and Full Stack Web development. Enjoys working with modern and open source technologies."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a Hacker News Clone with GraphQL, Hasura and React - Part 2 Description: A detailed tutorial on how to build a hackernews clone using GraphQL, Hasura and ReactJS. Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Abhijeet Singh and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.In part-1 of this series, we setup our backend and Auth0. In this part, we will setup our React app and connect it to our backend. React App SetupWe will start with user authentication. We will be using JWT (JSON \u00a0web tokens) for authentication. Let\u2019s first create some basic header in our react app for showing login button.Replace the contents of styles/index.css file as shown:\nThese styles will be used throughout our app so you don\u2019t have to worry about the styling.Setup Apollo GraphQL ClientReplace contents of App.js to use Apollo GraphQL client as shown below. (See apollo github repository for more help).In line 15 change the uri to your GraphQL Endpoint on Hasura, which you can find on hasura console (remember where you created tables). Here we have imported the header component which we will implement now.Create header component and use react-routerWe will use react-router to implement single-page application behaviour. Install react-router using: $ npm install react-router-domReact \u00a0Router, and dynamic, client-side routing, allows us to build a \u00a0single-page web application with navigation without the page refreshing \u00a0as the user navigates. React Router uses component structure to call \u00a0components, which display the appropriate information.By \u00a0preventing a page refresh, and using Router or Link, the flash of a \u00a0white screen or blank page is prevented. This is one increasingly common \u00a0way of having a more seamless user experience. (source)For using react-router in our app, we have to wrap whole app in BrowserRouter It is a context provider for routing, which provides several props necessary for routing (like match, location, history). See this if you are unfamiliar with context. Replace \u00a0the contents of index.js:Next, we will create a header component for navigation within app. Create a header.js file in components directory. The contents of header.js should be:Here we are creating a navbar similar to HackerNews navbar. Later we will add some routes to it for navigation. That\u2019s it! We have \u00a0successfully created a header navbar and used react-routes in our app.Right now your app should look like thisAuth0 JWT integration with React App Follow along with Auth0-react-quickstart guide as reference to include Auth0 in react app. Configure Auth0 client by setting Allowed Callback URLs, Allowed Web Origins, Allowed Logout URLs to http://localhost:3000 and add the custom API if you haven\u2019t done already. Now install auth0-spa-js :$ npm install @auth0/auth0-spa-jsNow we will include react-auth0-wrapper in our app, which is a set of custom react-hooks that enable you to work with the Auth0 SDK. Create a new directory src/auth and add file react-auth0-wrapper.js populate it with code from here.Now add another file as auth/auth_config.json in src/auth . Populate auth_config.json with following code (change the values accordingly):Now we are ready to include login functionality in our react app. Basically, we will be including a login button in header. This button will lead to login through Auth0 with redirect to our localhost once login/signup is completed. At the same time login/signup data will be updated in our users table in hasura backend due to the Auth0 rules we added earlier. Once the login is done, we will get the accessToken in JWT format using functions provided by Auth0 SDK in App.js . This accessToken will then be used as a authorization header in apollo client queries to \u00a0backend, thus every query which goes to backend will have authorization \u00a0header.Firstly, change the contents of index.js to the following: Here, we are using the Auth0Provider which is a context provider for Auth0 client. Any children components will now have access to the Auth0 client. Having provided the Auth0 client to our app, we now replace the contents of components/header.js file to the following: We are using useState hook(line 20) to set initial accessToken value to empty string. If the user is logged in, the token is fetched from the Aut0 SDK client using getTokenSilently() (line 31). Notice that this function returns a Promise and is asynchronous. This function attempts to return the current \u00a0access token. If the token is invalid, the token is refreshed silently \u00a0before being returned from the function. If thetry block successfully gets executed, accessToken value is set to the JWT access-token from Auth0 (line 32).The component re-renders when we get accessToken value. Thus after the async function has finished executing, we store the value of accessToken in state. The component re-renders and apollo-client gets the token value, thus re-rendering the whole ApolloProvider (context-provider) with new token value and the authentication header.Once we have accessToken, we will use this to make requests to our backend using apollo client. See apollo-docs for apollo authentication using headers. Basically, we are passing the accessToken as authorization header(line 50), in our apollo queries. This client is then used inside the ApolloProvider (context provider) to provide the child elements access to apollo client created here.Now, \u00a0you should be able to login and logout of our app. Clear cache and \u00a0login. You must be asked to give access to your auth0 tenant by our \u00a0hasura backend. Give the access and you\u2019re good to go.Note : If you are facing errors, remember to not keep any dependency on react-apollo . @apollo/react-hooks must be used instead.Now, our app looks like thisImplementing Post List and Upvote/Points(realtime updates of upvotes)We will implement a list of post and upvote post button. Create a new component components/PostList.js as :POSTS_LIST query(line 10) is being used to fetch details from post table in our database. We can fetch post description, url, id, user \u00a0details who created the post, and also the number of upvotes/points in \u00a0that post all in just one query. This is due to the object and array \u00a0relationships we created on our post table earlier. useQuery (line 31) is a custom apollo-client react hook. We get the query data in data object (line 31) which is then passed as a prop to the Post component, which we will implement now.Create a new component components/Post.js as:Two \u00a0things are happening here: we are displaying the post information using props passed by parent component, and we are writing a GraphQL mutation o upvote posts.Firstly, we are getting post information in props (line 18). This is used to display post description, url , points count and created_at . Secondly, we are making a mutation UPVOTE_POST using useMutation hook. For upvoting a post, we need current logged in user\u2019s userId and postId . userId is present in user object we get from useAuth0 hook (line19), which is being stored in loggedUserId variable (line 23). Once we get userId we will use it as variable in our mutation. See apollo mutation if you are unfamiliar with passing variables in mutation. The other variable in useMutation hook, refetchQueries is used to refetch the specified query after the mutation results have \u00a0arrived. Thus, upvotes/points will be recorded and shown in real-time.The mutation will insert a row in our point table, recording the upvote. Now, the same user cannot vote the given post again as (post_id + user_id) is the unique key for our point table, which we set earlier. Finally, we need to import PostList component in App.js. Change your App.js file to make the following changes:Switch is a part of react-router which is used to match components with their paths. Now, our react-app looks like thisTry upvoting posts, and see the real-time updates in upvotes, thanks to refetchQueries . We haven\u2019t yet implemented the user-profile and create-post \u00a0functionality, so the submit button and user-profile links do not work. \u00a0Next we will implement the same.Implementing Submit Post functionalityCreate a new file as components/SecuredRoute.js as:This \u00a0will help us to create some secured routes which can only be accessed \u00a0if the user is logged-in. We will use secured routes while routing. \u00a0Using secured route, if someone tries to access the url\u2019s without \u00a0logging-in, user will be redirected to login automatically.Now, let\u2019s create component components/NewPost.js as:This component will be used to submit a post. We have a form which has two input fields for description and url. We are using react state to store values of description, url and error (if mutation is not successful). The mutation accepts three variables which are being passed using variables parameter (line 34). If the mutation succeeds, we are using refetchQueries to refetch POSTS_LISTS query, thus implementing real-time updates of the list of post on the \u00a0home page. If the mutation is successful, data in our backend postgres \u00a0database gets updated with the new post data.Let\u2019s include route to new-post in our App.js :It is a secured route, so if we try to access localhost:3000/new-post without logging-in, we will be redirect to login screen.Now, \u00a0you can try submitting a post and see that the home page will be \u00a0updated with the latest post. In case of any error in mutation, the \u00a0error message will be shown on screen.http://localhost:3000/new-post should look like thisImplementing User ProfileLet\u2019s now implement user-profile. Create a new component components/Profile.js as : We are getting user_id as props which will be used to query our backend database for user info, for the given user_id. The data is then rendered in return() . The props (user_id) here is being passed in form of url, and we are using props.match.params.id to get that prop. These props are provided by the react-router BrowserRouter context provider, which is included in our index.js file. We need to now update App.js to redirect user to profile section:Here, we are passing id as props (path={\u201c/user/:id\u201d}), which is accessed via props.match.params.id . This is a react-router way of passing prop. See this example for more details.Finally, \u00a0we have our app ready with user-profiles section. You can navigate to \u00a0user-profiles, create new posts and see real-time updates of new-posts, \u00a0upvote posts and see real-time updates of upvotes.You should now have a working Hacker News clone. Incase you'd like to reference it, the final code for this app is hosted here. Visit the live demo here.About the author Abhijeet Signh is final year UG student in Computer \u00a0Science and Engineering from IIIT Kalyani. He has done work in Full \u00a0Stack Development, Android, Deep Learning, Machine Learning and NLP. He \u00a0actively takes part in competitive programming contests and has interest \u00a0in solving algorithmic problems. He is a startup enthusiast and plays \u00a0table tennis and guitar in spare time."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a Hacker News Clone with GraphQL, Hasura and React - Part 1 Description: A detailed tutorial on how to build a Hacker News clone using GraphQL, Hasura and ReactJS. Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Abhijeet Singh and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.Here is how I used Hasura GraphQL Engine and React to build a Hacker News clone. The final code is hosted here. Visit the live demo here.App FeaturesThis app will have the following features:It will display a list of all posts.Users can create account (using Auth0 authentication).Authenticated users can create new posts.Authenticated users can upvote posts.Realtime updates when other users upvote a post or create a new one (updating apollo cache).Initial SetupFirst we will be building the frontend of our app using react. To quickly start, we will create a basic react app with no build configuration using create-react-app. Run the following commands in terminal:$ npx create-react-app hackernews-clone\n$ cd hackernews-clone\n$ npm startNote : npx is a npm package runner. The typical use is to download and run a package temporarily or for trials. create-react-app is an npm package that is expected to be run only once in a project\u2019s lifecycle. Hence, it is preferred to use npx to install and run it in a single step. (source)The server will start and you should be able to see a welcome screen on http://localhost:3000 (like below).To improve the project structure, let\u2019s create two directories, both inside the src folder. The first is called components and will hold all our React components. Second one styles, that is for all the CSS files you\u2019ll use.Move the required jsx and css files to appropriate directories and change the references to these files in both index.js and App.js accordingly. Your directory structure should look like this:\u251c\u2500\u2500 node_modules\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 public\n\u2502 \u251c\u2500\u2500 favicon.ico\n\u2502 \u251c\u2500\u2500 index.html\n\u2502 \u2514\u2500\u2500 manifest.json\n\u251c\u2500\u2500 src\n\u2502 \u251c\u2500\u2500 App.test.js\n\u2502 \u251c\u2500\u2500 components\n\u2502 \u2502 \u2514\u2500\u2500 App.js\n\u2502 \u251c\u2500\u2500 index.js\n\u2502 \u251c\u2500\u2500 logo.svg\n\u2502 \u251c\u2500\u2500 serviceWorker.js\n\u2502 \u2514\u2500\u2500 styles\n\u2502   \u251c\u2500\u2500 App.css\n\u2502   \u2514\u2500\u2500 index.css\n\u2514\u2500\u2500 README.mdI\u2019ll be using react-bootstrap package for the styling. Install the package using:$ npm install react-bootstrap bootstrapMake sure to add the CDN links to stylesheets in /public/index.html. Copy the links from here.Also install react-apollo GraphQL Client using the following command:$ npm install apollo-boost apollo-link-context @apollo/react-hooks @apollo/react-hoc graphqlApollo Client helps you consume a GraphQL API from the front-end.Next we will setup the backend using the Hasura GraphQL Engine and Postgres. See this guide to quickly get started with Hasura GraphQL engine and Postgres running on heroku with one click deployment.Setting up backendOur Postgres Database structure : Creating tablesCreate the following tables : users table (id has type text)users tablepost table (id has type integer(autoincrement)):post tableAdd the foreign key constraint from user_id column to users.id . Set Update and Delete Violation as restrict.Creating RelationshipsCreate and object relationship by jumping to relationship section of post table, between user_id of posts and id of users table. This will let you query the information of user who created the post in a single fetch query. See this link for creating relationships through foreign key in hasura.point table (id has type integer(autoincrement)):point tableAdd foreign key and unique key as shown. user_id, post_id is a unique key as one user can upvote a post only once. It has foreign key relationship with both post table and users table.Also add the object relationships in post table as shown:These relationships will help us query the number of points for each post.Setting up PermissionsNow \u00a0we will set up the permissions for the tables, so that only logged in \u00a0users can do certain things. Set up the permissions as shown below:point tableusers tablepost tableSee access control basics if you are not much familiar with access control and how it works in Hasura.To make it less complicated, I have given all the row level permissions as without any checks and column level as all columns and aggregation queries (used to count number of points for a post). That\u2019s it for now.Using Auth0 JWT for AuthenticationThe following figure quickly explains how JWT authentication works. Here, Application Server (our backend) needs to be configured to verify that the incoming JWT are created by the authentication server (Auth0).We are using Auth0 as authentication server. It will create a JWT and pass to our react app. Then, using apollo client, we will send the auth_token (access_token in our case) to our backend (application server), i.e. Hasura GraphQL \u00a0Engine. Hasura is needed to be configured to verify that incoming JWT is \u00a0created by Auth0 authentication server.Let's get started by using @apollo/react-hooks and @auth0/auth0-spa-js. We have to follow these steps:Auth0 JWT integration with Hasura See this guide for Auth0 JWT Integration with Hasura and get the JWK (key for validation of incoming JWT). In Auth0 dashboard, set Allowed Callback URLs, Allowed Web Origins, Allowed Logout URLs to http://localhost:3000 and save changes.You may also need to disable OIDC Conformant in Auth0 > Applications > Your_app > Settings > Advanced Settings. \u00a0Once you have the key, next step is to activate the JWT mode in Hasura. \u00a0Go to your heroku dashboard, choose our app and go to settings. Here \u00a0you can see some Config Vars which we will set-up now. Add the config vars as following: Here, HASURA_GRAPHQL_ADMIN_SECRET key for the JWT mode to work. Put the key in HASURA_GRAPHQL_JWT_SECRETenvironment variable. Once you have added this, the GraphQL endpoints can only be queried using Authorization header or X-Hasura-Admin-Secret header. Finally, set HASURA_GRAPHQL_UNAUTHORIZED_ROLE to anonymous because we do allow unauthenticated users to read some data.Unauthorized role: used when access-key is not sent in access-key only mode or \u201cAuthorization\u201d header is absent in JWT mode. Example: anonymous. Now whenever \u201cAuthorization\u201d header is absent, request\u2019s role will default to \u201canonymous\u201d.Next step is to add rules in our Auth0 app. We will add 2 more rules to Auth0 app as shown below: Here we are replacing idToken with accessToken . Basically, auth0 provides different types of tokens and the auth0-spa-js does not expose the idToken anymore, so we will use accessToken. Update: From version 1.2.0 of auth0-spa-js, the raw value of idToken is exposed using getIdTokenClaims method.Read more about tokens here. The new SPA SDK only uses Authorization Code Grant + PKCE (not Implicit Grant anymore) Thus we have to use accessTokens in place of idTokens.(See this thread).Add one more rule for keeping our postgres database and Auth0 users in sync: Change admin_secret and url accordingly. Now, whenever a new user signs up or a user logs in, our \u00a0hasura postgres database will be updated automatically to store the user \u00a0information in our users table. See this for more details.Note: Here we are just doing a post request with graphql mutation queries, and we have x-hasura-admin-secret in header, thus the incoming request have full access to mutate the database.We also need to create a custom API in Auth0 Dashboard > APIs which will point to our Hasura GraphQl API. Create a new api as shown below and give it any name. Change Identifier with your graphql endpoint.Now that we have got our backend and Auth0 setup, we can get started on building out the React app. Head over to the 2nd part of this series to learn how. About the author Abhijeet Singh is final year UG student in Computer \u00a0Science and Engineering from IIIT Kalyani. He has done work in Full \u00a0Stack Development, Android, Deep Learning, Machine Learning and NLP. He \u00a0actively takes part in competitive programming contests and has interest \u00a0in solving algorithmic problems. He is a startup enthusiast and plays table tennis and guitar in spare time."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Join Identity Data from Okta with Postgres Using Hasura Remote Joins Description: Use Hasura Remote Joins to join data from your Postgres database with user data in Okta. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRUse Hasura Remote Joins to join data from your Postgres database with user data in Okta.This post is a part of our Remote Joins series. Remote Joins in Hasura allows you to join data across tables and remote data sources. Data Federation using Hasura Remote Joins is now available in v1.3.0 stable release.Try it out on Hasura Cloud and let us know what you think!Okta is a complete access management platform that provides secure identity management with Single Sign-On, Multi-factor Authentication, Lifecycle Management (Provisioning), and more. In a typical app, Okta is the gateway for connecting the app, devices and users via APIs. In this example, we will make use of Okta's API to fetch user's data mapping it with the user id in Postgres.Let's create a new table in Hasura called users with just id and created_at columns:Adding Okta API as Remote SchemaTo be able to query Okta data via Hasura, it needs to be added as a Remote Schema using the Hasura Console.Deploy Custom ResolverAdd the following environment variables in the .env file on glitch.OKTA_API_KEY=xxx\nOKTA_DOMAIN=xxx\nPORT=3000Get the Okta API Key by visiting the create an API Token docsSet the API key as OKTA_API_KEY environment variable.Get the Okta Domain by visiting the following find your domain docsWe need to input the Okta Domain as OKTA_DOMAIN environment variable.This custom resolver is used to accept a user's id as an argument and will return a data pertaining to that user.Get the GraphQL API Endpoint from Glitch and add it as remote schema.Now let's add the Remote Relationship called userInfoNow let's create a user on Okta. For simplicity, we use the API directly instead of building a client app. Note that, you have to replace OKTA_API_KEY and OKTA_DOMAIN with the values generated above. Additionally you have to add profile details in the body of the request.curl -X POST \\\n  'https://hasura.okta.com/api/v1/users?activate=true' \\\n  -H 'Authorization: SSWS OKTA_API_KEY\u2019 \\\n  -H 'Content-Type: application/json' \\\n  -H 'Host: OKTA_DOMAIN\u2019 \\\n  -d '{\n  \"profile\": {\n    \"firstName\": \"\",\n    \"lastName\": \"\",\n    \"email\": \"\",\n    \"login\": \"\"\n  },\n  \"credentials\": {\n    \"password\" : { \"value\": \"\" }\n  }\n}'Once a user is created on Okta, let's add the user id into Hasura's users table.Now the GraphQL query to fetch this data in a single API call would look like the following:query {\n  users {\n    id\n    created_at\n    userInfo {\n      email\n      firstName\n      lastName\n      status\n    }\n  }\n}Notice that, the nested query userInfo comes from Okta API and it will apply the filter of users.id = userInfo.id, there by only giving data about the current user in Okta.Checkout our other posts in the Remote Joins series.Remote Joins opens up a lot of exciting use-cases and these boilerplates are simple examples to explore the possibilities. You can fork the original example to extend the functionality for your app's requirements."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Improved logging Description: The v1.0.0-beta.3 release added several exciting enhancements to the GraphQL engine's logs. Author: Marion"}, {"role": "assistant", "content": "The v1.0.0-beta.3 release added several exciting enhancements to the GraphQL engine's logs. The following log types are now enabled by default: startup, http-log, webhook-log and websocket-log. The detailed documentation of these log types can be found here.Here is an example of an http-log of a successful query:http-log in case of successHere is an example of an http-log with a permission error. The level has been changed from info to error.http-log in case of errorNow we'll add query-log that was also introduced with the same release but is not enabled by default. We can set the following environment variable (in this example on Heroku):Adding environment variable for query-logHere is an example of a query-log:Query logMore details on this change can be found in the changelog."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Support for array operators in permissions Description: Following popular demand in the community, support for array operators like _in, _nin, _has_keys_any in permission rules has been introduced in v1.0.0-beta.3. Author: Marion"}, {"role": "assistant", "content": "Following popular demand in the community, support for array operators like _in, _nin, _has_keys_any in permission rules has been introduced in v1.0.0-beta.3. Using array operatorsLet's say we have a table users and a table organizations where a user can access zero, one or several organizations. Let's see how we can model this.We have a users table with some sample data:users tableAnd an organizations table with some sample data:organizations tableIn the permissions tab of the organizations table, we add a new role called user:In the expanded section, we can now add the following custom check for insert:Set permissions for user roleLet's break this down.We can use a session variable called X-Hasura-Allowed-Org-Ids , which contains an array of org_id values, to specify which organizations a particular user has access to. In the permissions rule, we say that a row in the organizations table can be accessed if the value in its `id` column is part of the list that the user has access to.Let's go to GraphiQL and pass the following request headers: the X-Hasura-Role is user (for which we added the permissions). Then we'll pass a X-Hasura-User-Id and X-Hasura-Allowed-Org-Ids containing an array of allowed organizations. For now, we just want to access one organization.Let's now access all four organizations:Note: the rules that say which organizations may be accessed by which users remains with an external system and will be passed to Hasura, e.g. inside a JWT token.The array operators are _in, _nin, _has_keys_any and _has_keys_all. More information on this change can be found in the changelog. Do check this feature out and let us know what you think!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: JWT Analyzer / Inspector in Hasura Console Description: With the v1.0.0-beta.3 release, a JWT analyzer is now part of the Hasura console. Learn how to use this feature! Author: Marion"}, {"role": "assistant", "content": "With the v1.0.0-beta.3 release, a JWT analyzer is now part of the Hasura console. Up until now, JWTs had to be decoded using an external tool like https://jwt.io, etc. Let's see how we can use this feature! This example uses Firebase JWT sample app (https://github.com/hasura/graphql-engine/tree/master/community/sample-apps/firebase-jwt) to generate a token. You can use any other JWT provider and configure Hasura with the JWT secret using the --jwt-secret flag or HASURA_GRAPHQL_JWT_SECRET environment variable. After logging in and getting a valid JWT token, we add it to the Authorization request header in the console's Graphiql page:Valid JWT token in request headerIf we click on the rectangular icon on the right, the JWT analyzer opens and we get information on the token validity, Hasura claims, headers and the full payload. Here a snapshot:Decoded JWT in JWT analyzerYou can find more information about this change in the changelog. Do try this out and let us know what you think about it!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Joining Postgres Data with FaunaDB Serverless using Hasura Remote Joins Description: Use Hasura Remote Joins to join data from your Postgres database with serverless data in FaunaDB. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRUse Hasura Remote Joins to join data from your Postgres database with serverless data in FaunaDB.This post is a part of our Remote Joins (available in beta) series. Remote Joins in Hasura allows you to join data across tables and remote data sources. Data Federation using Hasura Remote Joins is now available starting from the v1.3 beta release. Jump on our discord or comment on github and let us know what you think!FaunaDB is a serverless cloud database for modern applications. It is designed to focus on quick deployment, simplicity of daily operations, and developer\u2019s ease of use. It offers a GraphQL API which can be joined with Hasura using Remote Joins.In this example, we will look at how a schema in FaunaDB can be joined with existing data in Hasura. Assuming that there is a users table in Hasura with columns id and name, let's get started.Creating a GraphQL Schema in FaunaDBLogin to FaunaDB Cloud - This is the dashboard to manage your database and security.Create a new database to store data. Let's name it hasuraCreate a GraphQL Schema. Let's create a simple schema to manage user's todos. Download the below schematype Todo {\n   title: String!\n   completed: Boolean!\n   userId: Int!\n}\n\ntype Query {\n   allTodos: [Todo!]\n   todosByUser(userId: Int): [Todo!]\n}\nImport your GraphQL Schema by clicking on GraphQL on the left sidebar navigation and choose the GraphQL schema file that you downloaded above. Once the import is successful, you should see the GraphQL Playground to start playing with the APIs.Insert sample data in FaunaDBLet's insert sample data for the above schema. In the GraphQL Playground, execute this mutation to insert a todo.mutation CreateATodo {\n   createTodo(data: {\n   title: \"Build an awesome app!\"\n   completed: false\n   userId: 1\n   }) {\n       title\n       completed\n   }\n}This inserts a todo tagging to userId 1.Getting the Access KeyThe GraphQL endpoint requires authentication with a specific FaunaDB database. We need to create a new key with role Server Read-OnlyThe secret generated must be provided as an HTTP Basic Authorization header, encoded as a Base64 string. For example, if your FaunaDB secret is fnADMxRzydATDKibGAciQlNQWBs-HJdpJS1vJaIM, you can encode it like so:echo -n \"fnADMxRzydATDKibGAciQlNQWBs-HJdpJS1vJaIM:\" | base64\nZm5BRE14Unp5ZEFUREtpYkdBY2lRbE5RV0JzLUhKZHBKUzF2SmFJTTo=The trailing colon (:) is required.Then your Authorization header would look like this:Authorization: \"Basic Zm5BRE14Unp5ZEFUREtpYkdBY2lRbE5RV0JzLUhKZHBKUzF2SmFJTTo=\"Adding FaunaDB as Remote SchemaTo be able to query FaunaDB data via Hasura, it needs to be added as a Remote Schema using the Hasura Console.The GraphQL API Endpoint is:https://graphql.fauna.com/graphqlNow copy the secret key (base64 encoded) and use it in Authorization headers like below:Authorization: Basic <base64 encoded key>\n\nIn Hasura Console, head to Remote Schemas and enter GraphQL Server URL with the above endpoint. Under Additional Headers, enter the Authorization header with the secret as mentioned above.Now, let's go to users table -> Relationships and add the remote relationship todos.Under configuration for todosByUser, we declare a filter to say:userId: From column -> idThis ensures that while querying for todos in users, we get only the relevant todos written by the user who is queried for.Now the GraphQL query to fetch this data in a single API call would look like the following:query {\n  users {\n    id\n    name\n    todos {\n      data {\n        title\n        completed\n      }\n    }\n  }\n}Notice that, the nested query todos come from FaunaDB and it will apply the filter of users.id = todos.data.userId, there by only giving todos written by the user."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Join any OpenAPI or Swagger Rest API with Postgres using Hasura Remote Joins Description: Use Hasura Remote Joins to join data from your Postgres database with any OpenAPI/Swagger Spec GraphQL. Boilerplate - openapi-swagger-remote-schema Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRUse Hasura Remote Joins to join data from your Postgres database with any OpenAPI/Swagger compliant REST API. Boilerplate - openapi-swagger-remote-schemaThis post is a part of our Remote Joins series. Remote Joins in Hasura allows you to join data across tables and remote data sources. Data Federation using Hasura Remote Joins is now available in v1.3.0 stable release.Try it out on Hasura Cloud and let us know what you think!In this example, we will look at how data from a public API conforming to the OpenAPI spec can be joined with existing data in Postgres using Hasura Remote Joins. We are going to make use of this OpenAPI Service; it is a currency exchange rate API. Given a currency, it will return exchange rates for all other global currencies.Alright, now let's say there is a users table with columns id, name, currency. \u00a0Now we would like to query users data along with the exchange rate data in the public API. Let's see how this can be done using the openapi-to-graphql tool.Adding OpenAPI Service as Remote SchemaTo be able to query OpenAPI serice via Hasura, it needs to be added as a Remote Schema using the Hasura Console. Let's deploy the GraphQL server on Glitch.Change the OpenAPI schema in the openapi.json file, as necessary for modifications. It currently uses this exchange-rate API. Get the GraphQL API Endpoint from Glitch and add it as remote schema.Add Remote Relationship to JoinNow let's add the Remote Relationship called currency_ratesNow the GraphQL query to fetch this data in a single API call would look like the following:query {\n  users {\n    id\n    name\n    currency\n    currency_rates {\n      date\n      rates\n    }\n  }\n}Notice that, the nested query currency_rates come from exchange rate API following the OpenAPI spec. It will apply the filter of users.currency = currency_rates.baseCurrency, there by only giving exchange rates related to the current user's currency.Checkout our other posts in the Remote Joins series:Building a Music Playlist app with Gatsby, ContentfulUsing GraphQL to join data across Google Places API and PostgresBuild apps with rich content using Contentful and PostgresJoin data from Google Sheets API and Postgres"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Custom auth for Hasura with Netlify Description: How to add custom auth with Netlify identity and Hasura.  Author: Vladimir"}, {"role": "assistant", "content": "In the last couple of months, I published several blog posts on authentication with different providers such as Auth0 or AWS Cognito. I also summarized different authentication options with these providers in Hasura Authentication Explained blog postOne of the auth solutions that I've been missing is the one provided by Netlify. In this tutorial, we will create an auth solution for hasura.io using Netlify in a bit different way than we did it previously. The main difference is that Netlify Identity - \u00a0Netlify auth solution, even though supports JWT, it still does not support custom claims, which are crucial to make Hasura work with JWT tokens. We will talk about the different approach here - using custom Auth webhook to verify JWT token provided by Netlify Identity, extract data from it and send this data in the format Hasura can parse and use in permission system.Getting startedSetting up our HasuraFirst of all, we will set up our Hasura on Heroku. If you are new to Hasura you can read more about it in the following blog postWe will create two tables: posts and users. User table will have Text type id, because we will get that from Netlify identity. Getting users into HasuraIn this section we will make sure when signing up, the user will be written into users table.Create netlify siteWe will start simple by the creation of our netlify site:mkdir hasura-netlify-identity-auth\nnpm init\nnetlify initFor this custom auth solution we will need two things:Netlify function, which will run on signup and will add users to HasuraNetlify function which we will use as auth webhook to validate our token, extract data and send this data to HasuraFrontendI won't be creating frontend in this blog post, but will use react-netlify-identity-widget sample. You can check frontend code hereCreating signup hookWe will use netlify cli to create signup hook.netlify functions:createchoose identity signup. This will create functions folder and auto-generate identity-signup.js file with boilerplate code. Netlify identity will call identity-signup.js automatically whenever user confirmed his email.We will install node-fetch cause we will need it to execute our insertUser mutationnpm install node-fetch\non signup function hook we will execute insertUser mutation to insert a new user into HasuraLet's also add hasura secret in environment variables in Netlify to set admin secret for Hasura.If you want to watch this part, which we streamed together with swyx on https://twitch.tv/hasurahqIt's available here:Creating custom auth webhookIn this section, we will create a custom auth web hook by creating a new function to validate auto-generated jwt token whenever we sign up.What we will do first is to create a new function using netlify fuctions:create we will call it validate-userInside the function we will get identity and user object from context.clientContextNetlify automatically decodes JWT token and if a token is supplied properly, then we will be able to get a user object. If not, then we will set a role as anonymousYou can notice that we return x-hasura-role and x-hasura-user-id, so we will be able to use them in the permission system inside Hasura. Now the last thing left to do is to add HASURA_GRAPHQL_AUTH_HOOKenvironment variable to heroku or any other environment you run hasura at. Whenever you do so, the token will be validated with netlify identity and correct values will be sent to the permission system.Watch the part2 of our Twitch stream with swyx here:Signing in with GoogleSomething that we haven't covered in the stream, but you might want to do is to sign with Google or any other providers. For that, you cannot rely on identity-signup name convention, but you need to create a hook manually. So the process will be to rename identity-signup function to something else and then in Notification settings for identity, add our signup hook url triggered on signup event. SummaryThis blog post is a rough explanation of what we did during our twitch stream, so if you haven't subscribed to our Twitch streams or to Youtube channel I strongly suggest doing so, cause we stream this type of content on a weekly basis, having guests from different companies guiding me through live coding solutions for real-world problems. All of this is done in a fun, friendly environment. If you want to show me some really cool technique, product or solution involving Hasura, I would be happy to find the time to stream with you, so let me know on Twitter"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Optimistic UI and Clobbering Description: Optimistic UI is a front-end development paradigm wherein, after making a mutation request to an API, the client updates the UI optimistically assuming that the request is successful. Author: Rishichandra Wawhal"}, {"role": "assistant", "content": "What is Optimistic UI?Optimistic UI is a front-end development paradigm wherein, after making a mutation request to an API, the client updates the UI optimistically assuming that the request is successful.Optimistic UISee the example in the GIF below. It displays a counter value from the database and the increment button increments it. The counter on the left implements the traditional synchronous UI, while the one on the right implements optimistic UI.Optimistic UI exampleSynchronous UIThe increment request is made to the database.Once the response is successful, the counter value on the UI is updated.If the response is unsuccessful, the counter value on the UI remains unchanged.Optimistic UIThe increment request is made to the database.The counter value on the UI is updated immediately assuming the response will be successful.If the response is successful, the UI is updated with the data from successful responseIf the response is unsuccessful, the counter value on the UI reverts back to the previous state.Why should you use Optimistic UI?As you see above, in both cases, successful and unsuccessful responses are handled elegantly. The only difference is that optimistic UI seems faster irrespective of the network bottleneck. In most cases, there is no reason to wait for a successful response, because most requests are expected to be successful in production environment. You can also have a recovery mechanism for reverting back to original state if the request is unsuccessful. Clobbering with Optimistic UIThe ProblemClobbering is a software-engineering problem where a source of data is overwritten due to side effects. In case of optimistic UI, clobbering usually happens when the UI makes multiple mutations in quick succession and the optimistic UI for a mutation is overwritten with the response data from a different mutation.Consider a situation where a UI element that mutates from a value 1 to a value 2 and to a value 3 in quick succession. You can see the clobbering problem in the GIF below:The UI will go through the following states if we try to implement optimistic UI without accounting for clobbering:Initial stateValue in database: 1Value on UI: 1Mutation to value (2) initiatedValue in database: 1Value on UI (optimistic) : 2Mutation to value (3) initiatedValue in database: 1Value on UI (optimistic): 3Mutation to value (2) successfulValue in database: 2Value on UI (from successful response): 2Mutation to value (3) successfulValue in database: 3Value on UI (from successful response): 3This means that for a person just viewing the UI, the UI goes from 1 to 2 to 3 to 2 to 3, which is semantically just wrong. The UI should ideally be going from 1 to 2 to 3 and that's it.This is the clobbering problem with optimistic UI.The SolutionWe can solve this problem by checking for stale data before updating the UI. To achieve this, we associate every mutation with a unique comparable identifier such as a timestamp or a number. This means, along with each mutation, you associate an identifier (say, a number) slightly greater than the identifier associated with the previous mutation. This identifier should be a part of both, your optimistic response and the mutation response. Now, whenever the UI has to be updated, we just have to check if the new data has mutation identifier greater than that of the existing data. In this way, we avoid updating the UI with stale data.Now that we know how to account for clobbering, lets revisit the UI states when a value goes from 1 to 2 to 3:Initial stateValue in database: 1Mutation Identifier: 1252Value on UI: 1Mutation to value (2) initiatedValue in database: 1Mutation Identifier of optimistic data: 1253Mutation Identifier of existing data: 1252Value on UI (optimistic) : 2Mutation to value (3) initiatedValue in database: 1Mutation Identifier of optimistic data: 1254Mutation Identifier of existing data: 1252Value on UI (optimistic): 3Mutation to value (2) successfulValue in database: 2Mutation Identifier of data from successful response: 1253Mutation Identifier of existing data: 1254Value on UI (from successful response): 3Note that the UI does not update with the data from mutation response because the mutation identifier is less than the mutation identifier in the UI dataMutation to value (3) successfulValue in database: 3Mutation Identifier of data from successful response: 1254Mutation Identifier of existing data: 1254Value on UI (from successful response): 3As you see, the UI goes from 1 to 2 to 3 and that's it.The idea is to check and sanitise the data before updating a data source and it can be used to solve most clobbering issues. The only requirement for implementing this solution is that the server should support atomic increments along with updates.Error HandlingWith this solution to clobbering, it is easy to handle unsuccessful requests as well. Since every UI state is associated with a mutation identifier, we can roll back to the previous mutation identifier whenever an unsuccessful response is received from the server.Example Let us take an example of implementing this solution in case of a todo app.Imagine you have a todo app that reads data from a todo table in Postgres. The todo table would traditionally look something like this:Since we have to account for clobbering, we will add another field to this table called update_mutation_identifier.Now whenever a todo is updated from active to complete to active in rapid succession, the flow would look something like that of the diagram below.Simultaneous UI and Database states during subsequent rapid mutationsI've used Postgres (and Hasura for GraphQL) for building the examples above.Feedback, comments and questions are very welcome! Let us know in the comments below or on our discord server."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How FASTCUP built a high performance app with a complex database schema using GraphQL and Hasura Description: How fastcup.net leveraged GraphQL, Hasura, Postgres and Event Triggers to build a fast, realtime app. Author: Sandip"}, {"role": "assistant", "content": "How FASTCUP leveraged GraphQL, Hasura, Postgres and Event Triggers to build a fast, realtime app.FASTCUP is an esports league app that allows users to play Counter-Strike matchups against other teams on their game servers, with custom anti-cheating measures and rank-based matchmaking.Gamers have access to profiles of other players with details like ranks, statistics, matches, friends, advanced player statistics, etc. They can even enter leagues and tournaments, and earn cash prizes.Solving for a complex database schema and website speedIn 2018, FASTCUP decided to bring their gaming services to a larger audience by building an English language version of their existing web app. This new app, https://cs.fastcup.net, was required to query a really complex database schema with lots of relational data:You need a \u201cbird\u2019s-eye\u201d view to fit FASTCUP\u2019s database schema in a single pageOne of the major design goals for this app was to be extremely fast (gamers tend to expect speed from their websites too!), and consequently, structuring the response payload to exactly meet the requirements of the app\u2019s front-end was a strict requirement. GraphQL\u2019s proposition of avoiding under-fetching/over-fetching of data fit their needs perfectly, and the team started evaluating tools for building a GraphQL API.Evaluating GraphQL toolsBefore settling on Hasura, they considered a few other tools, but discarded them for being slow and inflexible, and for their limited support for GraphQL subscriptions and other arguments/operators (where, order by, limit, etc). A search for GraphQL tools with support for subscriptions led the team to discover Hasura and subsequent evaluations showed it to be a good fit for their requirements due to:Leveraging of Postgres features for a better developer experienceGraphQL engine\u2019s speed and performanceAuto-generated CRUD GraphQL API over a database schemaHasura now powers almost all of the app\u2019s GraphQL API requirements. Event triggers are also leveraged to kickstart a game server when the status of a match changes to \u201cLive\u201d.Feedback on HasuraFASTCUP believes that Hasura has contributed significantly to their feature throughput, largely due to the fact that they are required to write code only to support custom business logic in mutations. They also love the fact that there\u2019s a \"single source of truth\" derived from introspecting the database, and the fact that this \u201csnapshot\u201d can be version controlled. Next steps with HasuraThe FASTCUP engineering team is the go-to candidate for Hasura for feedback on performance-related features. They also actively shape the product roadmap with their feedback, especially on enhancements for event-triggers and iterations on access-control patterns.FASTCUP currently supports CS:GO and CS 1.6, and they are aggressively working on adding support for other popular games like PUBG, Dota 2, etc. with Hasura powering the entire catalog. Want to know how Hasura can help your team? Schedule a demo.This case study was prepared basis a discussion with the FASTCUP team.  "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Join data from Google Sheets API and Postgres using Hasura Remote Joins Description: Use Hasura Remote Joins to join data from your Postgres database with Google Sheets API in GraphQL. Boilerplate -> Google Sheets Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRUse Hasura Remote Joins to join data from your Postgres database with Google Sheets API in GraphQL. Boilerplate -> Google SheetsThis post is a part of our Remote Joins series. Remote Joins in Hasura allows you to join data across tables and remote data sources. Data Federation using Hasura Remote Joins is now available in v1.3.0 stable release.Try it out on Hasura Cloud and let us know what you think!In this example, we will look at how data from Google Sheets API can be joined with existing data in Postgres using Hasura Remote Joins. Let's say there is a users table with columns id, name, email. There is a Google Sheet, with a list of email IDs mapped to URLs. Now we would like to query users data along with the url data in Google Sheets mapped against each user's email id. Let's see how this can be done using a custom resolver.Adding Google Sheets API as Remote SchemaTo be able to query Google Sheets data via Hasura, it needs to be added as a Remote Schema using the Hasura Console.Deploy Custom ResolverAdd the following environment variables in the .env file on glitch.GOOGLE_SHEETS_API_KEY=xxx\nSPREADSHEET_ID=xxx\nPORT=3000Get the Google Sheets API Key by visiting the Authorize Requests Sheets API page.Set the API key as GOOGLE_SHEETS_API_KEY environment variable.We need to input the spreadsheet id from where we are going to fetch data. Set that as SPREADSHEET_ID environment variable. We can find the sheet ID in the url.This custom resolver is used to accept a user's email and will return a filtered list of data pertaining to that user.Get the GraphQL API Endpoint from Glitch and add it as remote schema.Now let's add the Remote Relationship called sheetsNow the GraphQL query to fetch this data in a single API call would look like the following:query {\n  users {\n    id\n    name\n    email\n    sheets {\n      urls\n    }\n  }\n}Notice that, the nested query sheets come from Google Sheets API and it will apply the filter of users.email = sheets.author, there by only giving data about the current user.Checkout our other posts in the Remote Joins series:Building a Music Playlist app with Gatsby, ContentfulUsing GraphQL to join data across Google Places API and PostgresBuild apps with rich content using Contentful and PostgresRemote Joins opens up a lot of exciting use-cases and these boilerplates are simple examples to explore the possibilities."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a Music Playlist App with Gatsby, Contentful and Hasura Remote Joins Description: Join music data in your database with audio source files in Contentful with GraphQL and Hasura Remote Joins. Auth0 handles the authentication part. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRJoin music data in your database with audio source files in Contentful with GraphQL and Hasura Remote Joins. Handle authentication using Auth0.Instant setup. App Source Code -> \u00a0gatsby-contentful-auth0In this example, we will look at how a music playlist app can be built with Gatsby and CMS as Contentful. Contentful is an API First CMS to build digital products. It offers a GraphQL API which can be joined with Hasura using Remote Schema. We will learn how these multiple data sources can be merged with existing data in Hasura. In the music playlist app, we store metadata about music tracks, albums, playlist and users in Postgres database and query it using Hasura GraphQL. We will use Contentful to store and retrieve audio tracks required to play music on the app. For user authentication we will make use of Auth0.Here's a simple schema representation of the music playlist app that we are going to build. There's an album table which can have multiple tracks. Similarly there's a playlist table which can have multiple tracks. A playlist is linked to a user via the user table.The track information is stored in Postgres but we use Contentful to store and manage actual audio tracks.Getting StartedThis post is a part of our Remote Joins series. Remote Joins in Hasura allows you to join data across tables and remote data sources.Deploy GraphQL Engine on Hasura Cloud and setup PostgreSQL via Heroku:\nGet the Hasura app URL (say whatsapp-clone.hasura.app)Now let's setup Auth0 for the Authentication part. Refer to the Auth0 integration guide for the configuration.In the sample app that you cloned above, you need to apply the rule which syncs user signups to the postgres database. So every time a user signs up on the app, the user data is inserted into the database.Storing Media Files in ContentfulLet's create necessary content types in Contentful. Under Content model, create a new content type for Tracks and add the following fields -> id and track. We will use the id field to connect this data with Hasura track table. Note that track field is of Media type and we will make use of it to create audio source files.Adding Contentful as Remote SchemaTo be able to query Contentful data via Hasura, it needs to be added as a Remote Schema using the Hasura Console.Get the GraphQL Content API Endpoint in the following format:https://graphql.contentful.com/blog/content/v1/spaces/<space-id>\nAnd replace with the appropriate space id.In Contentful dashboard, click on Settings. Under Space Settings click on API keys. Copy the Space ID and paste in the above endpoint.Now copy Content Delivery API - access token and use it in Authorization headers like below:Authorization: Bearer <access_token>\nIn Hasura Console, head to Remote Schemas and enter GraphQL Server URL with the above contentful endpoint. Under Additional Headers, enter the Authorization header with the access_token as mentioned above.Create Remote Relationship from playlist_track table to tracksCollection type of Contentful.Under configuration for tracksCollection, we declare a where clause to say:id: From Column -> track_idThis ensures that while querying for tracks, we get only the relevant audio source files.Now the GraphQL query to fetch this data in a single API call would look like the following:query PlaylistQuery {\n      playlist {\n        name\n        tracks {\n          track_details {\n            name\n          }\n          track {\n            items {\n              track {\n                url\n              }\n            }\n          }\n        }\n      }\n  }Note that the nested query track comes from Contentful and it will apply the filter of track.id = playlist_track.track_id, there by giving only the relevant audio track information.AuthorizationWe have merged two different data sources and retrieved data in a single GraphQL query. With role based permissions, we can take care of the permissions required for the app. For example, a user should be able to add tracks only to the playlist that they created. Insert some sample data to see how the response structure looks like for the above GraphQL query.The track URL information comes from Contentful and more metadata can be fetched about that track. Using the URL, we build the playlist app showing the tracks belonging to the user's playlist."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: BBVA uses GraphQL and Hasura to build an open source security product Description: The BBVA team saw significant reduction in build time, app extensibility and documentation requirements thanks to the adoption of Hasura and GraphQL Author: Hasura"}, {"role": "assistant", "content": "The project saw significant reduction in build time, app extensibility and documentation requirements thanks to the adoption of Hasura and GraphQL BBVA Innovation Labs is a multidisciplinary team within BBVA that was formed to provide advances in the technology area at BBVA. The team does not limit itself to a specific field - their scope spans everything from AI and infrastructure to Big Data and MicroServices. One of their most recent projects is Deeptracy, an open source tool for analysing security issues in third party libraries used in a project. It chooses the most suitable security tool for each language and notifies development teams of vulnerabilities spotted in the project dependencies.Adopting GraphQL DeepTracy originally started life as a research project, but had matured enough that the bank was considering integrating it with their software. Roberto Martinez, the project owner, wanted to rewrite the project to make the integration process easier. Apart from making it smaller and more manageable, there were a couple of key issues he wanted to address: Since he was the sole project owner, he did not want to spend a lot of time on support activities such as documenting the API or communicating back and forth with a team that was going to use it. He could not envision all the use cases where DeepTracy will be used in the future. He did not want to have to keep building REST APIs or have teams modify DeepTracy constantly for their specific requirements. He felt that both these problems could be addressed by simply adopting GraphQL. Adopting HasuraRoberto was very impressed by Hasura\u2019s quality of code, robustness, documentation and the community that had sprung up around it. He tried it out, felt it was very easy to get started, and decided to use it for his rewrite. Outcomes Hasura reduced Roberto\u2019s development time from 2 months to about 3 weeks. Thanks to the GraphQL API, his maintenance workload was much lesser: It will be much easier to iterate on, as they do not need to write new APIs to fetch specific data. Once BBVA integrates their other software with DeepTracy, their documentation and communication requirements will be significantly reduced. Going forward, Roberto is interested in implementing remote schemas to further reduce the size of Deeptracy. Want to know how Hasura can help your team? Schedule a demo.This case study was prepared basis a discussion with Roberto Martinez, IT Project Manager at BBVA. \u00a0"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How Hasura scales to 1 million active GraphQL subscriptions (live queries) Description: A look at the architecture that lets Hasura scale to 1 million active GraphQL subscriptions (live queries).   Author: Tanmai Gopal"}, {"role": "assistant", "content": "Hasura is a GraphQL engine on Postgres that provides instant GraphQL APIs with authorization. Hasura allows 'live queries' for clients (over GraphQL subscriptions). For example, a food ordering app can use a live query to show the \u00a0live-status of an order for a particular user.We designed hasura to be scalable right out of the gate. We recently tested Hasura's performance to handle a million active live queries. As part of the test setup multiple clients (a web/mobile app) subscribe to data or a live-result with \u00a0an auth token. The data is in Postgres. 1 million rows are updated in \u00a0Postgres every second (ensuring a new result pushed per client). Hasura \u00a0is the GraphQL API provider (with authorization).The results: At 1 million live queries (and hence 1 million open websocket connections to Hasura), Postgres is under about 28% load with peak number of Postgres connections at around 850.We've written about the architecture that enables this performance (and some more notes on our benchmarking process and tooling) on our github repo. Check it out here: https://github.com/hasura/graphql-engine/blob/master/architecture/live-queries.md"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using GraphQL to join data across Google Places API and Postgres to build location based apps Description: Use Hasura Remote Joins to join geography data from PostGIS extension in your Postgres database with Google Places API in GraphQL. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRUse Hasura Remote Joins to join geography data from PostGIS extension in your Postgres database with Google Places API in GraphQL. Boilerplate -> Google PlacesHasura Remote Joins with Google Places API and PostGIS\u00a0This post is a part of our Remote Joins. Remote Joins in Hasura allows you to join data across tables and remote data sources. Data Federation using Hasura Remote Joins is now available in our latest v1.3 release.Try it out with Hasura Cloud and let us know what you think!In this example, we will look at how data from Google Places API can be joined with existing geography data in Postgres with PostGIS extension using Hasura.PostGISTo store and compute on geo-location data, we are going to use PostGIS which is a spatial database extender for Postgres. It adds support for geographical objects on Postgres and also comes with utility functions such as distance, area, union, and intersection.Enabling PostGISBefore we begin, we need to enable the PostGIS extension on Postgres.Head to Data tab in the API Console and click on SQL in nav bar on the left. And execute the following SQL commands:-- User location data\nCREATE TABLE user_location (\n  user_id INTEGER PRIMARY KEY,\n  location GEOGRAPHY(Point)\n);Insert Sample DataGraphQL Mutationmutation insertUserLocation($user_location_objects: [user_location_insert_input!]!) {\n    insert_user_location(objects: $user_location_objects) {\n        returning{\n            location\n            user_id\n        }\n    }\n}Variable would be the following:{\n \"user_location_objects\": [\n   {\n     \"user_id\": 1,\n     \"location\": {\n       \"type\": \"Point\",\n       \"coordinates\": [12.9406589, 77.6185572]\n     }\n   },\n   {\n     \"user_id\": 2,\n     \"location\": {\n       \"type\": \"Point\",\n       \"coordinates\": [12.939553, 77.6183303]\n     }\n   }\n ]\n}Now we want to fetch the top restaurants nearby the coordinates of the user location. Google Places API is available as a REST API to get this data.Adding Google Places API as Remote SchemaTo be able to query Google Places data via Hasura, it needs to be added as a Remote Schema using the Hasura Console.Deploy Custom ResolverAdd the following environment variables in the .env file on glitch.GOOGLE_MAPS_API_KEY=xxx \nPORT=3000Get the Google Maps API Key by visting the Google Maps Platform page.Set the API key as GOOGLE_MAPS_API_KEY environment variable.This custom resolver is used to accept a location argument of type \"geography\" of PostGIS and returns restaurants around a radius with name, rating and address details.Refer to API documentation for integration with other information like Photos, Reviews etc.Get the GraphQL API Endpoint from Glitch and add it as remote schema.Now let's add the Remote Relationship called restaurantsNow the GraphQL query to fetch this data in a single API call would look like the following:query {\n  user_location {\n    user_id\n    restaurants {\n      name\n      lat\n      long\n      address\n      rating\n    }\n  }\n}Notice that, the nested query restaurants come from Google Places API and it will apply the filter of user_location.location = restaurants.location, there by only giving restaurants data near by the user."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How KintoHub uses GraphQL and Hasura to power their real-time API infrastructure Description: KintoHub, a cloud-native PaaS provider, powers their real-time API infrastructure using GraphQL and Hasura. Author: Sandip"}, {"role": "assistant", "content": "About KintoHubKintoHub is a workflow service that manages cloud-native application lifecycle from source to production. It helps individual developers or engineering teams build, combine, deploy, and manage APIs, microservices, databases, and tasks using KintoHub\u2019s interactive dashboard.Building the real-time API stackFor a service that can be used to manage global-scale deployments and workflows for even complex apps, having a reliable and scalable real-time API infrastructure was a must-have at KintoHub.In late 2018, the engineering team delivered a proof-of-concept for a real-time CI/CD build-pipeline. At this point, the user-facing dashboard, that served as the center-piece of the developer experience on KintoHub, had to be manually refreshed by a user to update its statuses and statistics. To polish the experience while scoping the product\u2019s beta version, the team started researching various real-time API tools and technologies.Their choices were narrowed down to the following:A raw websockets-based infrastructure to support all realtime use-cases.gRPCA GraphQL-based solutionIt felt premature to adopt gRPC for web development as the frontend javascript libraries were in alpha at the time. Also, based on the volume of realtime use-cases in various frontend apps - like notifications, etc., another requirement of the candidate real-time solution was to ease/standardize the process of accessing information. Therefore, the team narrowed their focus to a GraphQL-based solution.Evaluating GraphQL and HasuraInitially, an MVP was built using MongoDB over 6 months to test GraphQL waters. This MVP ran into issues with scale and direct access to the database. This is when the team decided to try out Hasura.The team was encouraged to get familiar with Postgres and Hasura, especially focusing on Hasura\u2019s role-based access control features. A sign-up/accounts-management service for the dashboard was the first service to use Hasura\u2019s GraphQL API. A reverse-proxy to sit in front of Hasura and a custom JWT server to handle authentication were also built to support this MVP. This version of the MVP met the team\u2019s requirements, and they decided to adopt Hasura for other microservices as well. The team now leverages Hasura\u2019s GraphQL API by logging real-time infrastructure related events from their various services/workflows into their database, and accesses this data in realtime wherever required. This part of the stack has been open sourced as argo-monitor (coming soon) which makes it easy to funnel any workflow kubernetes event to Hasura, Mongodb or Postgres directly. Results from the evaluation processThe biggest win for the team was that the 2nd iteration of the MVP, based on Hasura and Golang, took 6 weeks to build and was able to fully replace the earlier version (that took around 6 months to build!). After deploying Hasura in production, a cloud-native deployment management service, built on the same stack, was benchmarked to be 40% faster than its predecessor, further bolstering the team\u2019s confidence in the solution. Next steps with HasuraKintoHub is preparing to port the last of their major features to Hasura, in time for OSCON 2019.They are also closely shaping Hasura\u2019s product pipeline with feedback from their personal experience of running it in production. One of the major areas of collaboration between the two organisations is the ability to deploy a global-scale, one-click multi-region Hasura solution that uses YugaByte as the datastore. Want to know how Hasura can help your team? Schedule a demo.The following case-study is based on a Q&A with Joseph Cooper, the CEO at Kintohub.com"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Contributor Days is back - join us for our frontend frameworks edition! Description: GraphQL Contributor Days is back. This edition will focus on the front end! The event will be live streamed on 8th Aug August 9am PST. Author: Rajoshi"}, {"role": "assistant", "content": "Photo by \u00c9mile Perron on UnsplashNext month we are partnering with This Dot for a special edition of GraphQL Contributor Days. This edition is frontend focused and will be fully online, live-streamed, and available for all to join in! This is an opportunity to explore cross collaborations and learn from the different communities using GraphQL. \u00a0Panelists and speakers from various frameworks, like React, Angular, Ember, and more, will discuss how they\u2019ve been working with GraphQL, their tooling of choice and more. We will also have many key developers who are working on GraphQL at work discussing these topics.The goal of Contributor Days is to not only give important updates to the community, but also a forum to provide a voice for anyone using GraphQL to have direct access to core contributors and authors of various libraries and frameworks through live chat and an online broadcast.This event will take place online on August 8, 2019 from 9am - 1pm PST. Some topics we\u2019ll be discussing:Getting started with GraphQLExperiences maintaining a GraphQL app: Pros/cons; complexity trade-offThe \u00a0GraphQL API: Ownership, control & maintenancePrecise vs generic APIs The API lifecycle:Documentation/explorationRegressions and testingDeprecationMonitoring/usage-analyticsProcess changes (around debugging/performance)Our featured guests include\u2026Tracy Lee: Founder,\u2008This\u2008Dot\u2008&\u2008RxJS\u2008Core\u2008TeamRajoshi Ghosh: Co-Founder, HasuraTanmai Gopal: Co-Founder, HasuraRyan Chenkie: Developer/TrainerCarlos Rufo: OSS DeveloperSara Vieira: Developer, CodeSandboxMark Stuart: Web Infrastructure, PayPalShruti Kapoor: Software Engineer,\u2008PaypalChristina Yu: Software Engineer,\u2008PaypalJoey Nenni: Web Engineering Lead, PayPalGabriel McAdams: MTS2 Software Engineer, PayPalEve Porcello: Instructor, Moon HighwayAlex Banks: Instructor, Moon HighwayAnd more to be announced! Check out the full details of the event here and register to get the link for the livestream and live chat!Hope to see you there! "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build apps with rich content using Contentful and Hasura Remote Joins Description: Join application data in your database with content in Contentful with GraphQL and Hasura Remote Joins Author: Praveen Durairaju"}, {"role": "assistant", "content": "Join application data in your database with content in Contentful with GraphQL and Hasura Remote JoinsContentful is an API First CMS to build digital products. It offers a GraphQL API which can be joined with Hasura using Remote Schema.Recently we launched Remote Joins feature (available in preview) which allows to join data across tables and remote data sources.In this example, we will look at how a simple blog CMS in contentful can be joined with existing data in Hasura. For this example, the existing data in Hasura can just be users with their blog posts data coming from remote data source like Contentful. The same pattern of data coming from contentful can be used to build other apps with rich content, like for example a playlist app where video metadata come from contentful.Alright, let's get started by creating the necessary content types in Contentful. Under Content model, create a new Content type for Blog Post and add the following fields -> title, content, created, cover_image, description, is_active, author. We will use the author field to connect this data with Hasura users table. Note that content field is of Rich text type and we will make use of it to create blog post content.Adding Contentful as Remote SchemaTo be able to query Contentful data via Hasura, it needs to be added as a Remote Schema using the Hasura Console.Get the GraphQL Content API Endpoint in the following format:https://graphql.contentful.com/v1/spaces/<space-id>\nAnd replace with the appropriate space id.In Contentful dashboard, click on Settings. Under Space Settings click on API keys. Copy the Space ID and paste in the above endpoint.Now copy Content Delivery API - access token and use it in Authorization headers like below:Authorization: Bearer <access_token>\nIn Hasura Console, head to Remote Schemas and enter GraphQL Server URL with the above contentful endpoint. Under Additional Headers, enter the Authorization header with the access_token as mentioned above.Create Remote Relationship from Users to blogPostCollection type of Contentful.Under configuration for blogPostCollection, we declare a where clause to say:author: From column -> idThis ensures that while querying for blogs in users, we get only the relevant blogs written by the user who is queried for. Let's go ahead and create a row in users table and a simple blog post in Contentful.Insert a new userThis creates a new user with the id 1.Insert a new blog in contentfulUnder author, let's give the value 1, so that the data in Hasura is linked.Now the GraphQL query to fetch this data in a single API call would look like the following:query {\n  users {\n    id\n    name\n    blogs {\n      items {\n        title\n        content {\n          json\n        }\n      }\n    }\n  }\n}Notice that, the nested query blogs come from Contentful and it will apply the filter of users.id = blogs.items.author, there by only giving blog posts written by the user."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Why Rapid Tender chose GraphQL and Hasura to build a real-time, collaborative tender-response app Description: The story behind Rapid Tender's adoption of GraphQL and why they chose Hasura to implement it. Author: Sandip"}, {"role": "assistant", "content": "About Rapid TenderRapid Tender offers technology and services that let organisations streamline their tender estimation operations. Their products have helped price over \u00a3150 million of construction work. Rapid Tender\u2019s GraphQL journey is being shepherded by Aidan, the Technical Director, and Simeon Filipov, Software Developer.Evaluating GraphQL & HasuraIn 2018, the Rapid Tender engineering team decided to evaluate the costs and benefits of adopting GraphQL by building a prototype admin app for collaborative tender responses. This prototype was powered by a custom GraphQL API built using Graphene, a Python GraphQL library. The team was impressed with how empowered the frontend team was to query any of the data served by the GraphQL API, significantly reducing the scope of their dev iterations. The one drawback that they noticed was the effort spent on developing the CRUD layer of the GraphQL API.Bolstered by these early results, this year the team decided to double down on their first iteration and compare their approach with ones based on GraphQL tools in the market. Several tools like PostGraphile, Prisma, AppSync, Hasura, etc. were researched for their offerings and compatibility with the team\u2019s existing stack and skills.The team decided to go with Hasura because of the following reasons:The ready-to-use GraphQL CRUD API on Postgres would significantly reduce the time to add GraphQL support for the entire database schema. Live-queries are perfect for implementing collaboration features, with zero investment into building out a custom websockets based system.The Auth/permissions system at the application layer instead of in the database was indicative of a scalable design. The quality of documentation and a helpful developer community. As part of their research, the team worked on a second iteration of the admin app using ReactJS and Hasura, and found that the implementation took far less time than the evaluation process itself! Using Hasura in productionThe team is gearing up for using Hasura in production and is in the process of beginning user trials with their clients. As part of the next set of iterations of their product, they want to leverage the highly scalable live-queries and customise them to implement more advanced features of their app like modelling cursor movements from multiple users in the same document \u00e0 la Google Docs. \u00a0They also plan on using Hasura for several other upcoming projects within the organisation, and leveraging newly released features such as Remote data joins on OneGraph APIs for Stripe, etc. The team is also collaborating with Hasura on enhancing the developer experience for handling migrations, onboarding content, etc.Want to know how Hasura can help your team? Schedule a demo."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a WhatsApp Clone with GraphQL, React Hooks and TypeScript Description: Build a Clone of WhatsApp using Hasura GraphQL, React with Hooks, Typescript and GraphQL Code Generator. Author: Hasura"}, {"role": "assistant", "content": "TL;DRWhat to expect from this two part tutorial? Realtime GraphQL APIs from HasuraAuthentication with JWT and role based permissionsReact frontend; 100% functional components with hooksTypescript definitions auto-generated using GraphQL Code GeneratorThis is a two part tutorial. The first part will be about building the backend using Hasura GraphQL Engine. The second one will be about building the frontend using React Hooks and Typescript. This is about the backend. The frontend tutorial is coming soon. But the app's source code is available here.Let's start building the backend by deploying Hasura.Hasura is an open-source engine that gives you realtime GraphQL APIs on new or existing Postgres databases, with built-in support for stitching custom GraphQL APIs and triggering webhooks on database changes.Deploy Hasura on Hasura CloudDeploy GraphQL Engine on Hasura Cloud and setup PostgreSQL via Heroku:\nGet the Hasura app URL (say whatsapp-clone.hasura.app)Database ModellingLet's take a look at the requirements for the basic version of WhatsApp where a user can initiate a chat with another user / group. The core of whatsapp revolves around users. So lets create a table for usersTypically the users table will have an id (unique identifier), username and password for logging in and metadata like name, picture and created_at for whatsapp's use-case.Head to the Hasura Console -> Data -> Create table and input the values for users table as follows:Here we are ensuring that username is a unique column and we make use of postgres default for the timestamp column created_at. The primary key is id , an auto-incrementing integer. (you can also UUID).Now, the next important model for whatsapp is the chat table. Any conversation between 2 users or a group will be considered as a chat and this will be the parent table. The model for chat table will have the following columns id (unique identifier) , created_at and properties of a group chat name , picture and owner_id. Since the chat will not always be a group chat, the columns name, picture and owner_id are nullable. The owner_id is a column which decides whether the chat is a private chat or a group chat.But irrespective of whether the chat is private or group, we need to map the users involved in each chat. Let's go ahead and create a new table chat_users which will have chat_id and user_id as the columns and both would form a composite primary key.Now that we have users, chat and their mapping ready, let's finish the missing piece, message. The message table will have the following columns; id (unique identifier), content (message content), created_at, sender_id and chat_id.The core database tables required for a simple WhatsApp clone is ready.Relational modelling with constraintsWe are using Postgres database underneath and hence we can leverage the relational features like constraints and relations.Let's add foreign key constraints to all the relevant columns so that we can fetch related data in the same query.Head to Data->Chat->Modify and create a foreign key constraint for the column owner_id which has to be a value of users->idNow, lets create foreign key constraint for chat_users table for both columns chat_id and user_idFor example, chat_id column would have a constraint on chat table's id column. Similarly create a foreign key constraint for user_id column which will point to users table's id column.Let's move on to message table to create the last two foreign keys required.sender_id :: users -> idchat_id :: chat -> idGreat! Now we are done with creating the necessary constraints for all the tables.Create relationships using consoleWe need to tell Hasura that relationships exist between two tables by explicitly creating them. Since we have foreign keys created, Hasura console will automatically suggest relationships available. For the foreign keys that we created just above, let's create corresponding relationships so that we can query related data via GraphQL.We need to create relationships from chat to users as an object relationship. It is a one to one relationship.Similarly we need to create array relationships from chat to chat_users and message tables. A chat can have multiple messages and can involve multiple users.Using the suggested array relationships from the console, let's create the following chat_users.chat_id -> chat.id and message.chat_id -> chat.idAnd in users table, we create the necessary array relationships for the following:chat.owner_id -> users.idchat_users.user_id -> users.idmessage.sender_id -> users.idQueriesHasura gives instant GraphQL APIs over Postgres. For all the tables we created above, we have a ready to consume CRUD GraphQL API.With the above data model, we should be able to query for all use cases to build a WhatsApp clone. Let's consider each of these use-cases and see what the GraphQL query looks like:List of all chats - In the landing page, we would like to see the list of chats along with the user information. The chat list should be sorted by the latest message received for each chat.query ChatsListQuery($userId: Int!) {\n    chat(order_by:[{messages_aggregate:{max:{created_at:desc}}}]) {\n      id\n      name\n      picture\n      owner_id\n      users(where:{user_id:{_neq:$userId}}) {\n        user {\n          id\n    \t  username\n    \t  name\n    \t  picture\n        }\n      }\n\t  \n    }\n}We make use of relationship users to fetch relevant user information and filter by user who is already logged in and belongs to the same chat.List of all users/groups - We would like to also list down all the users or groups that the user belongs to, so that a conversation can be initiated.query ExistingChatUsers($userId: Int){\n    chat(where:{users:{user_id:{_eq:$userId}}, owner_id:{_is_null:true}}){\n      id\n      name\n      owner_id\n      users(order_by:[{user_id:desc}],where:{user_id:{_neq:$userId}}) {\n        user_id\n        user {\n          ...user\n        }\n      }\n    }\n}List of messages in a chat - This query will give a list of messages for a given chat, along with sender information using a relationship. query MessagesListQuery($chatId: Int!) {\n\tmessage(where:{chat_id: {_eq: $chatId}}) {\n        id\n        chat_id\n        sender {\n          id\n          name\n        }\n        content\n        created_at\n  \t}\n}MutationsNow that the queries are ready to fetch data, let's move on to Mutations to make modifications like insert, update or delete in the database.Insert chat/group (nested mutation) - The first time a conversation occurs between two users, we need to make a mutation to create a record in chat and create two records in chat_users . This can be done using Hasura GraphQL's nested mutations.mutation NewChatScreenMutation($userId: Int!,$currentUserId: Int!) {\n    insert_chat(objects: [{\n      owner_id: null,\n      users: {\n        data: [\n          {user_id: $userId},\n          {user_id: $currentUserId}\n        ]\n      }\n    }]) {\n      affected_rows\n    }\n  }Insert message - To insert a message, we can issue a simple mutation to message table with the relevant variables.mutation MessageBoxMutation($chatId: Int!, $content: String!, $sender_id: Int!) {\n    insert_message(objects: [{chat_id: $chatId, content: $content, sender_id: $sender_id}]) {\n      affected_rows\n    }\n}Delete chat - To delete a chat, (either one-one or a group), we can issue a simple delete mutation. But to avoid dangling data, we would issue a delete to cascade all rows from chat_users and message as well apart from chat.mutation deleteChat($chatId: Int!) {\n    delete_chat_users(where:{chat_id:{_eq: $chatId}}) {\n      affected_rows\n    }\n    delete_message(where:{chat_id:{_eq: $chatId}}) {\n      affected_rows\n    }\n    delete_chat(where:{id: {_eq: $chatId}}) {\n      affected_rows\n    }\n}Note: We could also create an on delete constraint in postgres which takes care of this automatically. But the above mutations are shown for demonstration.Update user profile - Finally, we need a mutation to update user's profile data like the name, profile picture etc.mutation profileMutation($name: String, $picture: String, $userId: Int) {\n    update_users(_set: {name: $name, picture: $picture}, where: {id: {_eq: $userId}}) {\n      affected_rows\n      returning {\n        id\n        name\n        picture\n        username\n      }\n    }\n  }SubscriptionsNow comes the exciting part! Realtime data. We need a way to notify the user when a new message has arrived. This can be done using GraphQL Subscriptions where the client watches for changes in data and the server pushes data to client whenever there is a change in data via websocket. We have two places where we need realtime data. One for new messages, and one for users who are registered.Subscribe to latest messagessubscription MessageAdded {\n    message_user {\n        id\n        chat_id\n        sender {\n        id\n        name\n        }\n        content\n        created_at\n    }\n  }Subscribe to userssubscription UserUpdated {\n    users(order_by:[{id:desc}]) {\n      id\n      username\n      name\n      picture\n    }\n}Permissions and AuthThe Auth API for signup and login is provided by a simple JWT server. The source code for the server is available here.Now coming to Authorization, Hasura allows you to define role based access control permissions model. In our data model, there is one role for the app and its the user role. We need to define permissions for this role for the tables created above. To insert into chat, a user role needs to have the following permission - The user should be the owner of the chat or the user should belong to the chat that they are creating.Similarly for users table, insert permission looks like the following:We are substituting session variables to match the id of the user.Similarly we can define permissions with the same condition for all operations.Check out the metadata for the complete list of permissions.What's Next?The app data model currently doesn't have the following features: Typing indicator and read receipts. This can be introduced with minor updates to schema.Now that the backend is ready, we can move forward with the integration of GraphQL APIs with the React Frontend with Hooks, Typescript and GraphQL Code Generator.Watch out for this space for the second part of the tutorial involving the frontend!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Common access control patterns with Hasura GraphQL Engine Description: In this post, we will look at some access control patterns that can be used with Hasura to granularly allow/restrict the data Author: Rishichandra Wawhal"}, {"role": "assistant", "content": "IntroductionIn this post, we will look at some access control patterns that can be used with Hasura to granularly allow/restrict the data. This is a summary blog post from Hasura Streams and the video has been uploaded to Youtube. If you prefer watching, similar examples have been covered in this video:Hasura GraphQL EngineHasura GraphQL Engine is a thin GraphQL server that sits on any Postgres database and allows you to CRUD the data with realtime GraphQL and access control.This post assumes that you have basic understanding of Hasura and relational data models. Check out this guide if you have never used Hasura before.Hasura enables role based access control which can be integrated with most Auth providers. The access control rules in Hasura are functions of session variables. Session variables are x-hasura-* variables like x-hasura-role, x-hasura-user-id that can be decoded from the request headers of the GraphQL request. You could have any number of session variables to make the rules more granular.Auth flow with GraphQL EngineLet us see how to set up access control rules as functions of these session variables. Hasura infers the GraphQL schema from the Postgres schema, which means that setting access control rules on the tables, columns and their relationships corresponds to setting access control rules on the fields of your GraphQL schema.PermissionsThe access control rules are referred to as Permissions in the Hasura console. When you go to any table, you can see a permissions tab for that table where you can set permissions.UIThe Hasura console has a neat UI for setting permissions. It also has a filter-builder that will make building filters and checks very joyful.RolesWith each GraphQL request made to it, Hasura checks the role of the client user in the session variable x-hasura-role. Looking at this role, Hasura looks for the permissions for this role and allows or restricts CRUD accordingly.The roles in Hasura have nothing to do with the Postgres roles and users. These roles are implemented at the Hasura Layer.You can define these roles in the Hasura console and there is no limit on multiple roles being defined.Insert PermissionThe permissions to insert rows in a table is composed of three parts: Check constraint: This is a boolean value built out of session variables, values of the fields of the row being inserted and all the logical operators. For example, to insert into users table, you want to set a condition like { \"id\": { \u00a0\"_eq\": \"x-hasura-user-id\" } }. This condition enforces that whenever a row is being inserted to the users table, it can only be inserted if the id of the row is equal to the x-hasura-user-id value in the session variables.Columns: You can restrict insertion to only particular columns (rest of them being null, default or being inferred from column presets)Column Presets: Column presets are values that you can set to be assigned to fields while they are being inserted. For example, when an entry is being inserted in a users table, we can take the id from x-hasura-user-id session variable. This helps to avoid parsing the session information on the client.Select PermissionFilter: This is a boolean value built out of session variables and the values of the fields being selected. For example, in an articles table, you want all users to be allowed to query all published articles, but only their own unpublished articles. To implement that, you would add a filter like:{\n  \"or\": [\n    {\n      \"author_id\": {\n        \"_eq\": \"x-hasura-user-id\"\n      }\n    },\n    {\n      \"is_published\": true\n    }\n  ]\n}Columns: Sometimes you want some roles to not have access to particular columns of the table. In such cases, you can explicitly choose which columns to allow and restrict.Update PermissionFilter: This is a boolean value built out of session variables and the values of the fields being updated. For example, in an articles table, you want users to modify only their own articles. To do that, your update filter would look like:{\n  \"author_id\": {\n    \"_eq\": \"x-hasura-user-id\"\n  }\n}Columns: You can restrict which columns can be updated. This comes handy in cases when you do not want created_at field to ever be updated, but you might want to update the title field in the articles table.Column presets: Like with insert, if you want to automatically update certain fields with certain values without it being explicitly mentioned in the request.Delete PermissionFilter: Like with update, the filter on delete permisson is a boolean condition which needs to be satisfied before the the row can be deleted.ExamplesWith the above ideas, let us look at some specific cases and how they can be modelled. Firstly, lets use a base schema for a HackerNews like application. This is the postgres schema:Let me show you a sample GraphQL query to get all the articles along with their authors, comments, upvotes and downvotes.query {\n  articles {\n    id\n    title\n    content\n    content_type\n    author {\n      id\n      name\n    }\n    comments {\n      body\n      id\n      author {\n        id\n        name\n      }\n    }\n    article_upvotes_aggregate {\n      aggregate {\n        count\n      }\n    }\n    article_downvotes_aggregate {\n      aggregate {\n        count\n      }\n    }\n  }\n}Now let us try to target some specific access control use cases with the above schema.Enforcing users to insert articles as themselvesSay I am a user with id = 123. This means that the value of x-hasura-user-id in my session information would be 123. Now, when I am inserting an article in the articles table, you would want the author_id to be 123, which means you would want me to insert the article with myself as the author. To enforce that, you could take two approaches:Through check constraint: In the insert permission of the articles table, you can set a check constraint like { \"author_id\": { \"_eq\": \"x-hasura-user-id\" } }. This would ensure that the article would be inserted only if the author_id in the insert payload matches the x-hasura-user-id in the session variables.Through column presets: In the insert permission of the articles table, you can disable inserting into author_id column and set a column preset such that the author_id is automatically taken from the x-hasura-user-id session variable. In this way, the author_id would be inserted appropriately without the client needing to explicitly mention it.You can use exactly the same approach for restricting users from updating articles that are not published by them.Multiple rolesSay you have two roles: user and moderator. A moderator should be allowed to update the title of every post unconditionally while a user should be allowed to update the title only for their own posts. So the update permission for:moderator: Should be allowed to update title and content of without any filter.user: Should be allowed to update the title and content with the filter { \"author_id\": { \"_eq\": \"x-hasura-user-id\" } }.Also, the moderator should be allowed to delete every post while a user should be allowed to delete only their own posts. So the delete permission for:moderator: Should be allowed to delete without checks.user : Should be allowed to delete \u00a0with the filter { \"author_id\": { \"_eq\": \"x-hasura-user-id\" } }Access control through viewsIn a typical news discussion app, the downvotes or upvotes on an article must be anonymous. This means, that the users should not be able to get all the information from article_downvotes and article_upvotes tables. To achieve anonymity, you can create a view that just has the article_id and the number of upvotes or downvotes. For article_downvotes, the view would look like:CREATE VIEW article_downvote_count as\nSELECT article_id,\n  count(*) AS downvotes\nFROM article_downvotes\nGROUP BY article_id;Now, once this view is created, you can set a select permission on this view such that it can be selected without any checks. You can implement the restriction similarly for article_upvotes table as well.Downvoting based on KarmaIn a website like HackerNews, you get down-voting privilege only when you reach a particular Karma (say 500). So on a table like article_downvotes which is a many to many relationship between articles and users, your insert permission would have a check:{\n  \"_and\": [\n    {\n      \"user\" : {\n        \"id\": {\n          \"_eq\": \"x-hasura-user-id\"\n        }\n      }\n    },\n    {\n      \"user\": {\n        \"karma\": {\n          \"_gte\": 500\n        }\n      }\n    }\n  ]\n}The above insert permission ensures that:The user is inserting an entry as themselves (author_id is equal to x-hasura-user-id)The insert is successful only if the inserting user has karma greater than or eual to 500.Enforcing fields to have only particular valuesYou can emulate enum-like behavior using Hasura Permissions. For example, in the articles table, the content_type field should be either \"url\" or \"text\". Any other value is invalid. Therefore, the check condition in the insert permission on articles table looks like:{\n  \"_and\": [\n    {\n      \"user\" : {\n        \"id\": {\n          \"_eq\": \"x-hasura-user-id\"\n        }\n      }\n    },\n    {\n      \"content_type\": {\n        \"_in\": [\"text\", \"url\"]\n      }\n    }\n  ]\n}This permission makes sure that:The user is allowed to insert only as themselves. (author_id is equal to x-hasura-user-id)In the article being inserted, the content_type field must be either text or url.You can similarly add different complicated access control rules for different roles and secure your data. Let us know if you have any questions in comments or join our Discord channel where we are super active. Also, let us know if you need more examples in the comments.ReferenceDocsGraphQL EngineVideo"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build and Deploy Realtime Svelte 3 Apps Using GraphQL Description: Build a Realtime Svelte.js 3 App using GraphQL (svelte-apollo and Hasura) and Deploy on Now 2.0 Platform. Instant setup/sample-app ? svelte-apollo Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRBuild a Realtime Svelte.js 3 App using GraphQL (svelte-apollo and Hasura Cloud) and Deploy on Now 2.0 Platform. Instant setup/sample-app -> svelte-apolloWhat is Svelte?Svelte is a component framework that compiles components at build step (unlike React/Vue) into highly efficient imperative code that updates the DOM. Svelte gives a single bundle.js that can be referenced in the app's HTML which doesn't contain any framework specific dependencies (magically disappearing framework was their tagline before). The bundles are usually smaller than other frameworks with greater emphasis on accessibility.This also means that there is no virtual DOM and no extra work like virtual DOM diffing and it looks like there are performance benefits to it. State management is made easy since its just javascript variables and updating them would re-render the component by updating the DOM directly (truly reactive)You will also see in the sample app that there's less boilerplate code than other frameworks to achieve the same functionality. Svelte released their stable version 3 last month and here's an interesting blog written by Rich Harris if you would like to learn some details.Alright! Now let's see how a simple realtime app can be made using Svelte and GraphQL. To get started, we will deploy Hasura.Setup Hasura Cloud projectHasura Cloud is a fully-managed GraphQL-API-as-a-service that gives you realtime GraphQL APIs on new or existing Postgres databases, with built-in support for stitching custom GraphQL APIs and triggering webhooks on database changes.\nFollow the instructions in the readme to create the tables author and article required for the app. Note the Hasura URL for GraphQL Endpoint. You will be configuring this in the app.Clone and Run AppThe demo app walks you through to build a simple realtime app, listing all authors and articles. In this example, we will look at building a simple app to Fetch articles and authors using GraphQL Queries, Adding an author using GraphQL Mutations and Demonstrate realtime capabilities using GraphQL Subscriptions by updating authors list in realtime.To integrate GraphQL into Svelte, we will make use of svelte-apollo module to perform queries.You can clone and run the app for the demo.$ git clone git@github.com:hasura/graphql-engine.git\n$ cd graphql-engine/community/sample-apps/svelte-apollo\n$ npm installAfter installation, you will have to configure the GraphQL endpoint that you got above after creating a new project on Hasura Cloud.Open src/apollo.js and configure the endpoint.Now run the app usingnpm run devNote: This app was forked off from the standard template and svelte-apollo dependency was added to integrate GraphQLHere's an example of a Svelte component making a GraphQL query to fetch list of articles:Here, we are importing the query method from svelte-apollo and passing in the client and the GraphQL query to fetch the list of articles.There's also the restore API to restore a previously executed query into Apollo Cache. The previous query came through preload method called during app initialisation.And subscriptions are pretty easy as well:If you looked at it carefully, this would be slightly similar to the React hooks style of implementation in the sense of less boilerplate code. Bundle using RollupSvelte comes with default Rollup bundling and configuring Rollup is also fairly easy. When you run the command npm run build , rollup generates a bundle.js, which is referenced in index.html and bundle.js doesn't contain any framework specific dependencies like React or Vue, since Svelte bundles at compile time.DeploymentDeploying a Svelte app is as simple as deploying a simple static HTML app with javascript and css files.Let's use now-cli to deploy the app. Install the now cli, if you don't have already.npm install -g nowAnd from within your project folder, execute the following command:now\nThis will deploy our svelte 3 app on Now using the configuration available in now.jsonObviously it can be deployed on popular providers like Netlify, Heroku etc.Final ThoughtsSvelte comes with least learning curve despite deviating from JSX style familiar to React developers and I would personally compare the syntax to Vue, as in you focus on writing HTML separately.If you are familiar with React/Vue and other virtual DOM based frameworks, you can try out Svelte for less boilerplate code and potential performance benefits. Of course GraphQL integration is pretty easy and you get all the benefits with lesser effort.There's also an SSR framework for Svelte named Sapper, if you are concerned about SEO.Take the demo for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on twitter, github or on our discord server."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Triggering Serverless Zeit Now on Postgres Events with Hasura Description: Build a 3factor app with Hasura\u2019s instant realtime GraphQL and leverage serverless deployments of Now 2.0 for deploying GraphQL APIs and webhooks. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRBuild a 3factor app with Hasura\u2019s instant realtime GraphQL and leverage serverless deployments of Now 2.0 for deploying GraphQL APIs and webhooks. Read more about building 3factor apps with event-driven programming.In this post, we will look at building a 3factor app which leverages Hasura for the real-time GraphQL layer and Now 2.0 for the serverless deployments. The app is a simple demo featuring a GraphQL API to fetch profile informationAn external API providing weather information, extending Hasura API (deployed to Now)A serverless function which is a utility to reverse the name of the profile.Before we deep dive into building an app with GraphQL and Serverless, let\u2019s quickly look into what a 3factor app is, and why serverless could be beneficial.3factor App3factor is an architecture pattern for modern full stack apps. The architecture of a 3factor app is composed ofRealtime GraphQLReliable Event SystemAsync serverlessWith these, we can build apps with high feature velocity and scalability. In short, a 3factor app dictates removing state from your code, and storing it in your datastore and/or event queue as much as possible.Why ServerlessServerless is an exciting functional paradigm. It helps minimize backend ops with lesser maintenance. Business logic can be written as smaller independent functions with automatic failover. Microservices can be deployed as these independent stateless functions.It also helps you scale by being cost efficient. Effectively you pay for usage rather than reserved time. It covers the use case of Async events. Serverless platforms like Now take care of the runtime. With Now, it has become easier than ever to deploy serverless functions and build rich experiences.Hasura GraphQL + NowLet us build the demo app to see the benefits of having a serverless architecture.Realtime GraphQL on Postgres Hasura gives you instant realtime GraphQL on Postgres. It runs as a container that you can host next to your database provider. Both Hasura and the Postgres database can be hosted on production-ready cloud providers like AWS. For simplicity of this demo, we are going to deploy Hasura GraphQL Engine on Heroku. We are going to build this demo the 3factor way.Quickstart with Heroku1. Click on the link below to deploy to Heroku. (You can sign up for free) \u00a0 \u00a0Deploy to Heroku2. Head to https://<YOUR_HEROKU_APP>.herokuapp.com and open your app. You should see the Hasura console.3. Now let\u2019s create a table called profile. Head to Data->Create Table and enter the following details to create a profile table. It will have the following columns: id, name, email , is_active, created_at. Select id as the primary key and click on Create.4. Now insert some sample data into the table by going to the Insert Tab.5. Let\u2019s query the data we have inserted by making a GraphQL Query. Head to GraphiQL tab and enter the following query and click on Play.query {\n  profile {\n    id\n    name\n }\n}Notice how easy it was to get an API that your client can consume. Similarly, you can make a mutation query to perform insert/update/delete. This means that you do not need to write any code to get CRUD APIs for your client app. All of it has been generated by Hasura. You can also configure permission rules for your users, at both row level and column level. Read more about access control with examples here. Authorization works with setting up webhooks or with JWT and hence it is not tied down to any Auth provider. For brevity of this post, we are skipping Auth.Extend Hasura with Remote SchemasAs you learned above, Hasura gives you instant GraphQL APIs over Postgres. But now your app will have custom logic that cannot be implemented via CRUD APIs. A common use case would be to make payments, doing some data processing etc. You can easily extend Hasura by adding your custom logic as Remote Schemas.For this example, we are going to extend the above Hasura GraphQL API with a custom weather API which can be queried upon by the client. Clone the source for the weather API from here.Deploy GraphQL API on ZeitOnce you have cloned the above repo, you can deploy to Zeit Serverless using the following command. Navigate to remote-schema folder and execute the following command.$ nowThis will give a GraphQL Endpoint for the custom weather API. Again notice how easy it is to deploy your custom logic to a serverless backend. Now let\u2019s test the API by adding it as a Remote Schema to Hasura, so that the client can query both using the same endpoint.Create Remote Schema on Hasura ConsoleHead to Remote Schemas tab on the Hasura console and add the weather API. The GraphQL server URL will be the now.sh deployment URL, that we just did above.Now, let\u2019s make a query in GraphiQL to see if all the APIs have been merged.query {\n  cityWeather(city_name:\"Bangalore\") {\n    max_temp\n    min_temp\n    applicable_date\n  }\n}Query GraphQL schema deployed on ZeitThe above is an example to fetch weather information for a given city name. (Bangalore in this case). This way, you can effectively write custom business logic in any language and deploy it as a GraphQL API to be added as a Remote Schema. You can deploy as many custom logic backends on Now 2.0 and extend Hasura with it.Trigger async eventsThere are parts of your application which can be asynchronous. A simple use case would be to send an email to a user who has just signed up or send an email to a customer after order placement in an e-commerce flow or do custom validations or data manipulations and scheduled jobs.Hasura can be used to create event triggers on tables in the Postgres database. Event triggers reliably capture events happening on the specified tables and then call configured webhooks to carry out some business logic. This business logic is recommended to be written in serverless backends, because they scale from the beginning, easy to deploy with less maintenance. Let\u2019s now deploy the custom logic written in Node.js to trigger based on an insert on profile table.Deploy functions on ZeitClone the Node.js function from here, which reverses the name of the profile.This function will be triggered on insert into profile table. Whenever a new row is inserted into profile table, we will capture that event and reverse the name of the profile.Deploy this function to Zeit using the following command:$ now -e HGE_ENDPOINT='https://myapp.herokuapp.com/v1/graphql'Replace it with your Hasura GraphQL endpoint.You have now deployed your serverless function on Zeit.Create event trigger on Hasura ConsoleLet\u2019s add the serverless function to the Hasura console as follows:Enter the trigger name, select the profile table and operations (insert) and finally enter the webhook URL as the now.sh URL that was deployed in the above step.Setup retry configTo handle errors or external dependency downtimes, you can setup retry config to a specific interval under Advanced Settings.Now let\u2019s insert a user into the table and see if this function is being triggered. Go to Insert row tab and insert a new row. Head over to Events tab and look at the Processed Events.You will find that there is a new event that was triggered on insert into profile.The name of the user inserted has been reversed. You can verify this by making a query to the profile table or by visiting the Browse tab.The above workflow completes the 3rd factor of a 3factor.app - Async serverless.Now 2.0 helps in deploying the serverless functions and custom GraphQL APIs easily. Since these functions are deployed in a serverless environment, it can be scaled on the go."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Resetting Hasura migrations Description: A guide to squash or reset migrations for your Hasura project. Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "If you have too many migrations and want to squash them or your current migration state on the local machine is corrupted, you can reset the state and create new migrations from the state that is on the server.Step 1: Nuke local migrationsDelete all the contents of your local migrations directory.$ rm migrations/*Step 2: Reset the migration history on serverOn the SQL tab of console, execute the following statement:TRUNCATE hdb_catalog.schema_migrations;Step 3: Pull the schema and metadata from serverSetup fresh migrations by taking the schema and metadata from the server:## (available after version alpha45)\n## create migration files (note that this will only export public schema from postgres)\n\n$ hasura migrate create \"init\" --from-server\n\n## note down the version\n## mark the migration as applied on this server\n\n$ hasura migrate apply --version \"<version>\" --skip-executionIf you are using schemas other than public, use --schema \"schema_name\" flag to indicate each one of them in the create command. This flag can be used multiple times. See more details about the usage in the docs.Step 4: Verify the statusExecute the following command to verify status of migration:$ hasura migrate statusYou have brand new migrations now!This can also be used to combine (kind of squash) all of your migration files into a single one. You're snapshotting the state of a server and adding it as a new migration."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: ARTEMIS Description: ARTEMIS uses Hasura to implement a real-time GUI to fetch data from a Postgres database.  Author: Harsha"}, {"role": "assistant", "content": "ARTEMIS uses Hasura to implement a real-time GUI to fetch data from a Postgres database. Photo by Chris Barbalis on UnsplashARTEMIS is a tool that monitors BGP for potential hijacking attempts against a network's own prefixes. It is an open source project by the INSPIRE group, FORTH, Greece and the Center for Applied Internet Data Analysis, University of California San Diego, USA.The system detects such attacks within seconds, enabling immediate mitigation. ARTEMIS has been tested at a major greek ISP, a dual-homed edge academic network, and a major US R&E backbone network. ARTEMIS is built as a multi-container docker application. The code is divided into backend and frontend sections. The ARTEMIS team needed a middle layer to implement a real-time GUI to fetch data from their Postgres database. They considered several solutions until they came across Hasura on a few discussion sites. After doing a dry run on Hasura\u2019s console and deploying a working demo, the team decided to migrate to Hasura. Hasura is now used as the main API to access the Postgres database on the frontend. Adopting Hasura lets ARTEMIS take full advantage of GraphQL\u2019s features, and drop REST when not needed.This blog post was prepared basis a discussion with Vasileios Kotronis, a Postdoctoral Researcher at Foundation for Research & Technology \u2013 Hellas (FORTH) and a member of the INSPIRE Group"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: Delicious Simplicity Description: A boutique design & development company uses Hasura, Gatsby and JAMstack to build out a blazing fast static site. Author: Harsha"}, {"role": "assistant", "content": "A boutique design & development company uses Hasura, Gatsby and JAMstack to build out a blazing fast static site.IntroductionDelicious Simplicity (DS) is a boutique design & development company based in Los Angeles. They specialize in building websites and webapps with a heavy emphasis on great UX. Their client base is mainly large non-profits and news & media outlets. For a recent project, they were tasked with building out a static web app that contained a lot of real-time functionality. The web app included a lot of functionality in common with popular video platforms (such as Youtube) - for example, user accounts, playlists and watched video history. In addition, the backend for this project also had to keep in mind a mobile app that the client was building separately with another team. It was decided to use the same backend for both the static web app and the mobile app. This project presented DS with a number of technical challenges that they were able to navigate thanks to GraphQL and Hasura. Adopting GraphQL and HasuraDS decided to use the JAMstack for their static web app, with Gatsby for the frontend. DS also decided to use GraphQL for their APIs so that the same backend could be used (without too much iteration on the APIs) for building the mobile app. One question they had was to understand how user-authenticated actions work inside a static site - at first they were not even sure if it was possible to add YouTube like functionality to a static site. DS attended JAMstack Conf to get an answer to this question, and it was here that they were introduced to Hasura. After trying Hasura out over a weekend, DS was amazed at how quickly they were able to setup GraphQL and an authentication system for a Gatsby site using Hasura, with the kind of extremely fine grained access control system that they would need for the user-authenticated actions they had in mind.OutcomesDespite being their most ambitious \u00a0JAMstack project to date, DS was able to finish the project with very few developer resources on a very tight deadline. What\u2019s more, the Hasura backend that they set up was also flexible enough that it served most of the needs of the mobile app that was being built by a separate team outside of DS. \u201cHasura was integral to the success of this project. It allowed our team to setup a complete static site with authentication and real-time functionality. Not to mention the support and communication we received from Hasura throughout the process was flat-out incredible!\u201d \u00a0- Brian W.Future RoadmapFor this project, Delicious Simplicity plans to add more functionality to the app, such as the ability to download videos and watch offline, and expanding the functionality of playlists and watchlists. They are confident that Hasura will work for them as the application becomes more complex. Delicious Simplicity also plans to use Hasura for their future projects - \u00a0next up is a realtime chat app! Want to know how Hasura can help your team? Schedule a demo.This case study was prepared basis a discussion with Brian Webster, Founder of Delicious Simplicity. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Contributor Days - Enterprise Edition! Description: A special edition of GraphQL Contributor Days focused on the challenges, nuances and benefits of adopting GraphQL inside an Enterprise organization. Happening on 15th May 2019.  Author: Rajoshi"}, {"role": "assistant", "content": "Photo by Chris Leipelt on UnsplashWe are ecstatic to announce that next month we will be partnering with This Dot for a special edition of GraphQL Contributor Days. This special edition of GraphQL Contributor Days is enterprise focused, so if you\u2019re implementing GraphQL at your enterprise company, please join us!This is also the first GraphQL Contributors Days that is fully online and available for all to join in! We\u2019ve invited a few companies who have implemented GraphQL and are at different stages of their GraphQL journey to discuss the challenges and nuances on using this technology.Another topic that will be addressed during this event will be how to collaborate with other GraphQL language libraries in the ecosystem. Key contributors to language libraries for GraphQL Java, GraphQL PHP, and GraphQL TypeScript have been invited to discuss and share their best practices and concerns within GraphQL. Along with conversations around enterprise implementations and varying language libraries, we will be discussing the new GraphQL Foundation will special guests like Lee Byron.The goal of Contributor Days is to not only give important updates to the community, but also a forum to provide a voice for anyone writing GraphQL to have direct access to core contributors and authors of various libraries and frameworks through live chat and an online broadcast.This event will take place online on May 15, 2019 at 11am - 3pm PST. Our featured guests include\u2026Lee Byron, Co-creator of GraphQLTanmai Gopal, Founder of HasuraHQBenedikt Franke, Co-Maintainer of LighthouseConnor Hastings, Senior UI Engineer NetflixEllie Day, Senior Product Engineer AtlassianEran Kampf, Tech Lead Manager at EbatesJon Wong, Staff Software Engineer at CourseraRyan Chenkie, Developer / TrainerShruti Kapoor, Software engineer at PayPalTim Griesser, Senior Software Engineer at CypressTyler Martinez, Frontend engineer at Airbnb Ivan Goncharov, GraphQL Consultant at APIs.guruRohan Deshpande, Principal Engineer at AWS AppSyncRafael Staib, Founder of ChilliCreamMichael Staib, Founder of ChilliCreamMadhu Jahagirdar, Principal Architect at Philips Healthcare Hugh Wilson, Core Developer at ApolloTracy Lee, Founder of This\u2008Dot\u2008&\u2008RxJS\u2008Core\u2008TeamRajoshi Ghosh, Co-Founder of HasuraHQAnd more to be announced! Check out the full details of the event here and register to get the link for the livestream and live chat!Hope to see you there! "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build a realtime mobile chat app using 3factor architecture Description: A guide to building a chat app using the 3factor architecture Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Umar Abdullahi and published as part of the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.What is 3factor architecture?3factor architecture design pattern leverages on realtime communication, eventing and serverless computing to allow you build full stack apps quickly and efficiently. The architecture pattern is composed of three factors, obviously :), which are realtime GraphQL, reliable eventing (eg. Hasura event triggers) and async serveless.Building scalable and high feature velocity apps sounds nice, but when directly dealing with servers and tightly coupled components, achieving scalability and high feature velocity can be quite challenging. With the 3factor architecture, GraphQL and serverless allows you to ship new features very quickly by improving developer experience on the front end and abstracting away the development operations respectively. Serverless also means auto-scaling.Read more about building 3factor apps with event-driven programming.PrerequisitesBefore we continue. We are going to use the Hasura GraphQL Engine, Hasura event triggers, and Google cloud functions to satisfy the three factors of the 3factor architecture. A prior knowledge of GraphQL, Go, and React native is needed. As our cloud functions would be written in Go and our mobile app would be written in React native and Expo.ModellingHasura GraphQL engine would automatically generate our GraphQL schemas and resolvers based on our tables and views in Postgres. So we\u2019ll have to model the necessary tables and views for a minimal chat app.UserWe\u2019ll need a user table to store our users.user (\n  id SERIAL PRIMARY KEY\n  username TEXT UNIQUE\n  push_token TEXT\n  last_seen timestamp with time zone\n  last_typed timestamp with time zone\n  date_created timestamp with time zone DEFAULT now() NOT NULL\n)\n\nTo create a user, only a username is required for our simple chat app, no passwords and email address. The last_seen and last_typed fields would be used to implement features like typing indicators and user online status.RoomTo be able to categorize discussions we\u2019ll need to store messages of similar topics in a room table.room (\n    id SERIAL NOT NULL PRIMARY KEY\n    name TEXT NOT NULL\n    room_type TEXT NOT NULL\n)\n\nAll rooms are public by default i.e any user can join, we won\u2019t be needing fields like is_private to identify private rooms in this example.MessageMessages sent by users will be stored in the message table.message (\nid SERIAL NOT NULL PRIMARY KEY\n\"text\" TEXT NOT NULL\nusername INT FOREIGN KEY REFERENCES user(username) NOT NULL\nroom_id INT FOREIGN KEY REFERENCES room(id) NOT NULL\n\"timestamp\" timestamp with time zone DEFAULT now() NOT NULL\n)\n\nThe room_id and username would be used to identify which user sent the message and which room it belongs to respectively.User_roomsWe are creating a user_rooms table to be able to get a list of rooms a user belongs to.user_rooms (\n    id SERIAL NOT NULL PRIMARY KEY\n    user_id INT FOREIGN KEY REFERENCES user(username) NOT NULL\n    room_id INT FOREIGN KEY REFERENCES room(id) NOT NULL\n)\n\nEach single row would represent a user and a room joined. We can easily remove a user from a room by using the delete_user_rooms mutation.Note: To fetch all users online we\u2019ll need an online_users view. An SQL script would be provided to create all the tables and views necessary.Setting up Hasura and PostgresRunning the Hasura GraphQL engine requires access to a Postgres database, fortunately, with Heroku free tier you can setup Hasura GraphQL engine with a Postgres database in a few minutes. Alternatively you can run Hasura GraphQL engine and Postgres using Docker.Deploy Hasura and Postgres on HerokuHere is how you can setup Hasura GraphQL engine and a Postgres database on Heroku;Click this link to deploy to Heroku. If you don\u2019t have an account on Heroku, you\u2019ll need to create one. Once you sign up, you\u2019ll be redirected to Heroku\u2019s app creation page automatically.Choose an app name and a region. A free Heroku Postgres add-on would be provided by default. Then Click deploy. Once deployment is done, click view. Or head to https:// <YOUR_HEROKU_APP_NAME>.herokuapp.com, you should see the Hasura console.Apply Schema on PostgresOn the Hasura console you\u2019ll notice that there are no GraphQL queries, this is because we don\u2019t have tables or views in our Postgres database. The models we created above have been combined to form a single SQL script below;To apply the schema on our Postgres database, select the Data tab in the Hasura console, click on SQL and paste the script above. Make sure you select the track this checkbox before you run.Once SQL is executed successfully, the GraphQL queries, mutations, and subscriptions would appear.Note: Hasura will detect relationships via foreign keys automatically. Make sure you track all relationships.Cloud functionsServerless functions are a crucial part of 3factor architecture. We are going to use Google cloud functions to handle some of our backend functions. Our chat workflow would consist of functions from user registration to sending messages.Chat workflow: sign up -> add user to rooms -> send message -> send push notification.Signing up users and sending messages can be performed from the frontend directly using the generated GraphQL API. Whereas we need to write backend logic to add users to defaults rooms and also send push notifications. This backend logic would be composed as cloud functions which would be triggered by Hasura event triggers.Building and testing locallySince Google cloud functions execute in runtime environments based on open source languages and frameworks, we can build and test our Go functions locally before deploying them. We won\u2019t go into details on how to set up your Go development environment so let\u2019s begin writing our functions right away.Google cloud functions expects a http.HandlerFunc to be the entry point of a HTTP function executed in a Go runtime environment hello_http.goIn order to test our function locally we would need a http.HandleFunc to register our handler functions. Create a new directory and paste the following code in your main.go file;main.goWe can\u2019t run our code just yet. We made reference to two handler functions rooms.AddToRooms and notifications.SendPushNotifications which we haven\u2019t created. Each function would be deployed separately so we need to put them in different packages. Let\u2019s create the rooms package first. Create a new directory \u201crooms\u201d in your root directory. The following code would be in rooms/fn.go;rooms/fn.goThe file above contains our handler function for adding users to default chat rooms. Immediately a user is created, the event trigger is supposed to call this function and the user would be added to the default chat rooms. Notice we are getting our database credentials from the environment variables, since we are testing locally we need to create a .env file in our root directory.DB_HOST=\nDB_PORT=\nDB_USER=\nDB_NAME=\nDB_PASSWORD=\n\nGet your Postgres database credentials from Heroku and place them in the .env file in your root directory.In order to map objects or data structures to our database tables we need to declare models, we would create our models using Golang structs. Create a file types.go in the rooms directory and paste the following code;rooms/types.goOur rooms package is now complete. We can create the notifications package where our handler function for sending push notification would be. Create a new directory called \u201cnotifications\u201d in your root directory; notifications/fn.goNotice we have two functions in notifications/fn.go, SendPushNotification is the main function triggered by the HTTP request, while getUsersPushToken gets the token of all users that are in that particular chat room.Results of the query performed to get all the user belonging to the specific chat room are mapped to a struct type named Result. We are going to create the struct type in the directory notifications/types.go:notifications/types.goBefore we run our functions, first let\u2019s create a Go module at the root of our project directory, open your terminal within the root directory and run go mod init tutorial/3factor-chatapp. This would create a Go module for us. Run go build to ensure all dependencies are downloaded. Finally run go run main.go. This would start the application. If the application started successfully you can follow the next step, else if you run into an error, ensure you are running a version of Go 1.11 or above and all the dependencies have been downloaded.Setting up event systemThe event system brings together the different parts of the entire application: from the frontend which initiates the events, and our cloud functions that are triggered when events are emitted. We\u2019ll need to capture events in two cases;When a new user signs up; we need an event to trigger our backend function to add the user to the existing default chat rooms.When a user posts a message in a room; we need an event to also trigger our backend so that we can notify the users in that room (via push notifications).We can set up our events system either through the hasura console or through Hasura API. In other to set up these events through Hasura API, create a file event_triggers.json and paste the following code; event_triggers.jsonNotice the webhook field is not set yet, this is because these triggers would be deployed on our Hasura engine which is running on Heroku and they cannot call our functions that are running on localhost. To solve this we are going to use a tool call ngrok, this would provide us with a public URL to our localhost server. Install ngrok and run the following command;$ ngrok http 5070\n\nThis would expose our functions which are supposed to be running on port 5070 to the public. Once you get the public URL, update your event_triggers.json, replace the <YOUR_NGROK_URL> with the URL provided by ngrok. Next, run the following command to set up the event triggers via Hasura API;$ curl -d @event_triggers.json -H 'Content-Type: application/json' https://3factor-chatapp.herokuapp.com/v1/query\n\nOnce the above command runs successfully, we can now run our first GraphQL query from the Hasura console. Select the GraphiQL tab in the console and run the following query;insert_user_queryYou\u2019ll see a new user added in the users table and also added to default rooms by our backend. To confirm that our backend was actually triggered, visit http://localhost:4040 to inspect the traffic of your ngrok public URL.Deploying cloud functionsMoving away from localhost to production would require us make a few changes to ensure our functions execute well in the production environment. On each function call we attempt to connect to the database because we require a database connection to execute our logic, this is not very efficient because database connections are very costly.The Golang init functions are defined in a package block and are used for running one time computations and many more. So let\u2019s move away our database connection to an init function;file-init-funcThis ensures that the database connection is attempted once and reduces execution time greatly. The database client is also declared globally so we\u2019ll need to add the following variables in types.go;var once sync.Once\nvar db *gorm.DB  \nNow our cloud function would connect to the database on startup, and wouldn\u2019t have to attempt connection again each time the function is triggered.Since each package would be deployed as a separate function we have to create a Go module within each package. Run the following command in each package to create a Go module;$ go mod fn.go\n\nI\u2019ve setup a GitHub (link to be provided) repo so that you can refer to the code.One of the easiest ways to deploy your Google cloud function is using the gcloud cli tool, it allows your to perform various operations using a few commands.This requires you to have a Google cloud platform account, if you don\u2019t have an account, sign up and create a project.Once you have the gcloud tool setup. Run the following commands in the rooms and notifications directory respectively (Note: Make sure you replace xxx with the right environment variables value);$ gcloud functions deploy AddToRooms --region=us-central1 --trigger-http --runtime=go111 --set-env-vars DB_HOST=xxx,DB_PORT=xxxx,DB_USER=xxx,DB_NAME=xxxx,DB_PASSWORD=xxxx --entry-point=AddToRooms\n\n$ gcloud functions deploy SendPushNotifications --region=us-central1 --trigger-http --runtime=go111 --set-env-vars DB_HOST=xxx,DB_PORT=xxxx,DB_USER=xxx,DB_NAME=xxxx,DB_PASSWORD=xxxx --entry-point=SendPushNotifications\n\nDeployment might take a few minutes.Implementing the front endWe are going to use React native and Expo to develop the frontend that would communicate with our GraphQL backend. You can view and run the final code of the frontend on Expo snack (https://snack.expo.io/@codebyomar/3factor-chatapp).Setting usernamesSigning up a user would be handled by a single component, SignUp. The code can be found in components/SignUp.js in the Expo snack project.First let\u2019s create our GraphQL mutation and wrap it in gql;const SIGNUP = gql`\n mutation SignUpUser($username: String!) {\n   insert_users(objects: { username: $username }) {\n     returning {\n       id\n       username\n       last_seen\n       last_typed\n     }\n   }\n }`;\n\nNote that the inner mutation, insert_users, is a mutation we are calling from our backend.The mutation above would be executed on submit of the form and the callback below would be executed once the mutation completes successfully;async handleResponse(response) {\n   let { insert_users } = response;\n   let { returning } = insert_users;\n   let user = returning[0];\n   this.props.navigation.dispatch(\n     NavigationActions.navigate({\n       routeName: 'Chat',\n       action: NavigationActions.navigate({\n         params: { ...user },\n         routeName: 'Rooms',\n       }),\n     })\n   );\n }\n\nThe SignUp component would render a mutation component imported from react-apollo. We would pass the mutation we created above as a prop and the callback to be executed once mutation completes successfully.render() {\n   const { username } = this.state;\n   return (\n     <Mutation\n       mutation={SIGNUP}\n       ignoreResults={false}\n       onCompleted={data => this.handleResponse(data)}>\n       {(signup, { loading }) => {\n         return (\n           <View style={styles.container}>\n             <Text style={styles.leadText}> Sign up </Text>\n             <TextInput\n               label=\"Choose a username\"\n               value={username}\n               onChangeText={name => this.setState({ username: name })}\n             />\n             <View style={{ paddingVertical: 15 }}>\n               <Button\n                 loading={loading}\n                 mode=\"contained\"\n                 onPress={() => signup({ variables: { username } })}>\n                 Continue\n               </Button>\n             </View>\n           </View>\n         );\n       }}\n     </Mutation>\n   );\n }\n\nCreating chat roomsA list of chat rooms will be listed to users after signing up. We will need to write a mutation to create new chat rooms.export const ADD_ROOM = gql`\n mutation create_room($room: String!, $type: String){\n   insert_rooms(objects: { name: $room, room_type: $type }) {\n     returning {\n       id\n       name\n     }\n   }\n }`;\n\nWe simply retrieve the room name from the input field and pass it as a variable to our mutation to create a new room. In other to get a fresh list of rooms we are polling data every 3000ms by passing a pollInterval prop to the Query component.<Query query={GET_CREATED_ROOMS} pollInterval={3000}>\n  // \u2026 other code here\n</Query> \n\nSubscribing to chat roomsWe have two separate lists of rooms listed. The first is a list of rooms the user has joined and he can send messages in, the others are rooms he has not joined. We need to subscribe to the user_rooms table so that we can quickly update his list anytime he joins a new room.export const ROOM_SUBSCRIPTION = gql`\n subscription onUserJoinRoom($offset: Int!, $userId: Int) {\n   user_rooms(\n     offset: $offset\n     where: {\n       user_id: {\n         _eq: $userId\n       }\n     }\n   ){\n     id\n     roomsByroomId {\n       id\n       name\n     }\n   }\n }\n`;\n\nSending messagesWe are using a react native library to render our messages and chat user interface. We will pass a function that would receive the message and user who sent it, then we would call our insert_messages mutation.export const SEND_MESSAGE = gql`\n mutation send_msg($objs: [messages_insert_input]){\n   insert_messages(objects: $objs){\n     returning{\n       id\n       msg_text\n       msg_timestamp\n       room_id\n       usersByusername{\n         id\n         username\n       }\n     }\n   }\n }\n`; \n\nA room_id is also passed to the mutation to identify which room the message belongs to.Typing IndicatorWe update the last_typed field in the user table to the current timestamp anytime the user types in a few characters in the input field.export const SET_USER_TYPING = gql`\n mutation set_user_typing($uid: Int!) {\n   update_users( _set: { last_typed: \"now()\" }\n     where: {\n       id: {\n         _eq: $uid\n       }\n     }\n   ) {\n     affected_rows\n     returning {\n       id\n       last_typed\n     }\n   }\n }\n`;\n\nBy updating the last_typed every time a user types, we can subscribe to view the last user that typed. In other not for us to appear in the list we exclude ourselves from the list.export const SUBSCRIBE_USER_TYPING = gql`\n subscription get_user_typing($myId: Int ) {\n   user_typing (\n     where: {\n       id: {\n         _neq: $myId\n       }\n     },\n     limit: 1\n     order_by: {\n       last_typed: desc\n     }\n   ){\n     last_typed\n     username\n   }\n }\n`;\n\nSubscribing to messagesWe first send a query to fetch all the messages in a particular room.export const GET_MESSAGES = gql`\n query fetch_messages($room_id: Int!) {\n   messages(where:{room_id:{ _eq: $room_id }}){\n     id\n     text\n     room_id\n     msg_text\n     msg_timestamp\n     usersByusername{\n       id\n       username\n     }\n   }\n }\n`;\n\nAfter the query results arrive, the subscribeToMore function becomes available to us. With the subscribeToMore function we will be notified with the content of the new message any time there is an update. All we need to do is append it to the previous messages.     <Query query={GET_MESSAGES} variables={{ room_id }}>\n       {({ subscribeToMore, data, loading, error }) => {\n         if (loading) return null;\n         const { messages } = data;\n         return (\n           <MessageList\n             messages={this.mapMessages(messages)}\n             room_id={room_id}\n             user={this.user}\n             subscribeToNewMessages={(append) =>\n               subscribeToMore({\n                 document: MESSAGE_SUBSCRIPTION,\n                 variables: { rid: room_id },\n                 updateQuery: (prev, { subscriptionData }) => {\n                   if (!subscriptionData.data) return prev;\n                   const newMessages = this.mapMessages(\n                     subscriptionData.data.messages\n                   );\n                   return append(newMessages);\n                 }\n               })\n             }\n           />\n         );\n       }}\n     </Query>\n\nSending push notificationsTo send push notifications we will need the user\u2019s push token to know the device we will be delivering the messages to. We will need the user\u2019s permission to do that. Fortunately Expo provides a solution to do that easily. The function below is called on mount of the component to get the user\u2019s permission; async registerForPushNotificationsAsync() {\n   const { status: existingStatus } = await Permissions.getAsync(\n     Permissions.NOTIFICATIONS\n   );\n   let finalStatus = existingStatus;\n   if (existingStatus !== 'granted') {\n     const { status } = await Permissions.askAsync(Permissions.NOTIFICATIONS);\n     finalStatus = status;\n   }\n   // Stop here if the user did not grant permissions\n   if (finalStatus !== 'granted') {\n     return;\n   }\n   // Get the token that uniquely identifies user's device\n   let token = await Notifications.getExpoPushTokenAsync();\n   const { navigation } = this.props;\n   return this.props\n     .saveToken({\n       variables: { token, userId: navigation.getParam('id', 'undefined') },\n     })\n     .then(response => response)\n     .catch(err => console.log(err));\n }\n\nAfter a user grants us permission, \u00a0we save their token using a mutation, by updating the push_token field in the user table.export const SAVE_PUSH_TOKEN = gql`\n mutation save_token($token: String!, $userId: Int!) {\n   update_users(_set: {push_token: $token}\n     where: {\n       id: {\n         _eq: $userId\n       }\n     }\n   ) {\n     returning {\n       id\n     }\n   }\n }\n`;\n\nRemember we set an event trigger for everytime a new message is posted in a room, our cloud function gets the push token for all the users in that rooms and notify them.User online featureAs long as a user is logged in we update their online status every 3 seconds. This help us to identify users that were seen in the last 10 seconds. By simply subscribing to the online_users view we can get realtime update of each user\u2019s online status.const SUBSCRIBE_ONLINE_USERS = gql`\nsubscription isUserOnline($userId: Int) {\n online_users (\n   where: {\n     id: {\n       _eq: $userId\n     }\n   }\n ) {\n   id\n   username\n }\n}\n`;\n\nNow when you click a user\u2019s avatar, a dialog would pop up displaying their online status and other details.ConclusionWe have been able to build a scalable chat application with Hasura graphql engine which generates most of our backend functions, while Hasura event system allowed us to glue other functionalities to the whole application. The structure of the 3factor architecture makes it possible to add new features to our chat app very quickly and also build multiple frontends, you can check out the full project on GitHub https://github.com/CodebyOmar/3factor-chatappAbout the author Umar Abdullahi is a full stack software engineer based in Abuja, Nigeria. He loves building software products and developing them into \u00a0startups. He also occasionally writes about JavaScript and Go on his personal blog. He watches football and movies in his spare time. You can reach out to him on Twitter."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Getting started with Hasura and Flutter Description: A guide to building apps using Flutter, GraphQL and Hasura  Author: Harsha"}, {"role": "assistant", "content": "This tutorial was written by Junyu Zhan and published as part of the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.For those who aren't familiar with Flutter yet, Flutter is an open-source mobile application development framework created by Google, which allows you to build beautiful native apps on iOS and Android from a single codebase. Just like Hasura, Flutter has many great features that \u00a0improve both developer and user experience. Just take 2 examples: Hot \u00a0Reload helps you iterate with different designs really fast, without \u00a0losing the state of your app. Skia (the 2D graphics engine used by \u00a0Chrome) and all the built-in widgets provide performant, beautiful and \u00a0customizable UIs.Hasura and Flutter are a really good match for building 3factor apps. In this tutorial, I will show you how to get started with this powerful \u00a0duo. We are going to port the voting app in my last tutorial to iOS and Android. The main goal is to show you how to integrate \u00a0Hasura into Flutter apps, so we are only going to implement the basic \u00a0voting features.PrerequisitesYou need to know Hasura and a little bit of Flutter to get started. \u00a0Since you are reading this article, I assume you are already familiar \u00a0with Hasura. For Flutter, I recommend going through the official doc to learn how to install and build a simple demo app with Flutter.First thing firstCreate a new flutter project is pretty simple. Just run flutter create hasura_vote_flutter and you are good to go. We need to add a GraphQL library first. Believe \u00a0it or not, this might be the trickiest part of this tutorial. Since \u00a0Flutter is in its early stages, there isn't a mature library like Apollo \u00a0or Relay for the web. I picked flutter-graphql just because it works for this project. I expect as time goes on a \u00a0great GraphQL library for Flutter will appear. By the way, all of us can \u00a0help to make this happen!Back to the topic. Open pubspec.yaml and add the dependency:dependencies:\n  flutter:\n    sdk: flutter\n\n  cupertino_icons: ^0.1.2\n\n  flutter_graphql: ^1.0.0-rc.3\n\nIf you are using IDEs recommended by the official doc, the package will be installed automatically. Otherwise, just run flutter packages get in the command line.Prepare GraphQL documentsNo matter which GraphQL library you use, the process is similar. You \u00a0write some GraphQL documents and the library helps you send them and get \u00a0some result back. Create two files queries.dart and mutations.dart in the lib folder same as your main.dart file. Then copy and paste the two documents from our last tutorial.queries.dart:mutations.dart:This might seems a little bit redundant. But as the amazing ecosystem \u00a0of GraphQL keeps going, I expect more code-sharing between Flutter and \u00a0Web in the future.Hook up the clientThe auto-generated main.dart includes a simple counter app. Run it to make sure everything works. Then replace it with code below:Much cleaner! Let's focus on the MyApp widget first, \u00a0which initializes our GraphQL client. We don't need authentication in \u00a0this project, so a simple HttpLink will do. The URI here is the same as \u00a0in the last tutorial. If you deployed your own version, use your URI \u00a0instead. We then make GraphQLProvider at the top of our widget tree, in order to use the Query and Mutation widget in the children.Add QueryBecause Flutter takes inspiration from React to provide a tree-like \u00a0structure to build UI, the workflow of adding GraphQL data is very \u00a0similar to that of Apollo React. Just wrap the children with a Query widget:Nothing surprising here. We hook up the ReadProgrammingLanguages query and use that data to build a List. Hit save and you should see \u00a0the list appear in you developing device. But since we haven't added the \u00a0Vote mutation yet, clicking the list item will do nothing. What's more, because the subscription API of this flutter-graphql is not stable yet, we use polling to achieve real-time.Add MutationFinally, let's add the Vote mutation to make our app fully functional. Change the return part of _MyHomePageState like this:We use our prepared mutation to vote and update the data manually \u00a0when the result comes back. At this stage, the app should work \u00a0correctly. Try playing with it and see the change reflect on both the \u00a0Flutter app and the Web app.Wrapping UpGood developer tools simplify common tasks but also provide ways to \u00a0customize when in need. To me, Flutter and Hasura are two of the best \u00a0examples of this quality. In this tutorial, we build a fairly simple \u00a0real-time voting app. Utilizing the power of Hasura and Flutter, there \u00a0really is no limit on what kinds of amazing apps you can build.About the author Junyu Zhan is a \u00a0full-stack web developer living in China. Right now he is building a \u00a0cross-platform App with Flutter, React and Hasura. \u00a0He loves taking long walks and swimming in his spare time. You can contact him at his email \u00a0address (thezjy@gmail.com) or Twitter Account (https://twitter.com/thezjy1)."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: Credimi Description: How a Fintech startup uses Hasura as a high performance read layer over their existing database.  Author: Harsha"}, {"role": "assistant", "content": "How a Fintech startup uses Hasura as a high performance read layer over their existing database. The Credimi websiteCredimi is a web platform that turns invoices into cash advances in a simple, fast and reliable way. Corporates and SMEs can apply in minutes and get the advance within just a few hours.The existing Credimi application REST APIs are based on a MySQL database. Credimi decided to shift the read layer to GraphQL. They use a Kafka based streaming setup to hydrate a Postgres database and then use Hasura to expose a high performance GraphQL read layer.Adopting GraphQL While deciding what stack to use for the read layer, Credimi had two important considerations: 1) Time to market 2) Feature velocity post launch - they did not want to have to build new APIs everytime they wanted to expose new data to the frontend. GraphQL solves this problem - all they had to do was define the graph, expose the data with the relevant authorizations in place and front-end could just request for whatever data it needed. This helped them build features quickly and reduced future demands on the backend team. Another drawback with using REST was that to fetch certain types of data on the front end, they had to run complex queries that would take a long time to run and would result in unnecessary data being returned to the client. GraphQL lets you run optimised queries that fetch only as much data as you need. Adopting HasuraThe team looked at other GraphQL vendors before trying Hasura out and deciding to use Hasura. The developer experience was great: bringing up a Hasura instance was very easy. it was very easy to configure and get started with. Community mattered a lot to their team and they were very impressed by Hasura\u2019s community and responsive support. The team started with Hasura in the second week of January, and have almost completed their work. They expect to go into production in the second week of April. Want to know how Hasura can help your team? Schedule a demo.This blog post was prepared basis a discussion with Pamela Gotti, Senior Software Engineer, and the rest of the engineering team at Credimi. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: Cynthesize Description: A community app focused on supporting people building products Author: Harsha"}, {"role": "assistant", "content": "A project that uses Hasura with an Angular Frontend to build a community application around building products Q & A with \u00a0Siddhant Srivastav, founder at Cynthesize. Follow him on Twitter.What are you building at Cynthesize?Being students, my co-founder and I constantly see our friends coming up with ideas for applications and projects but not following through on them. In our opinion, this is mostly because they don\u2019t really have experience going from idea to prototype to product. This is the problem that we built Cynthesize to solve. Cynthesize is a platform which intends to channelise the power of community driven ideation and development so that everyone can take their ideas to the product level. This is a two staged process:The first part involves the refining an idea by holding community discussions around it. The second \u00a0stage, the crux of our product, is a mentored, checkpoint based system of working towards the ultimate of goal of having a finished, complete product, which can then be spun off into its own startup. It\u2019s a platform to support and guide users to turn their ideas into a full fledged product. We help prevent a project from going extinct or from getting lost amongst GitHub repos.Why did you choose to go with Postgres and GraphQL?We've been working on the project since July 2018 and in that time, it\u2019s gone through many iterations. Over time, we've used several frameworks/technologies for our backend. However, our backend development speed has been lagging behind the frontend slowing down our entire project. We have stuck with Postgres throughout thanks to its efficiency. We've been developing our frontend in Angular 7 and we require the backend only for providing API endpoints. Using GraphQL has proved to be the best choice for us, as we can query data frontend having to add additional code for different data requirements. This has boosted our development velocity considerably. Interacting with the backend through endpoints written in frontend also helps in cross-platform compatibility towards code conventions and data formats.What made you choose Hasura when building Cynthesize?Hasura makes working with GraphQL and Postgres very easy. The migration from Django to Hasura's GraphQL engine has been a super simple - it took only couple of hours to migrate completely. Hasura provides a console to interact with the database and makes it very easy and smooth to update and modify the database. Integration with OAuth services and maintaining sessions and authentications to database queries has been seamless. The documentation provided with Hasura has proved to be of great help a well. It's precise and covered all of our use cases. What has your experience with Hasura been like?My experience with Hasura has been very great. It's easy to get the work done and even teach my fellow contributors about GraphQL. The community has been very helpful as well.What is on the roadmap at Cynthesize, and how might Hasura help?We're currently working in the direction of launching the project as soon as possible for mentors. We're looking forward for students to take full advantage of the resources provided by Hasura to help build some amazing features for Cynthesize.Want to know how Hasura can help your team? Schedule a demo."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How ProPublica Illinois built The Ticket Trap Description: How GraphQL, Hasura, Gatsby and JAMstack helped ProPublica Illinois build The Ticket Trap. Author: Harsha"}, {"role": "assistant", "content": "Building the Ticket Trap, one of their most popular 2018 projects, using the JAMstack with HasuraThe Ticket Trap AppA case study of ProPublica Illanois ProPublica Illinois is an independent, nonprofit newsroom that produces investigative journalism. Headquartered in Chicago, it is the first regional publishing operation of ProPublica, dedicated to stories about big issues that affect people living and working in the state of Illinois. In addition to their regular reporting, ProPublica Illinois builds interactive news applications that give their readers a deeper insight into their stories. An example is their recent release of the Ticket Trap, an interactive database that lets you search through records of parking tickets issued in Chicago since 1996. The Ticket Trap was built by David Eads, a news applications developer at ProPublica Illinois. Hasura had a significant impact on the development of the Ticket Trap in terms of reducing the time to go live & the development resources required, and cutting down on the maintenance efforts required once the app had been deployed.Choosing the technology stackDavid was very careful with his technology choices, wanting to ensure that the project\u2019s maintenance debt was as low as possible. At the same time, he wanted the pages to load fast (mission critical for a news organization where each view matters) and cost as little as possible to serve. David decided to use the JAMstack with GatsbyJS for the frontend. While Gatsby solved a lot of problems, David was having trouble interfacing the frontend with really large data sets - \u00a0an ideal API would return large volumes of data, potentially millions of rows at a time. That\u2019s when he came across Hasura. David found that Hasura was great for his data journalism use-case. He really appreciated the ease of use and ease of deployment.I've been using Hasura for a week and I'm impressed both with the software itself and the team's friendly and inclusive approach to community development. Plus, it solves a critical need. Might be a fun one to hack on! I know I'd like to learn more about Go and Haskell. https://t.co/GfW385vQph\u2014 David Eads (@eads) October 1, 2018\n\nResults from using the JAMstack with HasuraBy using PostgreSQL and querying with GraphQL, David was able to significantly cut down on time spent dealing with data management and spent more of his time on other parts of development like design and frontend performance.The JAMstack architecture of sourcing data at build time, and deploying a static site, dramatically simplified the application development & deployment process. David was able to take end-to-end ownership without needing to call on other expertise within the organization, which saved resources and added flexibility.Released just before Christmas 2018, the Ticket Trap turned out to be one of the top 5 projects in terms of traffic for the entire year for ProPublica Illinois. Do read David\u2019s in-depth blogpost about building the Ticket Trap and the tech stack here!Going forwardDavid is currently working on another news application in which he will give API access to multiple stakeholders (other investigative teams and maybe even the public) to examine data. He will be using Hasura and GraphQL to provide this API. He now uses Hasura regularly to serialize complex relational data into JSON for a variety of daily data-journalism tasks. This is one of Hasura\u2019s most simple features, but an extremely important one for day-to-day data journalism.He expects to use the Gatsby + Hasura stack extensively for his work going forward.Want to know how Hasura can help your team? Schedule a demo.This case study was prepared basis a discussion with David Eads, News Applications Developer at ProPublica Illinois. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Exploring Hasura as a data access layer in serverless architecture Description: Exploring Hasura as a data access layer in serverless architecture Author: Harsha"}, {"role": "assistant", "content": "Using serverless + GraphQL to maximize efficiency for a product with inconsistent traffic patterns. This tutorial was written by Himshwet Gaurav and published as part of \u00a0the Hasura Technical Writer Program - an initiative that supports authors who write guides and tutorials for the open source Hasura GraphQL Engine.IntroductionThis post summarizes the issues we faced and how Hasura helped us resolve those issues while developing our graphql API using the serverless framework with AWS Lambda.Our product is a niche CRM tool catering to a specific group of professionals. At its peak usage, the system is expected to receive 200 requests/sec while remaining idle for most of the time. Based on the inconsistent traffic pattern and to keep the server management tasks to a minimum we decided to go with serverless framework. We chose GraphQL over REST as it better fit our data schema.ChallengesOur team was using GraphQL and AWS Lambda for the first time. While these technologies fit our bill of requirements we were not thorough with our research of their shortcomings and constraints imposed by them on the overall system architecture.Issues with GraphQLGraphQL is a good solution if you have to expose nested and interconnected resources through your API. It offers the front-end and mobile developers flexibility in terms of querying, saves them the task of stitching together data from multiple API calls and hence also reduces the number of calls made to the backend.The above benefits however, come at a cost. The task of stitching together data from multiple calls is essentially offloaded to the backend developers.Complex SQL queriesWe were relying on KnexJS and ObjectionJS for interacting with our Postgres database. The complexity of the code that generated the sql queries increased with every argument or field we added in our GraphQL query schema to offer more flexibility to our API consumers.One can argue that the complexity would have been similar had we chosen to build a REST API but if you truly want to harness the full power of GraphQL api, you definitely have to level up your SQL skills.N+1 query problemWhen dealing with nested resources in a single GraphQL query we discovered that we had created a huge collection of N+1 query issues. Though we were satisfied in terms of the functionality we offered through our API we had no clue how to solve all the N+1 query issues hidden in our resolver functions.Dataloader was the solution that popped up frequently in our searches and had we not discovered Hasura we would have gone with it.Issues with AWS LambdaMost examples showcasing AWS Lambda or serverless framework use DynamoDB as the data store. We ruled out DynamoDB as AWS relational database service (RDS) PostgreSQL was a better fit for our data schema. We had no clue the nightmare we were walking into by choosing AWS RDS and AWS Lambda to build our GraphQL backend.No database connection poolingDatabase connection pooling does not exist in the land of AWS Lambda. Lambda is essentially a stateless function running inside a container managed for you by some AWS magic. As a developer we do not have any control over the lifecycle of the container in which our Lambda function runs. There is no straightforward way to keep the database connections alive once this container is either destroyed or frozen in between subsequent requests.Apart from adding to the latency and execution time, the inability to use DB connection pooling also puts a limit on the concurrent execution of the lambda function. In theory you might be able to run 1000 lambda functions concurrently but if your DB is not prepared to accept 1000 connections be ready for sending out HTTP 500 responses.In our search to find a solution we came across this post by Jeremy Daly. He goes into the details of how to manage DB connections inside a lambda function. Jeremy\u2019s strategies did help us resolve the situation temporarily but we knew inside our hearts that we had deployed a solution that would eventually send us the dreaded 2:00 am alerts when the system went into production.Issues with AWS VPC & AWS LambdaAWS RDS has to be deployed inside the AWS virtual private cloud (VPC). Think of VPC as your own private network inside the AWS infrastructure. From a security perspective it made complete sense, to deploy our RDS postgres instance inside VPC but when it came to accessing the RDS through the lambda functions the problems began to compound.Either the VPC or the InternetIn order to access the RDS either you expose your database port to the Internet or deploy the lambda functions inside the same VPC. We chose to deploy our lambda functions inside our VPC.When a lambda function is deployed inside the VPC all resources inside the VPC are accessible but it can no longer access dozens of AWS services including the AWS simple notification service(SNS), AWS simple queuing service(SQS) or any other resource on the internet anymore.You have to either provision a NAT gateway or SNS/SQS endpoints inside your VPC to give your lambda functions access to these services or any other resource on the internet. Provisioning the gateway or endpoint incurs added cost.We had a significant number of lambda functions that accessed Postgres as well as published messages to either a SNS topic or a SQS queue. We have 4 separate accounts for dev, test, staging and production. Adding one VPC endpoint would have raised our monthly bill by roughly 60 Eur/month.Cold start issue is magnifiedIf you aren't aware of the cold-start issues and are planning to go serverless, I strongly recommend going through this post by Yan Cui. TLDR; Cold start happens once for each concurrent execution of your AWS Lambda function. Depending on the programming language you wrote your function in and the dependencies it has, the cold start can add significantly to the latency.When you deploy a lambda function inside VPC it is assigned an elastic network interface (ENI), think of it as a virtual ethernet card. Cold start times are magnified as provisioning the ENI can sometimes take up to 10 seconds!!ENI further complicates your life if you are using cloudformation. We won't go into the gory details here, suffice to say keep your lambda away from VPCsAWS Cognito waits for no oneOur misery continued. We are using AWS Cognito, a managed user authentication and authorization service. Cognito allows you to hook into the signup and signin process using triggers. Triggers are lambda functions executed at certain stages of the signup/signin process.We had two lambda functions which were triggered during the pre-signup and post-confirmation stages. When those triggers started exhibiting erratic behaviour AWS Cognito imposes the following constraint on the trigger lambda functions: \u00a0\u201cAmazon Cognito invokes Lambda functions synchronously. When called, your Lambda function must respond within 5 seconds. If it does not, Amazon Cognito retries the call. After 3 unsuccessful attempts, the function times out. This 5-second timeout value cannot be changed\u201dNow imagine a situation where you have your lambda function deployed inside a VPC. You are bound to timeout for significant percentage of requests because of the added delay in cold-start due to provisioning of ENI. You have to be very lucky for your lambda to respond within 5 seconds after a cold-start.After having wrestled with all these problems it was clear to us that our current architecture was headed for disaster. We either had to abandon the serverless architecture or find reliable and stable solutions to the host of problems we had encountered.Hasura to rescueOnce the proof of concept was complete, we were looking for solutions to:DB connection poolingAccessing the Postgres from lambda without deploying the lambda inside VPCA simpler solution to build sql queries for our graphql resolversN+1 query issueWe stumbled across Prisma and Hasura. Honestly, because of the time pressure we did not have the luxury of evaluating both in depth. We were looking for any solution that could resolve the aforementioned four issues with minimal cost of migration.Why HasuraWe were initially skeptical about Hasura because of the alpha release tag but the following facts won us over:It was written in Haskell. Coming from the world of JS where every other day you have to resolve an 'undefined' variable issue, we were bullish on reliability of Haskell.The Hasura team was extremely responsive to our queries on their Discord channel.Deploying the lightweight Hasura docker images was a piece of cakeIt was pure joy to query data using their auto-generated graphql API without worrying about SQL joins.It was performant for our needs. That little 'Analyze' button on their graphiql dashboard is very helpful.New architectureIn our graphql schema a lot of business logic is performed before the queries and mutations are processed. Hasura offers a way to hook into a remote schema but that would have put us back to square one.Hasura as a data access layerInstead we decided to use Hasura as a data access layer.We put it in front of our Postgres database inside the VPC.We configured Hasura to use a connection pool based on our AWS RDS specificationWe opened a single public port on the EC2 instance on which Hasura was deployed.All the lambda functions that accessed the Postgres were moved out of the VPC.We configured a strong x-hasura-admin-secret and stored it in the AWS system manager parameter store as a secure string. Each microservice/lambda function reads this value during deployment and configures its graphql client headers.Each microservice was assigned a x-hasura-role and the respective access permissions were configured on the Hasura engine.In the last step we dropped the KnexJS and ObjectionJS modules from our microservices. Each microservice now generates a graphql query to fetch data from Hasura instead of building SQL queries.We have a reliable and performant system now.Current challengesThere are two issues that still keep us awake.GraphQL query injectionSince we have dropped the KnexJS and are generating graphql queries to access data from our Postgres through Hasura, we need to be very careful about sanitizing the user input used to build those queries.Public port in VPCWe would have ideally wanted to keep the Hasura endpoint accessible only by our microservices. Currently it is open to the Internet. We are relying on Hasura's inbuilt security to block any unauthorized access and considering Hasura is designed to accept traffic from internet directly we are not in panic mode.In conclusionOur new architecture with Hasura as data access layer has been operational for last 2 months for our beta customers and we haven't had a single 'WTF' moment. Our team is currently working towards using Hasura to also manage our schema migrations to level up our CI/CD.If you are planning to use serverless architecture we strongly recommend to stay away from VPC's. We found Hasura, otherwise we were in a very sorry state. If you are planning to build a graphql API, we strongly recommend giving Hasura a chance. Although their current release is tagged alpha-40, the product is reliable and stable.In the worst case scenario if it behaves unexpectedly, rest assured of the responsiveness of the Hasura team on their discord channel.About the AuthorThis post was written by Himshwet Gaurav under the Hasura Technical Writer Pr0gram. He leads a digital product studio in Tallinn, Estonia. Right now he is prototyping a specification management tool for software development teams. You can contact him through his studio's website.Want to know how Hasura can help your team? Schedule a demo."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: Lineup Ninja Description: Migrating from Firebase to a new production ready backend powered by Hasura  Author: Harsha"}, {"role": "assistant", "content": "Migrating from Firebase Realtime Database to a new backend powered by Hasura. Lineup Ninja is a comprehensive abstract and agenda management platform for conferences, exhibitions and association events. It is sold directly to customers and as white labeled software. The first iteration of the Lineup Ninja product was built using Firebase Realtime Database. As Lineup Ninja saw increasing traction, they were facing a couple of issues with this MVP backend. Their large enterprise clients required them to meet with the security and regulatory compliance standards of their respective regions. They wanted to add powerful search and querying features to their app which Firebase Realtime Database did not make easy. The Lineup Ninja websiteAdopting GraphQL Lineup Ninja decided to use GraphQL as it has a similar query structure to Firebase and so they could migrate easily. They evaluated several GraphQL tools before deciding to go with Hasura because: It was very easy to get started with Hasura: click and run on Heroku helped them test it out quickly, and the learning curve was very short. Hasura provided realtime GraphQL over Postgres - providing an easy to use realtime API but with all the \u00a0benefits and advantages of an RDBMS. The fact that Hasura used Postgres meant it was easy to troubleshoot problems thanks to the large ecosystem. ResultsLineup Ninja was able to migrate out of Firebase and have a fully functional production grade backend ready with very few developer resources, on a very tight deadline. Their new backend enabled sales to different enterprise customer segments:European customers worried about GDPR could now store their data on their own Postgres databases. Chinese clients could now use it \u00a0- their largest customer has clients in China and would not have been able to use the MVP since it was built on Firebase. Large enterprise customers who care about security - they wanted a security policy and an overview of the architecture, and were much more comfortable with the database being Postgres rather than it being built using Firebase. Hasura also gave them 2 other benefits:Hasura enables them to leverage serverless and not worry about DevOps, which is important since the team is small. Hasura\u2019s high performance helped them save significantly on infrastructure costs. Going forward, Lineup Ninja will be adding several new features to their product over the coming weeks and months. They are very confident in their new backend\u2019s ability to keep up with their feature velocity and scaling demands. Want to know how Hasura can help your team? Schedule a demo.This case study was prepared basis a discussion with Gordon Johnston, CTO at Lineup Ninja. Gordon has written previously about using Hasura here and here. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Case Study: A Fortune 500 Healthcare Company Description: A Fortune 500 Healthcare company observes dramatic improvements in feature velocity and dev productivity using Hasura. Author: Harsha"}, {"role": "assistant", "content": "A Fortune 500 Healthcare company observes dramatic improvements in feature velocity and dev productivity using HasuraThe technology team at a Fortune 500 Healthcare Company (HC), headed by their Principal Architect and Head of Engineering and Design decided to adopt GraphQL as they felt GraphQL would significantly advance their chosen business KPIs. They considered several GraphQL solutions before eventually settling on Hasura. HighlightsApp development time has gone down from \u201cmonths and years to weeks\u201d. Simultaneously, team size for an app development project has reduced from 15 to 7-8. Skill set requirements have changed drastically. The need for backend devs and database experts has reduced, and now they hire full-stack developers with significant React experience. \u00a0Going forward, they will be implementing Hasura for apps across business lines. \u00a0 Adopting GraphQL HC had to keep certain key business priorities in mind when deciding on the architecture for their future applications:Reduce total time from concept to delivery. Enable external developers to build applications on top of their offerings easily. Increase their ability to adapt to changes as they go to market. For example, HC felt that GraphQL was an ideal solution across these three strategic aspects. They validated \u00a0this by running two pilots, one for a new prototype and another for an existing product. Evaluating GraphQL Tools HC initially considered developing their GraphQL APIs from scratch. However, they decided against this since they did not have enough developers with relevant experience. They also felt that there were enough tools available in the market for setting up GraphQL servers, and that they could leverage one of these solutions. HC tested several tools before eventually deciding to go with Hasura because: \u00a0Hasura has extensive support for Postgres and allowed HC to leverage all of the Postgres features. \u00a0Hasura worked in a plug-n-play fashion with their existing on-prem database, and was extremely easy to get started with, requiring minimal boilerplate. Hasura offered extremely high performance. Hasura did not require writing and maintaining backend code for most use-casesUsing Hasura Hasura is currently being considered for use in multiple \u00a0products. The most important consideration is performance, and being able to offer a single box solution. In the past, HC offered enterprise box solutions which made getting into clients a large barrier, requiring CapEx approval. With a single box solution, customer buy-in was easier to obtain. Hasura\u2019s high performance and low footprint enabled HC to offer a single box solution. HC also had some initial questions around the security and access control model. These were resolved over discussions with the Hasura team on the Hasura discord and the Hasura team is working closely with them to add more advanced enterprise-ready capabilities Results from using HasuraThrough building the beta version of the product using Hasura, HC has observed several dramatic benefits. By using Hasura, HC was able to significantly improve dev team efficiency. Application development time reduced to weeks from months and years - in the past, this app would have taken a year or more. Similarly, team size reduced from 15 developers to 7 or 8. The team doesn\u2019t need to worry about middleware or API changes and can avoid writing a lot of backend boilerplate. The skill sets required for application development has also changed. HC did not require any backend or DB specialists to build the app. Going forward, HC is looking mainly at full-stack React developers to fill out any vacancies. \u00a0The high performance Hasura affords, and the consequent ability to offer a single box solution, has reduced sales cycles, and will directly impact sales revenue. \u00a0Roadmap going forwardThe products are expected to go into production later this year. Going forward, HC plans to use Hasura across their business lines, as they try to significantly improve dev team productivity and feature velocity. Want to know how Hasura can help your team? Schedule a demo.This case study was prepared basis a discussion with the Principal Architect & Head of Engineering and Design of the corresponding business division of the Fortune 500 Healthcare company. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura Authentication Explained Description: Hasura supports various types of authentication and this post lays out the authentication options when using Hasura in production. Author: Gautam BT"}, {"role": "assistant", "content": "This post was published originally on Apr 2019 and has been edited (significantly) in May 2020Handling authentication correctly is a key step in ensuring the security of your application in production. In this post we will look at the various options for authentication with Hasura.\nIntroduction\n\nDifferent ways to authenticate with Hasura\nAuthorization and sesion variables\n\n\nAuthentication using admin secret\n\nConfiguring Hasura with an admin secret\nSession variables when using an admin secret\n\n\nAuthentication using JSON Web Tokens (JWT)\n\nConfiguring Hasura for JWT validation\nSession variables with JWT based authentication\nIntegrating with JWT based authentication providers\n\n\nAuthentication using webhooks\n\nConfiguring Hasura to use webhooks\nSession variables with webhooks\nBuilding apps that use webhook based authentication\n\n\nUnauthenticated access\nSummary\n\nIntroductionHasura supports 4 methods of authenticationAdmin secret based authentication: Use this method if you are doing server to server communication and the client is a trusted client.JSON web tokens (JWT) based authentication: Use this method if you are authenticating your end-users using a JWT based authentication provider like Auth0 or Firebase or AWS CognitoWebhook based authentication: Use this method if you need to roll out a custom authentication solution.Unauthenticated access: Use this method if you want to provide anonymous access to some data, for example if you want to make a public feed of events.We will understand how each of these methods works below, but first we must understand the related topics of Authorization & Session Variables. Authorization & Session VariablesWhile authentication deals with \"Is this user valid?\", Authorization deals with \"Does this (valid) user have permission to access this resource or perform this action?\". Hasura comes with a built-in role-based access control system. We have put together a post here explaining how this works:Every \u00a0request to Hasura executes against a set of session variables. These \u00a0variables are expected to be set by the authentication system. While arbitrary variables can be set and used in defining authorization rules, two variables are of particular interest in the context of authorization:X-Hasura-User-Id: This variable usually denotes the user executing the request.X-Hasura-Role: \u00a0This variable denotes the role with which the user is executing the \u00a0current request. Hasura has a built-in notion of a role and will \u00a0explicitly look for this variable to infer the role.Session variables are the link that ties authentication and authorization together. The exact mechanism of how these variables get set depends on the authentication mechanism.Authentication using admin secretThis is the simplest mode of authentication. Here's how authentication with an admin secret works:Hasura is configured with an admin password on startupWhen making an API request the client passes the admin password in the header X-Hasura-Admin-SecretHasura validates the admin secret and allows access to all resourcesThe secret is called admin-secret since the admin role is used to execute the request i.e the caller will have permissions to create/update/delete/view any data that is there. So use this method with caution!This method of authentication also takes precedence over JWT and webhook based authentication i.e if X-Hasura-Admin-Secret is set correctly, Hasura will always execute the request with admin privileges.Configuring Hasura with an admin passwordWhen creating a new instance of Hasura engine, the first thing you want to do is to Secure your GraphQL endpoint:To do this, we need to pass the environment variable HASURA_GRAPHQL_ADMIN_SECRET while starting Hasura. Hasura docs describe the instructions for doing this on Heroku, on Docker, and on Kubernetes. Once the environment variable is set, if you try to access the console, you will get a simple \"login\" page that will ask you to specify your secretYou can also now use the admin-secret to make API requests by setting the X-Hasura-Admin-Secret:curl 'https://<hasura-host>/v1/graphql' -H 'x-hasura-admin-secret: <admin-secret>'  --data-binary '<graphql-query>'Session variables when using an admin secretWe do not need to worry about session variables with an admin secret, because session variables are used for writing authorization rules and admin secret always gives access to all resources.Authentication using JSON Web Tokens (JWT)In this post, we assume you are familiar with what a JWT is. If you are new to JWT's here is a guide we have put together explaining how JWT's work in the context of front end GraphQL clients. It also covers the security aspects of using a JWT for authentication.Here's how JWT based authentication works:An end-user is authenticated on the app by your authentication serverOn successful authentication, the authentication server returns a JWT to the app with the user and role information embedded in the claims sectionThe app passes the JWT in the Authorization header to HasuraHasura validates the token and extracts the user and role informationConfiguring Hasura for JWT validationsome dataValidating the JWT Token in step 4. above requires a JWT secret. You can enable JWT mode by using the --jwt-secret flag or HASURA_GRAPHQL_JWT_SECRET environment variable while starting Hasura. The the value of the flag or environment variable must be a JSON object:{\n  \"type\": \"<optional-type-of-key>\",\n  \"key\": \"<optional-key-as-string>\",\n  \"jwk_url\": \"<optional-url-to-refresh-jwks>\",\n  \"claims_namespace\": \"<optional-key-name-in-claims>\",\n  \"claims_namespace_path\":\"<optional-json-path-to-the-claims>\",\n  \"claims_format\": \"json|stringified_json\",\n  \"audience\": <optional-string-or-list-of-strings-to-verify-audience>,\n  \"issuer\": \"<optional-string-to-verify-issuer>\"\n}\nThe type, key, audience and issuer fields are used for validating the JWT whereas the claims_* fields are used for extracting session variables.Session variables with JWT based AuthenticationHasura will look under https://hasura.io/jwt/claims namespace (or the namespace configured above) in the token, extract x-hasura-* prefixed variables and use them as session variables. For example for the below JWT...{\n  \"sub\": \"1234567890\",\n  \"name\": \"John Doe\",\n  \"admin\": true,\n  \"iat\": 1516239022,\n  \"https://hasura.io/jwt/claims\": {\n    \"x-hasura-allowed-roles\": [\"editor\",\"user\", \"mod\"],\n    \"x-hasura-default-role\": \"user\",\n    \"x-hasura-user-id\": \"1234567890\",\n    \"x-hasura-org-id\": \"123\",\n    \"x-hasura-custom\": \"custom-value\"\n  }\n}... Hasura will extract the session variables x-hasura-allowed-roles, x-hasura-default-role, x-hasura-user-id, x-hasura-org-id and x-hasura-custom. You can then write permission rules to use these session variables.Integrating with JWT based authentication providersHere are some resources on integrating JWT based Auth for some popular authentication providers: Auth0: Auth0 JWT Integration with Hasura GraphQL engineAuthGuardian: OneGraph\u2019s AuthGuardian JWT Integration with Hasura GraphQL engineAWS Cognito: Using AWS Cognito for authenticationFirebase: A tutorial for using Firebase to add authentication and authorization to a realtime Hasura appNextJS apps: Add Authentication and Authorization to Next.js 8 Serverless Apps using JWT and GraphQLAuthentication using webhooksHere is how webhook based auth works:The app calls a GraphQL API on Hasura passing on authentication credentials in headers. This can be a session token or an API key for something custom.Hasura calls a pre-configured webhook. Hasura will forward the headers to the API.The HTTP API uses the headers to authenticate the user and returns a success or failure along with the user and role informationConfiguring Hasura to use webhooksdocker run -d -p 8080:8080 \\\n-e HASURA_GRAPHQL_DATABASE_URL=DATABASE_URL \\\n-e HASURA_GRAPHQL_ENABLE_CONSOLE=true \\\n-e HASURA_GRAPHQL_ADMIN_SECRET=secret \\\n-e HASURA_GRAPHQL_AUTH_HOOK=https://auth-web-hook.example.com \\\nhasura/graphql-engine:v1.0.0-alpha41\n\nHasura will then execute a GET request on https://auth-web-hook.example.com whenever it needs to authenticate a request. The API is expected to return 200 Ok if authentication has succeeded or 401 Unauthorized to deny authentication.You can also configure the API to be a POST request by using the command line parameter --auth-hook-mode or the HASURA_GRAPHQL_AUTH_HOOK_MODE environment variable.The exact request/response structure is described in the docs.Session variables with webhooksThe API is expected to return a JSON response containing the session variables to use. For example if the API returns the response...{\n    \"X-Hasura-User-Id\": \"25\",\n    \"X-Hasura-Role\": \"user\",\n    \"X-Hasura-Is-Owner\": \"true\",\n    \"X-Hasura-Custom\": \"custom value\"\n}... Hasura will extract the variables X-Hasura-User-Id, X-Hasura-Role, X-Hasura-Custom, and X-Hasura-Is-Owner.Building apps that use webhook based authenticationThere are community contributed boilerplates available for a few systems:node-js-expresspassport-jspython-flaskAWS Cognito using lambda functionsFirebase cloud functionsUnauthenticated AccessHasura will treat a request as Unauthenticated access if:There is no webhook configuredThere is no Authorization headerX-Hasura-Admin-Secret header is not setThe role for unauthenticated access is configuredThe role for unauthenticated access can be configured either by setting the command line flag --unauthorized-role or the environment variable HASURA_GRAPHQL_UNAUTHORIZED_ROLE. This doc explains configuring permissions in unauthorized role.SummaryWe have covered the different authentication options available while executing a request. Hasura's authentication system is designed to be flexible. Hasura makes it easy to integrate whether you are using an third party auth provider like Auth0 or AuthGuardian or Firebase or rolling your own.Note: Hasura Cloud comes with a pre-configured admin-secret. Try it out now and check out the docs to get started!If you need help with setting up Auth with Hasura, ping us on Discord or tweet to us at @HasuraHQ!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Add Authentication and Authorization to Next.js 8 Serverless Apps using JWT and GraphQL Description: Learn how to add authentication to Next.js 8 Serverless App using a JWT server and add authorization by setting permissions using Hasura GraphQL Engine. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRDeploy a Node.js Express JWT service for authenticating requests to Hasura GraphQL Engine.Authorization using JWT and Hasura GraphQL permissionsA sample Next.js 8 app with login,signup and articles listing pageDeploy to Now.sh using serverless targetNext.js 8 + JWT + Hasura GraphQLTech StackNext.js 8 for building the sample serverless react appApollo Client for GraphQL queryingNode.js server for JWT authenticationHasura GraphQL Engine for GraphQL APIs with permissionsLet's get the backend up and running before configuring the Next.js 8 app.(You might enjoy our article on Implementing JWT Auth and Role-based Content Access via CF Workers + Hasura Actions)Deploy Hasura to Hasura CloudHasura is an open-source engine that gives you realtime GraphQL APIs on new or existing Postgres databases, with built-in support for stitching custom GraphQL APIs and triggering webhooks on database changes.Deploy GraphQL Engine on Hasura Cloud and setup PostgreSQL via Heroku:\nNote the Hasura URL for GraphQL Endpoint. You will be configuring this in the app later.Apply the migrations by following the instructions in this section to create the necessary database schema and permissions.Now the backend is ready! You will be able to instantly query using Hasura GraphQL APIs. The endpoint will look like (https://myapp.hasura.app/v1/graphql). We will come back to this during the integration with the Next.js app.Run JWT ServerWe will now run the JWT server locally to handle signup and login requests from the Next.js app.#clone the repo\ngit clone https://github.com/hasura/graphql-engine\n\n# Change directory\ncd community/boilerplates/auth-servers/passportjs-jwt-roles\n\n# Install NPM dependencies\nnpm install\n\n# Generate the RSA keys\nopenssl genrsa -out private.pem 2048\nopenssl rsa -in private.pem -pubout > public.pem\n\n# print the keys in an escaped format\nawk -v ORS='\\\\n' '1' private.pem\nawk -v ORS='\\\\n' '1' public.pem\nNow, get the database URL from Heroku by heading to the settings page of the app.export DATABASE_URL=postgres://postgres:@localhost:5432/postgres?ssl=true\n\n# Then simply start your app\nnpm startReplace DATABASE_URL with the value obtained from Heroku config. Do note that you need to append ?ssl=true at the end of the DATABASE_URL.Configure JWT with HasuraNow that the JWT server is ready, we need to configure Hasura to use the JWT Secret. The env HASURA_GRAPHQL_JWT_SECRET should be set like this:{ \"type\": \"RS256\", \"key\": \"<AUTH_PUBLIC_KEY>\" }\nWhere <AUTH_PUBLIC_KEY> is your RSA public key in PEM format, with the line breaks escaped with \"\\n\". This was generated just above.In order to configure the JWT Secret we also need to set the admin secret.Run the Next.js AppBoth the JWT server and Hasura GraphQL Engine has been setup and configured.Now let's run the Next.js app by running the following commands:# Change directory\ncd community/sample-apps/nextjs-8-serverless/with-apollo-jwt/app\nConfigure the Hasura app url in lib/init-apollo.js. Configure the Auth Server URL in pages/signup.js and pages/login.js. For example, // inside pages/signup.js\nconst apiUrl = 'http://localhost:8080/signup'\n\n// inside pages/login.js\nconst apiUrl = 'http://localhost:8080/login'Now install the dependenices, build the serverless bundle and run the app using the following commands.# Install dependencies\nyarn install\n\n# Build the app\nyarn run build\n\n# Run the app\nyarn run devOnce you run the app, you should be getting a screen like the one below:Authenticated QueryApollo Client has been configured with an Auth Middleware which sets the Authorization header (if available).const authMiddleware = new ApolloLink((operation, forward) => {\n    // add the authorization to the headers\n    operation.setContext({\n      headers: {\n        authorization: getToken(),\n      }\n})We call a utility function called getToken() which gets the auth token stored in the cookie and returns the token.const getToken = () => {\n  let token = null;\n  if (typeof document !== 'undefined') {\n    token = 'Bearer ' + cookie.get('token')\n  }\n  return token\n}The actual implementation of signup and login sets the cookie with the token.In the file utils/auth.js, we handle the logic of what needs to be done once the server returns the token after successful signup/login.Authorization using JWTNow that the user is logged in, we would like to fetch only the articles written by the same user. The permissions have been configured in such a way that only the user who wrote the article will be able to fetch the data. Head to the Hasura app URL to open Hasura console and navigate to \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Data->article->Permissions to see the permissions defined for the user role.The permission check looks like:{ \"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}This means that when a request is being sent with Authorization: Bearer <token> from the client, it will look for the X-Hasura-User-Id value from the token payload and filter it for the user_id column, ensuring that only logged in users get the data and also get only their data. The user has permissions to access all columns.Protected Routes using HOCThough we have handled authorized queries, we would like to restrict access to pages like /articles at the route level because it is available only for logged in users. We again make use of an HOC to handle this.When a navigation is triggered on a protected route, we call a utility helper method auth which will fetch the token from the cookie using the nextCookie module. In case the token is not available, then it will redirect to /login page.export const withAuthSync = WrappedComponent =>\n  class extends Component {\n    static displayName = `withAuthSync(${getDisplayName(WrappedComponent)})`\n\n    static async getInitialProps (ctx) {\n      const token = auth(ctx)\n\n      const componentProps =\n        WrappedComponent.getInitialProps &&\n        (await WrappedComponent.getInitialProps(ctx))\n\n      return { ...componentProps, token }\n    }\n\n    constructor (props) {\n      super(props)\n\n      this.syncLogout = this.syncLogout.bind(this)\n    }\n\n    componentDidMount () {\n      window.addEventListener('storage', this.syncLogout)\n    }\n\n    componentWillUnmount () {\n      window.removeEventListener('storage', this.syncLogout)\n      window.localStorage.removeItem('logout')\n    }\n\n    syncLogout (event) {\n      if (event.key === 'logout') {\n        console.log('logged out from storage!')\n        Router.push('/login')\n      }\n    }\n\n    render () {\n      return <WrappedComponent {...this.props} />\n    }\n  }\n\nexport const auth = ctx => {\n  const { token } = nextCookie(ctx)\n\n  /*\n   * This happens on server only, ctx.req is available means it's being\n   * rendered on server. If we are on server and token is not available,\n   * means user is not logged in.\n   */\n  if (ctx.req && !token) {\n    ctx.res.writeHead(302, { Location: '/login' })\n    ctx.res.end()\n    return\n  }\n\n  // We already checked for server. This should only happen on client.\n  if (!token) {\n    Router.push('/login')\n  }\n\n  return token\n}We wrap the protected components like articles using the withAuthSync HOC which will take care of the redirects on the server. After logging in, when you visit the articles page, you should be seeing the articles written by you.Don't be surprised to see data being empty. You would need to insert articles tagging your user id in the Hasura console.Deploy on NowWe can deploy this Next.js app on now.sh with the serverless target.Deploy it to the cloud with now (download):npm install -g now\nnow\nDo note that, the JWT server has to be deployed as well. The JWT server url has to be configured inside the Next.js app.I have put together a boilerplate so that you can get started quickly!Check it out on github.Take it for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on twitter, github or on our discord server."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Add Authentication and Authorization to Vue.js Apps with Auth0 and GraphQL Description: Add authentication to Vue.js app with Auth0. Authorization using JWT and Hasura GraphQL permissions.\nA sample Vue.js app protected by login to fetch articles written by logged in user. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRAdd authentication to Vue.js app with Auth0Authorization using JWT and Hasura GraphQL permissionsA sample Vue app protected by login to fetch articles written by logged in userSource code for sample appVue.js + Auth0 + GraphQL + HasuraTech StackThe app uses the following stack that needs to be setup and configured to get it working:Vue.js with vue-cli-plugin-apollo and vue-routerAuth0 for authenticationHasura GraphQL Engine for instant GraphQL APIsLet's deploy Hasura along with postgres to get our GraphQL APIs ready.Deploy HasuraHasura is an open-source engine that gives you realtime GraphQL APIs on new or existing Postgres databases, with built-in support for stitching custom GraphQL APIs and triggering webhooks on database changes.\nApply the migrations by following the instructions in this section to create the necessary database schema and permissions.Now the backend is ready! You will be able to instantly query using Hasura GraphQL APIs. The endpoint will look like (https://myapp.hasura.app/v1/graphql). We will come back to this during the integration with the Vue app. Create an application in Auth0 Head to Auth0 dashboard and create an application in Single Page Web App type.2. \u00a0In the settings of the application, add http://localhost:3000/callback as \"Allowed Callback URLs\" and http://localhost:3000 as \"Allowed Web Origins\" to enable local development of the appAdd rules for custom JWT claimsIn the Auth0 dashboard, navigate to \"Rules\". Add the following rules to add our custom JWT claims:function (user, context, callback) {\n  const namespace = \"https://hasura.io/jwt/claims\";\n  context.idToken[namespace] = \n    { \n      'x-hasura-default-role': 'user',\n      // do some custom logic to decide allowed roles\n      'x-hasura-allowed-roles': user.email === 'admin@foobar.com' ? ['user', 'admin'] : ['user'],\n      'x-hasura-user-id': user.user_id\n    };\n  callback(null, user, context);\n}\nGet your JWT signing certificateHead to https://hasura.io/jwt-config/ and generate the config for your Auth0 domain.Copy the JWT Config that is generated for the Auth0 app.Enable JWT Mode on HasuraThe config generated above needs to be used in the HASURA_GRAPHQL_JWT_SECRET environment variable. We also need to set the HASURA_GRAPHQL_ADMIN_SECRET key for the JWT mode to work.Once you have added this, the GraphQL endpoints can only be queried using Authorization header or X-Hasura-Admin-Secret header.Create Auth0 RuleEverytime user signups on Auth0, we need to sync that user into our postgres database. This is done using Auth0 rules. Create another Rule and insert the following code:function (user, context, callback) {\n  const userId = user.user_id;\n  const nickname = user.nickname;\n  \n  request.post({\n  headers: {'content-type' : 'application/json', 'x-hasura-admin-secret': '<your-admin-secret>'},\n  url:     'http://myapp.herokuapp.com/v1alpha1/graphql',\n  body:    `{\\\"query\\\":\\\"mutation($userId: String!, $nickname: String) {\\\\n          insert_users(\\\\n            objects: [{ auth0_id: $userId, name: $nickname }]\\\\n            on_conflict: {\\\\n              constraint: users_pkey\\\\n              update_columns: [last_seen, name]\\\\n            }\\\\n          ) {\\\\n            affected_rows\\\\n          }\\\\n        }\\\",\\\"variables\\\":{\\\"userId\\\":\\\"${userId}\\\",\\\"nickname\\\":\\\"${nickname}\\\"}}`\n}, function(error, response, body){\n    console.log(body);\n    callback(null, user, context);\n});\n}\nReplace the admin secret and url appropriately.Finally we have the full backend and auth setup ready. Let's configure the Vue.js frontend to make the GraphQL query with the right headers.Configure Vue-CLI-Apollo-PluginWe will be using the Auth0's sample app to get started with boilerplate code. The following command generates apollo client setup for a Vue app.vue add apolloThis will generate a file called vue-apollo.js in src. In this file, we will be configuring the options object, getAuth by defining the following:getAuth: tokenName => {\n    // get the authentication token from local storage if it exists\n    // return the headers to the context so httpLink can read them\n    const token = localStorage.getItem('apollo-token')\n    if (token) {\n      return 'Bearer ' + token\n    } else {\n      return ''\n    }\n},This configuration ensures that ApolloClient makes use of the token returned by Auth0 for Authorization header when making its query or subscription.Authenticated QueryApollo Client has been configured with the right headers in the above setup. So let's add a simple query to fetch list of articles written by the user who is logged in.export default {\n  apollo: {\n    // Simple query that will update the 'article' vue property\n    article: gql`query {\n      article {\n        id\n        title\n      }\n    }`,\n  },\n}Now we would like to show this only if the user is logged in to the app.So in our <template> tag of Home.vue, we will be using the following code snippet to list the articles<template>\n    ...\n    ...\n    <div v-if=\"isAuthenticated\">\n      <h1 class=\"mb-4\">\n        Articles written by me\n      </h1>\n      <div v-for=\"a in article\" :key=\"a.id\">\n        {{a.id}}. {{ a.title }}\n      </div> \n    </div>\n    ...\n    ...\n</template>Note that we are ensuring that this markup has to be rendered only if isAuthenticated returns true. To implement this, we emit an event after each successful login.Head to src/auth/authService.js to see the implementation details of Auth0 login and event emitting.In this file, an event is emitted once the login is done successfully.this.emit(loginEvent, {\n  loggedIn: true,\n  profile: authResult.idTokenPayload,\n  state: authResult.appState || {}\n});A plugin has been registered to handle this event in src/pluginsimport authService from \"../auth/authService\";\n\nexport default {\n  install(Vue) {\n    Vue.prototype.$auth = authService;\n\n    Vue.mixin({\n      created() {\n        if (this.handleLoginEvent) {\n          authService.addListener(\"loginEvent\", this.handleLoginEvent);\n        }\n      },\n\n      destroyed() {\n        if (this.handleLoginEvent) {\n          authService.removeListener(\"loginEvent\", this.handleLoginEvent);\n        }\n      }\n    });\n  }\n};\nSo once a loginEvent occurs, handleLoginEvent method is called. And in our Home.vue component, we handle that method to update isAuthenticated value. It is false by default and once login is successful, gets updated to true.methods: {\n    handleLoginEvent(data) {\n      this.isAuthenticated = data.loggedIn;\n      this.isLoading = false;\n    }\n},The GraphQL query above is sent using the token header returned by Auth0 and this takes care of the Authentication.Authorization using JWTThough the user is logged in, we want to show only the articles written by the same user. The permissions have been configured in such a way that only the user who wrote the article will be able to fetch the data. Head to the Heroku app URL to open Hasura console and navigate to \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Data->article->Permissions to see the permissions defined for the user role.The permission check looks like:{ \"user_id\": {\"_eq\": \"X-Hasura-User-Id\"}}This means that when a request is being sent with Authorization: Bearer <token> from the client, it will look for the X-Hasura-User-Id value from the token payload and filter it for the user_id column, ensuring that only logged in users get the data and also get only their data. The user has permissions to access all columns.Protected Routes using Vue RouterSince we are using Vue Router, we can add Navigation Guards using a Global Before Guard. This is called whenever a navigation is triggered and the navigation is considered pending until resolved.In src/router.js, we define the beforeEach guard which checks for the boolean auth.isAuthenticated() before resolving.router.beforeEach((to, from, next) => {\n  if (to.path === \"/\" || to.path === \"/callback\" || auth.isAuthenticated()){\n    return next();\n  }\n\n  auth.login({ target: to.path });\n});In case, the page is not /, /callback or the user is not authenticated, then the user is redirected to Login page using auth.login method.Running the AppWe need to configure the Hasura GraphQL Endpoint in the Vue.js app. Go to src/vue-apollo.js and modify the httpEndpoint and wsEndpoint values appropriately.Run the sample app by running the following commands:npm install\nnpm run serveYou should be seeing a screen like this:I have put together a boilerplate so that you can get started quickly!Check it out on github.Take it for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on twitter, github or on our discord server."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Asia: What you can expect ? Description: GraphQL Asia is almost here & we\u2019re super excited! Here is a quick look at what you can expect at Asia\u2019s very first GraphQL conference. Author: Rajoshi"}, {"role": "assistant", "content": "GraphQL Asia is almost here & we\u2019re super excited! Here is a quick look at what you can expect at Asia\u2019s very first GraphQL conference.Get your tickets at https://graphql-asia.org? TalksWe have some amazing talks lined up for you! Our speakers come from companies of all sizes from across the globe who have been through the GraphQL journey over the past few years. We have one of the creators of GraphQL \u2013 Lee Byron, delivering the keynote. ? Announcing Asia's first #GraphQL conference speakers ? To kickstart, who would be a better choice than the co-creator of GraphQL itself, @leeb would be starting off with the keynote at #GraphQLAsia #Bangalore pic.twitter.com/egfAUQB4XP\u2014 GraphQL Asia (@graphqlasia) March 18, 2019\n\nWe also have speakers from Twitter, Airbnb, Paypal, Adobe, Philips Healthcare, Treebo, Tokopedia, Cleartrip, Microsoft, Amazon & many more. .@PhilipsHealth is undergoing a GraphQL led #DigitalTransformation with GraphQL being used at every stage of the process to create value. Madhu will take us over a solid enterprise use-case of #GraphQL which will leave you ? with plenty to sell GraphQL to your ? team! https://t.co/18lpZOuFni\u2014 GraphQL Asia (@graphqlasia) March 25, 2019\n\n.@navyaagarwal from @Adobe would be talking about \"A glimpse of GraphQL: front end developers\u2019 life is easy now\" at #GraphQLAsia #Bangalore. ? pic.twitter.com/h80ypGU88g\u2014 GraphQL Asia (@graphqlasia) March 18, 2019\n\nApart from learning about how GraphQL has been implemented at these companies, you\u2019ll also get to meet the creators and engineers behind some of the most exciting GraphQL products & tools- Gatsby, AppSync, graphql-java, Nexus, and of course, Hasura :)Check out the full agenda here. ? WorkshopsGraphQL Asia will also let you get your hands dirty with four workshops ranging from beginner to advanced, that introduce you to GraphQL and tools to get you productive quickly. Your conference ticket gives you access to the workshops as well. You can check out the workshops that we have planned here. ?\ufe0f Speaker Office HoursExpect some major learning and stimulating conversations at GraphQL Asia at our informal, 20 minute, small-group interactions with the speakers. Ask them your questions and chat with them about what you love or hate about GraphQL, and anything else under the sun! It\u2019s like the stack overflow room, but IRL. :)So who should attend?The influence that GraphQL can have on your development process means that everyone from frontend developers to team leaders and CTOs can benefit from learning about GraphQL! We have 25 member teams from a single company attending the conference (yay to group discount tickets)!Of course, this conference is for GraphQL enthusiasts and practitioners, but it is also for those curious to learn about its benefits, and even for skeptics! (No, really \u2013 we have a whole talk by Robert Zhu called \u201cWhy you should use REST instead of GraphQL\u201d). .@rbzhu from @awscloud would be talking about \"Why you should use REST instead of GraphQL\" at #GraphQLAsia #Bangalore. ? pic.twitter.com/a8zD8B12g2\u2014 GraphQL Asia (@graphqlasia) March 18, 2019\n\nHow you can get involvedAttendGet your tickets here: https://www.graphql-asia.org/tickets. Prices rise from the 1st of April, so get them soon!Group Discounts: We have group discounts for large groups, so get your company to send in a large team! 15% off for 5 tickets of more20% off for 10 groups of 10 or more30% off for groups on 25 or moreDiscounts get automatically applied when you choose your number of tickets. Buy here. Diversity Tickets: We want the GraphQL conference to speak to a variety of voices. We\u2019re offering discounted tickets for underrepresented communities in tech. If you belong to one, sign up here.Student Tickets: While GraphQL Asia is largely geared towards working professionals, we would love some student participation! Apply for a student discount here. VolunteerWe\u2019re looking for volunteers in the run-up to and during the conference itself to help us conduct it smoothly. Sign up here.SponsorI would like to thank our sponsors without whose support this event would not have been possible! First time conferences are always hard to sponsor because of the lack of conference stats & prior track record. Our sponsors are truly committed to fostering this ecosystem & we\u2019re super thankful :)Silver Sponsors: DigitalOceanBronze Sponsors: Github, Tokopedia. We have a few more that we will be announcing soon. If your company would like to get involved, we still have a few sponsorship slots open. Reach out to me on Twitter. We can\u2019t wait to meet you in Bangalore ???! See you soon! "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building a Progressive Todo Web App with Vuetify, Vuex and GraphQL Description: Build a progressive todo app with Vuetify, Vuex, VueApollo and integrate GraphQL APIs using Hasura GraphQL Engine.  Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRBuild a progressive todo app with Vuetify, Vuex, VueApollo and integrate GraphQL APIs using Hasura GraphQL Engine. Instant setup. App Source Code -> vuetify-vuex-todo-graphqlVuetify + Vuex + Hasura GraphQLTech StackThis progressive web app uses the following frameworks/serversVue.js with Vuetify for Material Design UI, Vuex for state management, VueApollo for making GraphQL queriesHasura GraphQL Engine to get instant GraphQL APIs over PostgresVuetify FrameworkVuetify is a semantic component framework for Vue. It is built according to Google's Material Design Spec and has intuitive properties without complex classes and markup.In this app, we make use of Vuetify components like VGrid, VCard, VList, VTextField, VBtn among a bunch of available reusable components. The progressive todo web app is a fork of davidgaroro/vuetify-todo-pwa \u00a0which usesVuetify for the TodoMVC UI with material designVuex for state managementHere's how the todo app looks like with Vuetify's material design components:We added vue-apollo to this app and configured ApolloClient to make GraphQL queries for storing and managing the todos in the database. We will use Hasura to get instant GraphQL APIs.Hasura + vue-apolloHasura is an open-source engine that gives you realtime GraphQL APIs on new or existing Postgres databases, with built-in support for stitching custom GraphQL APIs and triggering webhooks on database changes.We will integrate Hasura GraphQL APIs with this todo app.Deploy Hasura to Hasura Cloud\nOnce we have Hasura deployed, we can configure the frontend to make GraphQL queries. We add all modules required to configure ApolloClient.npm install --save vue-apollo graphql apollo-client apollo-link apollo-link-http apollo-cache-inmemory graphql-tagNote: You can also use apollo-boost, if you don't need fine grained control over configurationWe define ApolloClient in src/apollo.js file in the todo app,import Vue from 'vue'\nimport VueApollo from 'vue-apollo'\nimport { ApolloClient } from 'apollo-client'\nimport { HttpLink } from 'apollo-link-http'\nimport { InMemoryCache } from 'apollo-cache-inmemory'\n\nconst httpLink = new HttpLink({\n  // You should use an absolute URL here\n  uri: 'https://myapp.hasura.app/v1/graphql'\n})\n\n// Create the apollo client\nexport const apolloClient = new ApolloClient({\n  link: httpLink,\n  cache: new InMemoryCache(),\n  connectToDevTools: true\n})\n\nconst apolloProvider = new VueApollo({\n  defaultClient: apolloClient\n})\n\n// Install the vue plugin\nVue.use(VueApollo)\n\nexport default apolloProvider\nWe need to just configure Hasura GraphQL Engine endpoint in the httpLink and import this apolloProvider in main.js and add it to the Vue instance like this: import Vue from 'vue'\nimport App from './App.vue'\nimport router from './router'\nimport store from './store'\nimport apolloProvider from './apollo'\nimport './registerServiceWorker'\nimport './vuetify'\n\nVue.config.productionTip = false\n\nnew Vue({\n  router,\n  store,\n  apolloProvider,\n  render: h => h(App)\n}).$mount('#app')\nOnce this is configured, the apolloClient can be imported and used across the app.We define the GraphQL queries/mutations and trigger them in Vuex store. For example, to fetch all the todos, we define a simple query to fetch todos sorted by latest id.const todoQuery = gql`{\n    todos(order_by: [{id: desc}]) {\n      id\n      text\n      is_completed\n    }\n}`and in our actions, we make this query using apolloClient and invoke the mutation handler using commit which will update the state.const actions = {\n  async fetchTodos ({ commit }) {\n    const { data } = await apolloClient.query({query: todoQuery})\n    commit('fetchTodos', data.todos)\n  },\n}Note that we are making use of async, await which makes the syntax cleaner.Let's define a GraphQL mutation to insert a new todo.const todoMutation = gql`\n  mutation insert_todos($text: String!) {\n    insert_todos(objects: [{text: $text}]) {\n      affected_rows\n      returning {\n        id\n        text\n        is_completed\n      }\n    }\n}`This mutation accepts a $text variable and inserts it into the todos table.Similarly we define GraphQL mutations for adding/updating/deleting todos and we define actions to make the mutation and update the state in the Vuex store.Progressive Web AppThis todo app is a PWA and comes with boilerplate setup forApp manifestService workerWorkbox options - Cache Google Fontswhich can be configured to get a Lighthouse score of 100.Todo App Source CodeI have put together the app with complete GraphQL integration so that you can get started quickly and extend it further!Check it out on github.Note that this app doesn't have any user authentication yet and its currently a demo to integrate GraphQL queries/mutations with Vuetify + Vuex with Hasura.Take it for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on twitter, github or on our discord server."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building an image processing app with GraphQL and async serverless Description: Here's how I used GraphQL to build an image processing app that converts an image to sepia asynchronously Author: Rishichandra Wawhal"}, {"role": "assistant", "content": "Here's how I used GraphQL to build an image processing app that converts an image to sepia asynchronouslyAsync image processingTL;DRImage processing is best done asynchronously so that the state is not lost when the app is refreshedServerless functions can be used to run the image processing logicRealtime GraphQL helps you be in sync with the processing stateHasura GraphQL Engine \u00a0gives instant realtime GraphQL APIs over PostgresSource code: client, image processing logic(We've also written a short tutorial on how to build an image storage integration with Hasura Cloud that auto-tags our images for us, with Cloudinary and Autocode.)IntroductionIn this post we'll build a basic image processing app that lets you upload an image and add a sepia tone to it asynchronously. We will not go into the code subtleties as this is more of a philosophical rant about how async image processing apps can be built using realtime GraphQL and serverless. \u00a0We'll use:Hasura GraphQL Engine as a free realtime GraphQL server over Postgres.Hasura gives you realtime GraphQL APIs over any Postgres databaseAn event sourcing system to trigger external webhooks on mutationsPrerequisitesKnowledge of consuming a GraphQL APIKnowledge of some front-end framework. An example is given with ReactJS, but you could use anything.ArchitectureThis app follows the 3factor.app architecture pattern. 3factor app is an architecture pattern for resilient and scalabale fullstack apps. Read more about building 3factor apps with event-driven programming. It involves:Realtime GraphQLReliable EventingAsync serverlessOlder architecture vs 3factor architectureIn case of our image processing app, the flow boils down to:The client uploads the image to cloud and inserts the image URL into the database with a GraphQL mutation. The client then watches the changes in the database for that particular image with GraphQL subscriptions.When the image URL has been inserted in the database, Hasura's event system calls an external webhook with the image detailsThe external webhook converts the image to sepia tone, uploads the converted image to cloud and updates the database with the converted image URLThe client receives the update when the converted image has been uploaded to the database (GraphQL subscriptions)BackendFirst step is to get a realtime GraphQL server running in the form of Hasura GraphQL Engine.Click here to deploy it to Heroku's free tier with free Postgres (no credit card required).Data modelWe need only one table for this app.When you create this table called images, Hasura provides the following root fields in its GraphQL schema:images: To query or subscribe to the data from images table (comes with clauses such as where, order_by, limit)insert_images: To insert one or more images into the images tableupdate_images: To update one or more images in the images tabledelete_images: To delete one or more images in the images tableImage processing logicWe need our image processing logic that takes our uploaded image and adds a sepia tone to it. This could be written in any language or framework and deployed on any platform. For example, in NodeJS, you can write this logic using JIMP and serverlessify it using Zeit. The code would look something like:The above function simply takes an object of the following form, converts it to sepia and stores the converted image at /tmp/<id>.png.{\n\t\"id\": 233,\n\t\"image_uri\": \"https://mycloudbucket.com/image\"\n}Once the image has been converted, you also want to update the converted image URI to the database.Event sourcingHasura lets you define event triggers that listen on mutations and invoke an external webhook with the mutation data. We will create one such event trigger that listens on insert_images mutation and calls the webhook that performs the logic discussed above.Creating an event trigger in HasuraWith this, our backend is ready.Frontend Most of the logic happens on the backend, so the front-end stays fairly clean. The front-end has two screens:Upload screen: Uploads image to cloud, inserts the image URL in the database and redirects to the Convert screen with URL param id=<image_id>Convert screen: Waits for the image to get processed and shows the converted imageUpload screenThis screen does the following:Takes image from userUploads image to cloudInserts this image in the database with GraphQL mutationRedirects to the new screen with URL parameter id=<image_id> where image_id is the unique id of the inserted imageThe GraphQL mutation for inserting the image to the database looks like:The above mutation inserts the image URI to database and returns the id of the inserted row. Next, you redirect to the Convert screen where you wait for the processed image.Convert screenIn the convert screen, you look at the id of the image in URL parameters and make a live query to the database with GraphQL subscriptions. The subscription looks like:While rendering the UI, you would check if the received \u00a0converted_image_uri is null. If it is, you show a loading indicator, or you show the converted image. If done in React with Apollo's Subscription components, it would look something like:As you see, in the above component:If there is an error in subscription, we render some error UI.If the subscription is in loading state, we show a loading UI.If the subscription response is empty i.e. there is no row in the database where id is equal to the given id, we say that the id in the URL parameters is invalid.If we get a response but the converted_image_uri is null, we assume that the processing is still in progressIf we have a valid converted_image_uri, we render it.Finishing upWe discussed a pattern to build async image processing applications. You could use this architecture to build mostly all kinds of async applications. Check out 3factor.app and hasura.io to know more about building resilient and scalable fullstack applications.If you have any questions, stack them up in the comments and they'll be answered ASAP."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Create a Remote Schema to wrap a REST API with Hasura Description: Easily write a wrapper GraphQL service which calls your REST API. Add this GraphQL service as a Remote Schema in Hasura for automatic schema-stitching.  Author: Tirumarai Selvan"}, {"role": "assistant", "content": "What is a Remote SchemaA remote schema is simply an external GraphQL service that you can stitch with Hasura GraphQL Engine for a unified GraphQL API. A common way to incrementally adopt GraphQL in an existing infrastructure, is to wrap pre-existing REST APIs within a GraphQL layer. In this blogpost, we will see how you can do this using Hasura easily. Example REST APISuppose, you have a REST API like the following:GET /users\nGET /users/:userId\nPOST /usersThe GET /users \u00a0 endpoint also takes an optional query param i.e. GET /users?name=abc and the POST /users endpoint expects a valid JSON payload in the body.Let's see how we can wrap this in GraphQL.Bootstrap a GraphQL ServiceWe can start by picking up a minimal boilerplate for a GraphQL server, say this: https://github.com/hasura/sample-remote-schema (or jump to final implementation here)This boilerplate is built using the Apollo Server framework. It is simply a hello schema i.e. on query { hello } it returns \u00a0{ data\": {\"hello\": \"world\"} }const typeDefs = gql`\n  type Query {\n    hello:  String\n  }`;\n\nconst resolvers = {\n    Query: {\n        hello: () => \"world\",\n    },\n};Build the Schema, Fill in the TypesFor our REST API (from above), lets start defining some new types and fields. The complete definition of types and fields is what makes our schema :const typeDefs = gql`\n  type User {\n    id:       String!\n    name:     String!\n    balance:  Int!\n  }\n\n  type Query {\n    getUser(id: String!): User\n    users(name: String): [User]\n  }\n\n  type Mutation {\n    addUser(name: String!, balance: Int!): User\n  }\n`;Roughly our REST to GraphQL mapping looks like the following:\n\n\nREST\nGraphQL\n\n\n\n\nGET /users\nusers (name: String) : [User]\n\n\nGET /users/:userId\ngetUser(id: String!): User\n\n\nPOST /users\naddUser(name: String!, balance: Int!): User\n\n\n\nWe can already notice some extra information about the APIs and this is one of the selling points of GraphQL: we can see input and output type information very clearly. Lets now move to implementing the APIs.Resolve!In a REST world, we usually talk about writing handlers for endpoints. In GraphQL, we do a similar thing but call it resolvers! For the schema that we defined in the previous section, lets add their corresponding resolvers:const resolvers = {\n    Query: {\n        getUser: async (_, { id }) => {\n            return await getData(MY_REST_URL + '/users/' + id);\n        },\n\n        users: async (_, { name }) => {\n            var nameParams = '';\n            if (name) {\n                nameParams = '?name=' + name;\n            }\n            return await getData(MY_REST_URL + '/users' + nameParams);\n        }\n    },\n\n    Mutation: {\n        addUser: async (_, { name, balance } ) => {\n           return await postData(MY_REST_URL + 'users', { name, balance } );\n        }\n    }\n};\nEasy! All we did was proxy the incoming GraphQL request to our REST API.Finally, we need to deploy this service somewhere and get a URL. There are plenty of cloud solutions that allow you to deploy a Node service easily. You can look at the final implementation with deployment instructions on Heroku here.Add to HasuraOnce you have gotten an HTTP URL for your above GraphQL service, just head over to your Hasura console and add it as a Remote Schema. In case your remote schema defines few types that are same as Hasura, they will get merged automatically too. Notice that you can also add Authorization and other headers to your Remote Schema in case your REST API expects such headers.Add a Remote Schema through Hasura ConsoleNow, you should be able to call your remote schema APIs alongwith the default Hasura APIs from the Hasura GraphQL Endpoint. And that's it!Querying Remote Schema from Hasura consoleConclusionIt is easy to wrap your existing REST API into a GraphQL API. The advantages of these are many: 1) you can incrementally adopt GraphQL in your existing infrastructure, 2) you get added benefits over REST APIs like type safety and more readability, 3) you get a unified API layer for all your APIs.With the guide presented above, you can get started with your GraphQL wrapped REST API in minutes. You can then add this GraphQL service as a Remote Schema in Hasura for automatic schema-stitching. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: graphql2chartjs: Realtime charts made easy with GraphQL and ChartJS Description: graphql2chartjs is an open source tool that reshapes your GraphQL data as per the ChartJS API. This makes building charts as easy as simply making a GraphQL query. Author: Rishichandra Wawhal"}, {"role": "assistant", "content": "Use GraphQL as a data source to build realtime charts with ChartJSExample realtime chartWe started using ChartJS with GraphQL so that we could leverage GraphQL's realtime subscriptions to build realtime charts. Soon enough we realised that we can automate this process of restructuring the GraphQL data to a form that ChartJS expects. graphql2chartjs is an open source tool that reshapes your GraphQL data as per the ChartJS API. This makes building charts as easy as simply making a GraphQL query.The idea behind this tool is to generate ChartJS compliant data object from your GraphQL response by simply adding a few aliases in your GraphQL query. Usage with HasuraHasura gives you an instant realtime GraphQL API on an existing Postgres database. You can create views to capture analytics and aggregations on your database and instantly turn them into charts.If you're using Postgres and Hasura, this is what using graphql2chartjs looks like:graphql2chartjs usageWatch this video below to see a demo/tutorial of using Hasura with an existing Postgres database, creating views and building charts.Sample client codeWhen using React, \u00a0Apollo and react-chartjs-2 on the client, your code for a realtime chart would look something like:import {Subscription} from 'react-apollo';\nimport gql from 'graphql-tag';\nimport graphql2chartjs from 'graphql2chartjs';\nimport {Bar} from 'react-chartjs-2';\n\nconst Chart = () => (\n  <Subscription\n    subscription={gql`\n      subscription {\n        Articles: articleStats {\n          label: title\n          data: num_likes\n        }\n      }`}\n    }>\n    {({data} => {\n      if (data) {\n        const g2c = new graphql2chartjs(data, 'bar');\n        return (<Bar data={g2c.data} />);\n      }\n      return null;\n    }\n  </Subscription>\n);Mapping GraphQL queries to ChartJS chartsDifferent types of charts need different structures in their datasets.For example a bar chart dataset needs labels and data associated for each label; the ChartJS API refers to this as labeland data. Once you alias fields in your graphql query to label and data, and pass the response through graphql2chartjs, your dataset is ready to be used by bar chart in chartjs.Bar / Line / Doughnut / Pie / Radar / Polar Area / AreaCharts of this type need 2 data inputs, label and data.query {\n\tArticleLikes : articles {     \n\t\tlabel: title\n\t\tdata: likes\n\t}\n}Scatter / BubbleCharts of this type need 2 data inputs: data_x, data_y (and data_r for bubble).query {\n\tArticleLikesVsComments : articles {\n\t\tdata_x: num_likes\n\t\tdata_y: num_comments\n\t}\n}Time series (line / bar)Charts of this type need 2 data inputs, data_x or data_t and data_y. Note that there is no label.query {\n\tStockPrices : stockprice {\n\t\tdata_t: created\n\t\tdata_y: price\n\t}\n}graphql2chartjs usage workflowgraphql2chartjs works in 3 steps:Initialise graphql2chartjs: const g2c = new graphql2chartjs()Add data from your graphql response: g2c.add(graphqlResponse.data, 'bar')Set your chart data to the data properly of the graphql2chartjs instance: g2c.data(Optional) Incrementally add data to the existing g2c instance: g2c.add(graphqlResponse.data, 'bar')The add() function can also take a transformer function as the second argument. This transformation function can be used to \u00a0add custom properties to your data points from the GraphQL response. graphql2chartjs maps your GraphQL response with this transformer function and adds the given properties to the existing data points before performing any restructuring. \u00a0For example, if you wish to add different background colours for different data-sets, you would do something like:g2c.add(graphqlResponse, (datasetName, dataPoint) => {\n\tif (datasetName === 'ArticleLikes') {\n\t\treturn {\n\t\t\tchartType: 'bar',\n\t\t\tbackgroundColor: 'red'\n\t\t}\n\t}\n\treturn {\n\t\tchartType: 'bar',\n\t\tbackgroundColor: 'bar'\n\t}\n});Whenever you are using a transformer function, the property chartType is mandatory because graphql2chartjs must know how it should add the given properties in the ChartJS data.Read more detailed examples and instructions in the readme.Take it for a spinDo try graphql2chartjs and let us know what you think :) Refer to the Hasura docs if you\u2019re looking for things like setting up security/access control on your GraphQL API.We hang on discord, on github and our website intercom. Feel free to ask us questions :)"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Git Push to Deploy Lambdas using CircleCI Description: We want to setup a git push workflow to deploy our functions on AWS Lambda with API Gateway.\n Author: Tirumarai Selvan"}, {"role": "assistant", "content": "TL;DR:We want to setup a git push workflow to deploy our functions on AWS Lambda:$ git push origin master\n\n * branch              master -> FETCH_HEAD\n   9e364190..0e14a1fd  master -> origin/master\nUpdating 9e364190..0e14a1fd\n\n$ #triggers a circleci job that deploys your serverless function\n\n$ curl aws-myapi-gateway.com/function\n\"Hello world\"We can do this in 4 easy steps:Setup a repo with functionsOne-time setup of AWS and CircleCIConfigure CircleCIGit pushIntroductionFunctions-as-a-service is a service offered by cloud platforms in which you can deploy functions and get API endpoints to invoke the same. Building on FaaS can be really simple and powerful for use-cases which do not require the need of a full-fledged web service. AWS Lambda is the most popular FaaS platform out there today. A function on AWS Lambda is also simply called: a lambda (we will use the terms interchangeably).In this post, we will solve one of the major pain points of using Lambda in production i.e. CI/CD. There are many tools available in the market which help you build and deploy Lambdas safely but, in this post, we will showcase the way to do it using the traditional approach of using git (aka gitops style).git push to deploy Lambdas with API GatewayOur aim is to keep 3 environments in the cloud:DevStgProdThe CI/CD system will deploy our application, comprised of multiple functions, to each environment based on the branch on which the code is pushed:\n\n\nbranch\nenvironment\n\n\n\n\nmaster\ndev\n\n\nstg\nstg\n\n\nprod\nprod\n\n\n\nThe end output is that we will have an environment specific HTTP endpoint for each of our functions.Repo structureBefore we jump straight to deploying our lambdas, lets talk a bit about code organization. Our deployment configuration will depend on this.Here is the repo structure which we will be using in this blogpost:.\n\u251c\u2500\u2500 functions\n|   |\u2500\u2500 echo\n|   |   |\u2500\u2500 index.js   \n|   |\u2500\u2500 helloWorld\n|   |\u2500\u2500 myFunc\n|   |   ...\n\u251c\u2500\u2500 .circleci\n|   |\u2500\u2500 config.yml            \n|   |\u2500\u2500 deploy.sh            \n|\u2500\u2500 .git\n\u251c\u2500\u2500 .gitignore    \nIt is a very simple structure: a git repo at the top level.a functions folder.a circleci folder (more on this later). Each lambda is a separate folder inside the functions folder. We will assume each function is complete in itself i.e. it can be independently deployed on AWS Lambda without any further dependencies. One important point: This structure is not optimized for code/library sharing across functions. We can easily workaround this by having a common folder with overlapping dependencies and use that during the build of each function. For simplicity, we will keep this setup out-of-scope of this tutorial.One-time setupOur CI/CD setup will require few resources which need to be setup one-time:1. Lambda Execution Role on AWSLog into AWS Console and create a IAM service role as per the docs here. This is the role with which our Lambdas will run. In case you already have a predefined role setup for your Lambdas, you can skip this step.2. API Gateway on AWSLog into AWS Console and go to API Gateway service page. Create a bare-bones API Gateway, like the following, per environment:Each deployed function will be attached to a new route on this API Gateway e.g. a function called hello will be routed to <api-gateway-url>/hello and a function called bye will be routed to <api-gateway-url>/bye.3. Add Repo in CircleCILog into CircleCI with Github/Bitbucket and you should see all your repos under Add Project:Add your repo by clicking Set Up Project and in the next page just click Start Building (ignore all other instructions). This will setup all the hooks required in your repo so that CircleCI is triggered every-time there is a push. Currently, this will result in build failure as we haven't configured CircleCI yet. We will set this up in the next section.4. Setup environment variables in CircleCIWe have to give few environment specific values and secrets to CircleCI build environment so that it can access the AWS resources. Head to your project settings in CircleCI:Add the following environment variables:#common\nAWS_ACCOUNT_ID\nAWS_REGION\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\n\n#dev api gateway id\nAWS_DEV_REST_API_ID\n\n#stg api gateway id\nAWS_STG_REST_API_ID\n\n#prod api gateway id\nAWS_PROD_REST_API_IDCircle CI configurationCircleCI runs a build using the configuration present in .circleci folder of the git repo. Our .circleci folder contains 2 files: config.yml and deploy.shconfig.ymlThe config.yml has metadata about the workflow like what are the steps to execute, on what branch to run, etc. At a high level this is what our config.yml looks like:\n\nversion: 2\njobs:\n  build_dev:\n    steps:\n      - checkout\n      - run:\n          name: Setup Environment Variables\n          command: |\n            echo 'export AWS_REST_API_ID=\"$AWS_DEV_REST_API_ID\"' >> $BASH_ENV\n      - run:\n          name: Install dependencies\n          command: |\n            apt-get update\n            apt-get install -y jq zip python3-pip\n            ...\n      - run:\n          name: Deploy functions\n          command: |\n            cd functions\n            ../.circleci/deploy.sh echo\nworkflows:\n  version: 2\n  full:\n    jobs:\n      - build_dev:\n          filters:\n            branches:\n              only: master\n\nWe have a workflow which calls a job build_dev only for the master branch. The build_dev job has few steps like setting up dependencies, building lambda, etcThis is the complete file for all 3 environments:\n\ndeploy.shThe deploy.sh is a bash script that builds and deploys a folder (assumed to be a nodejs function here) to Lambda and links it to a route in the API Gateway. If you look at the config.yml file above, in the final step we are calling deploy.sh with the appropriate folder. We will only use the aws-cli in the script for complete control and observability.At a high level, these are the steps that are being done in deploy.shCreate a Lambda with the name <functionName>_<environment>, if doesn't exist already.Zip the folder and upload the code to the above Lambda.Create an alias for the Lambda with the GITSHA of the commit.Create a route in the API Gateway, if doesn't exist already.Link the alias with the route.Here is the complete file:\n\nGit push and goWe are all set. Once you have committed the .circleci folder with the above files and pushed, your functions will be built and deployed on Lambda with API Gateway. You can check the logs of each build on the CircleCI dashboard. Congratulations, you have just set up a git push workflow for your Lambdas.PS: If you are using Hasura, then you can setup this workflow for your Remote Schemas and Event Triggers :)"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Building Stateful Apps with Serverless Functions and Postgres Description: This post explores the different ways to deal with application state when writing business logic in serverless functions. Author: Tirumarai Selvan"}, {"role": "assistant", "content": "How should your serverless functions interface with a database to manage application state?This post explores the different ways to deal with application state when writing business logic in serverless functions. We'll primarily use Postgres as the database when talking about these different approaches, and often use Hasura as one of the glue solutions between your serverless functions and your Postgres database. However, the ideas in this post are generic and can be carried forward to your favourite database solution.TL;DRApproach #1: Connect to Postgres directly from your serverless functionApproach #2: Use BaaS APIs from your serverless functionApproach #3: Trigger serverless functions on Postgres events with rich event payloadsApproach #4: Combine approaches above and use Postgres to orchestrate a workflow with the serverless functions driving state transitionsIntroductionServerless platforms do not allow the developer to write code that holds any long-term state. This is because the serverless runtime is launched on-demand and is short-lived i.e. all the resources are deallocated after a request is completed.These are few examples of things that you can't approach in the same way you did when you were writing a normal API server and may take for granted:Writing to disk directly, like saving a file to a folderA global variable that holds information across multiple requests, say a counter \u00a0that keeps track of how many requests this API server has seen.A database connection pool that your API server maintains with a database even when there are no requestsIn this post, we explore some of patterns with real world use-cases which can help you in building your next app with serverless functions. Let's cover them one-by-one!1. Connect to Postgres directlyThe most common way to store application state for a server-side application is by using a database. However, connecting to a database directly from a serverless function naively is not the wisest idea, because while you can suddenly invoke a 1000 serverless functions, the database can't suddenly scale to creating a 1000 database connections.Here are a few best practices to connect to Postgres to read/write data or run a database transaction from your serverless function:\nCreate a connection to your database by following the best practices from your cloud-vendor which helps serverless functions reuse a database connection across invocations (kind of like a connection pool):\n\nAWS Lambda\nGoogle Cloud Functions\nAzure Functions\n\n\nCreate your state in entirety at the beginning of the function.\n\nThis is to avoid executing multiple calls to the database which can incur performance cost as well as running cost. Gwen Shapira talks more about such database access patterns in this blogpost: https://thenewstack.io/managing-state-in-serverless/\n\n\nTry to use an ORM-free data layer as much as possible.\n\nThis is because with ORMs, you need to define the data models first and you will have to do this for every function. This could mean writing lot of boilerplate and keeping all of them in sync with the underlying database. As you do this, you'll realise that you were better off writing a monolithic API server instead of serverless functions that were independent of each other!\nConsider using good SQL clients or no ORM clients like massivejs.\n\n\nConsider using an external connection pooling agent for your database like pgBouncer\n\nIf you are using DB transactions, then keep in mind the concurrency limits of your database. Transactions are costly and don't scale easily. Depending on your load, you may have to leverage a reliable external connection pooler like pgBouncer [0]. In fact, DigitalOcean provides its managed Postgres database with connection-pooling built-in [1].\n\n\n\n2. Use BaaS APIsOne of the easiest ways to manage state is by not managing it yourself ? .Backend-as-a-Service (BaaS) solutions abstract away some of the complex performance and concurrency concerns when dealing with persisting state. Your code in the serverless function uses HTTP APIs (stateless) and treats the BaaS solution like a black-box. The BaaS itself deals with actually managing and persisting state even as thousands of concurrent pieces of code might be reading, writing or modifying data. Here are examples of BaaS services your serverless function might use:S3 style APIs to read/write object or file type data. All major cloud vendors now provide a S3 API.Hasura GraphQL APIs to read/write data from PostgresAlgolia to read/write to a search index and databaseOne of the neat things about using BaaS is that your function appears to be totally stateless. Consider the following code which uses Algolia to populate a search-index:\n\nconst algoliasearch = require('algoliasearch');\n\nexports.function = async (req, res) => {\n  const data = req.body.book;\n\n  var client = algoliasearch(process.env.ALGOLIA_ID, process.env.ALGOLIA_KEY); //from env vars\n  var index = client.initIndex('book_index');\n\n  index.addObjects([data], function(err, content) {\n    if (err) {\n      console.error(err);\n      res.json({error: true, data: err});\n      return;\n    } else {\n        res.json({error: false, data: content});\n    }   \n};\n\nAs you can see, this function is essentially making one API call to a BaaS which does the heavy-lifting with state. 3. Trigger serverless functions on Postgres events with \"rich\" event payloadsEvents are the underlying mechanism the cloud vendor uses to provision resources for a serverless function and then run it, by passing arguments to the function from the event payload. If the event payload contains all the data and context necessary for the serverless function to run, then you don't need to read data from an external source.This brings us to the third approach of writing serverless functions that need state. You can capture events from your database, like Postgres, and use them to trigger your serverless functions.However, to be useful in a real-world context, these events need to contain all the related data and relevant context. Just the id or the operation (insert/update/delete) details are typically not enough.A detailed event payload is more useful In our case of working with Postgres, event payloads should typically contain:Row dataRelated data to the row (joined in via other tables)Application user session information, if applicableHasura provides an eventing system on Postgres out-of-the-box that makes it easy to trigger events with more contextual and detailed payloads as described above! 4. Postgres & serverless functions in a workflow as a state-machineMost complex apps have workflows or progress via state-machine transitions in the backend. Consider a food-ordering application with the following data model:Order tableThe entire ordering workflow, comprising of important business logic steps like validation, payment, approval, delivery agent assignment can be thought of as a series of transitions to the state of the order \u00a0table.Orchestrating a workflow using Postgres and serverless functionsEach column in the order model can be considered to be updated by a specific serverless function that runs the complex business logic to actually make that transition appropriately and modify the state with more information. This approach then becomes a mix of approaches 1/2 and 3:Trigger serverless functions on database events (updates on specific columns trigger specific serverless functions)Serverless functions can go and update state in the database while running their business logic via approaches #1 and #2!This idea is also closely related to the 3factor architecture pattern (read more about building 3factor apps with event-driven programming).ConclusionBuilding stateful applications with serverless functions is not hard for most use-cases. The basic idea is to delegate the heavy-lifting to expert systems. Using some of the patterns described here, developers can extract the most amount of value in writing business logic in serverless functions."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build and Deploy Serverless Apps with Next.js 8 and Zeit Now Description: Configure Next.js 8 serverless mode to generate lambdas for each page. Use Hasura GraphQL API as data layer. Deploy to Zeit Now. Instant setup. Tutorial/boilerplate ? nextjs-8-serverless Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DR: Configure Next.js 8 serverless mode to generate lambdas for each page. Use Hasura GraphQL API as data layer. Deploy to Zeit Now. Instant setup. Tutorial/boilerplate -> nextjs-8-serverlessNext.js Serverless Apps on NowNext.js 8 landed last week with support for serverless deployment. With this release, Next.js splits an application into serverless lambdas helping improve both reliability and scalability. The \"pay for what you use\" model is one of the key benefits to going serverless. There are obvious other benefits like distributed points of failure and scale as you go. Out of the box support for serverless is exciting!To quickly test this out, I had created a Next.js 8 app integrating Hasura GraphQL Engine. The app had a couple of pages; index and about. The index page renders a list of authors fetched from postgres via Hasura GraphQL. The about page renders some static text. Here are the instructions for you to get started.Deploy Hasura on Hasura CloudDeploy GraphQL Engine on Hasura Cloud and setup PostgreSQL via Heroku:\nFollow the instructions in the readme to create the table author required for the app. Note the Hasura URL for GraphQL Endpoint. You will be configuring this in the app.Clone and Run appThe demo app walks you through to build a simple app, listing all authors. You can clone and run the app for the demo.$ git clone git@github.com:hasura/graphql-engine.git\n$ cd graphql-engine/community/sample-apps/nextjs-8-serverless\n$ npm installAfter installation, you will have to configure the GraphQL endpoint that you got above.Open lib/init-apollo.js and configure Hasura GraphQL Endpoint in uriNow run the app by using the following command:$ npm run devand visit http://localhost:3000 to view the app.Enabling Serverless ModeTo enable serverless mode, we add the serverless build target in next.config.js.module.exports = {\n  target: \"serverless\",\n};\nThat's it! Now build the serverless app by running the following command:$ npm run build\nThe build that is generated by Next.js has a very low footprint, with the base zip size of the serverless function being 50KB. The function that is output doesn't require any other dependencies, since everything required is already bundled in. Of course, depending on the page's internal dependencies, the final size would be more than 50KB.In the .next folder, you will see a serverless folder generated after the build. Inside that there is a pages folder, which will have outputs of lambda per page.pages/index.js => .next/serverless/pages/index.js\npages/about.js => .next/serverless/pages/about.jsThese pages can now be independently deployed onto other cloud providers.Deploy to Now PlatformThe Now Platform gives a seamless way to deploy it to the cloud with now (download): Run the following commands to deploy$ npm install -g now\n$ nowNote: Older versions of now-cli doesn't support serverless mode. Once the deployment is successful, you will be able to navigate to pages / and /about, with each one internally being a lambda function which now manages.The Now Platform takes care of the routing which is available across a few regions. The lambdas are all deployed onto AWS.The outputs of the build process can be static files or serverless lambdas. Static files will be served directly from Zeit's edge caches, while lambdas contain code that gets executed dynamically and on-demand.As you can see in the image below, about and index pages are lambdas. The static assets are being cached by Zeit.Migrating from older versionsNext.js 8 maintains backward compatibility and hence for most applications, all you need to do is run:$ npm i next@latest react@latest react-dom@latestAnd set up the config in next.config.js to have the serverless build target and you should be good to go. Final ThoughtsThis is an interesting shift towards deploying web apps as serverless lambdas. Since each page of the app is a lambda, you pay only for the number of invocations and you can choose to scale high traffic pages selectively. Static assets can be cached as well, providing an affordable and scalable option from day 1.Check out the sample app on github. Take the demo for a spin and let us know what you think about serverless for web apps.If you have any questions or run into any trouble, feel free to reach out to us on twitter, github or on our discord server."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Realtime GraphQL API on Azure with Hasura and Azure Database for PostgreSQL Description: Deploy Hasura with a new or exisitng Azure Database for PostgreSQL to get instant realtime GraphQL API on Azure. Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "We recently published a guide along with a Resource Manager template to deploy Hasura on Azure backed by Azure Database for PostgreSQL and Azure Container Instances. This can be used with an existing database or a new one that is automatically provisioned.Just click the button below to get started with the Hasura GraphQL Engine on Azure with Azure Database for PostgreSQL (the button takes you to the Azure Portal).\n\n\n\nRead more about the integration on the Azure Blog."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Build react-static sites using GraphQL on Postgres Description: Source data from your existing postgres database for your react-static site using GraphQL APIs. Instant setup. Tutorial/boilerplate. Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DR: Source data from your existing postgres database for your react-static site. Instant setup. Tutorial/boilerplate -> react-static-graphqlreact-static <- GraphQL <- Hasura <- Postgresreact-static is a progressive static site generator for React. It is designed to be highly performant and supports custom query layers like GraphQL. It is a popular JAMStack framework. In this post, we are going to focus on the API part of JAMStack which allows us to build powerful static sites. All database actions are abstracted into GraphQL APIs (Hasura in this case) and accessed over HTTPS with JavaScript. Hasura provides GraphQL APIs on top of Postgres. So it can be leveraged effectively to build a site with react-static, which will be the Markup part of JAMStack.JAMStack with Hasura GraphQL and react-staticDeploy Hasura on Hasura CloudDeploy GraphQL Engine on Hasura Cloud and setup PostgreSQL via Heroku:\nGet the Hasura app URL (say react-static.hasura.app)Follow the instructions in the readme to create the tables author and article required for the app. Note the Hasura URL for GraphQL Endpoint. You will be configuring this in the app.Clone and Run appThe demo app walks you through to build a simple blog app, listing all authors and articles written by each author. You can clone and run the app for the demo.$ git clone git@github.com:hasura/graphql-engine.git\n$ cd graphql-engine/community/sample-apps/react-static-graphql\n$ yarn installAfter installation, you will have to configure the GraphQL endpoint that you got above.Open src/apollo.js and configure the endpoint.Now run the app:$ yarn startYou should be seeing the react-static homepage looking like:react-static demo app with Hasura GraphQLIf you had added sample data to authors and articles as mentioned in the readme, you can navigate to /blog and see the data templated on the UI.Implementation DetailsNow that you have looked at the demo app, let's look at what is happening behind the scenes. For this app, we need to fetch data from Postgres and template it on a react component. The data to be fetched is simple; article and author.The GraphQL query in Hasura for fetching all authors would look something like query {\n  author {\n    id\n    name\n  }\n}and the query for fetching all articles written by an author would look something like thisquery($author: Int!) {\n    article(where: {author_id: {_eq: $author}}) {\n        id\n        title\n        content\n        created_at\n    }\n}That's it! These are the only queries required to build this demo app. Now what about the routes? Well, it is actually pretty easy to write them. react-static reads a config file called static.config.js which can export a bunch of config options and getRoutes is one of them. It should resolve an array of route objects.Our aim is to create a route for listing all authors and one route for listing all articles written by a given author. So we will have a /blog route which displays the list of authors and a /blog/:authorId route, which displays a list of all articles written by a particular author.Now using the above GraphQL queries, we can write routes that cover the above use case. The code snippet for static.config.js looks something like this:Notice that, we are importing the GraphQL queries defined to fetch authors and articles. In getRoutes, we are returning an array of routes constructed to match our routing requirement of /blog/:authorId. We are mapping over each author to create a route for individual authors and returning the response of the GraphQL query to the react component. The response should ideally contain the list of articles for the given author. The templated markup is now constructed during build time and react-static outputs a bunch of HTML files, each for one of the above routes.This can be used to create any kind of dynamic routing to suit your needs. We are using Apollo Client to make the GraphQL queries. Final ThoughtsThe idea of rendering static pages is to improve performance; time to first byte. With a CDN in front of these apps, scaling becomes much easier as you have to just cache and render the files in more regions. The developer experience is also better with more targeted development and debugging. GraphQL APIs and tooling make it more easier to build such apps.I have put together a boilerplate and tutorial so that you can get started quickly!Check it out on github. Read more about JAMStack and documentation of react-static to play around with the app.Take the demo for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on twitter, github or on our discord server."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Contributor Days is back! Description: GraphQL Contributor Days is back by popular demand! We are thrilled to announce\nthis quarterly event in partnership with This Dot Labs and welcome those\ninterested in the GraphQL [https://hasura.io/graphql/] ecosystem to join.\n\nThe next session will be in San Francisco on Feb. 8th, 2019 from 12PM - 4:30PM. \n\nThe purpose of the event is for the GraphQL community to come together and\ndiscuss some key topics in the GraphQL ecosystem, and facilitate cross\ncollaboration within the community.\n\nAs the  Author: Hasura"}, {"role": "assistant", "content": "GraphQL Contributor Days is back by popular demand! We are thrilled to announce this quarterly event in partnership with This Dot Labs and welcome those interested in the GraphQL ecosystem to join.The next session will be in San Francisco on Feb. 8th, 2019 from 12PM - 4:30PM. The purpose of the event is for the GraphQL community to come together and discuss some key topics in the GraphQL ecosystem, and facilitate cross collaboration within the community.As the GraphQL ecosystem grows, with new tooling emerging and newer companies adopting it, this event hopes to give practitioners and members of the GraphQL community an unbiased platform to discuss and learn from each other and push the community forward. Our last event included topics such as:GraphQL, Microservices, and organising teams around GraphQLStandardizing GraphQL subscription protocolGraphQL in the client - state management on the client & encapsulation in large appsWe will be publishing a blogpost with all of the learnings from the last event here. In next week\u2019s event, some of the topics that we will be discussing are:GraphQL Tooling & EcosystemCommunity collaboration, new initiatives & GraphQL FoundationGraphQL Gateway - architecture, evolution, and experiencesTanmai, our CEO will be co-hosting this event along with Tracy Lee, the founder of This Dot Labs. Our guests for the next event include creators of Gatsby, Graphene, Apollo, AppSync as well co-creators of GraphQL. We will also be joined by engineers from companies using GraphQL in production such as:Coursera\u200a\u2014\u200aJon Wong, Staff Software EngineerRohan Deshpande - Principal Engineer, AWS AppSyncSyrus Akbary - Founder & CEO, Wasmer/GrapheneKyle Mathews - \u00a0Founder/CEO, GatsbyDocker\u200a\u2014\u200aTyler Martinez, UI Engineer & DesignerPaypal\u200a\u2014\u200aMark Stuart, Web InfrastructurePaypal\u200a\u2014\u200aShruti Kapoor, Software EngineerTwitter\u200a\u2014\u200aSasha Solomon, Software EngineerNetflix\u200a\u2014\u200aConor Hastings, Senior UI EngineerMoon Highway \u2014 Alex Banks, Software Engineer, InstructorIntuit, Inc. \u2014 Greg Kesler, Principal Software EngineerMoon Highway \u2014 Eve Porcello, Software Engineer, InstructorFacebook \u2014 Dan Schafer, Software EngineerIf you\u2019re interested to attend, do sign up for an invitation here as this is a non-broadcasted and invite only event.Tell us in the comments if you have questions for the attendees. And follow us on Twitter for all the action :). "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Notes from GraphQL Contributor Days Nov'18 Description: GraphQL Contributor Days\n[https://www.thisdot.co/events/graphql-contributor-days] is a community\ndiscussion hosted by ThisDot and Hasura. The first event was in November 2018;\nwe invited folks from the GraphQL community in and around San Francisco, had\nlunch, discussed GraphQL, had coffee and discussed some more GraphQL. ?\n\nThe amazing folks at GraphQL contributor days - Nov 2018This is a (long overdue)\nblog post summarizing the super insightful discussions that I was delighted to\nbe a part of!\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "GraphQL Contributor Days is a community discussion hosted by ThisDot and Hasura. The first event was in November 2018; we invited folks from the GraphQL community in and around San Francisco, had lunch, discussed GraphQL, had coffee and discussed some more GraphQL. ?The amazing folks at GraphQL contributor days - Nov 2018This is a (long overdue) blog post summarizing the super insightful discussions that I was delighted to be a part of!In attendance were:\n\n\n\n\n\nSasha (ex-Medium, Twitter)\n\u00a0\u00a0\u00a0\n\nSyrus (Graphene)\n\n\n\nEran (eBay)\n\u00a0\u00a0\u00a0\n\nUri (The Guild)\n\n\n\nBrian (GitHub)\n\u00a0\u00a0\u00a0\n\nJon (Coursera)\n\n\n\nGarrett (Netflix)\n\u00a0\u00a0\u00a0\n\nConor (Netflix)\n\n\n\nKyle (Gatsby)\n\u00a0\u00a0\u00a0\n\nNick (GraphQL Creator, Elementl)\n\n\n\nTyler (Docker)\n\u00a0\u00a0\u00a0\n\nMark (PayPal)\n\n\n\nShruti (PayPal)\n\u00a0\u00a0\u00a0\n\nTanmai (Hasura)\n\n\n\nAnd with all these awesome folks in the room together, these are the topics we discussed:GraphQL & microservicesError handling in GraphQLGraphQL in production - authorization, caching, securityGraphQL on the clientGraphQL subscriptionsSampling GraphQL and GraphQL onboarding in teamsMiscellaneous discussionsGraphQL and microservicesDoes GraphQL make you think about your backend and microservices differently?Nick kicked off the discussion by asking everyone what their microservices and the boundary definitions looked like, and how GraphQL was being incorporated. These were the different approaches across the room:\n\nMicroservices: Almost automated GraphQL gateway with almost no \u201cbusiness logic\u201d being used as a \u201cBackend for frontend\u201d (BFF) to upstream services that speak REST/GraphQL/gRPC\n\nPros:\n\nGraphQL adoption was incremental and added instant value to client applications\n\n\nCons:\n\nGraphQL API was not as client-driven as it could have been, and in some cases the \u201corganisational\u201d structure that was reflected in the microservices would leak into the client API\n\nMore \u201chops\u201d could lead to increased operational complexity and less DRYness in the codebase.\n\n\n\n\n\n\n\nMonolith (Facebook, Github): Native GraphQL API provided by the \u201cbusiness logic\u201d service\n\nPros:\n\nGraphQL API can be as client-driven as required\n\n\nCons:\n\nRequires significant buy-in of \u201cbackend\u201d team and working with existing code\n\n\n\n\n\nHybrid approaches:\n\nApproach 1: Product infrastructure / platform team composed of backend, infra, frontend developers implement a GraphQL gateway.\n\nPer Sasha, this is a successful approach for Medium even as the monolith is being broken into microservices (that speak via protobuf). The GraphQL gateway does some level of data transformations, error-handling to make the API is client friendly, but does not implement any other business logic.\n\n\nApproach 2: Start by building a GraphQL gateway to REST services to provide immediate value to client teams, but gradually merge with the REST service.\n\nIn Uri\u2019s work with enterprise clients with large amounts of legacy code, this approach seemed to be feasible and the GraphQL service was planning to eat the REST service. Incidentally the REST service in this case was proxying to SOAP endpoints, so it made an unusual amount of sense to merge the GraphQL and REST service. Uri mentioned that they also try to move things towards a monorepo (Nick\u2019s face lit up like anything) to improve automation and workflow.\n\n\n\n\n\nNick expressed 2 key concerns that could crop up if GraphQL support is not added \"natively\":If you split up the microservices by organizational domains (Conway\u2019s law) and structure your GraphQL schema by putting them together, then you are leaking your organizational structure to the client. While building a GraphQL gateway atop existing microservices, you are adding a new component, thus potentially introducing new process and performance overheadsError handling in GraphQLTL;DR: Go watch Sasha\u2019s talk: https://www.youtube.com/watch?v=GYBhHUGR1ZY As far as possible, model errors in the type system. Union types are especially handy. This works for \u201capplication\u201d errors which are expected states that your application and user could find themselves in. However before everyone migrates to union types, Jon issued a fair warning: union types are a breaking change for the client and should be rolled out thoughtfully!(Other types of errors are transient errors in the system or system level errors where the application is in an unknown state. These are hard to model in the application type system.)Hot takes:Kyle: Most errors are just another \u201cstate\u201d in your application.Nick: If there are 3 things we would change about GraphQL when we launched it:Called GraphQL mutations \u201cGraphQL actions\u201dGraphQL would have had a built-in story for streaming dataErrors. Error handling was half-baked at the outsetGraphQL in productionI asked for a quick round-up of practices and methods around authorization, security, performance, caching and here\u2019s a quick rundown of what different teams were up to.Authorization:Most common approach: Authorization done as usual by the upstream service with null returned for fields that are inaccessibleInput provided to GraphQL resolvers (in some cases via schema directives to make it easy to review the schema) to enforce authorization in the resolverSecurity:Mark: Depth check and reject, with a gradual move to weightages planned.Jon: Long running queries trigger a latency alarmPerformance/caching:Jon: We enforced \u201cmulti-GET\u201d on the upstream REST APIs to make fetching from the upstream services faster for the GraphQL gatewaySasha: Observed that overall latency increased for some GraphQL queries because of introduce another hop with the GraphQL service that calls upstream services.Conor\u2019s performance tip on the client-side: Please don\u2019t create the GraphQL query string in the `render` function of your react apps. It gets rebuilt a million times and is a performance gotcha commonly seen with folks writing their first GraphQL apps!Brian: Use the graphql batch ruby gem.Caching:Jon: No server-side caching yet, but this could be a HUGE win for the teamGarett: Caching is also problematic when there is PII data so it\u2019s not easy to enable \u201cglobally\u201dEran: We do in-memory caching on the GraphQL server to cache resolver resultsTyler: HTTP GET caching works well, because Tyler has GraphQL On The Client\u2122\ufe0fOne of the key open questions that emerged was around improving sequential information access from the GraphQL service to upstream services? Can we fetch information parallely by improving query-resolver traversal?GraphQL on the clientClient-side state: Redux vs apollo-link-state:In most cases, for those using Apollo, most state that was being handled was remote state. In those cases where there was client-side state it is usually handled locally in the react component\u2019s state object. In case components need to share client-side state, the most common solution has been to create a parent component that manages the state and passes it down to children via props.Shruti mentioned that because apollo-link-state was painful to configure, they abandoned it in favour of react local state and it\u2019s been possible to get away with that.Sasha: We currently use redux for queuing up \u201ctoasts\u201d on the app, however it would seem that most things can be achieved with apollo-link-state.Redux middleware was awesomeUri reminded everyone that one of the most powerful and amazing things about redux was the ability to add your own middleware (some nostalgic nodding was observed). However, because apollo-link is \u201cbelow\u201d the cache, it\u2019s not easy to write a middleware type utility with apollo-link. Uri and team recently open-sourced Loona.js. It is a client library that wraps over apollo-client and apollo-link-state to bring back the easy flexibility that redux middleware offered. Loona allows the developer to define \u201ceffects\u201d which can be attached to a set of \u201cactions\u201d. Read more about it here and in the docs.RelayAlthough relay use was not very common (Eran mentioned that they\u2019ve been on Relay for a while), mostly because Relay seems to be very opinionated, Relay did have some interesting ideas. Uri mentioned that it would be great to explore how to bring those ideas to other clients like Apollo, or how to make Relay more flexible.By enforcing opinions on the GraphQL schema, Relay can automatically handle the cache, busting it and updating it as mutations happen.Relay can extract GraphQL documents from across a codebase, transform/optimize them, and generate build artifacts. After getting the basic client working, devs like having a statically analysable set of GraphQL queries that a client makes anyway, and Relay makes this happen this out-of-the-box. Use-cases include persisting optimized GraphQL on the server, runtime representations of the queries for clients like the Relay runtime, or generated source code for use with compiled languages (Java/Swift/etc).Hot takes:Jon: Look at BabelBlade to reduce double-declaration. The GraphQL query is \u201cinferred\u201d from the resulting data object being used in the react code!Eran/Syrus: What if we had a universal GraphQL client with webassembly! ?GraphQL subscriptionsGraphQL subscriptions are still not commonly used, per the sample in the room. However, there are many interesting ideas around live queries to ease client-side work. The @live and @defer tags are especially exciting in this regard, albeit need more client-side support.Jon had an interesting take where it might be possible to implement @live and @defer directly on the client without needing server-side buy in. Fields marked with @live would undergo a polling. Fields marked as @defer, which are potentially slower GraphQL queries, could be queried after the main query was run.Sampling GraphQL & GraphQL onboarding for teamsBrian kickstarted the conversation by describing his initial steps with getting used to GraphQL when starting out at Github, and asking what kinds of things could be done to make the onboarding for new developers to the team\u2019s GraphQL stack better. The room then discussed different approaches to GraphQL onboarding:Jon: On the frontend side, build a basic UI component with the GraphQL API. On the backend side, especially to get familiar with GraphQL schema designing, fill in a \u201chole\u201d in the existing GraphQL schema.Uri: Getting familiar with the GraphQL API using GraphiQL and GraphQL voyager to visualise the APISasha: Series of mini onboarding courses. From things like the React/GraphQL architecture and making queries from the app to improving GraphQL query performance and making additions to the schema.Mark: Start by pointing people to the docs and then do a mini-workshop for the client-side and the server-side.Open discussionJon posed the question to the room of what the new tools and resources that we need to build in the current ecosystem would look like.Garret: Improving tooling around query planningTanmai: Better content and tooling around implementing performant GraphQL queries with existing popular language/framework ORMsUri: Better vendor agnostic tutorialsBetter linting: Lint rules for unused query fields, pre-processors for IDE typeaheads and build-time setups, making eslint handle \u201casync\u201d processing"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Call for open-source contributions! Description: Calling all open-source contributors ( or would-be contributors) to come be a part of the GraphQL Engine project.  Author: Tanmai Gopal"}, {"role": "assistant", "content": "The Hasura GraphQL Engine is an open-source project that provides a ready-to-use GraphQL API on Postgres. Over the last few months, Hasura has been starred over 6000 times on Github, and has a nascent but vibrant community who hang out on the Discord server. Hasura \u2764\ufe0f communityCommunity and community-contributions are at the heart of any open-source project, and we are very grateful for and encourage contributions from the community ?. The recent edition of the popular Hacktoberfest saw 74 community-contributed PRs in the repo!We would like to continue making the GraphQL Engine project a welcoming place for contributors, especially first-timers. We will do our best to keep the repo and community contributor-friendly - issues that are ripe for contributions will be proactively labelled as such and we will actively engage with and encourage ideas from anyone looking to contribute.The Hasura GraphQL Engine stack and the repoThe GraphQL Engine repo is made up of multiple components (server, console UI, CLI and docs) and a sub-folder for community content (boilerplates, sample apps, etc.). Here\u2019s a list of these different components, the corresponding programming language/framework(s) used by each of them and their contribution guidelines:In the GraphQL Engine repo, you\u2019ll find the following labels that indicate contribution readiness of issues:c/community: Issues for community content where little or no familiarity with Hasura code is expected. For example, a boilerplate for an AWS Lambda NodeJS function to implement a simple use-case.help wanted: Issues where the community's input is crucial. These can either be requests for ideas/feedback or for code.good first issue: Issues that are perfect for those who are new to open source, or just want to become familiar with the GraphQL Engine code-base.Why HasuraThe community: We have been blessed by an extremely kind and helpful set of folks in the community. With an interesting mix of developers from veteran architects to those working on their first gigs, it\u2019s very gratifying to notice that most members generously contribute their time with community content and \u00a0helping others out on the community Discord server. The stack: As detailed above, the Hasura GraphQL Engine stack is an eclectic set of cutting-edge languages/frameworks (even if we do say so ourselves ?). There\u2019s something for everyone looking to contribute - from documentation to functional programming in Haskell!92% of all PRs submitted to the project eventually got merged. We care about ensuring that your efforts actually translate into adding value to the project.We\u2019ve got your backWe understand that contributing to a new project can be a daunting task, especially for first-time open-source contributors. That\u2019s why we taken a few measures to ensure your experience of contributing to the GraphQL Engine repo is as seamless as possible:Our contribution guidelines are extremely detailed and designed to get you started as quickly as possible.We have a dedicated community support channel for contributors on our Discord. It\u2019s a great place to ideate and seek help or feedback. Your PRs will be constructively reviewed within 48 hrs without fail. Also, most of the PRs in the repo get an automatically deployed playground instance of GraphQL Engine with just your changes for instant feedback.If you are looking to make your first contribution to an open-source project, we will also set up pair-programming sessions with the maintainers of the component you want to contribute to, so they can help you get started! Please fill out this form and we\u2019ll get back to you with a calendly link so you can set up a call at a mutually convenient time. And then there\u2019s the\u2026Swag. We are kinda nuts about our artwork (all thanks to Chaaya)! And we like to share it as much as we can. That\u2019s why every contributor to the project get our gratitude in the form of a swag pack with the awesome artwork!The Hasura swag pack!How to get startedOkay, so you\u2019re convinced and can\u2019t wait to pitch in to the project! The first thing you should do is take the Hasura GraphQL Engine for a spin and then go through the repo\u2019s contribution guidelines. You\u2019ll get the lay of the land and you\u2019ll know how to get started with any of the components listed above. You can then head to issues in the repo and find an interesting problem to solve!Or, if you have any ideas and want to sound us out or if you are unsure of which issue to pick, join the community Discord, head to the GraphQL-> #contrib channel and just drop us a note. We are super responsive on Discord and we promise you that someone from the community will get back to you as soon as possible. Alternatively, you can even create an issue on Github if that\u2019s how you like to roll.See on you on the other side\u2026I hope we\u2019ve convinced you to attempt contributing to the Graph Engine project. If you do make a start, please know that we\u2019ll do our very best to make sure that you get across the finish line, your contribution becomes a part of the project, and that you\u2019ll have fun while you do this. Now, what are you waiting for!Recommended ReadingIf you are someone who is unsure of whether to get involved with an open-source project or you want to make your first contribution to one, you should definitely read this awesome guide on why/how to contribute to open source.https://egghead.io/courses/how-to-contribute-to-an-open-source-project-on-github"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Now in GraphQL Engine: Native support for PostGIS topology operators Description: Announcing native support for PostGIS topology operators in GraphQL Author: Hasura"}, {"role": "assistant", "content": "A lot of the developers in the Hasura GraphQL Engine community use PostGIS, the popular spatial database extension for Postgres, for their apps, and it was only natural for them to expect first-class support for PostGIS topology operators like ST_Contains, ST_Within, ST_DWithin, etc. in Hasura\u2019s GraphQL API. We are very pleased to announce the availability of native support for these operators as boolean GraphQL operators!Using PostGIS topology operators in GraphQLLet\u2019s say you want to fetch a list of popular landmarks within a district/borough. In your database schema you have a table called \u00a0landmarks (a list of popular locations, with a column of type geometry called location) and you need to search landmarks that are within a polygon (how you may model a district/borough). This is how you can now implement this search in a query using PostGIS\u2019 ST_Within operator as a boolean GraphQL operator in the where argument:You\u2019ll have to pass the variable as JSON (GeoJSON ) representation of geometry values as shown below:You can use these operators in subscriptions and mutations too.List of supported PostGIS topology operatorsThe following is a list of all supported topology operators:We are all ears!We are huge fans of the PostGIS extension and very excited to contribute to the experience for developers using both PostGIS \u00a0and GraphQL Engine. We'd love to hear your thoughts on this release - please do try it out and let us know how we can further improve our PostGIS support."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Hasura Technical Writer Program Description: Write technical content about the Hasura GraphQL Engine or GraphQL and get compensated for each post published. Author: Hasura"}, {"role": "assistant", "content": "Photo by Alfons Morales on UnsplashTL;DR: Write technical content related to Hasura or GraphQL and we will compensate you for each article of yours that we publish.The Hasura Technical Writer program is a way for you to support the Hasura GraphQL Engine or GraphQL in general by writing technical content, and get paid for doing so! As with any open source project, great technical content is key to the success of the Hasura GraphQL engine. Simple, well written tutorials, integrations and guides can make a huge difference when someone is using a dev tool for the first time. Your articles will help us grow Hasura\u2019s public repository of resources, and make it easier for developers to start using the GraphQL Engine. Have an interesting idea for a blogpost? Apply hereThrough this program, authors can opt for a personal payout or for a donation to a charity or open source project of their choice for articles posted on the Hasura blog. Here\u2019s how the process worksApply with a topic/proposal here.Once your proposal is accepted, our editors will get in touch and help you refine your title and outline. We will also decide on the mechanisms of the payout at this stage. We will pay you 300$ for each post published. Write: We will share writing guides, best practices, and references that you can look at as you write your post. Our editors will also be in constant touch to help you anytime you need them. Once the draft is finalised, we will publish the piece on our blog. Every piece will be accompanied by your bio and social media links. All content will be published under the Creative Commons license. We will make the payout as we had agreed at the acceptance phase. We will also send you some special Hasura swag, and feature your work on our social media channels and newsletter. What we're looking forSome of the topics we are interested in are: Walkthroughs of how to build fullstack applications using Hasura and GraphQL along with frontend languages like Javascript frameworks and thier respective ecosystems (Angular, Vue, React, React Native, etc), mobile (Android, iOS, Swift and Flutter) or Elm and ReasonMLUsing Hasura and GraphQL along with 3rd party platforms such as Gatsby, Netlify, Auth0, OneGraph etcTutorials on how to use Hasura features such as Remote Joins, etcIn-depth technical articles about working with GraphQL on topics such as caching, monitoring, schema masking, thoughts on GraphQL mutation design, interfaces, performance optimization and other interesting insights about working with GraphQL .You can also propose other topics, so long as they relate to working with the Hasura GraphQL Engine or GraphQL in general. We do not publish posts that have appeared elsewhere, and we will check to make sure that content we publish does not violate any copyright. Who can apply: Short answer - anyone. We would especially encourage first time writers to apply. We realise writing a technical post can seem like a daunting task, but we\u2019ll give you all the support you need to come up with an amazing blogpost. Please don\u2019t let anything discourage you from applying. If you have a great idea, we\u2019ll help you put pen to paper. If you have any questions, would like clarifications on any of the information above, or generally want to chat about the program, you can either send us an email at harsha@hasura.io.Tell us your idea for a blogpost. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Speaking at a meetup or a conference about Hasura? We can help! Description: Looking to speak about Hasura at a conference or meetup? We can help you prep, practise and even send you swag to give away.  Author: Hasura"}, {"role": "assistant", "content": "TL;DR Giving a talk that involves Hasura? We'll help you prep, practise and even send you swag to give away! How to at the end of this post. Over the last few weeks, a lot of our users have been giving talks about Hasura at meetups and conferences around the world. Some of them had reached out to us for assistance on our Discord, and we thought it would be a good idea to compile all the ways we can help in one place. Side note: Our Discord server was the highlight of 2018 for us! It has been incredible to watch the Hasura community help each other with all things GraphQL and serverless, showcase the products they have built, and generally be awesome. Check it out!Here are all the ways in which we can help with your talk (if you would like us to): Proposal/AbstractEmail us (details below) your abstract or proposal, and we can give you pointers and tips. Our engineers have spoken at a lot of meetups and conferences around the world, and have a good handle on what works. \u00a0Need some inspiration for your talk proposal? This repo with talk proposals might help. Resources to help you prepareOnce your talk has been accepted, we have lots of slide decks, speaker notes and videos from previous talks that we can share with you to help you prepare. We can also schedule calls with Tanmai or Tiru if you would like to practise. Both of them are very experienced speakers, and spoke at dozens of conferences and meetups in just the last year. SwagWe can ship you some T shirts and stickers that you can distribute after the talk. This will take us 2-3 weeks, so we would appreciate it if you let us know well in advance of your talk. Spreading the wordPre talk, we can help spread the word via social media and on our newsletter, and post talk, we can feature the talk on our website, social media and blog. How you can reach outIf you would like us to help in any of the ways mentioned above (or in some other way that we have not mentioned), you can either send an email with details to build@hasura.io, or ping Harsha (username: #ChocolateBear#0834) on our Discord. "}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How To Build A Real-Time App With GraphQL Subscriptions On Postgres Description: \nBuilding real-time apps is a hard engineering problem to solve, especially as a frontend developer or someone who has recently made a\u2026\n Author: Hasura"}, {"role": "assistant", "content": "Building real-time apps is a hard engineering problem to solve, especially as a frontend developer or someone who has recently made a transition to becoming a fullstack developer. However, GraphQL is rapidly upending this status-quo.In his article published in Smashing Magazine, Sandip covers how to build a realtime app from first principles, the challenges involved and how emerging tooling is addressing them with elegant solutions that are easy to reason about.If you are new to GraphQL or are just looking to build a realtime app, this post is a great place to start.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Tweaking GraphQL performance using Postgres\u2019 Explain command Description: \nLeveraging Postgres\u2019 execution plan information to find performance bottlenecks\n Author: Sandip"}, {"role": "assistant", "content": "Leveraging Postgres\u2019 execution plan information to find performance bottlenecksAs the Hasura GraphQL Engine\u2019s API is meant to be consumed concurrently by many frontend client apps, we\u2019ve invested a significant amount of effort into ensuring that Hasura is super performant (in terms of latency and resource consumption) and inherently scalable. In this post, let\u2019s see how we introduced a performance analysis feature for users by leveraging Postgres\u2019 popular execution plan analysis tool, Explain.Performance in Hasura GraphQL\u00a0EngineThe lifecycle of a query in the GraphQL Engine\u00a0worldAn incoming request to Hasura goes through the following stages:The request body is parsed into a GraphQL ASTThe GraphQL query is validated (checks for semantic correctness and ACL/authorization), and, if it passes all checks,The validated query is converted to an SQL statement and is executed by Postgres (which also converts the resultset into JSON)The JSON response is returned to the client.We have previously blogged about the architectural decisions made for optimizing the performance of the Hasura GraphQL Engine. Network latencies are also negligible as Hasura and the configured Postgres instance are meant to be collocated in the same datacenter/network. So, that leaves database optimizations, like indexes, etc. as options critical to improving the performance of the GraphQL API. These optimizations are largely dependent on the database schema and the quantum of data being handled.Postgres\u2019 Explain command displays the execution plan generated for the supplied SQL statement. The execution plan shows how the table(s) referenced by the statement will be scanned, and if multiple tables are referenced, what join algorithms will be used to bring together the required rows from each input table, along with estimated statement execution cost (Postgres docs; check out this post on how to interpret a Postgres execution plan).In a recent version of Hasura GraphQL Engine, we introduced an API that takes a GraphQL query as input and outputs the generated SQL along with the execution plan prepared by the Explain command.The generated SQL and the execution plan for a given GraphQL\u00a0queryLet\u2019s see how this feature helps tweak the performance of GraphQL queries through an anecdote.From 30s \u2192 less than\u00a01sA user from the community reported alarmingly high latency numbers for one of their queries\u200a\u2014\u200a30 seconds! The query fetches data from two tables, MrSeries and MrVolume, each with a few million rows in it. Here\u2019s a modified version of that query:And, here\u2019s the corresponding execution plan for it from the Explain command:Performance bottlenecks are the steps with high associated costs (cost is an arbitrary unit of computation). In the above execution plan, it\u2019s easy to see that the bottlenecks are lines 11 and 14\u200a\u2014\u200aPostgres running sequential scans on the tables MrSeries and MrVolume, which is bad as there are millions of rows in them! To rectify this, we added a couple of indexes on these tables:Then, we ran the following SQL commands to update Postgres\u2019 stats about these tables:When we ran and analyzed the query again, here\u2019s what we saw:Notice how the aggregate cost came down from 2439653.09 to 174088.71 (a ~14x drop!). The latency also went down from ~30 seconds to less than 1 second! All this from just adding a couple of indexes. How awesome is that?! ???We are iterating on this feature to make the insights from Explain readily actionable, automatically highlight performance bottlenecks, and perhaps even automate the database admin tasks for developers! ?How to check out this\u00a0featureDeploy the Hasura GraphQL Engine (Heroku is the fastest way to try it out) and create a table or two using the admin console (also seed it with some test data). Head to the GraphiQL tab of the console and run any valid GraphQL query based on your tables. Click on the Analyze button to see the execution plan and the generated SQL for your GraphQL query! command built into the GraphiQL interfaceDo take this feature for a spin and let us know what you think. We\u2019d love to hear your thoughts on performance for GraphQL APIs in our very active Discord community channel!\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Setting up a production ready realtime GraphQL backend in just a few minutes Description: \nAnnouncing the DIY Realtime GraphQL BaaS using Hasura, DigitalOcean and Auth0\n Author: Praveen Durairaju"}, {"role": "assistant", "content": "Announcing the DIY Realtime GraphQL BaaS using Hasura, DigitalOcean and\u00a0Auth0Since Facebook announced GraphQL three years ago, GraphQL popularity has grown by leaps and bounds. In the 2018 state of javascript survey, 83% of JS developers had either used GraphQL or wanted to learn it, and it is well on its way to becoming a megatrend.npm trends for\u00a0GraphQLAdopting GraphQL can result in a significant increase in feature velocity, and any organization looking at new product development or re-architecting their existing products should seriously evaluate GraphQL and the benefits that it brings.Getting started with GraphQL\u00a0quicklyAt Hasura, we have just released the DIY Realtime GraphQL BaaS. It uses the Hasura GraphQL Engine, Auth0 and DigitalOcean, to help you setup a production ready GraphQL backend in a few minutes\u200a\u2014\u200asort of like Firebase, but for realtime GraphQL, over Postgres, on your own infrastructure. It offers you the ease of use of a BaaS (set up in a few minutes), while allowing you to retain flexibility and control going forward.Here\u2019s how you can set it up:Deploy the Hasura GraphQL engine on DigitalOcean using the one-click deploy. It comes with a Postgres database and Caddy webserver for easy and automatic HTTPS using Let\u2019s Encrypt.Deploy Hasura GraphQL on DigitalOcean2) Create your schema using the Hasura console. The Hasura GraphQL Engine will give you instant GraphQL APIs, for queries, mutations and subscriptions.Create schema3) Configure Auth0 rules and callback URLs, and integrate it with Hasura.Configure access control\u00a0rulesAnd that\u2019s it! You have a full-fledged backend with GraphQL APIs and Authentication/authorization on managed infrastructure. On the 5$ machine, you should be able to handle thousands of users. Of course, if you want to handle more users, you can just upgrade the underlying infra.We have used DigitalOcean and Auth0 for this example for their ease of use, you can use any other infrastructure provider or authentication provider.Next stepsOnce you have the BaaS setup, there are tons of other cool things you can do as you build complex applications, and take them to production:Write custom backend code or business logic and use schema stitching to merge with Hasura.Manage postgres migrations.Trigger webhooks on Database events to integrate with serverless functions.Set up logging and monitoring with DataDog and HoneycombIf you get stuck somewhere or you have some questions, feel free to talk to us on our Discord.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Create Next.js sites using GraphQL on Postgres Description: \nTL;DR: Fetch data from your existing postgres database for your next.js site. Instant setup. Tutorial/boilerplate ?\u2026\n Author: Karthik Venkateswaran"}, {"role": "assistant", "content": "TL;DR: Server side render websites using Next.js and GraphQL APIs over postgres using Hasura GraphQL Engine. Instant setup. Tutorial/boilerplate -> nextjs-postgres-graphqlNextjs-GraphQL-PostgresBuilding server side rendered react website is hard, next.js commits to solve this problem beautifully. Hasura GraphQL engine makes it a breeze to setup a GraphQL server on a postgres database. In this post we will take a look at how we can put the 2 together, to use GraphQL APIs to fetch data on the server and use this data in server rendered pages using next.js.Next.jsNext.js makes building server side rendered websites fast. It is developed by the awesome peeps at Zeit. This is how it works:Pages: Next.js comes with its own opinion on how your project directory structure should be. It assumes a directory called pagesand every\u00a0.js file inside it becomes a distinct route. It cleverly makes use of the filesystem as an API to generate these pages on the server side.Server side data fetching: Next.js provides a way to fetch dynamic data required for a page to completely render on the server. It respects a static async function called getInitialProps. This function resolves to data which is passed as props to the page component which then rendered on the server and returned to the user.GraphQL &\u00a0HasuraGraphQL gives you ability to fetch the exact data you need for your UI and is great for building UIs with encapsulated data requirements. Hasura gives you a secure GraphQL endpoint instantly on your Postgres database, which allows you to focus on building your website with data from your database and not worry about setting up a GraphQL server.next-apollo: GraphQL in\u00a0next.jsAdam Soffer has come up with the amazing next-apollo library which uses getDataFromTree from react-apollo to SSR pages that are configured with a GraphQL query.Next.js + Hasura + next-apollo = SSR react apps with\u00a0GraphQL!I have put together a boilerplate and tutorial so that you can get started quickly!Check it out on github.Take it for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on twitter, github or on our discord server.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Hasura vs Prisma Description: \nOver the last few weeks, many people have asked me what the difference between Hasura & Prisma is.\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "Over the last few weeks, many people have asked me what the difference between Hasura & Prisma is.I\u2019ve put together a few points below with help from how our users are using Hasura, to list out a few differences. These are qualitative points and in the interest of time I\u2019ve not been very exhaustive and I will gradually do so. Please feel free to reach out to me on twitter or on discord(@tanmaig) if you have any questions in the meantime.On the surface, Hasura and Prisma provide a GraphQL API over Postgres. However, there is a significant difference in terms of where Hasura ideally fits in your stack and how they add value to your project.Hasura is GraphQL for your frontend\u00a0apps.Prisma is a GraphQL ORM for your GraphQL (or REST) servers and not your frontend apps, kind of like a replacement for JDBC, SQL Alchemy, ActiveRecord and so on.Prisma as a GraphQL API over your database cannot be used directly by your frontend apps.That said, you can use Hasura as a GraphQL API for your microservices if you need to. Especially if you\u2019re looking for scalable and robust subscriptions.Adding business\u00a0logicPrisma has clients & bindings for a nodejs/typescript/go for you to build your own business logic that would talk to a database. Because Prisma is an ORM, instead of using, say, nodejs and knex/sequelize to build a GraphQL server, you can use nodejs and Prisma client to speak to the Prisma server to speak to the database. Your code can perform business logic as usual, exactly like you would when building a GraphQL server from scratch.With Hasura, there are 2 ways to write business logic:You write your own GraphQL service however you\u2019d like, in any language/framework, using your favourite ORM (if you need to speak to the database) and merge that GraphQL service with Hasura\u2019s GraphQL to create a unified GraphQL endpoint.You write a webhook, ideally as a serverless function, that gets triggered whenever any changes happen in the database. Note that you can setup change capture for any change in the database, not just when there are GraphQL mutations.Hasura aims to save you the grunt effort of writing a CRUD and scalable realtime backend without interfering with how you\u2019d write your business logic.Adding business logic with\u00a0HasuraAuth/access control and GraphQL\u00a0securityBecause Hasura is a GraphQL API for your apps, Hasura has access control and authorization features for you to be able to secure access to your database. We\u2019ve carefully designed the access-control system to be able to handle simple to complex use-cases. From users being able to CRUD their own data to enterprise-grade multi-tenant, multi-role applications.Hasura integrates with any auth system, including JWT systems like Auth0, firebase, forgerock.GraphQL security: We already support limiting response sizes to prevent scraping/denial-of-service and we\u2019re adding more security features prioritized by what our users want. There are lots of other features we have on our roadmap for our users to help them use GraphQL securely.Performance &\u00a0costHasura does not use resolvers or a data-loader in its architecture. Instead, Hasura compiles a GraphQL query to a single, very readable, SQL query that include access-control clauses (where user_id = <session-user-id>). This makes Hasura extremely fast, especially for complex queries. Read more about our high performance architecture here: https://hasura.io/blog/architecture-of-a-high-performance-graphql-to-sql-server-58d9944b8a87Because Hasura is actually a compiler fronted by a webserver, Hasura\u2019s resource footprint is insanely low, especially compared to the JVM that Prisma needs to run.Hasura can service a few thousand complex GraphQL queries per second well within a 100MB of RAM \u2192 Build a side-project that scales to a thousand concurrent users at 5$ a month running both Postgres and Hasura. Prisma cannot run on the Heroku free tier or on Zeit because of its image size and resource consumption.In comparison, Prisma uses the dataloader approach and is optimized for small queries only and can\u2019t handle complex queries efficiently.Simple scaling & auto-scalingHasura is designed to be completely stateless and does not have a master-slave architecture. This makes it painless to scale Hasura:Vertical scaling: Increase CPU/RAMHorizontal scaling: Increase the number of replicasAuto-scaling: Setup up auto-scaling on your favourite container provider for automatic 1 to n scaling depending on trafficBecause of this, setting up production architectures for Hasura that involve auto-scaling, multi-AZ deployments, HA configurations is a straightforward process.Horizontally scaling Prisma is more tedious and not as straightforward as changing a config variable to increase replicas or enabling auto-scaling on your cloud vendor dashboard (or declarative config file).Seamless support for\u00a0PostgresHasura is designed to be Postgres first. Postgres is a very mature database and Hasura ensures that you can keep using an existing Postgres instance the way you want without any changes to your team\u2019s workflow. Hasura supports Postgres rich type system and operators, the ability to create views, using custom SQL functions and very soon even materialized views!Automatic management of your GraphQL\u00a0schemaPrisma makes you manage a database exclusively via the GraphQL SDL which cannot support the kinds of up/down/customisable database migrations that most applications eventually need. Eg, in a GraphQL SDL driven database system: Creating a User GraphQL type is equivalent to creating a user table.Hasura does not drive your data model through the GraphQL schema definition language. Instead Hasura helps you use your database as you would and offers a simple UI to manage your database modelling, and then generates a GraphQL schema on top of that based on extra metadata you give it. You can use your own database migration system or Hasura\u2019s migration system (inspired by ActiveRecord ala Rails) to model your database.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using Hasura GraphQL Engine with a CI/CD system Description: Schema migrations using Hasura CLI can version control and reproduce schema along with sample data. Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "Applications using Postgres and Hasura GraphQL Engine (HGE) might need to run them in a CI/CD environment to conduct tests (like we do when building Hasura). Let\u2019s look at a general workflow in getting Postgres and HGE running with some test data already in place.Using Hasura with GitLab, Jenkins, Circle CI, Travis CI, Drone\u00a0CIPostgresFirst step is to run Postgres (\u22659.5) as HGE depends on it. Easiest way is to spin up the Docker container if your CI/CD platform supports it.docker run -d -p 5432:5432 postgresThis will start Postgres with a default database called postgres. The database url will be postgres://postgres:@localhost:5432/postgres.To wait until Postgres starts running, we can use a simple bash function using netcat:So, after running Postgres, we wait for it to start running.wait_for_port 5432If you have Postgres running elsewhere, you can use that database url directly.GraphQL EngineHGE is released as a Docker container. Set the database url as an environment variable and run the container:docker run -d -p 8080:8080 -e HASURA_GRAPHQL_DATABASE_URL=postgres://postgres:@localhost:5432/postgres hasura/graphql-engineWait for port 8080 to be ready:wait_for_port 8080GraphQL Engine endpoint will be http://localhost:8080/v1alpha1/graphql.MigrationsBy default, when you run HGE, you can use the console served at /console endpoint to make changes to your schema. However, if you have different environments or you are adding GraphQL to an existing database, and want to replicate the database, you can make use of the Migrations feature.Hasura GraphQL engine comes with powerful Rails-inspired migration tooling to help you keep track of the changes you make to your schema. As you use the Hasura console from CLI, the CLI will spit out migration files for the changes you make that you can put in version control and even edit manually.SchemaYou can follow the guides on the Migrations docs page to setup migrations for your existing or new project.FixturesIf you need to add sample test data, you can add them as SQL files along with migrations.While preparing your migration files (not on the CI/CD system), create a new migration:hasura migrate create sample_dataThis will create some files of the form <version>_sample_data.{up|down}.{sql|yaml}\u00a0. You can edit <version>_sample_data.up.sql and add SQL statements to insert required data. Remove all other files and commit this.Applying migrations during the CI\u00a0stepOnce you have your migrations, you can get them to your CI/CD, say using git clone, and apply these migrations on the HGE we\u2019re running here.First, install the Hasura CLI:chmod +x hasuracurl -L https://cli.hasura.io/install/linux-amd64 -o hasura\nLet\u2019s say the migrations and config.yaml is in a directory called hge, we can apply the migrations by executing the following command:cd hge\nHASURA_GRAPHQL_ENDPOINT=http://localhost:8080 hasura migrate apply\nThis will restore the schema on the database.MetadataIf you\u2019re using another migration system like knex, you might be using hasura with migration mode turned off [docs here]. In that case, you\u2019ll only need to apply the HGE metadata:cd hge\nHASURA_GRAPHQL_ENDPOINT=http://localhost:8080 hasura metadata applyMetadata is already included in the Hasura migrations if you\u2019re using them.Your applicationOnce the migration/metadata is applied, you can connect your application to connect to HGE and run your tests. For example, to run npm tests:HASURA_GRAPHQL_ENGINE_ENDPOINT=http://localhost:8080/v1alpha1/graphql npm run testAssuming, your application is configured to take the endpoint from the environment variable HASURA_GRAPHQL_ENGINE_ENDPOINT.Example applicationsHere are some projects that run HGE in their tests:GraphqurlSketch.shHasura GraphQL Engine"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: firebase2graphql: Moving from firebase to realtime GraphQL on Postgres Description: \nWhat if you could move from Firebase ? to a \u201creal\u201d open-source database ?preserving the same application development experience\u2026\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "What if you could move from Firebase ? to a \u201creal\u201d open-source database ?preserving the same application development experience including realtime features?Over the last few weeks we\u2019ve been working on a small open-source utility to help you migrate data from firebase realtime db into Postgres and then expose realtime GraphQL on it, in a single command!firebase2graphql -d firebase-data-export.json https://my-hasura-app.comWe use the Hasura GraphQL engine to expose GraphQL on postgres by dynamically creating the postgres schema and the GraphQL relationships from our CLI.The two main problems we\u2019ve solved are:Retaining the same query and data model structure as data is migrated from firebase into PostgresAutomatically normalizing the denormalized firebase data modelIn this post, I\u2019ll talk aboutOur motivation behind this toolThe engineering behind how this was doneMotivation:1. Firebase realtime vs.\u00a0GraphQLFirebase\u2019s realtime database is a revolutionary product for web and mobile app developers. We love the Firebase experience of being able to query whatever data is required on the frontend without needing to write backend APIs and have anything in our firebase \u201cdatabase\u201d be realtime to boot.GraphQL promises application developers the same thing!Frontend first development: The ability to iterate on your application querying for precise slices of data as required.The client-side SDK that made the developer experience of using Firebase awesome is analogous to the tremendously amazing experience of using a GraphQL client with GraphiQL to browse and explore the GraphQL API.Firebase\u2019s realtime features can now be captured as GraphQL subscriptions which offer a similar level of abstraction and convenience to us without having to much about with the underlying sockets, connections and things.This is an example of a live-query that fetches user with id 1, their articles and any changes to that slice of data (spanning multiple tables) will automatically be sent to the client: \u00a0subscription {\n  user(where: {id: {_eq: 1}}) {\n    id\n    name\n    email\n    articles {\n      id\n      title\n      content\n    }\n  }\n}While firebase is arguably still more convenient GraphQL is more generic and allows app developers to bring a similar joy in consuming a backend API to any backend.2. Firebase vs.\u00a0PostgresOne of the major problems with Firebase is the inability to scale to more complex requirements that invariably arise. Simple features like being able to query the database or create aggregations/transformations over large volumes of data become inconvenient and in some cases impossible.Considering that we have an opportunity to preserve the same developer experience with GraphQL, including realtime features and move to the world\u2019s most advanced open-source database, why not!The solutionWe built the Hasura GraphQL engine (github) to give app developers an instant realtime GraphQL API on postgres. This combines the awesomeness of Postgres as a database with the developer experience of GraphQL to be able to build applications without writing any code on the backend.Our CLI tool firebase2graphql leverages Hasura to export data from firebase to Postgres, and optionally also normalise your data model to remove data duplication so that you still get easy queries with GraphQL to fetch related information without having to make multiple writes (mutations).1. Phase I: Exporting data from Firebase to Postgres preserving GraphQL query structureWhen you run this command:firebase2graphql firebase-data-export.json https://my-hasura-app.com\u2026we need to get data from firebase into Postgres tables. The key ideas are:Each node of data maps to a \u201crow\u201dFor every nested node, we create another table with a relationship to the parent tableIf the nested node, is an \u201carray\u201d type we create what we call an array relationship, basically a one-to-many relationshipOtherwise, we create a \u201cobject\u201d relationship, basically many-to-one relationshipWith these high-level rules, we\u2019re able to map the following firebase data: \u00a0{\n  \"posts\": {\n    \"-LMbLFOAW2q6GO1bD-5g\": {\n      \"author\": \"Eena\",\n      \"authorPic\": \"...photo.jpg\",\n      \"body\": \"Content body of this article\",\n      \"starCount\": 0,\n      \"title\": \"My first post\",\n      \"uid\": \"4UPmbcaqZKT2NdAAqBahXj4tHYN2\"\n    },\n    \"-LMbLIv6VKHYul7p_PZ-\": {\n      \"author\": \"Eena\",\n      \"authorPic\": \"...photo.jpg\",\n      \"body\": \"Content body of this article\",\n      \"starCount\": 0,\n      \"title\": \"Whatta proaaa\",\n      \"uid\": \"4UPmbcaqZKT2NdAAqBahXj4tHYN2\"\n    }\n  },\n  \"user-posts\": {\n    \"4UPmbcaqZKT2NdAAqBahXj4tHYN2\": {\n      \"-LMbLFOAW2q6GO1bD-5g\": {\n        \"author\": \"Eena\",\n        \"authorPic\": \"...photo.jpg\",\n        \"body\": \"Content body of this article\",\n        \"starCount\": 0,\n        \"title\": \"My first post\",\n        \"uid\": \"4UPmbcaqZKT2NdAAqBahXj4tHYN2\"\n      },\n      \"-LMbLIv6VKHYul7p_PZ-\": {\n        \"author\": \"Eena\",\n        \"authorPic\": \"...photo.jpg\",\n        \"body\": \"Content body of this article\",\n        \"starCount\": 0,\n        \"title\": \"Whatta proaaa\",\n        \"uid\": \"4UPmbcaqZKT2NdAAqBahXj4tHYN2\"\n      }\n    }\n  },\n  \"users\": {\n    \"4UPmbcaqZKT2NdAAqBahXj4tHYN2\": {\n      \"email\": \"rishichandrawawhal@gmail.com\",\n      \"profile_picture\": \"...photo.jpg\",\n      \"username\": \"Eena\"\n    }\n  }\n}\u2026to this postgres schema: \u00a0users (\n\t_id text not null primary key,\n\temail text,\n    profile_picture text,\n\tusername text\n)\n\nposts (  \n\t_id text not null primary key,\n\ttitle text,\n\tbody text,\n\tstarCount int,\n\tauthor text,\n\tauthorPic text,\n\tuid text\n)\n\nuser_posts (  \n\t_id text not null,\n\t_id_2 text not null,\n\ttitle text,\n\tbody text,\n\tstarCount int,\n\tauthor text,\n\tauthorPic text,\n\tuid text\n)With this schema created and the data imported, Hasura now allows you to start making GraphQL queries (reads), mutations (writes) and subscriptions (realtime) on your data.Phase II: Automatically normalizing your firebase\u00a0dataIf you\u2019re familiar with Firebase, you know that Firebase prefers your models to be as denormalised as possible. Write-many, read-once is the Firebase philosophy. This means that the data you import into Postgres with Phase I will have a lot of data duplication.At this point, you can continue using GraphQL on postgres with the write-many approach (make a series of mutations, which will automatically be in a transaction) or we can ask firebase2graphql to attempt a normalisation.This is the rough normalisation algorithm we follow:Look at column names of each table, if another table has similar column names we try to see if they have overlapping data. This is done through a series of SQL queries on the imported data.If an exact subset or duplicate table is found, the subset containing table is deleted and any tables related to it, are now changed to be related to the superset table.This is more to help you get started with normalisation rather than be a final end. You can continue the normalisation process manually via the Hasura console by removing and creating relationships, dropping duplicate data and s o on.Do go through our README for a more exhaustive guide: https://github.com/hasura/graphql-engine/tree/master/community/tools/firebase2graphqlTake it for a\u00a0spinDo try firebase2graphql and let us know what you think\u00a0:) Refer to the Hasura docs if you\u2019re looking for things like setting up security/access control on your GraphQL API.We hang on discord, on github and our website intercom. Feel free to ask us questions\u00a0:)\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL Observability with Hasura GraphQL Engine and Honeycomb Description: Observability (in software world) means you can answer any questions about what\u2019s happening on the inside of the system just by observing\u2026\n Author: Praveen Durairaju"}, {"role": "assistant", "content": "Observability (in software world) means you can answer any questions about what\u2019s happening on the inside of the system just by observing metrics from outside of the system, without having to modify the working deployment to support this.GraphQL Observability with Honeycomb and\u00a0HasuraWhy an Observable System?Without an observable system, it would be difficult to understand what is going wrong with the system unless you are monitoring for known issues. At any point of time, you should be able to ask any arbitrary question about how your application works.GraphQL Observability: In a GraphQL application, these are the important metrics and context to capture:time of query and query execution timeactual query payload / query hashresponse status codes of queries/mutations/subscriptionsgraphql server versionip_address from which the query originatedand specifically in the case of Hasura GraphQL Engine, you might want to capture context likeuser_id of the user who made the queryrole of the usermetadata of the queryWith this external information available, you can ask meaningful questions in a production deployment to find what went wrong internally, or why your GraphQL backend is behaving the way it is. For example, if you see anomalies in query execution time for a particular query hash, you can try to identify what is wrong with the query (may be there is a database bottleneck that you need to optimise).Just having this information set up for monitoring also might not be enough. To make it meaningful, you can set up alerts\u200a\u2014\u200afor example: triggers for requests/min, errors/min with thresholds and get notified when the server is behaving differently.By the end of this tutorial, you should be able to setup Hasura GraphQL Engine with an observable system, Honeycomb in this case, and help you understand, optimise and control your GraphQL server. Here\u2019s a preview of what you will get at the end.Honeycomb Dashboard with GraphQL Engine\u00a0LogsYou can also choose to create a visual view of your logs based on filters. For example, to monitor error codes across a given time interval.Error Code Capturing across 2 day time\u00a0intervalSet up Hasura GraphQL Engine on\u00a0GKEThis documentation assumes that you have Hasura GraphQL Engine installed on Google Kubernetes Engine. Instructions for deployment is available here.Set up Honeycomb Agent on KubernetesSignup on Honeycomb to start the Kubernetes agent setup. Honeycomb uses kubernetes secret to store the API key. This will be available on Honeycomb after Signing up. kubectl create secret generic -n kube-system honeycomb-writekey \\\n\u2014 from-literal=key=<your-key>We will use Kubernetes DaemonSets to automatically deploy the Honeycomb Agent on all nodes.Once the DaemonSets extension is enabled, create a daemonset using the following command to deploy the agent$ kubectl apply -f https://honeycomb.io/download/kubernetes/logs/quickstart.yamlNote: in production clusters, remove mountpath configuration for `minikube-varlibdockercontainers`Sending cluster events and resource\u00a0metricsExecute the following command to start sending cluster events and resource metrics from kubernetes.$ kubectl create -f https://honeycomb.io/download/kubernetes/metrics/honeycomb-heapster.yamlSending cluster state\u00a0metricsDeploy kube-state-metrics and the Honeycomb kube-state-metrics adapter to collect more fine grained data.Start kube-state-metrics$ kubectl create -f https://honeycomb.io/download/kubernetes/metrics/kube-state-metrics.yamlStart the Honeycomb adapter$ kubectl create -f https://honeycomb.io/download/kubernetes/metrics/honeycomb-state-metrics-adapter.yamlCheck the status of your new pods:kubectl get pods -n kube-systemkube-state-metricsHead over to the Honeycomb UI and query the kubernetes-state-metrics dataset to verify if everything is working.Sending GraphQL Engine logs to honeycombVerify graphql-engine logs by executing the following command.$ kubectl logs -l app=hasura-graphql-engine -c graphql-engine`Here -c is specified because hasura-graphql-engine has multiple containers running.Apply the following configmap. echo \u2018\n \u2014 -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n name: honeycomb-agent-config\n namespace: kube-system\ndata:\n config.yaml: |-\n watchers:\n \u2014 dataset: kubernetes-logs\n labelSelector: app=hasura-graphql-engine\n containerName: graphql-engine\n parser: json\n\u2018 | kubectl apply -f -\n`After updating the configmap, restart the agent pods:$ kubectl delete pods -l k8s-app=honeycomb-agent -n kube-systemIn the Honeycomb UI, go to the kubernetes-logs dataset and open the Schema tab.Schema tab\u200a\u2014\u200aHoneycomb DashboardScroll down to see options. Check the `Automatically unpack nested JSON` and select the level of nesting to `5`.Unpack nested JSON to get detailed parsing of GraphQL Engine\u00a0logsYou can now get detailed report on the logs / apply filters depending on the requirements.You can see query_execution_time, query_hash, response_size, status code etc for inspection.Dashboard FiltersNow start apply filters to get granular details. Lets apply a filter to get all queries which resulted in an error. The filter would be detail.status\u00a0!= 200\u00a0. Here\u2019s what the dashboard looks like.Status code\u00a0!= 200 in GraphQL\u00a0EngineNow, lets get a performance overview by filtering/sorting queries using query_exection_time column. Here\u2019s what a typical filter/sort would look like:Sort most time consuming queriesAlerts for AnamoliesNow that we have got a holistic view of your GraphQL queries, let\u2019s set up an alert to get notified via Email when something is wrong.Make a\u00a0TriggerWe can apply a filter for query_execution_time \u22652 to get notified for slow queries. This is just an example. You can tweak the number of seconds to a desired value depending on the application requirements.Now, the focus can be on the application development and this observable system setup could notify us on whats going wrong with the unpredictable parts of your GraphQL server.For further reference, refer to Honeycomb\u2019s kubernetes integration docs"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Create Gatsby sites using GraphQL on Postgres Description: TL;DR: Source data from your existing postgres database for your gatsby site. Instant setup. Tutorial/boilerplate\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "TL;DR: Source data from your existing postgres database for your gatsby site. Instant setup. Tutorial/boilerplate -> gatsby-postgres-graphqlGatsby is an amazing framework to help you build statically rendered react sites. Static sites, like those built with Gatsby, are easy to deploy and scale using CDNs and provide users with an amazing experience because pages load \u201cblazing fast\u201d.In some cases though, the data you need for your Gatsby site might come from a database, which it probably will in many real-world scenarios.For example, in a recent conversation with David Eads, he told me that while Gatsby was being used effectively at ProPublica Illinois, there\u2019s been a pain-point in integrating some of large datasets (on Postgres) they work with.But, those days are now gone because:You can use the amazing gatsby-source-graphql plugin to set any GraphQL API as a gatsby data-sourceYou can use Hasura to expose an existing Postgres database over GraphQL instantlyHallelujah!! I just started using this source plugin last night and it works just as advertised. @HasuraHQ integration here I come!! https://t.co/2kAy4L4fBE\u2014 David Eads (@eads) September 25, 2018\n\nKarthik Venkateswaran has put together a neat little boilerplate and tutorial so that you can get started quickly!Check it out on github.Take it for a spin and let us know what you think. If you have any questions or run into any trouble, feel free to reach out to us on twitter, github or on our discord server.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Hacktoberfest 2018 with Hasura Description: \nWe\u2019re super excited to announce that Hasura will be participating in this year\u2019s Hacktoberfest! Get your PRs merged into the GraphQL Engine\u2026\n Author: Rajoshi"}, {"role": "assistant", "content": "Celebrating opensource software with #hacktoberfestWe\u2019re super excited to announce that Hasura will be participating in this year\u2019s Hacktoberfest! Get your PRs merged into the GraphQL Engine repo to get some cool Hasura swag ?.Hacktoberfest, brought to you by DigitalOcean, GitHub & Twilio, is a month-long celebration of open source software that happens every year during October. Hacktoberfest started in 2014 and rallies a global community of contributors, with last year\u2019s event drawing more than 30,000 participants and nearly 240,000 pull requests.Hasura & open\u00a0sourceWe open-sourced the Hasura GraphQL Engine just about 2 months back and have received an amazing response from the community.GraphQL is a rapidly emerging field and developers the world over are moving towards building applications using GraphQL. We look forward to being involved with the large open source community that\u2019s excited about GraphQL and the tooling around GraphQL. There is a lot of tooling, boilerplates and tutorials that can be created to help the community move forward faster.Hacktoberfest with\u00a0HasuraThe Hasura GraphQL Engine is a GraphQL server that gives you instant, realtime GraphQL APIs over Postgres, with webhook triggers on database events for asynchronous business logic. The GitHub repo is organised across the following components (I have also mentioned the programming language/framework used):Core engine (Server)\u200a\u2014\u200aHaskellConsole\u200a\u2014\u200aReactCLI\u200a\u2014\u200aGoDocs\u200a\u2014\u200areStructuredText with SphinxCommunity resources: NodeJS, Python, React, ReactNative, Flutter, Vue and anything and everything the community would like to see and contribute.Issues across all these components are available for taking up and have been marked as hacktoberfest. If you\u2019re new to open-source, or the respective programming language/framework, we suggest that you look at issues we have also labelled as good first issue.Here are some issues already marked:add more boilerplates for serverless triggers \u00b7 Issue #431 \u00b7 hasura/graphql-engineNeed more examples on serverless triggers for various platform on different languages aws-lambda node 8 java csharp\u2026github.comupdate checker bottom-bar hiding the run button under sql tab \u00b7 Issue #271 \u00b7 hasura/graphql-engineThe update checker bottom bar is hiding the Run button on SQL page. PFA the screenshot of the same. The update checker\u2026github.comClick on the link below to find more issues:hasura/graphql-engineBlazing fast, instant realtime GraphQL APIs on Postgres with fine grained access control, also trigger webhooks on\u2026github.comGet some Hasura\u00a0SWAG!To participate, you can submit a pull request to our GraphQL Engine repository between 1st to 31st of October. If you\u2019re taking up an issue, do comment on it displaying intent so that duplicate work can be avoided.We\u2019ll send a Hasura sticker pack to anyone who submits a PR. And if your PR to an issue tagged as \u2018hacktoberfest\u2019 gets merged, we will ship a Hasura t-shirt and a sticker pack to you (wherever in the world you are). We will send you a form to fill out for details as and when this\u00a0happens.Hasura SWAG!We\u2019ll share more updates over the next few weeks. In the meantime, post your questions on our Discord server or tweet @HasuraHQ with the hashtag #hacktoberfest.To learn more about Hacktoberfest from DigitalOcean, GitHub and Twilio, you can visit the official page. You can register on the page to get a limited edition Hacktoberfest shirt from DigitalOcean for completing five pull requests on public GitHub repositories. Contributions to the Hasura GraphQL Engine repo will count towards the five ?.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Uptime Monitoring for Hasura GraphQL Engine with DataDog on GKE Description: Uptime monitoring is an important set up to determine availability and reliability of the servers in which your application is running\u2026\n Author: Praveen Durairaju"}, {"role": "assistant", "content": "Uptime monitoring is an important set up to determine availability and reliability of the servers in which your application is running. Hasura GraphQL Engine can be deployed as a Docker container and hence integrations with monitoring tools is a lot easier since most of them come with their built-in solution. By the end of this post, you will be able to set up monitoring alerts with DataDog on a Kubernetes cluster.Set up Hasura GraphQL Engine on\u00a0GKEThis documentation assumes that you have Hasura GraphQL Engine installed on Google Kubernetes Engine. Instructions for deployment is available here.Install Datadog agent on KubernetesHead over to installation instructions of Datadog Agent on Kubernetes.We will use Kubernetes DaemonSets to automatically deploy the Datadog Agent on all nodes.Installing Datadog Agent on KubernetesOnce the DaemonSets extension is enabled, create a manifest datadog-agent.yaml using this manifest to deploy the Datadog Agent. The manifesDot also includes your Datadog account API key and hence it is recommended to use the same.You\u2019ll then be able to deploy the DaemonSet with the command$ kubectl create -f datadog-agent.yamlCheck the status of the datadog agent pod using$ kubectl get podskubectl get pods to verify datadog-agent statusOnce its up and running, you can go to your Datadog dashboard for Kubernetes to view metrics that are automatically collected by the agent.By default, the Agent reports basic system metrics to Datadog, covering CPU, network, disk, and memory usage.Fine grained metrics with kube-state-metricsFor more fine-grained metrics, you have to set up kube-state-metrics add-on to your cluster, which provides much more detailed metrics on the state of the cluster itself. Some of the metrics it collects are node status, node capacity (CPU and memory), number of desired/available/unavailable/updated replicas per deployment, pod status (e.g., waiting, running, ready), and so on.Once you clone the kube-state-metrics repo, you just need to do the following:An important note for GKE users is that GKE has strict role permissions that will prevent the kube-state-metrics roles and role bindings from being created. To work around this, you can give your GCP identity the cluster-admin role by running the following one-liner:$ kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud info | grep Account | cut -d '[' -f 2 | cut -d ']' -f 1)Once you get \u201ccluster-admin-binding\u201d created message, you can continue with the setup of this service by executing the following command:$ kubectl apply -f kubernetesinside that repo as given in the README. The kubernetes folder contains service and deployment configs.Setup kube-state-metricsYou can check the status of metrics deployment using$ kubectl get deployment -n kube-systemand it will list a pod named metrics-server-v0.2.1 (at the time of writing, v0.2.1). Now head over to the Datadog dashboard to see more specific metrics from your Kubernetes cluster.More metrics after enabling kube-state-metricsOnce that is running, we can start creating Monitors in Datadog for alerts via Email.Head over to DataDog Monitors and select Kubernetes or Docker integration to start creating monitoring alerts whenever a deployment or a pod is down.Setting up Monitors.Define the metric to see if pods are running in the GraphQL Engine deployment by setting the threshold values for Alert (typically below 1), and setup notification via email in subsequent steps by templating the content of email.You can create multiple monitors to track CPU and memory usage etc depending on the use case. To test if the alerts are working, let\u2019s scale the deployment of graphql-engine to 0.$ kubectl scale --replicas=0 deployment/hasura-graphql-engineNow depending on the metric setup, you will get an email alert because the pods are not running. Head over to manage monitors to see the status of each monitors.This is a quick and effective way to setup uptime monitoring on a production kubernetes cluster for GraphQL Engine.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Improving our Cypress test automation time by 3x Description: At Hasura, we use Cypress, an e2e testing framework to test our console ui, which is the interface powering our graphql-engine. Cypress\u2026\n Author: Praveen Durairaju"}, {"role": "assistant", "content": "At Hasura, we use Cypress, an e2e testing framework to test our console ui, which is the interface powering our graphql-engine. Cypress tests are written using Mocha and Chai, and hence the syntax is familiar territory for modern javascript developers, amongst other reasons to move from selenium.The Problem:We had written a bunch of spec files to test our UI, a few to simulate user behaviour and also verifying through our APIs to check the correctness of the UI. Now all of these tests ran synchronously which meant that it took more than 30 mins to test all our core UI features.Waiting for tests to\u00a0completeTo increase coverage, we had to add more tests and that meant the time for tests will only go up. This was blocking our quick test <-> iterate -> deployment process of graphql-engine.Parallel tests, multiple processesWe resorted to running parallel tests by spawning multiple processes in the same container, knowing that it depends on the hardware resource to run as many as it can.But we ran into the issue of Could not start Xvfb\u00a0, Could not stop Xvfb due to a smaller timeout (initially 2 seconds, which was increased to 5 seconds) in detecting if Xvfb was installed and running. There are a few open issues on github, which deals with the above. The only frustrating bit was that these issues kept happening randomly (few builds would succeed and few would fail) and hence this optimisation could never be merged to our deployment process.Parallel tests, multiple containersWhile we were figuring out how to optimise tests duration, Cypress released v3.1.0 last month, which enables parallelism on CI systems with a simple --parallel flag. By enabling a cluster of docker containers in your CI, cypress automatically detects your CI provider\u2019s environment variables and parallelises your tests.To setup parallel tests, we also had to enable --record flag and that required a project to be setup on Cypress Dashboard service to record our tests.Note: The dashboard service is free for open source projects.Cypress Dashboard Service\u200a\u2014\u200aTesting console on Hasura GraphQL\u00a0EngineThe docs are straight-forward to setup a project using the Test Runner. Each project will have a projectId which needs to be configured in cypress.json and a record key which needs to be passed to --key flag.We use CircleCI for CI/CD and we just had to enableparallelism: 4in our configuration file. (4 is the number of containers). Parallelism Docs for the same are straightforward as well.We had categorised our tests into different folders and spec files depending on the service.Finally the cypress command that we had to run to get all of this working:cypress run --spec 'cypress/integration/**/**/test.js' --key $CYPRESS_KEY --parallel --recordAnd the end result\u2026 our overall build times came down from ~45 mins to ~15 mins. Here\u2019s one of our recent PR build on CircleCI that builds console with parallelism enabled.CircleCI test time\u200a\u2014\u200aBefore enabling parallelismCircleCI test time\u200a\u2014\u200aAfter enabling parallelismNote the time taken for test console in before and after parallelism tests.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Announcing Event Triggers on Hasura GraphQL Engine Description: After much work and sweat over the last few months, I\u2019m extremely happy to announce the public preview of a feature that allows you to\u2026\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "After much work and sweat over the last few months, I\u2019m extremely happy to announce the public preview of a feature that allows you to trigger webhooks on database events!Check it out at https://hasura.io/event-triggers/ and try it out on Heroku or with Docker.This feature is a part of the open-source Hasura GraphQL engine\u00a0:)Event-triggers on Postgres with\u00a0HasuraWith Hasura, creating an event-trigger to listen to a database change on Postgres takes all of 60 seconds:Create an event trigger for a webhook on a\u00a0tableCoupled with serverless deployments, this allows our developers to build some really cool things very easily:Push notifications that get triggered when a new review is addedPurge your CDN or cache to re-render your static contentUpdate a search index when a product is inserted, updated or deletedTrigger business logic that can run asynchronouslyCheck out more use-cases, demos and tutorials on the website: https://hasura.io/event-triggers/How it\u00a0worksTriggering webhooks on events are a deceptively simple feature. However, creating events and triggering actions is something fraught with edge-cases that we\u2019ve spent a lot of time working through. Here are some of the things you _don\u2019t_ have to worry with Hasura event-triggers!Atomic generation of events: If something happened in the database, then the event is captured ?%. Any source of database changes is fair game. A GraphQL mutation, a legacy REST API, a cron-job, direct SQL changes. In fact, events are even captured if Hasura is not running, going through a restart or deployment rollout.Reliable delivery of events: You can configure retry logic and a retry interval.Horizontally scalable: You want to deliver more events faster? Just scale Hasura horizontally and stay calm.Debugging and Observability: Building async logic, especially with serverless functions, is so much fun when you\u2019re trying things out and simultaneously so much nightmare in production. A large part of this is because debugging and tracing events in the new sphagetti backend requires extra instrumentation. With Hasura, events & invocations are already instrumented and the highly structured and informative logs make it easy for you to trace everything from errors to latency issues with your favourite tools\u00a0:)Realtime GraphQL \u2764\ufe0f Serverless triggered by database\u00a0eventsI\u2019m personally the most excited about the killer combination of realtime GraphQL (live-queries) and async business logic.I\u2019ve put together a video describing what an app using serverless for complex business logic in a real world situation would look like and it\u2019s super fun to look at!Try it\u00a0out!This is a public preview release and we\u2019re eagerly waiting to hear what you think. Take it for a spin at https://hasura.io/event-triggers/ and try it out on Heroku or with Docker."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using TimescaleDB with Hasura GraphQL Description: \nUse Hasura GraphQL on top of TimescaleDB to build powerful real-time dashboards in minutes\n Author: Anon Ray"}, {"role": "assistant", "content": "TimescaleDB is the first open-source time-series database that natively supports full SQL and is deployed in production across industries all around the world for powering applications in DevOps, IoT, SaaS, and Machine Learning. It is packaged as a Postgres extension. As the Hasura GraphQL engine gives you GraphQL APIs instantly on any Postgres database, it also works with TimescaleDB.For example, you can use create powerful views using TimescaleDB functions and instantly build real-time dashboards using GraphQL live-queries that Hasura will automatically provide. You can leverage TimescaleDB\u2019s fast ingestion and use GraphQL mutations from your app to insert data quickly and easily.For this post, we are going to set this up on a Digital Ocean VM.TL;DRThese are the topics we\u2019ll cover in this guide:Create a Digital Ocean droplet and install docker and docker-composeRun Hasura with TimescaleDB using docker-composeLearn what hypertables areRun GraphQL queries on hypertablesRun GraphQL subscriptions and get real-time dataCreate a Digital Ocean\u00a0dropletOpen Digital Ocean console, and click on the \u201cCreate\u201d button, and select \u201cDroplet\u201d from the menu:In the next screen, choose the image to be \u201cUbuntu 16.04\u201d:Then choose a machine size and choose a region of your preference.Next, make sure to add a SSH key (you can add your existing SSH key on your computer), so that we can login to our server. Finally, click on the \u201cCreate\u201d button.Installing DockerGet the public IP of your droplet, and then SSH to the server:$ ssh -i <path-to-the-private-key> root@<public-ip-of-droplet>Once we are logged in, install Docker and Docker compose:$ apt-get update\n$ apt-get install docker.io docker-compose\n# verify docker is working\n$ docker infoRun TimescaleDB with\u00a0HasuraTo run TimescaleDB stand-alone, we can just run their docker image:$ docker run -d -p 5432:5432 timescale/timescaledb:latest-pg10This will pull the timescale/timescaledb docker image and run it, binding the host\u2019s port 5432 to the container\u2019s port 5432.But instead of this let\u2019s use docker compose to run TimescaleDB and Hasura GraphQL together.Copy and paste the following in a file, name it docker-compose.yaml\u00a0:Change the HASURA_GRAPHQL_ACCESS_KEY and POSTGRES_PASSWORD to your preference.Then run:$ docker-compose up -dCheck if everything is running, by:$ docker ps\n\nCONTAINER ID IMAGE                 ... CREATED STATUS PORTS          ...\n097f58433a2b hasura/graphql-engine ... 1m ago  Up 1m  8080->8080/tcp ...\nbccb3af615fd timescale/timescaledb ... 1m ago  Up 1m  5432/tcp       ...This starts both TimescaleDB and Hasura.Now in your browser open: http://your-droplet-public-ip:8080\u00a0, and you should see the Hasura console. It will prompt for the access key you have entered before.Setup the TimescaleDB extensionWe have to setup the timescaledb extension on our database:We need to use psql to create the extension. To do that, we need to exec into out container.Find out the container id from:$ docker psThen exec into the container with psql\u00a0, and create the extension:# exec into the container, with psql\n\n# once we are connected to the db, we get a psql prompt\n\n# create the timescale extension\n\n$ docker exec -it <container-id> psql -U postgrespostgres-# postgres-#\n \nCREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;Now, we are all set to use TimescaleDB.First steps with TimescaleDBSQL tables in Timescale are called hypertables. They are an abstraction over standard SQL tables for efficient managing and querying of timescale data. For the end user, they can use hypertables as normally they would with standard SQL statements.Note: hypertables are created by first creating a normal SQL table, then converting it into a hypertable via the function create_hypertableLet\u2019s create our first hypertable via the Hasura console: \u00a0-- We start by creating a regular SQL table\n\nCREATE TABLE conditions (\n  time        TIMESTAMPTZ       NOT NULL,  \n  location    TEXT              NOT NULL,  \n  temperature DOUBLE PRECISION  NULL, \n  -- in celsius  humidity    DOUBLE PRECISION  NULL  \n  -- in percentage\n);\nCreate a table in the SQL\u00a0tabWe are using raw SQL to create the table because we have data types that are not available in the console directly. We will also track the table, so that we can perform GraphQL queries on it.Next, convert it into a hypertable:-- This creates a hypertable that is partitioned by time\n--   using the values in the `time` column.\nSELECT create_hypertable('conditions', 'time');Convert the table into a hypertable-- This creates a hypertable that is partitioned by time\n--   using the values in the `time` column.\nLet\u2019s insert some sample data: \u00a0INSERT INTO conditions(time, location, temperature, humidity) VALUES (NOW(), 'office', 27.24, 48.15);\n\nINSERT INTO conditions(time, location, temperature, humidity)  VALUES (NOW(), 'home', 32.21, 77.23);We can use standard SQL to query our tables. TimescaleDB also offers specialized functions to query time-series data.The following query will fetch last three hours data, grouped in 15 minute windows with their max temperature and humidity, and grouped by each location: \u00a0 \u00a0 \u00a0SELECT time_bucket('15 minutes', time) AS fifteen_min,\nlocation, COUNT(*), MAX(temperature) AS max_temp, MAX(humidity) AS max_hum    FROM conditions    \nWHERE time > NOW() - interval '3 hours' \nGROUP BY fifteen_min, location    \nORDER BY fifteen_min DESC, max_temp DESC;Hasura GraphQL with TimescaleDBTo query our hypertables with TimescaleDB-specific functions like time_bucket\u00a0, we can just create a view, track the view and make GraphQL queries on the view.We can create this view: \u00a0CREATE VIEW last_fifteen_mins_stats AS (\n\tSELECT time_bucket('15 minutes', time) AS fifteen_min, location, \t\t\n    COUNT(*), MAX(temperature) AS max_temp, MAX(humidity) AS max_hum\n  FROM conditions\n  WHERE time > NOW() - interval '3 hours'    \n  GROUP BY fifteen_min, location    \n  ORDER BY fifteen_min DESC, max_temp DESC\n);Now we can make GraphQL queries on our view: \u00a0query {\n  last_fifteen_mins_stats {    \n    location,    \n    fifteen_min,    \n    max_temp,    \n    max_hum  \n  }\n}Which will result in: \u00a0{\n  \"data\": {    \n    \"last_fifteen_mins_stats\": [      \n      {\"max_temp\": 32.21, \"location\": \"home\", \"max_hum\": 77.23,        \t  \n       \"fifteen_min\": \"2018-08-22T11:15:00+00:00\"},\n      {\"max_temp\": 27.24, \"location\": \"office\", \"max_hum\": 77.23,        \t\t\t\"fifteen_min\": \"2018-08-22T11:15:00+00:00\"}  \n     ]\n  }\n}Real-time data with TimescaleDB & Hasura\u00a0GraphQLWe can make a simple Python script which ingests data into TimescaleDB. We can run a subscription query to fetch our data in real-time.As our conditions table is also tracked in Hasura GraphQL, we can make insert mutations\u00a0!Our script looks like this:If we save this script in a file timescale_ingest.py\u00a0, we can run this script by:$ python3 timescale_ingest.pyThis will run in an infinite loop and ingest random data into our TimescaleDB instance via Hasura GraphQL mutations.We can run subscribe to this query by using Hasura GraphQL subscription: \u00a0subscription {\n  last_fifteen_mins_stats {    \n    location,    \n    fifteen_min,    \n    max_temp,    \n    max_hum  \n  }\n}Hasura subscriptions with TimescaleDB ingestion in\u00a0actionSummaryUsing TimescaleDB and Hasura GraphQL engine, we could query our Postgres database for real-time data using GraphQL APIs in under 10 minutes. This makes building powerful, real-time dashboards extremely simple.To get started with TimescaleDB, check out their GitHub and feel free to join their Slack community.You can get started with Hasura here. We\u2019re very active on our community chat on Discord. You can also DM me on Discord @rayanon if you are trying this out and have any questions!\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Postgres triggers on GraphQL mutations Description: \nSetup Postgres functions and triggers to perform backend operations like validation and inserting/updating other related data\n Author: Anon Ray"}, {"role": "assistant", "content": "Postgres trigger on a GraphQL mutation in\u00a0actionTL;DRSetup Postgres functions and triggers to perform backend operations like validation and inserting/updating other related data, whenever some inserts/updates happens on tables. Now when you use GraphQL mutations, Postgres triggers run, performing the required backend operations.IntroductionThere are cases where you would want run some custom backend function after an insert happens.Some examples of this are:Perform some validation on the server-side. For e.g. in a banking application, validate that deposits cannot be than 0 or lesser.Insert some related data in a single transaction. For e.g. in a note taking application, create a revision whenever a note is updated.What is a Postgres trigger\u00a0?A trigger is associated with a table or view and is fired whenever an event occur. The \u2018events\u2019 are INSERT, DELETE, UPDATE or TRUNCATE.Trigger will eventually call a function which will automatically be invoked when a specific event occurs.Triggers can be invoked in the following scenarios:Before the operation.After the operation.Instead of the operation.Validation on the server-sideConsider the following data model for a banking-like application:customer\u200a\u2014\u200aid, nameaccount_savings\u200a\u2014\u200aaccount_no, customer_id, is_active, created_atdeposit_activity\u200a\u2014\u200aaccount_no, deposit_amount, deposit_timewithdraw_activity\u200a\u2014\u200aaccount_no, withdraw_amount, withdraw_timeSo to demonstrate the first scenario, we will perform basic validation before an insert into deposit_activity happens. We want to ensure that insert doesn\u2019t happen, if the deposit_amount is \u2264 0 or if the account is inactive.Let\u2019s create a trigger. You can copy-paste this in the \u201cSQL\u201d tab of Hasura console:Now to to make deposits into an account, we can just use this insert mutation, and the trigger will make sure that the amount is valid (\u22650) and the account is not an inactive account. \u00a0mutation deposit {\n  insert_deposit_activity(objects:[{\n    account_no: 1,deposit_amount: 1000  }]\n  ) {    \n    affected_rows  \n    }\n}\nMaking an insert where the account is inactive or amount < 0 will throw a Postgres exception: \u00a0mutation deposit {\n  insert_deposit_activity(objects:[{\n    account_no: 1, deposit_amount: 0 }]\n  ) {\n    affected_rows\n  }\n}Results in:...{\n  \"error\": { \n    \"exec_status\": \"FatalError\",\n    \"hint\": null,\n    \"message\": \"Deposit amount must be greater than 0\",\n    \"status_code\": \"P0001\",\n    \"description\": null\n  }\n}Insert related\u00a0dataConsider the following data model:user\u200a\u2014\u200aid, usernamenote\u200a\u2014\u200aid, user_id, title, data, created_at, updated_atnote_revision\u200a\u2014\u200anote_id, created_at, title, dataWhenever a user updates a note, we will insert into note_revision before updating the note table.Create the following trigger:Now let\u2019s create a note: \u00a0mutation {\n  insert_note (objects: [{\n    user_id: 1, title: \"my first note\", data: \"some long note data i have\"}]\n  ) {\n    affected_rows\n  }\n}\nNow when we update that note: \u00a0mutation {\n  update_note (where: {id: {_eq: 1}}, _set: {title: \"My new note\", data: \"some   other note data\"}){\n     affected_rows\n    }\n}\nA new row in the note_revision table will be created:ConclusionPostgres triggers can be used effectively to trigger operations/functions on the backend.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Ultimate Guide to Schema Stitching in GraphQL Description: \nAn example-driven guide for understanding schema stitching in order to customize existing GraphQL API(s)\n Author: Rishichandra Wawhal"}, {"role": "assistant", "content": "An example-driven guide for understanding schema stitching in order to customize existing GraphQL\u00a0API(s)TL;DRSchema stitching is the process of creating a single GraphQL schema from multiple underlying GraphQL APIs.This is an example-driven blog post that covers the following schema-stitching use cases:A basic setup for schema stitching that simply delegates (github)Merging two remote schemas (github)Custom resolvers: Adding new queries and mutations to an existing remote schema (github)Override existing queries and mutations of a remote schema (github)Schema transform: renaming root, root-fields and fields of a remote schema (github)Extend a remote schema with custom types (github)We're happy to announce the release of Remote Joins today. Data Federation using Hasura Remote Joins is now available in v1.3.0 stable release.Check out our Cloud docs to learn more!IntroductionSchema stitching is the process of creating a single GraphQL schema from multiple underlying GraphQL APIs.Shout-out to Apollo and their library graphql-tools for their implementation and hopefully we\u2019ll see more examples in other languages soon!Schema stitching can also be used to customise an existing GraphQL API. For example:You want to extend the schema of an existing GraphQL API by adding more fields to an existing type whose data comes from another data sourceYou want to rename the root fields and types of a GraphQL API, for example if you want to change \u201csnake_casing\u201d to \u201cCamelCase\u201dYou want to add custom resolvers or override existing resolvers of a GraphQL service such as Hasura GraphQL Engine.graphql-toolsBefore we get to the tutorials, let us look at some of the important functions and the APIs that graphql-tools provides for schema stitching.The following functions help us create a new schema from existing schemas, typedefs and resolvers.makeExecutableSchema: a function that takes type definitions and resolvers and instantiates a GraphQLSchema class. You can now run GraphQL queries against this object.const schema1 = makeExecutableSchema({\n  typeDefs: `type Query { hello: String }`,\n  resolvers: { Query: () => return [\"hello\": \"world\"] }\n});introspectSchema: an async function that returns a GraphQLSchema instance of a remote GraphQL service. It takes an instance of Apollo-Link with the uri of a GraphQL server, optional headers to be able to introspect the schema.const introspectionResult = await introspectSchema(\n  new HttpLink({ uri: 'https://api.graphql.com/graphql' })\n);makeRemoteExecutableSchema: a function that gives you a the schema of a remote GraphQL server and an instance of Apollo-Link with its uri; returns a remote schema that can be delegated toconst schema2 = await makeRemoteExecutableSchema({\n  schema: introspectionResult,\n  link: new HttpLink({ uri: 'https://api.graphql.com/graphql' })\n});mergeSchemas: a function that takes multiple executable schemas along with custom typedefs & resolvers you can define, to create a newly combined GraphQL schemaconst schema3 = mergeSchemas({\n  schemas: [\n    schema1,\n    schema2,\n    someTypeExtensions\n  ],\n  resolvers: customResolvers\n});Major use cases of mergeSchemas()Each section below is an independent tutorial that you can clone and run. Github links are available at the bottom of each section. We will use Hasura GraphQL Engine API for all our examples.If you are looking for a boilerplate to get started with schema stitching, here is one.Stitching multiple remote GraphQL\u00a0APIsOne of the major advantages of using GraphQL is to have a single endpoint for all the APIs. We lose this advantage if our client wants to query different GraphQL servers. Schema stitching helps us solve this by giving us the ability to expose multiple GraphQL APIs over a single endpoint.Lets consider a real world use case, say, your client is querying two GraphQL APIshttps://bazookaand.herokuapp.com/v1alpha1/graphqlhttps://schema-stitching-blog.herokuapp.com/v1alpha1/graphqlLet us try to expose these two APIs over a single endpoint using schema stitching. The algorithm to stitch multiple GraphQL APIs is as follows:List down your GraphQL APIs and their HTTP endpoint parameters:2. Create executable schemas from the remote GraphQL APIs using the makeRemoteExecutableSchema() function and HttpLink\u00a0:3. Merge all the remote schemas using mergeSchemas():4. Serve this schema:Check out this example on Github.Sometimes, the two remote GraphQL APIs might have similar types or root fields and thus cannot be merged. In such cases, you have to rename fields and types of one or both the GraphQL APIs. Check out Renaming root fields and resolvers section below.New Queries and\u00a0MutationSometimes you want to add some new queries and mutations on top of your existing GraphQL API. Some of the major use cases are:You are using a GraphQL layer your database such as Hasura GraphQL Engine and you want to expose a custom query that the GraphQL layer does not support.You want to expose a make_payment field in your e-commerce app.You want to expose an existing REST API with some custom logic.Consider that you have a user table exposed via the Hasura GraphQL Engine. \u00a0user (\n  id serial primary key,  \n  name text,  \n  age int,  \n  city text,  \n  email text\n)\nGraphQL Engine currently does not support aggregations over tables. Let us expose a field user_age_average that fetches the average age using custom SQL. We will use knex for connecting to Postgres.The newly \u201cstitched\u201d GraphQL API will have:All the queries and mutations from the user schema.An additional root field called user_age_average that gets the average age of usersTo achieve the above, the algorithm is as follows:Instantiate a knex client for postgres:We will be setting this knex client in the server context so that it is passed to all the resolvers. \u00a0const server = new ApolloServer({\n  schema,\n  context: {\n    knex: createNex()\n  }\n});2. Write the type definitions and resolvers for the user_average_age schema:3. Make an executable GraphQL schema out of the above type definitions and resolvers: const averageAgeSchema = makeExecutableSchema({\n  typeDefs,\n  resolvers\n});4. Create a remote executable schema from the the existing user API using the makeRemoteExecutableSchema() function:5. Finally merge the two schemas:6. Serve the newly created schema. Make sure to set the knex client in the context while creating a server instance.Check out this example on Github.Overriding existing queries and mutationsWhile tailoring a new schema from existing GraphQL schemas using schema stitching, we can also override the existing queries and mutations. This is particularly useful when:Adding custom validation to the resolversData clean-up and case transformationAdding custom\u00a0checksLet us consider a gaming room GraphQL API that serves the following schema# root\ntype Query {\n  game: game\n}\ntype game {\n  id: Int,\n  name: String,\n  type: String,\n  multiplayer: Boolean,\n  ip: String\n}We will now add a custom validation to the game query such that this query passes through only after 17:00 (so that kids don\u2019t game during the day\u00a0;-)).Create a remote executable schema from the the existing game API.2. Write a custom resolver to add custom validation to the game query.3. Merge the executable game schema with our custom resolvers:4. Serve the new schema:Check out this example on Github.Data cleanupWe might want to sanitize or modify some data in our GraphQL server before we delegate it to some underlying GraphQL API. This can be achieved with the help of a custom resolver as illustrated below.Let us consider a simple userprofile schema where the insert_user mutation has to be delegated to an underlying schema. Before delegating, the email in the data has to be converted to lowercase.The algorithm is as follows:Create a remote executable schema with the makeRemoteExecutableSchema() function:2. Create a custom resolver for converting the email to lowercase.3. Merge the executable schema with the resolver using the mergeSchemas function:4. Serve the merged schema with a new instance of the apollo-server:Try out this example on this Github.Renaming types and root\u00a0fieldsSometimes you might want to prefix, suffix or camelize the names of the root fields or types of a remote GraphQL API. This could be useful while stitching remote schemas that have conflicting root fields or types.In this example, we will rename the root fields and types of the Github API in our server. The algorithm is as follows:1. Create an executable schema out of the Github API and rename the root fields and types.2. Serve the newly created schemaCheck out this example on Github.Renaming root to Query, Mutation, SubscriptionThe GraphQL spec allows you to have custom names for the root of your GraphQL schema. However, some libraries expect the roots to be Query, Mutation and Subscription\u00a0. If you want to rename the roots of a GraphQL API to Query. Mutation and Subscription, you can simply make an executable schema out of your GraphQL API and pass it through mergeSchemas. mergeSchemas uses Query. Mutation and Subscription as roots.Schema ExtensionSchema extension is the process of extending your existing GraphQL schema with custom fields.Let us consider a user schema (Running at https://bazookaand.herokuapp.com/v1alpha1/graphql). type Query {\n  user: user\n}\ntype user {\n  id: Int,\n  name: String,\n  age: Int,\n  city: String,\n  email: String\n}Now we want to extend the type user such that we can also get the weather of user\u2019s city. This weather will come from the metaweather REST API.The new user schema should be: type Query {\n  user: user\n}\ntype user {\n  id: Int,\n  name: String,\n  age: Int,\n  city: String,\n  city_weather: city_weather\n  email: String\n}\ntype city_weather {\n  temp,\n  max_temp,\n  min_temp\n}The algorithm is as follows:Create a remote executable schema out of the existing user GraphQL API:2. Write custom logic to get the weather of a city3. Extend the type user of the existing GraphQL API with a field city_weather of type city_weather\u00a0.4. Write a resolver for field city_weather that gets the weather of the city:5. Merge the new type extensions and extension resolvers with the existing user schema using mergeSchemas() function:6. Finally serve this new schema:Check out this example on Github.If you have any questions or if you think that anything is handled wrongly, please share in comments or open an issue here.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL and UUID type on Postgres Description: \nHasura GraphQL Engine has implicit support for Postgres UUID type, which means that UUID values can be provided as strings.\n Author: Sandip"}, {"role": "assistant", "content": "Hasura GraphQL Engine has implicit support for Postgres UUID type, which means that UUID values can be provided as\u00a0strings.Every table in a database needs a primary key.A primary key\u2019s purpose is to provide a stable, indexable reference to an entity in a table. Common practice is to use serial ids as primary keys. Serial ids are auto-incrementing integers, which are usually auto-generated by the database.A robust alternative to a serial id is the UUID (Universally Unique Identifier) or the GUID (Globally Unique Identifier), which is a standard 128-bit number that is very hard to duplicate, thereby providing unique identifiers which are perfect for use as primary keys.[We've collected together a number of video shorts, extra details, and information about primary keys, primary key data types, and data types in general including links to mapping from Postgres types to JavaScript types via GraphQL and Hasura.]What is the Postgres UUID Type?The Postgres UUID data type stores UUIDs as defined by RFC 4122, which are 128-bit quantities generated by algorithms that minimize the probability of having duplicate identifiers.A UUID comprises of 32 hexadecimal digits, represented in groups of 8,4,4,4 and 12.Eg: \u201c12345678-abcd-ef12\u2013345a-12345678abcd\u201dWhy UUID?Exposing information: Very frequently, the primary id is exposed to the user (REST urls, anyone?). If the primary key is easily visible to the user, this means that anyone can find out your number of users by just looking at the primary key on a freshly created user. This leaks information about the size of your userbase.Enumeration of entries: Another problem is that it\u2019s very easy to enumerate all the entries of your table with a primary key. A malicious user or a bot could easily scrape identities of all your users, or spam your API with organised actions on all your users. A UUID is universally random, and is practically impossible for external parties to predict. To be very clear though, UUIDs are not a security mechanism, the actual solution to this problem is to put up stronger access control. UUIDs just provide more protection in this case.Independent Generation: The biggest problem with sequential ids is that the client doesn\u2019t know the id of an object being inserted without talking to the database. This is even more of a problem when inserting multiple related objects with foreign keys, and creates inelegant multi step inserts. A UUID on the other hand, can be generated directly by the client making inserts very simple, and solves the foreign key problem quite nicely too.Random distribution: The random distribution of the UUIDs can prevent disk hotspots, where a large number of objects are stored in a particular disk block, causing high usage on specific areas of the disk. This works very well for databases distributed over several nodes.Why not UUID?Size: The downside is that UUIDs are four times as big as a serial, and if you have size constraints, they can be a problem.Hard to debug: Harder to remember, hard to debug, a small but important QoL issue. It\u2019s easier to test/debug when you have simpler ids that you can remember compared to long complicated UUIDs.Lack of easy ordering: The primary key cannot be used to determine the order of insertion, another column with timestamps or a sequence becomes necessary. This also means that clustered indexes on UUID columns are very costly in terms of performance.Performance: And also, random UUIDs mean scattered indexes, and this can cause performance issues/high disk usage with high insert rates. (Databases have sequential UUID generation functions to deal with this, but this eliminates the advantage of being able to generate the UUID on the client)UUIDs in the Hasura GraphQL Engine:Hasura GraphQL Engine (HGE) is an open source tool that gives instant GraphQL APIs over Postgres.HGE supports UUID types as implicits, which means that UUID values can be provided as strings.Let\u2019s set up a simple schema with a UUID column to run some queries on:Here\u2019s a sample GraphQL query that fetches a specific object from a table with a UUID primary key:Inserting an object with a UUID generated using a client side library is quite simple too:You can try out all the above examples on a live HGE instance here!References:Hasura GraphQL Engine DocsPostgres Docs"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Postgres Date/Time types on GraphQL Description: \nDate/Time types are one of the basic types every database supports, and Postgres has robust support for a large set of date and time types\u2026\n Author: Hasura"}, {"role": "assistant", "content": "Date/Time types are one of the basic types every database supports, and Postgres has robust support for a large set of date and time types, as shown below:This allows for a fair amount of flexibility in how time/date can be stored in Postgres. This blog post lists some examples on using these date/time types in GraphQL using the Hasura GraphQL Engine (HGE), an open source tool that gives instant GraphQL APIs over Postgres.Date/Time InputsPostgres is very flexible in handling date/time inputs, and accepts several possible input methods.For the following examples, we focus on the ISO-8601 representation of time, and the Postgres timestamptz type\u200a\u2014\u200aEg. \u201c2018\u201308\u201310T14:14:57+05:30\u201dDate/Time inputs in HGEThe Hasura Graphql Engine supports the following date-time types:These consist of implicit timestamp types that can be used as strings in queries, or properly validated custom Graphql scalars Timetz and Timestamptz, which allow only properly formatted ISO-8601 values.HGE also supports creating columns that can have default timestamp values inserted at creation.While creating your schema, you can create a Timestamp column, and use a function to set it on object insertion:Creating a Timestamptz column with a default value in HGETimestamptz vs TimestampThe timestamptz field in Postgres is basically just the timestamp field where Postgres actually just stores the \u201cnormalised\u201d UTC time, even if the timestamp given in the input string has a timezone.If your input string is: 2018-08-28T12:30:00+05:30\u00a0, when this timestamp is stored in the database, it will be stored as 2018-08-28T07:00:00.The advantage of this over the simple timestamp field is that your input to the database will be timezone independent, and will not be inaccurate when apps from different timezones insert timestamps, or when you move your database server location to a different timezone.To quote from the docs:For timestamp with time zone, the internally stored value is always in UTC (Universal Coordinated Time, traditionally known as Greenwich Mean Time, GMT). An input value that has an explicit time zone specified is converted to UTC using the appropriate offset for that time zone. If no time zone is stated in the input string, then it is assumed to be in the time zone indicated by the system\u2019s TimeZone parameter, and is converted to UTC using the offset for the timezone zone.To give a simple analogy, a timestamptz value represents an instant in time, the same instant for anyone viewing it. But a timestamp value just represents a particular orientation of a clock, which will represent different instances of time based on your timezone.For pretty much any use case, timestamptz is almost always a better choice. This choice is made easier with the fact that both timestamptz and timestamp take up the same 8 bytes of data.Querying Date TimeFor the next couple of sections, we\u2019ll be using HGE, with the following schema:In this table, the id is an auto-incrementing integer (Though you might prefer using UUIDs, here\u2019s our thoughts on why), and the created column is a timestamptz field that gets filled with the timestamp at the instant of insertion. The edited field is another timestamptz field, but can be inserted into.Querying date time types is as simple as representing them as ISO-8601 formatted strings.Inserting into Date/Time fieldsInserting into a Timestamptz field in HGE is as simple as a normal insert, with the timestamp as a string:You can also insert just the date:Or the timestamp without the timezone:You can try out all the above examples on a live HGE instance here.References:Hasura GraphQL Engine DocsPostgres Docs"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Instant GraphQL on AWS RDS Description: \nLet\u2019s deploy Hasura GraphQL Engine on Amazon RDS , to get instant, blazing-fast GraphQL APIs on a RDS Postgres database.\n Author: Anon Ray"}, {"role": "assistant", "content": "Let\u2019s deploy Hasura GraphQL Engine on Amazon RDS\u00a0, to get instant, blazing-fast GraphQL APIs on a RDS Postgres database.TL;DRCreate an RDS instance or use an existing oneDeploy Hasura GraphQL on Heroku or EC2+dockerInstant GraphQL APIs on your RDSCreate an RDS\u00a0instanceIf you already have an RDS instance, skip this section and go to the next section.Log into your AWS Console, click the EC2 link to go to the RDS Console, and click the blue \u201cCreate Database\u201d button:Make sure to choose the \u201cPostgreSQL\u201d engine when creating an instance:Note: As of writing this article Postgres 10.3 and below work. Postgres 10.4 does not work.Click \u201cNext\u201d and choose \u201cProduction\u201d or \u201cDev/Test\u201d based on your use-case.Choose a suitable instance for your RDS database:Scroll down and setup the credentials. Keep note of the credentials (master username and password, database name etc.), which will be required later.Choose a VPC and security group:If you are going to deploy GraphQL engine on AWS, then make sure the VPC used here is same as the VPC used with your EC2/ECS instances.Note: If you are planning to deploy Hasura outside AWS (e.g. Heroku), then you have to make this DB instance publicly accessible.Note: Make sure, the security group you choose has appropriate rules for inbound connections from wherever you deploy the GraphQL engine.Finally, click on \u201cCreate Database\u201d button.Use an existing RDS\u00a0instanceIf you already have an RDS instance with existing data, you can deploy Hasura GraphQL Engine following the below steps.Note: If you are planning to deploy Hasura outside AWS, then you have to make this DB instance publicly accessible.Note: Make sure, the security group associated with your RDS instance has appropriate rules for inbound connections from the GraphQL engine.You also need to setup appropriate permissions for the Postgres user. See our docs for details about Postgres user permissions.Deploy Hasura GraphQL on\u00a0Hasura CloudFollow the steps outlined in our docs to deploy Hasura GraphQL on Hasura Cloud.You would have to configure the HASURA_GRAPHQL_DATABASE_URL environment variable to point to your RDS instance. That\u2019s all!Deploy Hasura GraphQL on\u00a0AWSTo deploy Docker containers on AWS, there are two possible ways:Use EC2 instances and install docker on themUse AWS ECSIf you are aiming for a production setup you should use ECS.We will be using Docker on EC2 to deploy in this post.Launch an EC2\u00a0instanceLog into your AWS Console, click the EC2 link to go to the EC2 Console, and click the blue \u201cLaunch Instance\u201d button:On the next page, you need to pick an Amazon Machine Image (AMI) to run on your EC2 Instance. For this tutorial, just pick the top option, which is the Ubuntu Server 16.04 LTS\u00a0:Next, you need to pick the Instance Type, which determines what kind of CPU, memory, storage, and network capacity your server will have. Stick with the default option, t2.micro, and click the gray \u201cNext: Configure Instance Details\u201d button:In the Instance Details configuration, make sure to choose the same VPC that is used by your RDS instance.Then keep clicking \u201cNext\u201d until you get to the \u201cConfigure Security Group\u201d page. A Security Group is a set of firewall rules that control network traffic for your instance. By default, all incoming ports are blocked, so use this page to add rules that allow incoming SSH (TCP, port 22), HTTP (TCP, port 80) and HTTPS (TCP, port 443) requests from any source (0.0.0.0/0). Give the Security Group a name such as graphql-engine-test-sg, and click the blue \u201cReview and Launch\u201d button:On the \u201cReview Instance Launch\u201d page, click the blue \u201cLaunch\u201d button. This will pop up a modal that asks you to pick a Key Pair. This used to connect to your EC2 instance over SSH. Select \u201ccreate a new key pair\u201d from the drop-down, give the Key Pair a name like my-ec2-key-pair, and click \u201cDownload Key Pair\u201d:NOTE: Save the Key Pair\u00a0.pem file to a safe and accessible location on your computer (once you close this modal, you will never be able to download this\u00a0.pem file again, so make sure to save it!).Now if you go to your EC2 dashboard, you should see your EC2 instances running.Installing DockerThe next step is to install Docker on your EC2 Instance. Open a terminal, cd over to the folder where you saved your Key Pair, and run the following commands:$ cd ~/my-aws-key-pairs\n$ chmod 400 my-ec2-key-pair.pem\n$ ssh -i my-ec2-key-pair.pem ubuntu@<EC2-INSTANCE-PUBLIC-IP-ADDRESS>Once you are logged in, install Docker:$ sudo apt-get update\n$ sudo apt-get -y install docker docker-composeThen let\u2019s add this user to the docker group, so we can run docker commands without sudo:$ sudo usermod -a -G docker ubuntuExit the shell and login again. Check if docker works:$ exit\n$ ssh -i my-ec2-key-pair.pem ubuntu@<EC2-INSTANCE-PUBLIC-IP-ADDRESS>\n$ docker infoDeploying Hasura GraphQL\u00a0EngineSave the following docker-compose snippet into file docker-compose.yamlChange the HASURA_GRAPHQL_DATABASE_URL to point to your RDS instance, and HASURA_GRAPHQL_ACCESS_KEY to a long random string.Then run:$ docker-compose up -dCheck if the container is running:$ docker ps\n\nCONTAINER ID IMAGE                 ... CREATED STATUS PORTS          ...\n097f58433a2b hasura/graphql-engine ... 1m ago  Up 1m  8080->8080/tcp ...The status should be \"Up\". If the status is not up, check logs with docker logs <container-id>Access GraphQL engine\u00a0consoleNow go to the public IP address of the EC2 instance on your browser and it should open up the Hasura console.It should prompt for the access key, enter the access key you used in the above steps. Access the console and make GraphQL queries!\n\nTroubleshootingIf you can't access the console, check the GraphQL Engine docker logs. By running docker logs <container-id>. Postgres connection failed / Connection refusedIf the public IP is not accessible, check the docker logs. If the logs say postgres connection failed or something along those lines, it is most likely a security group issue. You would have to edit the security-group associated with the RDS instance.Go to the security-group of the RDS instance, and then to the \"Inbound\" rules. Click on \"Edit\", and then click on \"Add Rule\". Select \"PostgreSQL\" from the type dropdown and add the Private IP of your EC2 instance. You have to add it in a CIDR format, so it should be x.x.x.x/32. Then click \"Save\". Now check the logs again if graphql-engine sucessfully connects to the RDS instance."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL schema on Postgres with foreign keys and without foreign keys Description: \nHasura GraphQL Engine allows creating relationships over Postgres tables with or without foreign keys\n Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "While modeling a graph using a relational database system, tables can be considered as nodes and the links between nodes are often derived as foreign key constraints over the columns of these tables. The Hasura GraphQL Engine lets you query data by creating relationships using single-column foreign keys, multi-column foreign keys or even without foreign keys on a Postgres database.Single-column foreign\u00a0keyLet\u2019s look at an example. The schema contains two tables, user and address.user_id column in address table is a foreign key to id column of user table. Once we have a schema like this, Hasura GraphQL Engine console suggests relationships over these columns.Relationship prompts from the Hasura GraphQL Engine\u00a0consoleWhen we track these relations, the derived GraphQL schema contains the relation names and both tables can be queried in a single query:Querying over array relationshipsHere, addresssByuserId is an \u201carray relationship\u201d suggested by console for the foreign key we created. It\u2019s an array because a user can have many addresses. A \u201creverse\u201d \u201cobject relationship\u201d called userByuserId will be added to address table. It\u2019s an object since there will be only one user for an address.The object relationship to user from address\u00a0tableRelationships can have custom names if they are added manually instead of relying on console to auto-generate them. Checkout the docs for more options in customizing and creating relationships.Relations without foreign\u00a0keysIn some cases you might want to be flexible in defining relations between tables (nodes), without constraining it with a foreign key first. Hasura GraphQL Engine allows creating relationships manually, without a constraint.Let\u2019s look at a book-author schema. We have a table with lot of books, but we don\u2019t have author details for all authors. If we add a foreign key for author column in book table to the name column in author table, we\u2018ll be required to create an entry in author table for all authors for whom we have books for.Instead, if we do not add a foreign key, we are free to add books for which authors are not in the author table. Using Hasura, we can define a relationship over required columns manually, thereby enabling GraphQL queries over those extra info.Adding a relationship manually for author_detailsOnce we add the relationship, we can make a single query to get all books and the corresponding author details, if it\u2019s present.Fetching author details without a foreign\u00a0keyIn the above example, we can see that author_details are present for William Shakespeare and Anne Frank, but not for Dan Brown, since there is no entry for Dan Brown in the author table.On the other hand, by defining an \u201carray relationship\u201d from author table to book table, we can query all books for an author. Here is how we can create an array relationship using console:Adding an array relationship called\u00a0booksOnce we crate this relationship called books on the author table, we can write a GraphQL query that can fetch all books for authors. Here is an example query for getting all books by William Shakespeare:Getting all books for an author using an array relationshipBook-author data taken from Wikipedia"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using Custom SQL functions for queries with Postgres and GraphQL Description: Access custom PostgreSQL functions in GraphQL API queries and subscriptions Author: Rikin Kachhia"}, {"role": "assistant", "content": "Directly query custom PostgreSQL functions in the GraphQL APITL;DR You can now run queries (or subscriptions) such as the following:search_articles is a custom SQL function that takes a text input to search an article table (detailed example below)Custom SQL functionsCustom SQL functions are procedures that you can define which are stored by in database and can be invoked to run on the database and return the final result.This can be used to run some common business logic directly on the database itself which might otherwise:have to be replicated on all of the database\u2019s clientsrequire multiple queries to the database to computeFeature overviewCurrently, only the following types of Postgres functions can be queried in the GraphQL API (terminology from Postgres docs):Function behavior: ONLY STABLE or IMMUTABLEReturn type: MUST be SETOF <table-name>Argument modes: ONLY INNote: You can also support functions that need to return a custom type i.e. row set by creating and tracking an empty table with the required schema to support the function- see the 2nd example below.Access control for custom functionsAccess control permissions configured for the SETOF table of a function are also applicable to the function itself. How to query functions in GraphQL APILet\u2019s see you can use this feature through two examples:Example: A custom text-search functionIn the usual context of an article/author schema, let\u2019s say we\u2019ve created and tracked a custom function, search_articles with the following definition:(This is just an example function, you could have used the ilike argument to filter results from the article table too ? )This function filters rows from the article table based on the input text argument, search i.e. it returns SETOF article. Once you\u2019ve created and tracked this function (using the Data -> SQL section of the Hasura console), you can call the custom function in the GraphQL API (using the example query in the TL;DR section):query custom function, search_articles, directly in GraphQL APIExample: A PostGIS-based functionThere may be instances when your custom function returns a result-set or rows of a table that is not part of your existing schema. In this scenario, you are required to create and track this table to add support for your function in the GraphQL API. Let\u2019s see how we can do that using a PostGIS based example.Say you have 2 tables, for user and landmark location data, with the following definitions:You can populate these tables with test data using the following mutations and variables:In this example, we want to fetch a list of landmarks that are near a given user, along with the user\u2019s details in the same query. PostGIS\u2019 built-in function, ST_Distance can be used to implement this use case.Since our use-case requires an output that isn\u2019t a \u201csubset\u201d of any of the existing tables i.e. the SETOF table doesn\u2019t exist, let\u2019s first create this table and then create our location search function using the following definitions (once again, we\u2019ll use the Data -> SQL section of the Hasura console):This function fetches user information (for the given input userid) and a list of landmarks which are less than distance_kms kilometers away from the user\u2019s location as a JSON field. We can now refer to this function in our GraphQL API and get the corresponding response as shown below:query:example response:Please note that you can also:Use aggregations on custom functions using the using the <function-name>_aggregate field.Use arguments like where, limit, order_by, offset, etc. with function-based queries.Try it outDo take this feature for a spin and let us know what you think! Support for more functions types and for functions in mutations is on its way. Give us a shoutout on on the community Discord or Github if you want to be notified of updates on the iterations to follow!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL with Postgres views and materialized views Description: \nPostgres views are awesome. You can link them to regular tables using relationships and then make a single nested query to fetch related\u2026\n Author: Sandip"}, {"role": "assistant", "content": "Postgres views are awesome. You can link them to regular tables using relationships and then make a single nested query to fetch related\u00a0data.TL;DRHere is a summary of what is covered in this postIntroduction to views\u200a\u2014\u200aViews are basically virtual tables. There are a lot of advantages to using them.What makes working with views on Hasura GraphQL engine special?\u200a\u2014\u200aThe Hasura GraphQL engine is an open source tool that lets you setup your own GraphQL server over Postgres in minutes. Using the graphql-engine, you get the ability to add views as relationships to tables or other views. This makes it extremely simple to fetch all the required data with a single query.Example use-case\u200a\u2014\u200aWe put everything we discussed in the earlier sections to build a backend for a blog engine where authors can post articles and other authors can upvote this article. We try to model the database in a way that lets us get all of the data with a single query.What are views in\u00a0SQL?A view is a named query.Let\u2019s say that you have a complex query that you do not want to repeat everywhere, you can create a view over this query. Creating a view gives the query a name and now you can SELECT from this view as you would from an ordinary table.You can use views to represent joined tables or a subset of a table, selecting only the required columns and rows from a table.Views can be used in almost any place a real table can be used and are very common in SQL database designs.Why are views\u00a0useful?Helps encapsulate the details of the structure of your tables behind a consistent interface. It also lets you structure your data in an intuitive way.Provides more readability since a named query can set the context much faster than a raw SQL query. You can use a view instead of littering your client code base with complex queries.Views have separate permissions and hence can be used to restrict access to the tables such that users are only allowed to see specific rows and columns.Avoid making multiple queries and performing complex calculations on the client by specifying the logic in the DB. Querying a view is also much faster than making multiple queries on your client and then crunching the data.Since views are not REAL tables, you can only perform SELECT queries on them.Now that we know what views are and why they\u2019re awesome. Let\u2019s take a look at how Hasura makes working with them even better!Working with views on the Hasura GraphQL\u00a0EngineThe Hasura GraphQL engine can be used with any Postgres. You just have to provide a Postgres connection and you instantly get:Instant GraphQL APIs to store and retrieve data from tables and views.Ability to add a relationship between a view and a table. You can then query the table (or the view) and get the respective data from both the table as well as the view.Example use-caseLet\u2019s build a backend for a blog engine to see everything mentioned above in action.Feature setWe will have a list of authors and each author will have articles that they can publish.Every article can be \u201cupvoted\u201d by other authors.We want the ability to fetch the list of articles by an author along with the total upvotes received by each article, in one query.Execution overviewAn author table to store information about the authors.An article table which stores the content of each article.The upvotes will be tracked in the upvote table.A article_upvotes_count view which sums up all of the \u201cupvotes\u201d an article has received. Add an object relationship from the article table to the view.Fetch the articles (along with the upvotes) for an author using the GraphQL APIs provided by Hasura.Now that we know what needs to be done, let\u2019s get started.Getting GraphQL APIs over\u00a0PostgresWe will use the Hasura GraphQL engine for instantly getting GraphQL APIs over Postgres. Click on the button below to deploy the GraphQL engine to Heroku\u2019s free tier.Click this button to deploy the GraphQL engine to\u00a0HerokuThis will deploy the graphql-engine to Heroku. You might need to create a Heroku account if you don\u2019t have one. The graphql-engine will be running at https://your-app.herokuapp.com (replace your-app with your heroku app name).ConsoleThe Graphql engine comes with an admin UI called the Console\u00a0. You can use the Console to build the backend for your application.It is running at the /console endpoint of your graphql-engine URL, which is, in this case, https://your-app.herokuapp.com/console. The landing page of the console looks something like this:Creating tablesHead to the Data tab and click on Create Table to create a new table.Let\u2019s start with the author tableuser_id Integer Primary Keyusername TextThe article tableid Integer (auto increment) Primary Keyauthor_id Integertitle Textcontent Textcreated_at Timestamp Default now()upvote tableuser_id Integerarticle_id IntegerCompound Primary Key: user_id and article_idCreating a\u00a0viewNow that we have our tables created, let\u2019s create our view which shows the total upvotes for each article.The SQL statement to create this view will beCREATE VIEW article_upvotes_count AS \nSELECT article_id, COUNT(user_id) as total_upvotes\nFROM upvote\nGROUP BY article_id;To run this SQL statement, head to the Data tab and click on SQL from the panel on the left.Fetching data from\u00a0viewFetching data from a view or a table is the same, in this case to fetch a list of articles and the total upvotes for each article: \u00a0query {\n  article_upvotes_count {\n    article_id\n    total_upvotes\n  }\n}\nFetching total upvotes for all articlesAlternatively, you can also add this view as a relationship to the article table and fetch the article details as well the total upvotes in one query.Creating a relationship between a table and a\u00a0viewHead to the Data tab and click on article. Click on the Relationship tab and hit the Add a manual relationship button.In the form that comes up, select the following:Now, you can query the article table for all articles by an author along with the total_upvotes for each article. \u00a0query fetch_articles_by_author {\n  article (where: { author_id: 1 } order_by: [\"-upvotes.total_upvotes\"]) {      \n    id\n    author_id\n    title\n    content\n    created_at\n    upvote { \n      total_upvotes\n    }  \n  }\n}Note: The order_by condition is used to list the articles ordered by the number of upvotes it has received. - sign is to list them in descending order and + will be for ascending.And with this we have built a simple backend to a blog engine.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Atomic set and increment operators for GraphQL mutations Description: \nUse atomic set and increment operators in GraphQL mutations generated by Hasura GraphQL to update your data\n Author: Anon Ray"}, {"role": "assistant", "content": "When using GraphQL, writes or updates to data are made via mutations. In this post we will discuss the update mutations and related operators that are available on the Hasura GraphQL Engine.TL;DR:Hasura allows you to update multiple objects in one shot using a powerful where argument which allows you to create a boolean filter to select the objects to be updatedHasura supports a set operator to set or replace an existing valueHasura supports an increment operator to atomically increment numerical values by a given amount to prevent common race conditions where multiple clients are incrementing the same attributeHasura supports native JSON/JSONB operators that you can use to modify attributes in a JSON/JSONB value of a postgres column without needing to set the full JSON valueSchema and generated mutationsLet\u2019s consider the following schema for our example:When the above tables are tracked in Hasura GraphQL Engine, it will automatically generate the following update mutations:As you can see above, there are 3 key arguments in the update mutation, _set\u00a0, _inc and where.where is a boolean expression, which is mandatory. where will help you select the rows on which the mutation is to be applied. Otherwise the update mutation will have to be applied for all rows in the table or only on one particular row, either of which are not particularly flexible options. You can read more about \u201cbulk update\u201d mutations.Using the \"_set\"\u00a0operatorThe _set operator \u201csets\u201d the value of one or more columns of a row by replacing the existing value. Consider 2 illustrative examples below:Let\u2019s say we want to update the title and content of an article that the user is editing. We can run the following GraphQL query using the _set operator (assuming we know the id of the article)\u00a0:If we want to update all articles of an author, i.e. update data using nested objects\u2019 fields, our query would be like the following:Using the \"_inc\"\u00a0operatorThe _inc operator atomically increments the existing value of one or more columns by a given amount. This operator is only for columns that have a \u201cnumeric\u201d type, like integer, numeric, float, double etc.Let\u2019s say we want to update the likes of an article. As it is a numeric field, we can take advantage of the _inc operator. We can run the following GraphQL query (assuming we know the id of the article)\u00a0:Why do we need\u00a0\u201c_inc\u201d?The advantage of using _inc is we can avoid race conditions.If we were using _set directly\u200a\u2014\u200afetch previous likes, increment them on the client, and then run a set query with the new value\u200a\u2014\u200awhich can lead to race conditions when two clients simultaneously fetched the same likes and tried to update. One of our likes will be lost. As the _inc query is guaranteed to run atomically, we know that using that we will not run into race conditions.Using mutations to update JSON/JSONB dataHasura GraphQL Engine has first-class support for updating JSON/JSONB data in your Postgres tables. You don\u2019t have to form the JSON in your client and use _set\u00a0. You can directly use any of _prepend, _append, _delete_key, _delete_elem, and _delete_at_path operators to update nested keys and values in your JSON data.You can read more about these operators in this blog\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Bulk insert mutations with GraphQL Description: \nGraphQL insert mutations are used to create new objects in your back-end (typically a database) which can then be queried.\n Author: Rikin Kachhia"}, {"role": "assistant", "content": "GraphQL insert mutations are used to create new objects in your back-end (typically a database) which can then be\u00a0queried.In any typical application, you will often need to insert multiple objects into the database in one go. These could be objects of the same type or objects of different types.Some things to keep in mind in such a scenario would be:How many network requests are needed to achieve the inserts?How many and how efficient are the insert queries that will actually be run on the database?Is each insert independent of others? i.e. is the order of execution important.If some of the inserts fail, what happens to the rest of the inserts? Should the rest of the inserts go through or should all the inserts be rolled back?The Hasura GraphQL\u00a0EngineThe Hasura GraphQL Engine provides instant GraphQL APIs over any Postgres database.With the above points considered, the Hasura GraphQL Engine provides the following features for bulk insert mutations which guarantee a high performance and that the database is always in a consistent state:multiple objects of the same type can be inserted using a single insert mutation field within a mutation. This will be executed as a single SQL statement in the database.multiple objects of different types can be inserted using multiple insert mutation fields within the same mutation.if there are multiple mutation fields within a mutation, they will be executed sequentially on the database.if any of the mutation fields within a mutation fail, all the mutation fields in that mutation will be rolled back. i.e. all the mutation fields within a mutation are run as a transaction.Setup a GraphQL ProjectTry this out by setting up a GraphQL backend using Hasura Cloud. You will get a Hasura GraphQL Engine instantly that you can connect to a Postgres. Get started here.Once your GraphQL Project is up and running, let us assume we have the following tables in our database:The following are the insert mutation fields generated by Hasura GraphQL engine for the above tables:As you can see, the insert mutation fields expect an array of objects to be passed as arguments which will be inserted into the database.Now let\u2019s see a few examples of bulk insert mutations using the Hasura GraphQL engine:Insert multiple objects of same\u00a0typeExample: Insert authors in bulk using a single insert_author mutation field in a mutation:Notes:If insertion of any row fails (for example due to a duplicate primary key), none of the rows will get inserted.Insert multiple objects of different typesExample: Insert author and articles written by that author using the insert_author and insert_article mutation fields in the same mutation:Notes:In this example, the articles can be inserted only after the author is inserted as it would otherwise violate the author_id foreign key constraint. Hence the sequential execution of inserts is necessary here.If the inserting of author fails, the execution of the entire mutation halts and inserting of articles is not even attempted.If the inserting of articles fails, the insertion of the author is rolled back and the database state is restored to what it was before the mutation was attempted."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Postgres JSON and JSONB type support on GraphQL Description: Hasura GraphQL Engine supports Postgres schema-less (NoSQL) column types JSON and JSONB, enabling Queries, Mutations & Subscriptions over\u2026\n Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "TL;DR;hstore, JSON and JSONB types enable storing schema-less (NoSQL) on PostgresHasura GraphQL Engine supports JSON/JSONB columnsJSONB operators are supportedPostgres views created using derived data from JSONB columns can be queried through GraphQL, with filter operators like whereTable of\u00a0contentsSchemaQueriesJSONB Boolean OperatorsInsert MutationsUpdate Mutations_append [||]_prepend [||]_delete_key [-]_delete_elem [-]_delete_at_path [#-]Derived data using viewsConclusionYou can explore this Heroku App to play around with the data/queries referred to in this blog post.Postgres supports storing schema-less (NoSQL) data as JSON columns through dedicated hstore, JSON or JSONB column types. Even though type safety is a major push for moving to GraphQL, there are real-world use-cases where you\u2019d still need some columns which can store data for which schema is not decided already.For example,An analytics system that logs events from various sources:\u00a0The payload from each source could be different and multiple clients might be writing queries to analyze this data.An e-commerce website\u2019s database where each product has a strict set of basic parameters, like name, category, price, but a varying range of specifications (or specs). It is nearly impossible to create columns (schema) to capture all possible spec-fields for all kinds of products out there.\nSchemaLet\u2019s take a look at how we can use JSONB to model this product table and make some queries through Hasura GraphQL Engine.Here is the schema for product table on Postgres:The spec column can either be JSON or JSONB\u00a0. Both will make sure only a valid JSON can be inserted here, but with JSONB columns, we can query based on the JSON keys. For more insight into hstore, JSON and JSONB columns, and notes on when to use them, checkout this blog post from Citus Data.QueriesOnce we deploy Hasura GraphQL Engine on this database, a GraphQL query and it\u2019s response looks like this with some sample data:Example query after adding some sample\u00a0dataThe JSONB column spec is resolved as the underlying JSON object. This gives clients flexibility to render these fields, without knowing the schema at all.\nQuery with JSONB Boolean OperatorsSince v1.0.0-alpha17, we have added support for JSONB Boolean operators. You no longer need to query data inside these JSON documents using Postgres views. These operators are available in GraphQL queries directly.For example, let\u2019s say you want to query all laptops with 8GB RAM. This is what you can do to to make that query in GraphQL:Query all laptops with 8GB\u00a0RAMThe _contains operator is applied over the spec variable to filter out all rows which contains that particular key-value pair at the top-level in the JSONB column. For more complex operations not supported by these operators, you can still use a view.For more details about these operators, check Postgres docs.MutationsInsertInserting arbitrary JSON into the table is also straightforward using Mutations. Here is an example:Example for an insert mutation containing JSONSince GraphQL input arguments cannot be extended beyond String types, the JSON object can only be sent as a variable. GraphiQL might show some errors since it does not know how to handle the jsonb type. Nevertheless the queries works fine on the GraphQL Engine.UpdateHasura also supports Postgres JSONB operators. Available JSONB operators are _append (||), _prepend (||), _delete_key (-), _delete_elem (-) and _delete_at_path (#-).You can read more about JSONB operators here._append and _prependLet\u2019s say there is an admin interface for our e-commerce application, where product owners can add or remove new specs.Here\u2019s how a new key can be added to spec:Adding a new field to the spec using\u00a0_append_prepend can be used instead of _append to add the new key at the beginning the object.The variables can be nested JSON objects too:Here\u2019s how arrays can be appended:_delete_keyRemoving an existing top-level key using _delete_key:Deleting the key `driver_units` from\u00a0spec_delete_elem_delete_elem can be used to delete elements from arrays (only if the top object is an array in the JSONB column) by their index. Read more at docs._delete_at_pathWe can delete a key specified by a JSON path using this operation. Here\u2019s how we can delete spec.frequency_response.min\u00a0:Delete a nested key using `_delete_at_path`Derived data using Postgres\u00a0viewsThere could be use cases where the data stored in JSONB columns need to be queried using some known keys. We can create views for these purposes.For example, you need to show a separate listing for laptops for which you know that processor, ram and disk will always be present in the spec.We create a view called laptop_listing extracting certain field from the JSONB columns:When this view is tracked by Hasura GraphQL Engine, it can be queried using GraphQL, making use of all powerful filter operations:Simple query on the view with a limit\u00a0argumentLet\u2019s say we want show laptops with 8GB RAM only. A where clause can be used to express the particular condition:Filtering results based on filed values that are derived form JSONB\u00a0columnConclusionJSONB columns are useful for storing arbitrary data on Postgres. Using Hasura GraphQL Engine, you can access these columns via GraphQL, including JSONB operator support through mutations. Further inspection on the contents of the column can be carried out by creating views and these views can be queried though GraphQL as well.*Product data taken from dell.com and flipkart.com"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL for building internal applications quickly Description: \nOriginally published in App Developer Magazine on 8th August 2018.\n Author: Hasura"}, {"role": "assistant", "content": "Originally published in App Developer Magazine on 8th August 2018.The transition from I.T\u2019s role in organizations, from a mere horizontal function to one of a strategic advantage, is playing out the world over. However, most change management techniques behind these efforts tend to disproportionately focus on end-user facing software\u200a\u2014\u200aoften at the cost of intra-organization or internal applications (apps).Internal apps propel data-driven decision-making or support an end-user facing service. They can also help you become a fast-moving, agile organization that today\u2019s consumer expects and competition demands. It is critical for businesses to focus equally on the quality and feature velocity of internal apps.In this post, let\u2019s see how GraphQL can help a business do just that.Challenges with internal apps in organizationsThe development process for internal apps generally faces the following problems:Budget & Staffing: Compared to user-facing apps, internal apps are often under-budgeted and understaffed. This shortcoming places a high premium on the productivity of the app dev team.Compatibility with existing technology: Intra-organization apps are usually not greenfield projects, so they need to interact with data in existing systems. This requires different teams to coordinate with each other for every release. Having this kind of complexity and multiple stakeholders in the mix typically does not bode well for feature velocity.The brunt of bad \u201cbuild vs. buy\u201d decisions is more acutely felt in a constrained environment. Therefore, a digression into targeting the right set of apps to build internally versus purchasing off the shelf (OTS) products is warranted.Apps can be classified based on the complexity of analysis or business logic applied to data and the nature of their interaction with the underlying databases. The following is a useful guide to making build versus buy decisions based on this classification:Now that we know what apps to build, let\u2019s see how they can be built even better. But first, a brief introduction to GraphQL.What is\u00a0GraphQL?GraphQL is a specification for a query language for APIs and a server-side runtime for executing queries. The open source spec is centered around a typed schema that is available to users of the API (mostly front-end developers) to make any CRUD queries on the exposed fields. It\u2019s also agnostic to the underlying databases or any other source of data.The key features of GraphQL that make it attractive for app development are:Fewer errors: The typed schema leads to fewer errors, as commonly seen errors can be caught during the development stage itself.Flexibility: Once supported, a schema can serve any permutations of queries against it. To use a simple example of a blog app, a schema that covers the tables for authors and articles, will serve both of these queries at no additional cost:Fetch a list of authors and each author\u2019s articlesFetch a list of articles and each article\u2019s authorUniformity: Regardless of the source of data, GraphQL provides a uniform query interface for standardized app development workflows.Extensibility: GraphQL can also be used to stitch together multiple sources of data to such a degree that it can interact with these disparate sources in the same API call.Thanks to these reasons, GraphQL adoption has been growing by leaps and bounds. It is now being touted as an alternative to traditional REST APIs, because of its impact on API Lifecycle Management.How GraphQL can help transform internal app development?The impact of GraphQL on app development is rather apparent on API Lifecycle Management. Here\u2019s how it affects the different stages of an API development cycle:Fewer APIs: GraphQL\u2019s schema-driven approach leads to fewer APIs having to be developed. As shown in the above blog app example, there is no incremental cost to a new API that interacts with the same schema. This flexibility also works where new APIs are needed purely to transform the format of the output on the server side; the GraphQL spec grants frontend apps the ability to dictate the format of the output and query for any slice of data they want.Impact on client apps: The general front-end developer experience for consuming GraphQL APIs is fast becoming the gold standard that other dev tools are expected to emulate.Client-side code can be auto-generated from a GraphQL schema.Use-cases like real-time data, etc., from live queries are very easy to handle.Front-end developers spend much less time integrating with GraphQL APIs as compared to the same effort for REST APIs.Automatic API documentation: With a published schema, GraphQL has API discovery or documentation implicitly built into the server, with tools that take advantage of this.Testing & Deployment: GraphQL APIs need to be tested only when there\u2019s a change in the schema or a fresh schema (the client still needs to be tested). This also holds true for deployments. Compared to REST APIs, where every iteration is a candidate for testing and deployment, the cost of Q.A and devOps is significantly lower.GraphQL also helps reduce run-time errors and related testing due to its typed schema. Using client-side tooling/SDKs to exploit this benefit and provide compile-time, client-side query validations is the norm in the GraphQL ecosystem.This is where my image \u201cQuery validation in an IDE preventing errors before reaching formal testing\u201d should go with the caption below itStandardized code and workflows: Working with a specification like GraphQL, enforces a certain amount of standardization in codebases and processes. Considering the attrition rates in IT, this is a solid benefit of GraphQL.Compatibility with existing technologyMaking apps work with the existing technology stack is an important use-case. In addition to leveraging existing investments, data from these different sources, especially legacy apps/DBs, often needs to be queried together. This almost always introduces a technical and operational nightmare.GraphQL is great for placing different sources behind a uniform query interface as it is agnostic to the underlying database.Here\u2019s an anecdote from James Baxley from his experience with using GraphQL at NewSpring Church:\u201cWe wanted to use the Meteor account\u2019s system and reactive data, get financial accounts, show giving history, and show profile info from Rock, have all of our images and content stored in our CMS, and we wanted in-app search using Google\u2019s site search API. Using the power of GraphQL, in one week a single developer was able to connect all of those databases and systems together into a single endpoint.\u201dStitching together multiple sources of data is usually a one-time effort, that once handled, saves the need for expensive, cross-functional operations.Concluding remarksGraphQL\u2019s efficiency, and the extent of automated tooling delivered, means that teams working on internal apps can work better with the typical budgetary/resource constraints, as dependencies on backend developers are significantly reduced. In fact, with some open-source tools auto-generating ready-to-use GraphQL APIs, GraphQL enables front-end developers to transition into full-stack developers who can manage entire apps by themselves.Several well-known organizations like The New York Times, Github, Spotify, etc., have taken the plunge, adopting GraphQL as their primary API technology. The ecosystem is ripe with tools that handle most of the heavy-lifting. For example, Hasura provides a ready-to-use GraphQL server that automatically infers the schema from a database and manages the underlying implementation.Therefore, it is imperative that organizations evaluate GraphQL and, if their findings are positive, begin to chart out an org-wide adoption roadmap. Trying out GraphQL on internal apps is a great, risk-free opportunity for any organization to do so.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Bulk update and delete mutations in GraphQL Description: \nSafely update and delete data in your database over GraphQL\n Author: Rishichandra Wawhal"}, {"role": "assistant", "content": "Safely update and delete data in your database over\u00a0GraphQLThis blog post will be purely based of examples about bulk update and delete mutations using the Hasura GraphQL Engine.IntroductionIf you bulk edit the state of your application, there is a significant chance of messing up the state if the server implementation does not ensure a transaction. What is a transaction? Roughly put, a transaction is an independent task that must be processed independently and without loss of data in case of failures.Hasura GraphQL\u00a0EngineWe will use the open source Hasura GraphQL Engine (HGE) for showing examples of bulk mutations.It gives instant GraphQL APIs over Postgres without us needing to write a single line of code.It resolves all the bulk queries and mutations as transactionsSetup a GraphQL ProjectTry this out by setting up a GraphQL backend using Hasura Cloud. You will get a Hasura GraphQL Engine instantly that you can connect to a Postgres. Get started here.Bulk UpdateLet us consider a simple blog schema where we have the following article and author tables with the author_id of the article table being the foreign key from the author table\u2019s id\u00a0.The following are the update mutation fields generated by Hasura GraphQL Engine for the above tables:Now if an author decides to deactivate their author profile. In that case, we have to update the author table to deactivate the user and also update the article table to set that author\u2019s articles to unpublished. If the author has the id = 5\u00a0, the bulk mutation will look like:In the above mutation, we are first updating the author table and then the article table. In both cases we are requesting the number of rows affected by the mutation. The entire mutation is treated as a transaction; which means, even if one of the queries fail, the entire query fails.Bulk DeleteLets consider the same blog schema as mentioned above. The Hasura GraphQL Engine generates the following delete mutation fields for the two tables:If the author decides to delete their profile, we have to delete their entry in the author table and also delete all the articles by that author in the article table.But in this case, the order of deletion matters. We cannot delete the entry from the author table before deleting the entries from the article table. This is because all the rows in the \u00a0article table with author_id = 5 depend on the row in the author table with id = 5 (foreign key constraint). Therefore, we will first delete all the articles with author_id = 5 and then delete the author with id = 5\u00a0. The mutation will look something like:Update and Delete\u00a0togetherConsider a simple e-commerce schema which has the tables cart\u00a0, order, product, user and more. order and product tables are related through a many-to-many relationships using an order_product table.Say the user has placed an order for two items and just initiated the payment. At this time, the state of the cart\u00a0, order and order_products related to this order is:cart: [(1111, 2222, 3333), (1112, 4444, 3333)]\norder: [(5555, 3333, true, false)]\norder_products: [(6666, 5555, 2222), (6666, 5555, 4444)]As soon as the payment is complete, we have to update the order table to update the payment_processing and payment_successful\u00a0. We also have to delete the items from the cart of the user. Let us do this using a bulk update and delete mutation.In the above mutation, we have updated the row ( id=5555 ) of the order table with payment_processing to false and payment_successful to true. We have also deleted all the items from the cart of user with id = 3333\u00a0.ReferenceGraphQL queries and mutations docsHasura mutations docs"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Client-side GraphQL schema resolving and schema-stitching Description: \nThis blogpost talks about the why and how of:\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "This blogpost talks about the why and how of:Using GraphQL on the client by mapping GraphQL to REST APIsUsing multiple GraphQL APIs on the client with schema-stitchingWithout building a GraphQL serverTL;DR:Head to this github repo-tutorial to see how to setup a GraphQL schema, resolvers and schema-stitching on your client.The journey to\u00a0GraphQLIn the journey to GraphQL, mainly for existing applications, a few common approaches are emerging:GraphQL gateway service that proxies to RESTGraphQL gateway service that stitches different GraphQL schemas served by different services into one1 is common when you already have REST endpoints that you want to make API calls to and the team cannot invest in writing a GraphQL backend that runs business logic and queries data-sources.2 is common when you\u2019re using different GraphQL services on the backend and want to present a unified GraphQL endpoint as opposed to multiple ones.However, in a lot of cases, when moving to GraphQL the app dev team might not be able to create their own backend service because of the increase in operational effort and process that this might require.Is it then worth doing GraphQL on the client only, without needing a GraphQL server?Is GraphQL on the client worth\u00a0it?GraphQL on the client will nullify 2 traditionally espoused GraphQL benefits:Reduced API callsLesser data on the networkBut, even without these 2 benefits, GraphQL only on the client-side only (esp. with React) does have some real benefits.GraphQL improves UI architecture:Tyler\u2019s lists motivations in his talk \u201cGraphQL outside the box\u201d where he built GraphQL on the client because the benefits of GraphQL on the UI architecture were very real:A better data layer: Mapping remote data to the client-side shape becomes easier with GraphQL and using Apollo\u2019s react clientBetter component APIs: UI components can use GraphQL types, components can describe their static data dependencies as GraphQL fragmentsData modelling upfront: Using GraphQL in your client involves that you think about your GraphQL types first and makes the data-modelling step explicit.Incremental migrationSetting up a GraphQL backend might take time, especially for legacy applications and GraphQL on the client represents a neat incremental step.As we are working with users of the Hasura GraphQL engine, we noticed that similar problems were emerging, especially developers working in environments where building a new backend service is not feasible.Working with them we created a simple tutorial, using the existing community tooling, that allows a GraphQL schema and resolver to be written on the client itself.GraphQL on the\u00a0clientA Typical GraphQL setup:2. A client side GraphQL setup:3. How to build GraphQL on the clientRishichandra Wawhal has put together a tutorial as a github repo of how to use Apollo schema-link, graphql-js and other tools from the nodejs ecosystem to be able to do client-side graphql using Apollo client.Write a GraphQL schema and resolvers using graphql-toolsIf required, set up schema stitching with graphql-toolsUse schema-link to make Apollo client query the schema directly instead of using fetch.// Create a GraphQL schema and resolvers\n\nimport { makeExecutableSchema } from 'graphql-tools';\nconst typeDefs = `\n  type Hello {\n    message: String\n  }\n`;\nconst resolvers = {\n  Query: {\n    message: (root, args, context, info) => \"hello-world\"\n    // Or make a remote API call\n  }\n};\nconst schema = makeExecutableSchema({\n  typeDefs,\n  resolvers\n});\n\n// Setup Apollo client as usual, but use SchemaLink\nimport { SchemaLink } from 'apollo-link-schema';\nimport { InMemoryCache } from 'apollo-cache-inmemory';\nimport ApolloClient from 'apollo-client';\nconst client = new ApolloClient({\n  link: new SchemaLink({ schema }),\n  cache: new InMemoryCache()\n})Next on the roadmap is to add subscription support per: https://github.com/apollographql/apollo-link/issues/374\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Use GraphQL subscriptions from your NodeJS code Description: \nSetting up your NodeJS code to use GraphQL subscriptions involves going through a few hoops.\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "Setting up your NodeJS code to use GraphQL subscriptions involves going through a few hoops.Using GraphQL subscriptions from your NodeJS code is especially useful if you want to react to events or changes on a GraphQL backendWe\u2019ve setup a simple boilerplate to help you get GraphQL subscriptions setup at: https://github.com/hasura/nodejs-graphql-subscriptions-boilerplate."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Moving from apollo-boost to GraphQL subscriptions with apollo-client Description: \nThis is a simple tutorial on how to use GraphQL subscriptions with Apollo.\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "This is a simple tutorial on how to use GraphQL subscriptions with Apollo.Let\u2019s say you have the basic apollo-client working and now you want to enable subscriptions.This is what you probably have:import { ApolloProvider } from \"react-apollo\";\nimport ApolloClient from \"apollo-boost\";\n\nconst client = new ApolloClient({\n  uri: \"https://w5xlvm3vzz.lp.gql.zone/graphql\"\n});\nReactDOM.render(\n  (<ApolloProvider client={client}>\n   <App />\n  </ApolloProvider>),\n  document.getElementById('root'));Now, to move to subscriptions, this is what you need to change your apollo-client setup to:import { ApolloProvider } from \"react-apollo\";\n// Remove the apollo-boost import and change to this:\nimport ApolloClient from \"apollo-client\";\n// Setup the network \"links\"\nimport { WebSocketLink } from 'apollo-link-ws';\nimport { HttpLink } from 'apollo-link-http';\nimport { split } from 'apollo-link';\nimport { getMainDefinition } from 'apollo-utilities';\nimport { InMemoryCache } from 'apollo-cache-inmemory';\nconst wsurl = wss://hasura-demo.herokuapp.com/v1alpha1/graphql;\nconst httpurl = https://hasura-demo.herokuapp.com/v1alpha1/graphql;\n\nconst wsLink = new WebSocketLink({\n  uri: wsurl,\n  options: {\n    reconnect: true\n  }\n});\nconst httpLink = new HttpLink({\n  uri: httpurl,\n});\n\nconst link = split(\n  // split based on operation type\n  ({ query }) => {\n    const { kind, operation } = getMainDefinition(query);\n    return kind === 'OperationDefinition' && operation === 'subscription';\n  },\n  wsLink,\n  httpLink,\n)\n\nconst client = new ApolloClient({\n  link,\n  cache: new InMemoryCache()\n});\n// Use the client just as before\nReactDOM.render(\n  (<ApolloProvider client={client}>\n   <App />\n  </ApolloProvider>),\n  document.getElementById('root'));This reads as:Create a wsLinkCreate an httpLinkCreate a new apollo-link which delegates all GraphQL queries to httpLink unless they are subscription queriesCreate a ApolloClient which uses the link object we created aboveEnable InMemoryCache because that\u2019s what we had with apollo-boostThese are the packages you need to install:npm install --save apollo-client apollo-link-ws apollo-link-http apollo-link apollo-utilities apollo-cache-inmemoryAnd that\u2019s it. You\u2019re ready to start using Subscriptions!\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Introducing the Hasura GraphQL Engine Description: \nGet Instant Realtime GraphQL APIs on any Postgres based application, new or existing!\n Author: Hasura"}, {"role": "assistant", "content": "Get Instant Realtime GraphQL APIs on any Postgres based application, new or existing!Last updated: August 6th, 2018Instant Realtime GraphQL APIs on any Postgres\u00a0databaseWe are super thrilled to announce the launch of the Hasura GraphQL Engine, an open source product that gives you instant realtime GraphQL APIs on Postgres. You can try it out here\u200a\u2014\u200ago ahead, check it out, it will take exactly 30 seconds to deploy it to Heroku\u2019s free tier (yes\u200a\u2014\u200awe counted ?).The complete open source repo is available here.The Rapid Adoption of GraphQL in the EnterpriseSince Facebook announced GraphQL three years ago, GraphQL\u2019s adoption has grown exponentially, perhaps best illustrated by this npm trends chart.Fortune 500 enterprises such as Intuit, Walmart, the New York Times and NBC and global startups, such as Airbnb, Docker, Github, Twitter, Pinterest, Uber and Yelp are among those who have adopted GraphQL to shrink product development cycles.The Hasura GraphQL\u00a0EngineOur foray into GraphQL started earlier this year. We added GraphQL support to the Hasura Kubernetes platform\u2019s data APIs. Over the last few months, we have been working toward releasing the GraphQL engine as a standalone product that can instantly add GraphQL APIs to existing Postgres databases.The Hasura GraphQL Engine is an extremely lightweight, high performance product that gives you instant realtime GraphQL APIs on a Postgres database. This can be used to quickly build new applications on Postgres or fast-track the move to GraphQL for existing applications on Postgres.It comes with a UI (an API explorer, a phpMyAdmin of sorts for Postgres) that lets you create and view tables on your database and make GraphQL queries using the embedded GraphiQL interface.The Hasura Console lets you create and view tables in your Postgres database and query them using the embedded GraphiQL interface.August 06, 2018 update: The Hasura GraphQL engine now supports subscriptions for all kind of queries. Any query can be converted into a subscription by replacing query with subscription as the operation type. Read more here.GraphQL Subscriptions in\u00a0actionGetting StartedOur aim with the Hasura GraphQL engine is to help you setup a GraphQL server with the least amount of friction, so that you can start building applications without any setup. With that in mind, we present the Hasura deployment on Heroku\u2019s free tier.30 seconds to GraphQL zen!You can also use the docker container to run it anywhere.Adding GraphQL to your existing applicationsWe designed the Hasura GraphQL Engine to allow developers running existing Postgres based applications to migrate to GraphQL without having to write any backend code.Developers can simply choose tables from their new or existing database that they want to use with GraphQL and link those tables into a \u201cgraph.\u201d This gives companies already using Postgres an easy, incremental way to start migrating to GraphQL.In this video, we show you how you can query your existing gitlab instance with GraphQL.Querying Postgres directly from your front-end appsThis is every backend developer\u2019s nightmare!The Hasura Engine comes with a granular, dynamic access control system that integrates with existing authentication systems such as Auth0 or custom implementations. This further aids in the incremental journey toward adding GraphQL to your existing applications. The Hasura Engine enables developers to define who can access what data (admin, anonymous users, etc.).Add dynamic access control rules using the UI so that your front end apps can directly query the database via HasuraArchitected for high-performanceWhile designing Hasura, we had two objectives:Hasura should be able to run anywhere, from your local machine to the Heroku free tier to a server on Amazon, Google or Microsoft.You should be able to extract maximum performance/dollar.Consequently, we optimised Hasura for low memory footprint & latency. On the Heroku free tier, it consumes ~50MB of RAM even while serving more than ~1000 requests/per second.If you\u2019re interested in the architecture decisions involved in building this, you can read about it here.How we stand apart from existing solutions.Hasura was designed specifically to allow companies with existing Postgres applications to incrementally migrate to GraphQL, without affecting the application, database or users in any way.It works seamlessly with existing Postgres databases, i.e, adding Hasura to your current project which uses Postgres should not limit or change the way in which you use Postgres.The GraphQL Engine can also be exposed directly to clients without writing an intermediate graphql server. You can define the access control rules using a system inspired from Postgres\u2019s RLS (Row Level Security) and integrate it with your existing auth backends with a simple webhook.These two design choices distinguish Hasura\u2019s graphql-engine from similar offerings, such as Prisma and Postgraphile.Prisma aims to be an ORM like interface between your graphql server and the datastore. It abstracts away the underlying datastore with its own tooling.Postgraphile can also be exposed to clients directly, however the access control of these APIs is through Postgres\u2019s RLS which may or may not play well with your existing auth system.Behind the\u00a0scenesOver the last few months, several folks from the GraphQL community have taken out time to review the GraphQL Engine and provided us their feedback! Several calls and coffees later, we can confidently say that for anyone looking to get started with GraphQL, the developer ecosystem around GraphQL is thriving and the community is super helpful! You\u2019re in good hands! ?We would especially like to thank some of our Kubernetes platform users (Josh, Sonicrida, Dylan, we ?you) who took the plunge and started building their projects and stealth startups on the Hasura GraphQL Engine.We\u2019d love to hear your stories! Do try Hasura out and let us know your thoughts. We are very active on our Discord server. You can also ping us on our website chat.You can also check out our launch on hackernews here: https://news.ycombinator.com/item?id=17540263\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Working with schemaless data using GraphQL on Postgres Description: \nExploit Postgres schemaless JSON types in your GraphQL queries using the open-source Hasura GraphQL Engine.\n Author: Hasura"}, {"role": "assistant", "content": "Exploit Postgres schemaless JSON types in your GraphQL queries using the open-source Hasura GraphQL Engine.TL;DRHere is a summary of what is covered in this postSchemaless data is data that does not conform to a rigid schema. Usually stored as key-value pairs or as JSON. You can use the JSON and JSONB column type to store JSON data in your tables.With Hasura GraphQL Engine, you get instant GraphQL APIs on Postgres which you can use to store and retrieve data from the table.We use Postgres views to filter on the JSON dataMutation operations on the JSON data using GraphQL is not possible at this point of time and hence, to update you need to replace the whole JSON with the new data.What is schemaless data?Schemaless data is data that does not conform to a rigid schema. It is usually stored in the form of key-value pairs or as JSON documents.In a relational database like Postgres, this would mean working with a JSON data type.When to use schemaless data?Relational modeling can be used for most applications. But there are some use-cases where storing data as a JSON document makes sense:You can avoid complicated JOINS on tables that hold isolated data by storing them as a JSON document.If you are dependent on data coming from an external API as JSON, you can avoid the process of normalizing this data into different tables. Instead, you can store this data in the same format and structure that you received it in.It also helps in cases where you are dependent on data whose schema is not fixed.Working with schemaless data on\u00a0PostgresPostgreSQL provides two data types to store JSON elements: JSON and JSONB. The main difference between them is their efficiency. JSON is stored as text and JSONB is first decomposed into binary components and then stored. This makes inserts into JSONB slower but parsing of sub elements faster.Example use-caseLet\u2019s take an example of a case where we have a third party API which provides us with a user\u2019s name and address. The address field can have multiple fields like apartment number, street name, pincode etc. address in this case does not follow a rigid schema.Problem Statement:Find a way to store the complete information provided by the API into a table in our database.Provide an API to filter users based on their address pincode.Solution Overview:Create a user table with columns id primary key, name and address. address will be of type JSONB.Use the GraphQL insert mutation to insert data into this table.Create a view user_address which shows the id of the user and their pincode by retrieving it from the data in the address column of the user table.Fetch data from user_address using GraphQL.Alright, let\u2019s get to implementing our solution.Getting GraphQL APIs over\u00a0PostgresWe will use the Hasura GraphQL engine for instantly getting GraphQL APIs over Postgres. Click on the button below to deploy the GraphQL engine to Heroku\u2019s free tier.Click this button to deploy the GraphQL engine to\u00a0HerokuThis will deploy the graphql-engine to Heroku. You might need to create a Heroku account if you don\u2019t have one. The graphql-engine will be running at https://your-app.herokuapp.com (replace your-app with your heroku app name).ConsoleThe Graphql engine comes with an admin UI called the Console\u00a0. You can use the Console to build the backend for your application.It is running at the /console endpoint of your graphql-engine URL, which is, in this case, https://your-app.herokuapp.com/console. The landing page of the console looks something like this:Creating a table to store JSON\u00a0dataOn the Console, head to the Data tab and click on Create Table to create a new table.user table:id Integer Primary Keyname Textaddress JSONBHit the create button to create the table.Inserting JSON data into the\u00a0tableThe GraphQL mutation to insert data into the user table will be \u00a0insert_user(objects: $objects) { \u00a0 \u00a0affected_rows \u00a0}}mutation addUser($objects: [user_input]) {\n insert_user(objects: $objects) {\n    affected_rows\n  }\n}Variable {\n\"objects\": [\n    {\n      \"id\": 1,\n      \"name\": \"Jack Smith\",\n      \"address\": {\n        \"house_number\": \"112\",\n        \"house_name\": \"XYZ Apartments\",\n        \"street_name\": \"ABC Street\",\n        \"city\": \"Bengaluru\",\n        \"pincode\": \"123456\"\n      }\n    }\n  ]\n}Note: You can try out GraphQL APIs in the API Explorer of the API Console.Fetching data based on JSON properties using\u00a0GraphQLCurrently, we cannot directly filter data by the different properties of the stored JSON. We can however, create a view which holds all of this data and then query that view.Head to the Data tab on the API Console and click on SQL from the left. Run the following SQL command.CREATE VIEW user_address AS\nSELECT id as user_id, address->>'pincode' as pincodeFROM \"user\";Note: Ensure that you check the Track Table checkbox before running the query so that you can use Data APIs to query the view.This will create a view called user_address with user_id and pincode as columns.We can now fetch data from this view just like you would from a table. query user_on_pincode{\n user_address(where: {pincode: \"123456\"}) {\n    user_id\n    pincode\n  }\n}Filtering from the user\u00a0tableYou can also directly filter and fetch data from the user table based on the pincode. You need to add the user_address view as an object relationship to the user table.To do this, head to the Data tab in the API Console. Click on user from the panel on the left. Click on the Relationship tab and hit the Add a manual relationship button. In the form that comes up, enter the following informationRelationship Type will be Object RelationshipRelationship Name can be \u201caddress_info\u201dConfiguration: id\u00a0:: user_address -> user_idYou can now filter the user table by pincode.query get_user_by_pincode {\n  user(\n    where: {\n       address_info: { pincode: \"123456\" }\n     }\n   ) {\n    id\n    name\n    address_info {\n      pincode\n    }\n  }\n}Updating JSON\u00a0dataCurrently, there is no direct support to directly manipulate data inside of the JSON. To update, you will have to replace the whole JSON document using the an update mutation.mutation update_user_address {\nupdate_user(\n    where: {id: 1}\n    _set: $new_address\n  ) {\n    returning {\n      id\n      address\n    }\n  }\n}Variable{\n\"new_address\": {\n    \"address\": {\n        \"house_number\": \"120\",\n        \"house_name\": \"NEW XYZ Apartments\",\n        \"street_name\": \"NEW ABC Street\",\n        \"city\": \"Bengaluru\",\n        \"pincode\": \"456789\"\n    }\n  }\n}Hopefully, this served as a good introduction to working with schemaless data on Hasura. To learn more about the JSON and JSONB data types, you can check out the Postgres docs here.If you have any questions or suggestions, do not forget to drop in a comment!\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL and Tree Data Structures with Postgres on Hasura GraphQL engine Description: \nModelling a tree data structure on postgres and using GraphQL to add and get data from the tree.\n Author: Hasura"}, {"role": "assistant", "content": "Modelling a tree data structure on postgres and using GraphQL to add and get data from the tree.When are tree data structures used\u00a0?Recursive or Tree Data Structures are commonly used to model your schema to store data that has a hierarchy.Common use-cases of tree data structures include:Storing a directory structure for an application like Google Drive where you would have a root directory. Within this root directory you may have other directories or files. Every directory at any level of nesting can have any number of files or directories inside it.Comment threads for blogs or forums where each post or topic can have comments and each comment can in turn have comments (or replies).Finding common connections or friends for a networking application like LinkedIn or Facebook.In this blog post we are going to take a look at how you can model your database to build a comment thread feature for your application. Moreover, we will also be using GraphQL to add and delete comments.Let\u2019s take an example of a very simple blog and work on enabling comments for each post. To elaborate on what we needEvery post can have commentsEvery comment can have replies (child comments)We want to list these comments in an ordered fashion sorted by when they were created.We are going to go with Hasura GraphQL engine as it provides a Postgres database with instant GraphQL queries.Getting startedWe will use the Hasura GraphQL engine for instantly getting GraphQL APIs over Postgres. Click on the button below to deploy the GraphQL engine to Heroku\u2019s free tier.Click this button to deploy the GraphQL engine to\u00a0HerokuThis will deploy the graphql-engine to Heroku. You might need to create a Heroku account if you don\u2019t have one. The graphql-engine will be running at https://your-app.herokuapp.com (replace your-app with your heroku app name).API ConsoleEvery Hasura cluster comes with an API Console that you can use to build the backend for your application.The API Console is available at https://your-app.herokuapp.com/consoleCreating a\u00a0tableHead to the Data tab and click on Create Table to create a new tableLet\u2019s call our table post_comments to store the comments for each post. The table will have the following columnsid Integer(auto increment) Primary Keyparent_id Integer Nullablecomment Textcreated_at Timestamp default now()post_id Integeruser_id IntegerHit the create button to create the table.This is the table where we will store all the comments for each of the blog posts.Creating a self referenceNext, let\u2019s define a foreign key constraint on the parent_id column to the id column.To do this, head to the Modify tab and click on the Edit button next to parent_id. Check the Foreign Key checkbox, select post_comments as the reference table and id as the reference column.Adding a relationship to fetch the child\u00a0commentsNext, click on the Relationship tab and click on the Add button under the Suggested Array Relationship column. Name the relationship children_commentsHit Save to add this relationship.Fetching and adding\u00a0commentsThe mutation to insert a comment will be mutation add_comment {\n  insert_post_comments(objects: $objects) {\n    returning{\n      id\n      parent_id\n      comment\n      created_at\n      user_id\n      post_id\n    }\n  }\n}The $objects variable will be {\n  \"objects\": [\n    {\n      \"user_id\": 1,\n      \"post_id\": 1,\n      \"comment\": \"First comment on post 1\",\n      \"parent_id\": null\n    }\n  ]\n}Similarly, the $objects variable to add a reply to a comment \u00a0{\n \"objects\": [\n    {\n      \"user_id\": 1,\n      \"post_id\": 1,\n      \"comment\": \"First comment on post 1\",\n      \"parent_id\": 1 //Or id of the comment to which this comment is a reply to\n    }\n  ]\n}Fetching commentsIf we are aware of the level of nesting in our comments, then the GraphQL query to fetch comments and all children comments for a post would bequery get_comments_for_post {\n  post_comments(\n    where: {\n      post_id: 1\n      parent_id: null\n    }\n     order_by: [\"+created_at\"]\n  ) {\n    id\n    parent_id\n    comment\n    created_at\n    user_id\n    post_id\n    children_comments (\n      order_by: [\"+created_at\"]\n    ){\n      id\n      parent_id\n      comment\n      created_at\n      user_id\n      post_id\n    }\n  }\n}Here, we are fetching all the comments for a post with an id value of 1 and whose parent_id is null. We are then fetching all the replies (children_comments) as a relationship. The +created_at in the order_by field denotes that the comments should be fetched in the ascending order (based on the value of created_at ). Alternatively, - would denote descending. If no symbol is specified in the query, then + is assumed by default.Similarly, in case you had another level of nesting, the query would be query get_comments_for_post {\n post_comments(\n    where: {\n      post_id: 2\n      parent_id: null\n    }\n     order_by: [\"+created_at\"]\n  ) {\n    id\n    parent_id\n    comment\n    created_at\n    user_id\n    post_id\n    children_comments (\n      order_by: [\"+created_at\"]\n    ){\n      id\n      parent_id\n      comment\n      created_at\n      user_id\n      post_id\n      children_comments(\n        order_by: [\"+created_at\"]\n      ) {\n        id\n        parent_id\n        comment\n        created_at\n        user_id\n        post_id\n      }\n    }\n  }\n}Working with unknown levels of\u00a0nestingSites like https://news.ycombinator.com/ allow any level of nesting. Which means that every comment can have one more child comments. In this case, fetching our comment like we did above does not work since we do not know how many levels we need to fetch.One of the ways of handling this is to fetch the complete list of comments for the particular topic (in this case a blog post) and then arrange it in memory on your client.query get_comments_for_post {\n  post_comments(\n    where: {\n      post_id: 1\n    }\n     order_by: [\"created_at\"]\n  ) {\n    id\n    parent_id\n    comment\n    created_at\n    user_id\n    post_id\n  }\n}You could also have another table that keeps a track of ancestry, something like post_comment_ancestrycomment_idancestor_idHere, for each comment you will store a list of all of its ancestors. For eg: if comment A has two child comments B and C and comment C has a child comment D,A\n| - B\n| - C\n    | - Dthe entry in the post_comment_ancestry table would be+------------------------+\n|comment_id | ancestor_id|\n+------------------------+\n|     B     |      A     |\n|     C     |      A     |\n|     D     |      C     |\n|     D     |      A     |\n+------------------------+As you can see, comment D has two entries for A and C respectively. Using this table you can fetch a list of all child comments to any arbitrary amount of nesting for a particular comment.ConclusionIn this blog post we took a look at one of the ways in which we can work with tree data structures on Postgres.If you would like to see any other use-cases or suggest improvements to the ideas mentioned above, let me know in the comments.Hasura gives you instant realtime GraphQL APIs over any Postgres database without having to write any backend code.For those of you who are new to the Hasura GraphQL engine, this is a good place to get started."}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GraphQL and geolocation on Postgres (PostGIS using Hasura) Description: \nA primer to building backends for location-aware applications using postgres and GraphQL\n Author: Hasura"}, {"role": "assistant", "content": "A primer to building backends for location-aware applications using postgres and GraphQLIn this post, we are going to go over how you can use Postgresql and GraphQL to build location aware applications. We are going to go over a simple use case where we are building a GraphQL backend for an application which shows a list of landmarks around you.We are going to use the Hasura GraphQL engine for this as it provides out of the box GraphQL APIs over postgres.Getting startedWe will use the Hasura GraphQL engine for instantly getting GraphQL APIs over Postgres. Click on the button below to deploy the GraphQL engine to Heroku\u2019s free tier.Click this button to deploy the GraphQL to\u00a0HerokuThis will deploy the graphql-engine to Heroku. You might need to create a Heroku account if you don\u2019t have one. The graphql-engine will be running at https://your-app.herokuapp.com (replace your-app with your heroku app name).API ConsoleEvery Hasura cluster comes with an API Console that you can use to build the backend for your application.The API Console is available at https://your-app.herokuapp.com/consolePostGISTo store and compute on geo-location data, we are going to use PostGIS which is a spatial database extender for Postgres. It adds support for geographical objects on Postgres and also comes with utility functions such as distance, area, union, and intersection.Enabling PostGISBefore we begin, we need to enable the PostGIS extension on Postgres. (You\u2019ll need a Postgres instance with PostGIS installed. Heroku Postgres comes with the installation, you just need to create the extension. If you\u2019re using Docker based setup, checkout this docker-compose file)Head to Data tab in the Api Console and click on SQL in nav bar on the left. And execute the following SQL commands:Please ensure that you select the Track this checkbox before running the query so that this change is tracked as a migration.Creating tablesSay we need 2 tables, for user and landmark location data, with the following definitions:Run the above SQL in the same SQL section of the console as shown above, and don't forget to check the Track this checkbox (this is because we want GraphQL Engine to track this change in the db schema). We are storing the location of the landmark and the user in a column named location of type GEOGRAPHY(Point) for each table. Inserting Data using\u00a0GraphQLYou can populate the tables we just created with test data using the following mutations and variables:Fetching list of landmarks around a\u00a0userIn this example, we want to fetch a list of landmarks that are near a given user, along with the user\u2019s details in the same query. PostGIS\u2019 built-in function, ST_Distance can be used to implement this use case.Since our use-case requires an output that isn\u2019t a \u201csubset\u201d of any of the existing tables i.e. the SETOF table doesn\u2019t exist, let\u2019s first create this table and then create our location search function using the following definitions (once again, we\u2019ll use the Data -> SQL section of the Hasura console):This function fetches user information (for the given input userid) and a list of landmarks which are less than distance_kms kilometers away from the user\u2019s location as a JSON field. We can now refer to this function in our GraphQL API and get the corresponding response as shown below:query:example response:ConclusionPostGIS also comes with a variety of other utility functions which you can use. You can find that list here.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Why you should use GraphQL instead of a JSON DSL Description: \nGraphQL is awesome but I didn\u2019t always see it that way. The first few times I came across GraphQL I was very skeptical of its benefits\u2026\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "GraphQL is awesome but I didn\u2019t always see it that way. The first few times I came across GraphQL I was very skeptical of its benefits because the cost of putting together a GraphQL system seemed too high and it seemed to me that all the benefits of GraphQL could be gained by having a just creating a custom JSON DSL (a query language using JSON).However, there is a singular feature of a GraphQL system that is the key differentiator: the GraphQL schema that is served by any GraphQL server. This fact coupled with the rise of server side tooling, is why a GraphQL system is something you should seriously look at if you\u2019re thinking of writing a JSON based query language for your APIs.The rise of GraphQL is also perhaps a signal of the increasing amount of \u201ctype safety\u201d tooling in Javascript community.TL;DRA JSON DSL can be as expressive (actually too expressive) as GraphQLA JSON DSL requires no change in client-side and server side toolingGraphQL: Not just a query language but forces the server to publish a schema making the query/API spec a part of the codeGraphQL: Schema can be used by the client to create tooling that automatically validates queries or \u201ccodegen\u201ds typesafe SDKsGraphQL killer-design: API contract is a part of the codeGraphQL killer-feature: Community tooling can thrive because of that designIs a JSON DSL actually a feasible alternative?For example, a \u201cread\u201d query might look like this in GraphQL:But if we were to use a custom JSON query language, we might have come up with this ourselves:As you can see, the JSON DSL is \u201cuglier\u201d but it\u2019s a lot more flexible, no additional tooling and you can create your own \u201cquery language\u201d.Benefits of a JSON DSL:No change in tooling on the clientJSON objects can be easily manipulated, especially in a language like Javascript where JSON can easily be parsed into a Javascript object.No change in tooling on the serverDirectives and fragments in GraphQL would not even be required in JSON because you can just manipulate a POJO (plain old javascript object ?) exactly the way you need to.What are the problems with a JSON\u00a0DSL?However, the JSON DSL approach has a few problems that start emerging when you deploy it over a long running software project or a large team.Problems on the client-side:Where is the \u201cgrammar\u201d of your JSON DSL? REST-ish endpoints propose a simple, if slightly ad-hoc, grammar with HTTP verbs and simple JSON being chucked back and forth. But in a custom JSON DSL what if the \u201cquery language\u201d changes on the server side? How you do create a specification for the client to create queries in JSON that are valid?With a REST-ish design, there\u2019s no query language. The JSON that is sent back and forth adheres to a \u201cshape\u201d or a skeleton but not really a dynamic query language. For example, you \u201cPOST\u201d an \u201cobject\u201d. This allows the creation of things like a \u201cswagger\u201d spec. For example, a \u201cswagger\u201d specification of your API would look like this:However, what is the \u201cswagger\u201d equivalent for a single HTTP endpoint that takes our JSON query? A query language follows a \u201cgrammar\u201d and not really a \u201cshape\u201d.On the server-side:Serving a JSON DSL based API requires that you:Need to parse the incoming JSON query andAnalyse the parsed query to perform type-checksConvert it to an internal form that you can break down and process (\u201cresolving\u201d)This is suddenly turning from a fun \u201coh-hey-i\u2019ll-just-use-some-clever-if-else-statements-to-validate-the-JSON\u201d into \u201cwhy-is-this-a-1000-line-conditional-block\u201d or even worse \u201cwho-will-ever-maintain-this-random-new-language-that-exists-only-in-the-mind-of-the-first-employee\u201d situation.So then once you start off writing it in a naive way, you\u2019ll soon realise that it\u2019s easier to write a mini-compiler into your API server as a query parser + \u201cresolver\u201d that can actually understand the JSON DSL and process it in phases as opposed to dealing with the JSON DSL in an ad-hoc way.Welcome to\u00a0GraphQLOnce you go through the effort above, you\u2019ll realise that you\u2019ve invented a GraphQL. But it\u2019s \u201cuglier\u201d (because there seems to be some key-value cruft) and there\u2019s no standard spec.GraphQL approaches the problem of being able to make queries against a server by enforcing a few key ideas.Idea #1: A standard \u201cgrammar\u201d for queryingGraphQL defines a standard grammar for a query language to read/write data. It kind of looks like a neater JSON with just the keys and allows you to pass arbitrary parameters in the query.Idea #2: Force the server to publish a schemaThe GraphQL spec mandates that a GraphQL server should publish a schema describing the types of the data and the \u201cparameters\u201d (arguments per GraphQL).Image taken from graphql.orgWhen the server publishes a schema, suddenly a whole host of tools and plugins become possible which can start solving the problems that were fundamental to using APIs. In fact, this suddenly makes a GraphQL server + client system better even if you didn\u2019t have to make \u201cqueries\u201d and could have just gotten away with simple REST.The API specification is a part of writing the source code!Awesome problem solved #1: API validation at build timeWhether you\u2019re making \u201cqueries\u201d or simple REST calls this idea of having a schema now allows you (or even better, the community\u00a0;)) to write tooling that can validate your API calls.In fact, you can even add IDE tooling to do that validation as you write code!eslint configured with Atom to validate graphql\u00a0queriesFor typed language clients like Java (Android) or Swift (iOS) you can even generate the SDK code directly and nuke a large amount of boilerplate.Awesome problem solved #2: Reduction in async client code boilerplateAsync API calls from the client can now be specified as declarative queries and the boilerplate that fetches the data and renders the UI can be neatly abstracted away. This allows you to write much cleaner UI code!Check out a list of awesome react clients here: http://graphql.org/code/#graphql-clientsImportant note: GraphQL doesn\u2019t itself solve these problems. It\u2019s design choices allow the community to solve these problems.So GraphQL is\u00a0awesome?Yes, but even more so the GraphQL community is awesome! Client side tooling would have been a nightmare for whole new query language but amazing client-side tools are paving the way. Apollo\u2019s react/react-native GraphQL client library even reduces the boilerplate required to use the results of the GraphQL query in your react component!GraphQL servers are still a bit of a pain to write compared to writing simple REST-ish servers. A backend developer can\u2019t think with URL paths and good old JSON and instead needs to deal with an abstract syntax tree and \u201cresolvers\u201d up front. Add authentication to the mix and the resolution logic becomes a little more complicated. Performance also gets harder to reason about because of the recursive nature of processing a GraphQL query. exAspArk talks about the (n+1) problem and approaches to solving it here.But these are teething costs and not really pain-points that will remain with us forever. Better and better tooling, and more discussions around different patterns will start emerging and making life easier for the backend developer!SummaryGraphQL is an amazing leap forward for consuming APIs! GraphQL is a million-times better than writing your own query language in JSON especially if you care about your colleagues and the longevity of your project. In fact, consuming GraphQL is far better than consuming REST because of the type-safety tooling that\u2019s built into the GraphQL server and the client-side tooling. GraphQL servers are still a little hard to write, but it\u2019s getting better day by day thanks to the tireless work of folks in the community!GraphQL at\u00a0HasuraWe\u2019ve just released a GraphQL engine for Postgres that creates a GraphQL API server automatically based on the postgres schema and accepts declarative annotations to specify access control rules and relationships.Our approach to resolving GraphQL queries is a little different, because instead of recursive resolution we compile the GraphQL query into a single SQL query that is then run against Postgres.As a user, you don\u2019t need to write a backend GraphQL schema, or write any code. Monitoring or debugging is also easy if you start hitting slow queries because it is just a matter of EXPLAIN-ing the generated SQL query and performing standard tuning operations on Postgres.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Architecture of a high performance GraphQL to SQL engine Description: \nThe Hasura platform\u2019s data microservice provides a HTTP API to query Postgres using GraphQL or JSON in a permission safe way.\n Author: Sandip"}, {"role": "assistant", "content": "Update: Tanmai spoke about this in more detail, including newer updates at the 2019 GraphQL Summit conference in SF.The Hasura platform\u2019s data microservice provides a HTTP API to query Postgres using GraphQL or JSON in a permission safe way.You can exploit foreign key constraints in Postgres to query hierarchical data in a single request. For example, you can run this query to fetch \u201calbums\u201d and all their \u201ctracks\u201d (provided the \u201ctrack\u201d table has a foreign key to the \u201calbum\u201d table): \u00a0{\n  album (where: {year: {_eq: 2018}}) {\n    title\n    tracks {\n      id\n      title\n    }\n  }\n}As you may have guessed, the queries can traverse tables to an arbitrary depth. This query interface combined with permissions lets frontend applications to query Postgres without writing any backend code.This API is designed to be fast (response time) and to handle a large throughput (requests per sec) while being light on resources (low CPU and memory usage). We discuss the architectural decisions that have enabled us to achieve this.Query lifecycleA query made to the data microservice goes through these stages:Session resolution: The request hits the gateway which resolves the authorization key (if any) and adds the user-id and role headers and then proxies the request to the data service.Query parsing: Data service receives a request, parses the headers to get the user-id and role, parses the body into a GraphQL AST.Query validation: Check if the query is semantically correct and then enforce permissions defined for the roleQuery execution: The validated query is converted to an SQL statement and is executed on Postgres.Response generation: The result from postgres is processed and sent to the client (the gateway adds gzip compression if needed).GoalsThe requirements are roughly as follows:The HTTP stack should add very little overhead and should be able to handle a lot of concurrent requests for high throughput.Fast query translation (GraphQL to SQL)The compiled SQL query should be efficient on Postgres.Result from Postgres has to be efficiently sent back.Processing the GraphQL\u00a0requestThese are the various approaches to fetch the data required for the GraphQL query:Naive resolversGraphQL query execution typically involves executing a resolver for each field. In the example query, we would invoke a function to fetch the albums released in 2018 year and then for each of these albums, we would invoke a function to fetch the tracks, the classic N+1 query problem. The number of queries grows exponentially with the depth of the query.The queries executed on Postgres would be as follows:SELECT id,title FROM album WHERE year = 2018;This gives us all the albums. Let the number of albums returned be N. For each album, we would execute this query (so, N queries):SELECT id,title FROM tracks WHERE album_id = <album-id>This would be a total of N + 1 queries to fetch all the required data.Batching queriesProjects like dataloader aim to solve the N + 1 query problem by batching queries. The number of requests are not dependent on the size of the result set anymore, they\u2019ll instead be dependent on the number of nodes in the GraphQL query. The example query in this case would require 2 queries to Postgres to fetch the required data.The queries executed on Postgres would be as follows:SELECT id,title FROM album WHERE year = 2018This gives us all the albums. To fetch the tracks of all the required albums:SELECT id, title FROM tracks WHERE album_id IN {the list of album ids}This would be 2 queries in total. We\u2019ve avoided issuing a query to fetch track information for each album and instead used the where clause to fetch the tracks of all the required albums in a single query.JoinsDataloader is designed to work across different data sources and cannot exploit the features of a single data source. In our case, the only data source we have is Postgres and Postgres like all relational databases provides a way to collect data from several tables in a single query a.k.a joins. We can determine the tables that are needed by a GraphQL query and generate a single SQL query using joins to fetch all the data. So the data needed for any GraphQL query can be fetched from a single SQL query. This data has be transformed appropriately before sending to the client.The query would be as follows:This would give us data as followsalbum_id, album_title, track_id, track_title\n1, Album1, 1, track1\n1, Album1, 2, track2\n2, Album2, NULL, NULL\nThis data has to be converted into JSON response with the following structure:Optimising response generationWe discovered that most of the time in handling a request is spent in the transformation function (which converts the SQL result to JSON response). After trying few approaches to optimise the transformation function, we\u2019ve decided to remove this function by pushing the transformation into Postgres. Postgres 9.4 (released around the time of the first data microservice release) added json aggregation functions which helped us push the transformation into Postgres. The SQL that is generated would become something like:The result of this query would have one column and one row and this value is sent to the client without any further transformation. From our benchmarks this approach is roughly 3\u20136x faster than the transformation function in Haskell.Prepared statementsThe generated SQL statements can be quite large and complicated depending on the nesting level of the query and the where conditions used. Typically any frontend application has a set of queries which are repeated with different parameters. For example, the above query could be executed for 2017 instead of 2018. Prepared statements are best suited for these use cases, i.e when you have complicated SQL statements which are repeated with a change in some parameters.So, the first time this GraphQL query is executed: \u00a0{\n  album (where: {year: {_eq: 2018}}) {\n    title\n    tracks {\n      id\n      title\n    }\n  }\n}\nWe prepare the SQL statement instead of executing it directly, so the generated SQL will be (notice the $1):followed by executing this prepared statement with:EXECUTE prep_1('2018');When the graphql query changes to the year 2017, we simply execute the prepared statement directly:EXECUTE prep_1('2017');This roughly gives us a boost of 10\u201320% depending on the complexity of the GraphQL query.HaskellHaskell is a good fit for various reasons:Compiled language with great performance (this)Very performant HTTP stack (warp, warp\u2019s architecture)Our prior experience with the languageSummaryAll these optimisations put together result in some serious performance benefits. Here\u2019s a comparison of Hasura\u2019s architecture with Prisma and Postgraphile.Hasura\u2019s GraphQL API compared with Postgraphile and Prisma. This post was first published in April 2018 and these benchmarks are from older versions of Prisma & Postgraphile. Performance optimizations have since gone into all the projects featured below.In fact, the low memory footprint and negligible latency when compared to querying postgres directly, you could even replace the ORM with GraphQL APIs for most use-cases on your server-side code.Benchmarks:Details of the setup:An 8GB RAM, i7 laptopPostgres was running on the same machinewrk was used as a benchmarking tool and for different types of queries, we tried to \u201cmax\u201d out the requests per secondA single instance of the Hasura GraphQL engine was queriedConnection pool size: 50Dataset: chinookQuery 1: tracks_media_some \u00a0query tracks_media_some {\n  tracks (where: {composer: {_eq: \"Kurt Cobain\"}}){\n    id\n    name\n    album {\n      id\n      title\n    }\n    media_type {\n      name    \n    }  \n  }\n}\nRequests per second: 1375 req/sLatency: 17.5msCPU: ~30%RAM: ~30MB (Hasura) + 90MB (Postgres)Query 2: tracks_media_all \u00a0query tracks_media_all {\n  tracks {\n    id\n    name\n    media_type {\n      name    \n    }  \n  }\n}\nRequests per second: 410 req/sLatency: 59msCPU: ~100%RAM: ~30MB (Hasura) + 130MB (Postgres)Query 3: album_tracks_genre_some \u00a0query albums_tracks_genre_some {\n  albums (where: {artist_id: {_eq: 127}}) {    \n    id    \n    title    \n    tracks {      \n      id      \n      name      \n      genre {        \n        name      \n      }    \n    }  \n  }\n}\nRequests per second: 1029 req/sLatency: 24msCPU: ~30%RAM: ~30MB (Hasura) + 90MB (Postgres)Query 4: album_tracks_genre_all \u00a0query albums_tracks_genre_all {\n  albums {    \n    id    \n    title    \n    tracks {      \n      id      \n      name      \n      genre {        \n        name      \n      }    \n    }  \n  }\n}\nRequests per second: 328 req/sLatency: 73msCPU: 100%RAM: ~30MB (Hasura) + 130MB (Postgres)Try it out!If you're not running Hasura, setup a GraphQL project using Hasura Cloud and read the docs to get started.If you're already running Hasura, make sure you're on version 1.2 or above and you're good to go!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Debugging TCP socket leak in a Kubernetes cluster Description: \nLearnings from debugging a network error in Hasura\u2018s Kubernetes cluster on GKE\n Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "Parts of Hasura's infrastructure runs on Google Kubernetes Engine (GKE). This includes around microservices which power the Hasura website among other internal services.Last month, we started noticing delayed API responses and connection refused errors. The errors were more significant if the response body size is larger. For e.g. downloading a file became extremely slow, as slow as ~5kbps, and sometimes will time out altogether without even downloading a single byte.This post outlines how I debugged the issue.TL;DRAPI responses were flaky, sudden slowdowns, occasional timeouts, ERR_CHUNKED_ENCODING HTTP errorsKubernetes services/pods/endpoints were all fine, reporting healthy, but network connectivity was flaky seemingly without a patternAll nodes were reporting healthy (as per kubelet)thockin recommended checking dmesg logs on each nodeOne particular node was running out of TCP stack memoryIssue filed at kubernetes#62334 for kubelet to monitor tcp_memLesson learned: for failures that are hard to detect, check underlying nodeshttps://www.xkcd.com/1495/We run hasura.io on the Hasura platform itself. First, I checked out the API gateway powered by Nginx, which routes requests to the corresponding microservices. Connections proxied from gateway to certain pods times out. It was as if the network performance of the entire cluster has degraded. Little to no connectivity among certain pods.All Hasura platform components are grouped together in hasura namespace, while all custom workloads are in the default namespace. While trying to establish a correlation between these pods, I restarted all the pods in default namespace and everything started working normally. I had no clue of what the root cause was, but everything was back up as if nothing had happened. My initial assumption was this has something to do with k8s endpoints.Two days later, it happened again, certain pages on the our docs page were not loading along with some assets on other services as well. This time, I didn\u2019t restart all the pods, but tried to find out a correlation between the pods that were having the network connectivity issue. I kubectl exec-ed to many pods and tried to ping/download data from each other. But, nothing was making sense. Endpoints are fine, DNS is fine, services are discoverable, pods are running, no issues from localhost, yet, communication between some them were timing out, or stays hung forever.We found a similar issue earlier where if a request hits a k8s service, but the endpoint is not available, it just hangs, instead of showing connection refused. The discussions can be found at the Github issue:Service hangs when endpoints are not available \u00b7 Issue #60124 \u00b7 kubernetes/kubernetesIs this a BUG REPORT or FEATURE REQUEST?: /kind bug /sig network What happened: If we access a service which has no\u2026github.comHaving tried everything, I took it to twitter to get some help on debugging this issue. I tweeted out tagging a few of the core K8s developers.How do you usually debug network issues (slow/no connection) between pods in Kubernetes clusters\u2753We have been experiencing this on #GKE. Restarting the pod or service works though ?cc @kubernetesio @jbeda @brendandburns @thockin @kelseyhightower @Kris__Nova\u2014 Shahidh K Muhammed (@shahidh_k) March 13, 2018\n\nThat helped! Kris Nova, Tim Hockin and Bowei Du got involved with the debugging. thockin suggested that it might likely be conntrack exhaustion and asked to check dmesg. Kubernetes and GKE says all nodes are healthy and ready. But, on checking nodes one by one, one of the node\u2019s dmesg had many lines saying:TCP: out of memory -- consider tuning tcp_memGoogling around to see what tcp_mem does revealed that this error can happen when a machine\u2019s TCP stack runs out of memory pages allocated by the kernel or when there are lot of orphaned/open sockets. Next step was to checkout TCP stack memory usage and figure out who was using it up.The following command shows how many kernel pages can be allocated to TCP stack. When max is hit, things won\u2019t go well.$ cat /proc/sys/net/ipv4/tcp_mem\n43257\t57676\t86514\n(min) (pressure)(max)The current usage was beyond max! The number after mem indicates it.$ cat /proc/net/sockstat\nsockets: used 1386\nTCP: inuse 24 orphan 0 tw 58 alloc 863 mem 87916\nUDP: inuse 3 mem 3\nUDPLITE: inuse 0\nRAW: inuse 0\nFRAG: inuse 0 memory 0Clearly, some program was eating up all the memory pages available to the TCP stack. Identifying the component was pretty easy since restarting it clearly brought the numbers back to normal. The program was a task queue which opens a lot of TCP sockets, but doesn\u2019t close them under particular edge cases.This is when it hit me, underlying machines are also part of the equation. I have been checking everything on the Kubernetes layer, nodes, deployments, pods, services, endpoints etc., but did not think about going beneath that. Since kubelet said the node was healthy, I did not bother to think beyond. It\u2019s easy to miss fundamental issues like kernel TCP memory pages while working with higher level abstractions like Docker and Kubernetes. As thockin said:There are still things that are not perfectly containerized.  It's sort of a game of whack-a-mole, and will be for a while.\u2014 Tim Hockin (@thockin) March 14, 2018\n\nkubelet is the component responsible for monitoring health of a node. It continuously watches node\u2019s CPU, memory and disk, and report the status to API server, and if an unhealthy status is reported API server can mark the node as non-schedulable. Fundamentally, the node was unhealthy in this case, but kubelet was not monitoring the network health. Small network packets were going through, so the node was reporting ready status to API server. But, connections with larger payloads were getting lost.I opened an issue for kubelet to monitor tcp_mem also, since it is an important factor affecting node health, along with CPU/RAM/disk. There could be other factors like conntrack etc. that affects network performance/health. You can join the discussion here:kubelet should track tcp_mem stats also along with cpu/ram/disk \u00b7 Issue #62334 \u00b7\u2026kind feature /sig node What happened: A program started leaking TCP memory, which filled up the node&#39;s TCP stack\u2026github.comHave you faced such issues when running Kubernetes in production? Let me know in the comments.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using Gatsby for a web app with dynamic content \u2014 A case study Description: \nThis is a case study of how we can use a static site generator like Gatsby to build out a web app with dynamic content.\n Author: Hasura"}, {"role": "assistant", "content": "This is a case study of how we can use a static site generator like Gatsby to build out a web app with dynamic content.If you are looking for a boilerplate app setup ? gatsby-postgres-graphql. This sources data from your existing postgres database for your gatsby site. TL;DRHere is a quick summary of what this blog post covers:A basic overview of what a static site is and how it is different from a dynamic one.What Gatsby is and its various features.An action plan on what it takes to build a website with dynamic content using Gatsby.Caveats and current solutions for it.A step by step tutorial where we build out a product listing app with pagination and also programmatic page creation from an external data source. You can find the code to the whole app here.Final thoughts on using Gatsby for such an app.ContentIntroductionThe Action PlanCaveats and current solutionsThe TutorialFinal thoughtsWhy a static\u00a0site?Usually when you visit a website which usually has some dynamic content, the browser sends a request for that page to a server. The server receives this request, fetches the data for the page from a database, it then loads a UI template. The template and the data are then combined and finally a response for that page is sent. This is how a dynamic site works.What is this Gatsby you speak\u00a0of?Gatsby is a blazing-fast static site generator for React. So you get to build a React app and get server side rendering automagically because of Gatsby.A Gatsby site is also highly performant because it gives you client side code splitting and lazy loading of assets. This means that Gatsby gives the browser a complete page as a single file. All the necessary CSS and images are embedded in the page. Only the CSS really needed for that page is included. Gatsby also performs prefetching. After a page is loaded, it fetches data for all the other pages that are linked on the loaded page. This makes user navigation extremely fast.A complete list of Gatsby features can be found here.What about a website with dynamic\u00a0content?Alright, so static websites are awesome because they\u2019re fast. Gatsby is also awesome because it provides SSR for React apps and a whole bunch of other features. But all of this works for a website where the content does not change often.What about websites with dynamic content? An e-commerce website, where you have a paginated product listing page or a lot of pages with product content? It would be awesome to get the same speed and performance for a dynamic web app!Turns out, that since Gatsby is basically built on React, this is very easily doable. Let\u2019s take a look at how this can be done and if it makes sense to do so.The Action\u00a0PlanGatsby maintains an internal GraphQL data store. This internal data store is query-able via GraphQL APIs in your react components. During build Gatsby fetches data for each page and then bundles them up as individual pages.So the things we need to figure out are:Adding our external data into Gatsby\u2019s internal data store. This way our react components can then query this data using GraphQL.Generate a static listing page with the external data.Dynamically create static pages based on our data. For example, for an e-commerce web app, we would require individual pages for each product that we list on our listing page.Make sure that our listing page is paginated.CaveatsData is only fetched during the build step. So if your external source has any new data to be shown, it won\u2019t come up until we run another build.There are multiple ways around this:You can have a cron job running that rebuilds your app every 30 minutes or at a time interval of your choosing or just triggers a rebuild each time your data updates.Gatsby is built to behave almost exactly like a normal React application, which means that you can make network APIs in your react components and update the data shown after the page loads. These are called Hybrid Components, you can read more about them here.Hasura gives you instant GraphQL APIs and can be used to query your Gatsby data store. Here's a video on how to get started with Hasura, if you need it.Executing the action\u00a0planWhat are we going to\u00a0build?We are going to build a web app that will do the following:Have a paginated listing page which gets its data from an external sourceFor each item shown on the listing page, create another page which will show more details about the itemWe are going to use Algolia as our external source here and are also going to use their Javascript API client to fetch the data.Note: You do not need to know how Algolia works or the various things you can do using it to make the most of this blog post. All you need to understand is that Algolia is a full fledged search engine. You can feed data to it and then easily query data from it using their APIs or their clients. You can easily replace Algolia with an external database and query that database instead as your data source.Getting startedBefore we begin, let\u2019s install the gatsby-cli tool on our local machine. To install, run the following in your command shell:$ npm install --global gatsby-cliOnce you have the cli tool installed, create a starter project$ gatsby new my-listing-appNote: you can replace my-listing-app with any name that you want your project to be called.The above command will create a new directory with the specified project name, in this case my-listing-appTo run this app locally, run gatsby develop inside the project directory and your app will be available at localhost:8000Moreover, you can also explore the site\u2019s data and schema using GraphiQL which will be running at http://localhost:8000/___graphqlOk, now that we have our basic gatsby app running locally, let\u2019s build out our app one feature at a time.Adding the external data to the gatsby data\u00a0storeFirst, lets add the algoliasearch library to our project. In your root project directory (where your package.json file lives). Run the following:$ npm install --save algoliasearchThe code to fetch data from an Algolia Index, looks like so:const algoliasearch = require('algoliasearch');\n\nalgoliaIndex.search({\nconst algoliaClient = algoliasearch(ALGOLIA_APP_ID, ALGOLIA_API_KEY);\nconst algoliaIndex = algoliaClient.initIndex(ALGOLIA_INDEX);\n      query: '', //empty query string returns all the data\n      hitsPerPage: 1000 // This is the maximum data that can be fetched\n    },\n function searchDone(err, content) {\n      if (err) {\n        console.log(err);\n      } else {\n        console.log(content);\n      }\n    });In the above code snippet, we are querying Algolia for all the data from a particular index (specified by the empty query string) and fetching 1000 results per query (1000 is the maximum that we can fetch in a single query). ALGOLIA_APP_ID\u00a0, ALGOLIA_API_KEY and ALGOLIA_INDEX are self explanatory constants.Note: Since this is a simple implementation, we are going to assume that our data set is lesser than 1000. If this is not the case, we can either paginate on this request or use the browse feature provided by Algolia to fetch the complete data.To add this data into the gatsby data source, we need toFetch the data during the gatsby build step.Create new nodes for the fetched data.Open up the gatsby-node.js file from your root project directory and add the following to itconst algoliasearch = require('algoliasearch');\n\nconst ALGOLIA_API_KEY = '<YOUR_ALGOLIA_API_KEY>';\n\nconst algoliaClient = algoliasearch(ALGOLIA_APP_ID, ALGOLIA_API_KEY);\n\nexports.sourceNodes = async ({ boundActionCreators }) => {\nconst ALGOLIA_INDEX = '<YOUR_ALGOLIA_INDEX_NAME>';\nconst ALGOLIA_APP_ID = '<YOUR_ALGOLIA_APP_ID';\nconst algoliaIndex = algoliaClient.initIndex(ALGOLIA_INDEX);\n  return new Promise((resolve, reject) => {\n    algoliaIndex.search({\n      query: '', //fetch all\n      hitsPerPage: 1000\n      },\n      function searchDone(err, content) {\n        if (err) {\n          console.log('\\nError');\n          console.log(err);\n          reject();\n        } else {\n          console.log('\\nSuccess');\n          console.log(content);\n          resolve();\n        }\n      }\n    );\n  });\n};Note: We have wrapped the query in a promise. This is so that gatsby knows that it needs to wait for your query to resolve during the build process. Otherwise, it will not wait until the callback gets called. Alternative, you can also use async await and return after the required operation is complete.Save the following and run gatsby develop again. If your query executes successfully, you will see the content being printed out in your command shell.In this case, the query fetches a list of products with its name, stock and price.Next, we need to add this fetched data into the existing internal gatsby data storeconst algoliasearch = require('algoliasearch');\n\n\n.....\n\nexports.sourceNodes = async ({ boundActionCreators }) => {\nconst crypto = require('crypto');\n  const { createNode } = boundActionCreators;\n  return new Promise((resolve, reject) => {\n    algoliaIndex.search({\n      query: '', //fetch all\n      hitsPerPage: 1000\n      },\n      function searchDone(err, content) {\n        if (err) {\n          console.log('\\nError');\n          console.log(err);\n          reject();\n        } else {\n          content.hits.forEach(content => {\n            const productNode =  {\n              id: content.objectID,\n              parent: null,\n              children: [],\n              internal: {\n                type: `Product`,\n                contentDigest: crypto\n                  .createHash(`md5`)\n                  .update(JSON.stringify(content))\n                  .digest(`hex`),\n              },\n              name: content.name,\n              stock: content.stock,\n              price: content.price,\n            }\n            createNode(productNode);\n          });\n          resolve();\n        }\n      }\n    );\n  });\n};Note: We are using another javascript library called crypto\u00a0. Run the command npm install --save crypto in the directory with the package.json file to include it in your project.Here, the data being fetched is a list of products. We create a new node by passing an object to the createNode method. id, parent, children, internal.type and internal.contentDigest are mandatory parameters to the createNode method. Ensure that the id is unique for each node. The Type is the unique GraphQL node type. In this case, all the data being added is of the type Product. You can read about what the parameters are in the gatsby docs.Run gatsby develop again and then open up the GraphiQL service running at http://localhost:8000/___graphqlExplore the schema docs by clicking on the docs button on the top right. You will see that a new query called allProduct is now available.The GraphQL query to fetch all the products will be the following:query {\n  allProduct {\n    edges {\n      node {\n        id\n        name\n        stock\n        price\n      }\n    }\n  }\n}Note: Use GraphiQL to figure out the query.Accessing the data in the react component using GraphQL\u00a0queriesAlright, now that we have added our external data to the gatsby data source. Lets create a component to access this data. Inside the src/pages directory, create a new file named productList.js with the following content:import React from 'react'\n\nexport default class ProductList extends React.Component {\n\nrender() {\n\n}\n\nexport const query = graphql`\n    return (\n      <div>\n        <h1>Products</h1>\n        {\n          this.props.data.allProduct.edges.map((edge, index) => {\n            const product = edge.node\n            return (\n              <div key={product.id}>{product.name}</div>\n            )\n          })\n        }\n      </div>\n    );\n  }\n  query AllProducts {\n    allProduct {\n      edges {\n        node {\n          id\n          name\n        }\n      }\n    }\n  }\n`Note: We have not imported anything to access the graphql tag. This is because these queries are extracted out of the component by gatsby during the build process. You can read more about it in their docs.The data returned from the query is available to the component via its props. You can find the structure of the data by running the query in GraphiQL.Since gatsby supports hot reloading, you don\u2019t really have to do anything else, other than save the productList.js file. This component will now be available at http://localhost:8000/productListAnd voila! We have our listing page.Creating dynamic pages for each\u00a0productNext, let\u2019s create another set of pages for each of our products where we can show more information about the product, like the remaining stock quantity and the price. Let\u2019s say we want the url of the page to be of the type /product/<product-id>Open up the gatsby-node.js file and add the following...\n\n\nexports.sourceNodes = async ({ boundActionCreators }) => {....}\n\nexports.createPages = ({ graphql, boundActionCreators }) => {\nconst path = require(`path`);\n...  const { createPage } = boundActionCreators\n  return new Promise((resolve, reject) => {\n    graphql(`\n      {\n        allProduct {\n          edges {\n            node {\n              id\n            }\n          }\n        }\n      }\n    `  ).then(result => {\n    result.data.allProduct.edges.forEach(({ node }) => {\n      createPage({\n        path: `product/${node.id}`,\n        component: path.resolve(`./src/templates/productDetail.js`),\n        context: {\n          productId: node.id\n        },\n      })\n    })\n    resolve()\n    })\n  }).catch(error => {\n    console.log(error)\n    reject()\n  })\n};To create dynamic pages, we use the method createPage. The method accepts an object with the following keys:path: This is the path we want for our page. In this case, it is going to be /product/<productId>.component: This is the location of our react component which will be used to render the page. For the time being, just leave this as shown in the code snippet. We will create the component next. Note: we are using a gatsby provided library called path for this. Ensure that you have imported it with const path = require('path').context: is the data you want this component to get by default. In this case, we are sending it the product id, so that it can use it to fetch the complete data for the particular product.Note: You can read more about the createPage method here.Next, let\u2019s create our ProductDetail component. Create a new directory inside the src directory called templates. Inside the templates directory, create a file named productDetail.js with the following content inside it:import React from 'react'\n\nexport default class ProductDetail extends React.Component {\n\nrender() {\n\n}\n\nexport const query = graphql`\n    const product = this.props.data.product;\n    return (\n      <div>\n        <h1>{product.name}</h1>\n        <div>Price: {product.price}</div>\n        <div>Stock: {product.stock}</div>\n      </div>\n    );\n  }\n  query ProductDetails($productId: String!) {\n    product(id: { eq: $productId }) {\n      id\n      name\n      stock\n      price\n    }\n  }\n`Note: the $productId is the data being sent from the context key specified in our createPage method inside the gatsby-node.js file, as shown in the previous step. Gatsby automatically uses that as the value for $productId when making the GraphQL query.Save both the gatsby-node.js and productDetail.js files and run gatsby develop again. You can now access the detail page for each product at a url of the type, http://localhost:8000/product/<product-id>. Let\u2019s try http://localhost:8000/product/1Awesome, let\u2019s now link these pages from our listing page.Head to the productList.js file inside src/pages and make the following changeimport React from 'react'\n\n\nexport default class ProductList extends React.Component {\n\nrender() {\n\n\n}\n\nexport const query = graphql`\nimport Link from 'gatsby-link'\n    return (\n      <div>\n        <h1>Products</h1>\n        {\n          this.props.data.allProduct.edges.map((edge, index) => {\n            const product = edge.node\n            return (\n              <Link key={product.id} to={`/product/${product.id}`}>\n                <div>{product.name}</div>\n              </Link>\n            )\n          })\n        }\n      </div>\n    );\n  }\n  query AllProducts {\n    allProduct {\n      edges {\n        node {\n          id\n          name\n        }\n      }\n    }\n  }\n`There are two major additions made hereImport the Gatsby Link component with import Link from 'gatsby-link'Wrap <div>{product.name}</div> inside the Link component.Save the file and you would have your listing page now link to the detail pages.Adding pagination to the product listing\u00a0pageNext, let\u2019s add pagination to our listing page.To add pagination, we are going to leverage gatsby\u2019s community support and use one of their plugins gatsby-paginate.Run the following in the root project directory to include it in your project:$ npm install --save gatsby-paginateNext, open the gatsby-node.js file and make the following changes....\n\n\nexports.sourceNodes = async ({ boundActionCreators }) => {...}\n\nexports.createPages = ({ graphql, boundActionCreators }) => {\nconst createPaginatedPages = require(\"gatsby-paginate\");\n  const { createPage } = boundActionCreators\n  return new Promise((resolve, reject) => {\n    graphql(`     ...    `  ).then(result => {\n    createPaginatedPages({\n      edges: result.data.allProduct.edges,\n      createPage: createPage,\n      pageTemplate: \"src/templates/productList.js\",\n      pageLength: 10,\n      pathPrefix: \"productList\",\n    });\n    result.data.allProduct.edges.forEach(({ node }) => {\n      createPage({\n        path: `product/${node.id}`,\n        component: path.resolve(`./src/templates/productDetail.js`),\n        context: {\n          productId: node.id\n        },\n      })\n    })\n    resolve()\n    })\n  }).catch(error => {\n    console.log(error)\n    reject()\n  })\n};We are calling the createPaginatedPages with the following parameters:edges: is the list of products we have fetched from our GraphQL querycreatePage: is the createPage function we retrieve from boundActionCreators.pageTemplate: is the path to our react component which will render this page. In this case it is src/templates/productList.js. We will get to this in a bit. For the time being, let this be.pageLength: is the number of products we want to show per page. (defaults to 10)pathPrefix: is the route we want this list to be rendered at. In this case, /productList/Note: You can read the documentation for the plugin here to get more information about the customisations you can do.Next, let\u2019s create our productList.js template. Move the productList.js file from src/pages/productList.js to src/templates/ and make the following changes to it.import React from 'react'\n\nconst NavLink = props => {\n\nexport default class ProductList extends React.Component {\n\nrender() {\n\nreturn (\nimport Link from 'gatsby-link'\n  if (!props.test) {\n    return (\n      <Link to={props.url}>\n        {props.text} > {props.url}\n      </Link>\n    )\n  } else {\n    return (\n      <span>\n        {props.text} > {props.url}\n      </span>\n    )\n  }}\n    const { group, index, first, last, pageCount } = this.props.pathContext\n    const previousUrl =\n      index - 1 <= 1 ? '/productList/' : `${(index - 1).toString()}`\n    const nextUrl = `${(index + 1).toString()}`\n      <div>\n        {\n          group.map(edge => {\n            const product = edge.node;\n            return (\n              <Link key={product.id} to={`/product/${product.id}`}>\n                <div>{product.name}</div>\n              </Link>\n            )\n          })\n        }\n        <div className=\"previousLink\">\n          <NavLink test={first} url={previousUrl} text=\"Go to Previous Page\" />\n        </div>\n        <div className=\"nextLink\">\n          <NavLink test={last} url={nextUrl} text=\"Go to Next Page\" />\n        </div>\n      </div>\n    )\n  }\n}We have added a navigation component and also made the necessary changes to handle the data being sent to this component by the gatsby-paginate plugin.Save both the files and run gatsby develop again. After the build, open up http://localhost:8000/projectListAnd there we have it! A listing page app with pagination and dynamic pages!You can find the code for the whole app in the following repository.jaisontj/gatsby-listing-examplegatsby-listing-example - An example gatsby app showcasing how a listing app can be made using gatsbygithub.comFinal ThoughtsAlthough building a dynamic app using Gatsby is possible, the current version of Gatsby (v1) is suitable to build a site that is not \u201ctoo dynamic\u201d (data changes every 30 mins or so).Furthermore, there are a few features that would be really useful:Faster build times by only rebuilding pages whose data has changed. Having this feature will make every rebuild for data changes the same as updating a cache.Enabling fetching data directly from external GraphQL sources with the help of Schema Stitching. This way you can query the data directly from your source with your own GraphQL query.Support for \u201clarge websites\u201d. This means that sites with >1k pages will not perform well with Gatsby. Although, Gatsby does mention that there are people who have successfully built websites with more than 1k pages.We have been looking at moving hasura.io to gatsby for the past few days. This blog post is a primer to a series of upcoming blogposts where we will live blog each step of the migration process.If this attempt fails, this death will not be in vain. We will have learnt things about gatsby and about life. Which is always a good thing.While you're here, why not try out Hasura? It's free to try and you can get started in 30 seconds or so.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Draft vs Gitkube vs Helm vs Ksonnet vs Metaparticle vs Skaffold Description: \nKubernetes tools for application developers\u200a\u2014\u200aFeatures and use cases.\n Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "A comparison of tools that help developers build and deploy their apps on KubernetesTL;DRDraft- deploy code to k8s cluster (automates build-push-deploy)- deploy code in draft-pack supported languages without writing dockerfile or k8s manifests- needs draft cli, helm cli, tiller on cluster, local docker, docker registryGitkube- deploy code to k8s cluster (automates build-push-deploy)- git push to deploy, no dependencies on your local machine- needs dockerfile, k8s manifests in the git repo, gitkube on clusterHelm- deploy and manage charts (collection of k8s objects defining an application) on a k8s cluster- ready made charts for many common applications, like mysql, mediawiki etc.- needs helm cli, tiller on cluster, chart definition locally or from a repoKsonnet- define k8s manifests in jsonnet, deploy them to k8s cluster- reusable components for common patterns and stacks, like deployment+service, redis- needs jsonnet knowledge, ksonnet cliMetaparticle- deploy your code in metaparticle supported languages to k8s (automates build-push-deploy)- define containerizing and deploying to k8s in the language itself, in an idiomatic way, without writing dockerfile or k8s yaml- needs metaparticle library for language, local dockerSkaffold- deploy code to k8s cluster (automates build-push-deploy)- watches source code and triggers build-push-deploy when change happens, configurable pipeline- needs skaffold cli, dockerfile, k8s manifests, skaffold manifest in folder, local docker, docker registryWant to know more? Read ahead.Kubernetes is super popular nowadays and people are looking for more ways and workflows to deploy applications to a Kubernetes cluster. kubectl itself has become like a low-level tool, with people looking for even easier workflows. Draft, Gitkube, Helm, Ksonnet, Metaparticle and Skaffold are some of the tools around that help developers build and deploy their apps on Kubernetes.Draft, Gitkube and Skaffold ease the developer effort, when you are building an application, to get it running on a Kubernetes cluster as quickly as possible. Helm and Ksonnet help the deployment process once your app is built and ready to ship, by defining applications, handling rollout of new versions, handling different clusters etc. Metaparticle is an odd one out here, since it combines everything into your code\u200a\u2014\u200ayaml, dockerfile, all in the code itself.So, what should you use for your use case?Let\u2019s discuss.ContentsDraftGitkubeHelmKsonnetMetaparticleSkaffoldDraftSimple app development & deployment\u200a\u2014\u200aon to any Kubernetes cluster.As the name suggests, Draft makes developing apps that run on Kubernetes clusters easier. The official statement says that Draft is a tool for developing applications that run on Kubernetes, not for deploying them. Helm is the recommended way of deploying applications as per Draft documentation.The goal is to get the current code on the developer\u2019s machine to a Kubernetes cluster, while the developer is still hacking on it, before it is committed to version control. Once the developer is satisfied by the changes made and deployed using Draft, the code is committed to version control.Draft is not supposed to be used for production deployments as it is purely intended to be used for a very quick development workflow when writing applications for Kubernetes. But it integrates very well with Helm, as it internally uses Helm to deploy the changes.ArchitectureDraft: architecture diagramAs we can see from the diagram, draft CLI is a key component. It can detect language used from the source code and then use an appropriate pack from a repo. A pack is a combination of Dockerfile and Helm chart which together defines the environment for an application. Packs can be defined and distributed in repos. Users can define their own packs and repos as they\u2019re present as files in the local system or a git repo.Any directory with source code can be deployed if there is a pack for that stack. Once the directory is setup using draft create(this adds dockerfile, Helm chart and draft.toml), draft up can build the docker image, push it to a registry and rollout the app using Helm chart (provided Helm is installed). Every time a change is made, executing the command again will result in a new build being deployed.There is a draft connect command which can port forward connections to your local system as well as stream logs from the container. It can also integrate with nginx-ingress to provide domain names to each app it deploys.From zero to\u00a0k8sHere are the steps required to get a python app working on a k8s cluster using Draft. (See docs for a more detailed guide)Prerequisites:k8s cluster (hence kubectl)helm CLIdraft CLIdockerdocker repository to store images$ helm init\nUse caseDeveloping apps that run on KubernetesUsed in \u201cinner loop\u201d, before code is committed onto version controlPre-CI: Once development is complete using draft, CI/CD takes overNot to be used in production deploymentMore details here.GitkubeBuild and deploy Docker images to Kubernetes using git pushGitkube is a tool that takes care of building and deploying your Docker images on Kubernetes, using git push. Unlike draft, gitkube has no CLI and runs exclusively on the cluster.Any source code repo with a dockerfile can be deployed using gitkube. Once gitkube is installed and exposed on the cluster, developer can create a remote custom resource which gives a git remote url. The developer can then push to the given url and docker build-kubectl rollout will happen on the cluster. The actual application manifests can be created using any tool (kubectl, helm etc.)Focus is on plug and play installation & usage of existing well known tools (git and kubectl). No assumptions are made about about the repo to be deployed. The docker build context and dockerfile path, along with deployments to be updated are configurable. Authentication to the git remote is based on SSH public key. Whenever any change in code is made, committing and pushing it using git will trigger a build and rollout.ArchitectureGitkube: architecture diagramThere are 3 components on the cluster, a remote CRD which defines what should happen when a push is made on a remote url, gitkubed which builds docker images and updates deployments, and a gitkube-controller which is watching on the CRD to configure gitkubed.Once these objects are created on the cluster, a developer can create their own applications, using kubectl. Next step is to create a remote object which tells gitkube what has to happen when a git push is made to a particular remote. Gitkube writes the remote url back to the status field of the remote object.From zero to\u00a0k8sPrerequisites:k8s cluster (kubectl)gitgitkube installed on the cluster (kubectl create)Here are the steps required to get you application on Kubernetes, including installation of gitkube:$ git clone https://github.com/hasura/gitkube-example\n\n$ cd gitkube-example\n\n$ kubectl create -f k8s.yaml$ cat ~/.ssh/id_rsa.pub | awk '$0=\"  - \"$0' >> \"remote.yaml\"\n\n$ kubectl create -f remote.yaml\n\n$ kubectl get remote example -o json | jq -r '.status.remoteUrl'\n\n$ git remote add example [remoteUrl]\n\n$ git push example master\n\n## edit code\n## commit and pushUse caseEasy deployments using git, without docker buildsDeveloping apps on KubernetesWhile development, WIP branch can be pushed multiple times to see immediate resultsMore details here.HelmThe package manager for KubernetesAs the tag suggests, Helm is a tool to manage applications on Kubernetes, in the form of Charts. Helm takes care of creating the Kubernetes manifests and versioning them so that rollbacks can be performed across all kind of objects, not just deployments. A chart can have deployment, service, configmap etc. It is also templated so that variables can be easily changed. It can be used to define complex applications with dependencies.Helm is primarily intended as a tool to deploy manifests and manage them in a production environment. In contrast to Draft or Gitkube, Helm is not for developing applications, but to deploy them. There are a wide variety of pre-built charts ready to be used with Helm.ArchitectureHelm: architecture diagramLet\u2019s look at Charts first. As we mentioned earlier, a chart a bundle of information necessary to create an instance of a Kubernetes application. It can have deployments, services, configmaps, secrets, ingress etc. all defined as yaml files, which in turn are templates. Developers can also define certain charts as dependencies for other charts, or nest charts inside another one. Charts can be published or collated together in a Chart repo.Helm has two major components, the Helm CLI and Tiller Server. The cli helps in managing charts and repos and it interacts with the Tiller server to deploy and manage these charts.Tiller is a component running on the cluster, talking to k8s API server to create and manage actual objects. It also renders the chart to build a release. When the developer does a helm install <chart-name> the cli contacts tiller with the name of the chart and tiller will get the chart, compile the template and deploy it on the cluster.Helm does not handle your source code. You need use some sort of CI/CD system to build your image and then use Helm to deploy the correct image.From zero to\u00a0k8sPrerequisites:k8s clusterhelm CLIHere is an example of deploying Wordpress blog onto a k8s cluster using Helm:$ helm init\n$ helm repo update\n$ helm install stable/wordpress\n## make new version\n$ helm upgrade [release-name] [chart-name]Use casePackaging: Complex applications (many k8s objects) can be packaged togetherReusable chart repoEasy deployments to multiple environmentsNesting of charts\u200a\u2014\u200adependenciesTemplates\u200a\u2014\u200achanging parameters is easyDistribution and reusabilityLast mile deployment: Continuous deliveryDeploy an image that is already builtUpgrades and rollbacks of multiple k8s objects together\u200a\u2014\u200alifecycle managementMore details here.KsonnetA CLI-supported framework for extensible Kubernetes configurationsKsonnet is an alternate way of defining application configuration for Kubernetes. It uses Jsonnet, a JSON templating language instead of the default yaml files to define k8s manifests. The ksonnet CLI renders the final yaml files and then applies it on the cluster.It is intended to be used for defining reusable components and incrementally using them to build an application.ArchitectureKsonnet: overviewThe basic building blocks are called parts which can be mixed and matched to create prototypes. A prototype along with parameters becomes a component and components can be grouped together as an application. An application can be deployed to multiple environments.The basic workflow is to create an application directory using ks init, auto-generate a manifest (or write your own) for a component using ks generate, deploy this application on a cluster/environment using ks apply <env>. You can manage different environments using ks env command.In short, Ksonnet helps you define and manage applications as collection of components using Jsonnet and then deploy them on different Kubernetes clusters.Like Helm, Ksonnet does not handle source code, it is a tool for defining applications for Kubernetes, using Jsonnet.From zero to\u00a0k8sPrerequisites:k8s clusterksonnet CLIHere\u2019s a guestbook example:$ ks init\n$ ks generate deployed-service guestbook-ui \\     --image gcr.io/heptio-images/ks-guestbook-demo:0.1 \\     --type ClusterIP\n$ ks apply default\n## make changes\n$ ks apply defaultUse caseFlexibility in writing configuration using JsonnetPackaging: Complex configurations can be built as mixing and matching componentsReusable component and prototype library: avoid duplicationEasy deployments to multiple environmentsLast-mile deployment: CD stepMore details here.MetaparticleCloud native standard library for Containers and KubernetesPositioning itself as the standard library for cloud native applications, Metaparticle helps developers to easily adopt proven patterns for distributed system development through primitives via programming language interfaces.It provides idiomatic language interfaces which helps you build systems that can containerize and deploy your application to Kubernetes, develop replicated load balanced services and a lot more. You never define a Dockerfile or a Kubernetes manifest. Everything is handled through idioms native to the programming language that you use.For example, for a Python web application, you add a decorator called containerize (imported from the metaparticle package) to your main function. When you execute the python code, the docker image is built and deployed on to your kubernetes cluster as per parameters mentioned in the decorator. The default kubectl context is used to connect to cluster. So, switching environments means changing the current context.Similar primitives are available for NodeJS, Java and\u00a0.NET and support for more languages are a work in progress.ArchitectureThe metaparticle library for the corresponding language have required primitives and bindings for building the code as a docker image, pushing it to a registry, creating k8s yaml files and deploying it onto a cluster.The Metaparticle Package contains these language idiomatic bindings for building containers. Metaparticle Sync is a library within Metaparticle for synchronization across multiple containers running on different machines.JavaScript/NodeJS, Python, Java and\u00a0.NET are supported during the time of writing.From zero to\u00a0k8sPrerequisites:k8s clustermetaparticle library for the supported languagedockerdocker repository to store imagesA python example (only relevant portion) for building docker image with the code and deploying to a k8s cluster: \u00a0 \u00a0@containerize(\n    'docker.io/your-docker-user-goes-here',\n    options={\n        'ports': [8080],\n        'replicas': 4,\n        'runner': 'metaparticle',\n        'name': 'my-image',\n        'publish': True\n    })\ndef main():\n    Handler = MyHandler\n    httpd = SocketServer.TCPServer((\"\", port), Handler)\n    httpd.serve_forever()Use caseDevelop applications without worrying about k8s yaml or dockerfileDevelopers no longer need to master multiple tools and file formats to harness the power of containers and Kubernetes.Quickly develop replicated, load-balanced servicesHandle synchronization like locking and master election between distributed replicasEasily develop cloud-native patterns like sharded-systemsMore details here.SkaffoldEasy and Repeatable Kubernetes DevelopmentSkaffold handles the workflow of building, pushing and deploying an application to Kubernetes. Like Gitkube, any directory with a dockerfile can be deployed to a k8s cluster with Skaffold.Skaffold will build the docker image locally, push it to a registry and rollout the deployment using the skaffold CLI tool. It also watches the directory so that rebuilds and redeploys happen whenever the code inside the directory changes. It also streams logs from containers.The build, push, deploy pipelines are configurable using a yaml file, so that developers can mix and match tools that like for these steps: e.g. docker build vs google container builder, kubectl vs helm for rollout etc.ArchitectureSkaffold: overviewSkaffold CLI does all the work here. It looks at a file called skaffold.yaml which defines what has to be done. A typical example is to build the docker image using dockerfile in the directory where skaffold dev is run, tag it with its sha256, push the image, set that image in the k8s manifest pointed to in the yaml file, apply that manifests on the cluster. This is run continuously in a loop triggering for every change in the directory. Logs from the deployed container is streamed to the same watch window.Skaffold is very similar to Draft and Gitkube, but more flexible, as it can manage different build-push-deploy pipelines, like the one shown above.From zero to\u00a0k8sPrerequisites:k8s clusterskaffold CLIdockerdocker repository to store imagesHere are the steps to deploy an example Go application that prints hello-world:$ git clone https://github.com/GoogleCloudPlatform/skaffold\n$ cd examples/getting-started\n## edit skaffold.yaml to add docker repo\n$ skaffold dev\n## open new terminal: edit codeUse caseEasy deploymentsIterative builds\u200a\u2014\u200aContinuous build-deploy loopDeveloping apps on KubernetesDefine build-push-deploy pipelines in CI/CD flowMore details here.Let me know in the comments if I missed something out or got something wrong. I have not mentioned tools like Ksync and Telepresence, as I plan to write another post about them soon. But, if there are other tools that fit into the category of those mentioned here, please drop a note in the comments.You can find the discussion on Hacker News here.We\u2019ve been trying to solve similar problems around deployment with the Hasura platform and our current approach is to:Bootstrap a monorepo of microservices that contains Dockerfiles and Kubernetes specsgit push to build and deploy all the microservices in one shotThis method helps you get started without knowing about Dockerfiles and Kubernetes (using boilerplates) but keeps them around in case you need access to them. It also has a very simple (git push based) deployment method.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Strategies for Service Worker Caching for Progressive Web Apps Description: \nService worker caching strategies before and after deployment for react apps.\n Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRThis post goes through the issues with service worker caching, strategies that can be followed to have an ideal experience with service workers and what can be done to recover from a broken deployment. Also focusses on issues with create-react-app\u2019s opt-out behavior causing unintended side effects.Here\u2019s a quick summary of what this post covers:Service Worker caching enables offline first apps. In case its a broken deployment you can\u2019t quickly apply a patch because service-worker.js file itself might be cached, in which case your new deployment might not be rolled out to the end user till the original service-worker.js file\u2019s cache is invalidated (usually ~24h).create-react-app includes a service worker by default and makes your app work offline by default. (can throw up unexpected behavior if you are not aware of what is happening).Set cache-control to max-age 0, no-cache as response header for service-worker.js file. Set appropriate cache headers for other cached assets.Do not cache bad responses (3xx, 4xx, 5xx).Use Clear-Site-Data kill switch to reset your application cache and localstorage for your users. Call unregister() for unregistering your service worker that is already live.What is a service worker?A service worker is a JavaScript file that runs separately from the main browser thread, intercepting network requests, caching or retrieving resources from the cache, and delivering push messages.Here\u2019s the lifecycle of a service worker:The installation phase is the first one that happens and it happens once. Depending on your validity of service worker code, it could go into Activated phase or Error phase (rejected). Once activated, it will be running in the background and will wait for fetch / message events and respond to them. Finally it can be terminated (unregistered) naturally when the user closes the browser tab.The ProblemLets say you deployed your app with service worker(s) enabled, not knowing that service-worker.js file itself is cached. If you make a change to your service worker, then reloading the page won\u2019t kill the old one and activate the new one (Your service worker is considered updated if it\u2019s byte-different to the one the browser already has), leaving you somewhat confused as to why your changes haven\u2019t taken effect. This is because the old window is never actually closed, meaning there\u2019s never a time to swap them out\u200a\u2014\u200ayou need to kill the tab completely then reopen it.What does this mean to your app and your users?Reloading the page doesn\u2019t work as expected. Users will get the cached version (worst in case its a broken deployment) of your app instead of what you intended them to see. You cannot add a new script and deploy to clear the caching mess. You need to wait out the cache expiry time (usually ~24h if no http cache header is set) to invalidate the service-worker.js file cache and to refetch a new version, unless the user decides to clear cache themselves which is a highly unlikely scenario and one that can\u2019t be relied on. By the way, Shift + Reload will temporary skip caches to load the latest version of the app.create-react-app service worker\u00a0issuesReact\u2019s project scaffolding tool, create-react-app includes a service worker by default and this created issues for people who were not aware of what was happening. Like this example of a reddit post where the user is complaining that their react app was running despite uninstalling node, restarting pc and what not. A lot of users were just not aware or informed about this service worker addition and debugging was hard in this case.There is a github issue in create-react-app with a discussion heading towards disabling service workers by default.Note: Create React App includes a service worker by default as of today and they are planning to change to opt-in from react-scripts v2.0.Another horror story example was demonstrated at last year\u2019s JS Conf EU, where a production app was perpetually throwing errors due to a bad initial service-worker deployment.Smart CachingAny asset that fails to load during the installation phase will reject the entire caching registration. For example,const PRECACHE_URLS = ['/asset.js','/asset.css'];\n\nself.addEventListener('install', event => {\n\n    event.waitUntil(\n\n        caches.open(VERSION)\n\n            .then(cache => cache.addAll(PRECACHE_URLS))\n\n            .then(self.skipWaiting())\n\n    );\n\n});If any one of cache.addAll(PRECACHE_URLS) fails, then its a rejection and none of them will be cached.Do not cache certain pages to avoid weird behavior. For example, if you are caching your login page and if the user goes offline after logging in and the client side logic redirects to /login on network failure, the user will be presented the login page offline (not ideal user experience)Do not cache 3xx redirect response pages. Do not cache bad responses 4xx or 5xx. All of these will result in unintended side effects and will be bad UX.Pre-deployment strategyAssuming you haven\u2019t deployed service worker before, to avoid all the above headaches and horror stories, you need to make sure that requests to service-worker.js is not cached.Use the following cache-control policy for service-worker.js file.cache-control: max-age=0,no-cache,no-store,must-revalidateThis response header can be set inServer side nginx location directiveslocation ~* (service-worker\\.js)$ {    \n    add_header 'Cache-Control' 'no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0';    \n    expires off;    \n    proxy_no_cache 1;\n}Server side express.js response headersres.setHeader('Cache-Control', 'no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0');and if you are using a CDN, apply the setting to use origin cache headers.A banner showing that \u201cYou are currently offline\u201d message should also be implemented so that users are aware whenever they are browsing your app offline.Here\u2019s a simple offline message example by Google Chrome Devs that can be implemented in your service worker.Always keep clearing old cached versions of your assets to free up memory for your users and also to not cross storage limits. Addy Osmani has a good post about storage limits for different browsers/devices in a PWA app context.Post deployment strategyOk, I have already deployed my service worker (knowingly/unknowingly) and it seems to be broken. What do I do now?Kill SwitchThere is a W3C spec with clear-site-data header that can be used as a kill switch to clear your site cookies, cache etc. You can add this header and deploy to clear all.Clear-Site-Data: cookies, cacheThe goal of the kill switch is to terminate and deregister service workers registered for an origin. You will still need to wait for expiry of the HTTP cache header (~24h if nothing is set) for the original cache invalidation. You can add this header in the same way we discussed above (nginx directive or express.js response headers).Unregister service workerYou can remove all registered service workers in your next update by calling the unregister() method. The following piece of code will be inside your service worker: navigator.serviceWorker.getRegistrations().then(function(registrations) {\n for(let registration of registrations) {\n  registration.unregister()\n} })You can also have a service worker check for a newer version and do a force update. Don\u2019t forget to apply the pre-deployment strategies mentioned above.Problems aside, service workers are still a good way to manage cache for apps if used smartly. There are a lot of apps running in production with service workers enabled. If you are using Chrome, you can use this internal URL to look at the apps you visited that are running service worker.chrome://serviceworker-internals/\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: GKE vs AKS vs EKS Description: \nThe state of managed Kubernetes by industry\u2019s top cloud providers.\n Author: Tirumarai Selvan"}, {"role": "assistant", "content": "The state of managed Kubernetes by industry\u2019s top cloud providers.Update: This post was last updated on June 15, 2018.In this post, we will try to keep up-to-date information on Google Container Engine (GKE) by Google Cloud Platform, Azure Container Service (AKS) by Microsoft Azure and Elastic Container Service (EKS) by Amazon Web Services.Here is a table for quick reference: Though this is non-exhaustive, it covers what is required for most use-cases.PS: Hasura has been running Kubernetes clusters big (~100nodes) and small on Azure, AWS & GCP for few years and are dedicated to being vendor agnostic. Our dream come true would be GKE\u2019s onboarding and Azure\u2019s resource group based IAM + billing.Now, lets explore some of the main aspects of using these platforms:OnboardingCreating a Kubernetes cluster is a straightforward experience with both GKE and AKS. Using their native CLI, you can create, upgrade or delete clusters with a single command. Definitely, a very simple and smooth process to get started. For more advanced setups, GKE provides a fairly comprehensive setup through their UI as well. AKS seems to have kept their UI very minimal and doesn\u2019t offer anything different from their CLI.EKS does not have a one page onboarding experience like the others. The process has multiple steps and needs configuration beyond the web console (like connecting the worker nodes to the cluster via kubectl). This is clearly a bottleneck if you are in the business of creating and deleting clusters on demand. But respite is not very far away with the community building tools like \u2018eksctl\u2019 that simplify the provisioning process considerably.With regards to available Kubernetes versions, GKE and EKS are providing Kubernetes v1.10 which is the latest stable upstream version. GKE also offers K8s in alpha mode, which is a fully enabled (aka alpha features included) Kubernetes package. AKS is not very far off, just one major version behind: v1.9.From our usage experience we found GKE provisioning to be the fastest, followed by AKS and EKS respectively. Typically, creation took ~2 mins on GKE, ~5 mins on AKS and ~15 mins on EKS.Post provisioning, all the vendors provide full access to the clusters using kubectl.AvailabilityAvailability is one thing that is very crucial if you are running business critical applications on Kubernetes.GKE provides high availability of their clusters in two modes: multi-zonal and regional. In multi-zonal mode, there is only one master node but there can be worker nodes in different zones. In regional mode, the master nodes are also spread across all the zones to provide even better HA.AKS does not have high availability for their master nodes, as of date. The worker nodes are part of Availability Zones so they provide HA\u00a0. We are not sure what Azure\u2019s roadmap looks like regarding complete HA in the future.EKS also provides HA master and worker nodes spread across multiple availability zones very similar to GKE\u2019s regional mode.ScalabilityGKE, AKS, and EKS all provide you the ability to scale up nodes very easily, just by using the UI.GKE and EKS provide further customisation in their ability to scale up. Unlike AKS, where you can only scale up to similar nodes, GKE and EKS provide the ability to add different \u2018nodepools\u2019 (or \u2018nodegroups\u2019) which allows different machine types to join the worker pool. Whereas in GKE addition of a nodepool is a single step process, in EKS you need to additionally connect the new nodepool to the cluster manually.Also in GKE, you can configure your cluster to use the Cluster Autoscaler automatically which will scale your nodes up or down based on the required workload. This is great if you repeatedly run short-lived processes like batch jobs. Note that although both AKS and EKS have their worker nodes running as \u201cauto-scaling\u201d groups, the auto-scaling policies are not Kubernetes aware. The Cluster Autoscaler can also be setup for these providers manually.Add-onsThere are many add-on features that enhance the usability of a Kubernetes cluster. The availability of these features as turn-key services can differentiate these providers more saliently.For instance, all three providers have the option of running K8s on GPU powered nodes. This enables heavy machine-learning/image processing frameworks like TensorFlow to leverage their full potential.GKE provides support for Calico as its network plugin which enables Network Policies to be defined for inter-pod communication. AKS supports Network Policies through kube-router project which has to be installed manually. EKS also provides Calico integration though it has to be setup manually. Network policies are crucial for securing the platform especially in a multi-tenant environment.AKS offers a new and incubating feature on Kubernetes called Service Catalog to its users. With Service Catalog, a client can request for Azure services which are then bound and the credentials provided to an application running on the cluster.AKS also supports integration with the virtual-kubelet project. With the virtual-kubelet project, pods can be backed by the provider\u2019s CaaS services (in this case, ACI) instead of VMs. This enables a \u201cserverless\u201d experience for workloads on Kubernetes. Along the same lines, EKS is also working on providing integration with its own CaaS: Fargate.We foresee many more features from upstream Kubernetes and incubators to make its way to these managed providers.PricingBoth GKE and AKS charge you only for the infrastructure that is visible to you. This means the master nodes, cluster management and other services are free of cost.EKS differs here from the others as it charges for the master nodes as well. With 0.20 dollars per hour for the master nodes (combined), EKS is considerably costlier.Clearly, even from a pricing perspective, if you want to run a K8s cluster on these providers it makes sense to use their managed service instead of deploying it yourself. Of course, having a self-deployed cluster might be inevitable for few custom cases.Here is a simple pricing table for a 8 node (1 core, 3.5gb) cluster:  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n \u2551           \u2551 Short-term (100 hrs) \u2551 Long-term (3yrs committed)\u2551\n \u2551           \u2551 /per month           \u2551 /per month                \u2551\n \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n \u2551    GKE    \u2551        40$           \u2551        125$               \u2551\n \u2551 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 -\u2551\n \u2551    AKS    \u2551        60$           \u2551        150$               \u2551 \n \u2551 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 -\u2551\n \u2551    EKS    \u2551 50$ + 20$* (* master)\u2551 150$ + 144$* (* master)   \u2551\n \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255dEndnoteIt\u2019s very exciting to see big cloud providers getting into the space of managed Kubernetes. These clusters come with cluster management, high availability, scalability, automation and many other plugin services. The Kubernetes ecosystem is evolving with great speed with such platforms competing with each other and providing solid value to end users.Do you have any other criteria which you\u2019d like to see in this qualitative comparison of Kubernetes offerings? Please let me know in the comments below!\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How to Write Dockerfiles for Python Web Apps Description: \nStep by step guide to writing highly optimized Dockerfiles for Python web app development and deployment.\n Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRThis post is filled with examples ranging from a simple Dockerfile to multistage production builds for Python apps. Here\u2019s a quick summary of what this guide covers:Using an appropriate base image (debian for dev, alpine for production).Using gunicorn for hot reloading during development.Optimising for Docker cache layers\u200a\u2014\u200aplacing commands in the right order so that pip install is executed only when necessary.Serving static files (bundles generated via React/Vue/Angular) using flask static and template folders.Using multi-stage alpine build to reduce final image size for production.#Bonus\u200a\u2014\u200aUsing gunicorn\u2019s --reload and --reload_extra_files for watching on changes to files (html, css and js included) during development.If you\u2019d like to jump right ahead to the code, check out the GitHub repo.ContentsSimple Dockerfile and\u00a0.dockerignoreHot Reloading with gunicornRunning a single python scriptServing Static FilesSingle Stage Production BuildMulti Stage Production BuildLet\u2019s assume a simple directory structure. The application is called python-app. The top level directory has a Dockerfile and an src folder.The source code of your python app will be in thesrc folder. It also contains the app dependencies in requirements.txt file. For brevity, let\u2019s assume that server.py defines a flask server running on port 8080.python-app\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 server.py\n    \u2514\u2500\u2500 requirements.txt1. Simple Dockerfile ExampleFor the base image, we have used the latest version python:3.6During image build, docker takes all files in the context directory. To increase the docker build\u2019s performance, exclude files and directories by adding a\u00a0.dockerignore file to the context directory.Typically, your\u00a0.dockerignore file should be:.git\n__pycache__\n*.pyc\n*.pyo\n*.pyd\n.Python\nenvBuild and run this image:$ cd python-docker\n$ docker build -t python-docker-dev .\n$ docker run --rm -it -p 8080:8080 python-docker-devThe app will be available at http://localhost:8080. Use Ctrl+C to quit.Now let\u2019s say you want this to work every time you change your code. i.e. local development. Then you would mount the source code files into the container for starting and stopping the python server. $ docker run --rm -it -p 8080:8080 -v $(pwd):/app \\\n             python-docker-dev bash\nroot@id:/app# python src/server.py2. Hot Reloading with\u00a0Gunicorngunicorn is a Python WSGI HTTP Server for UNIX and a pre-fork worker model. You can configure gunicorn to make use of multiple options. You can pass on --reload to the gunicorn command or place it in the configuration file. If any files change, gunicorn will automatically restart your python server.We\u2019ll build the image and run gunicorn so that the code is rebuilt whenever there is any change inside the app directory.$ cd python-docker\n$ docker build -t python-hot-reload-docker .\n$ docker run --rm -it -p 8080:8080 -v $(pwd):/app \\\n             python-hot-reload-docker bash\nroot@id:/app# gunicorn --config ./gunicorn_app/conf/gunicorn_config.py gunicorn_app:appAll edits to python files in theappdirectory will trigger a rebuild and changes will be available live at http://localhost:8080. Note that we have mounted the files into the container so that gunicorn can actually work.What about other files? If you have other types of files (templates, views etc) that you want gunicorn to watch for code changes, you can specify the file types in reload_extra_files argument. It accepts an array of files.3. Running a single python\u00a0scriptFor simple single file scripts, you can run the python script using the python image with docker run.docker run -it --rm --name single-python-script -v \"$PWD\":/app -w /app python:3 python your-daemon-or-script.pyYou can also pass some arguments to your python script. In the above example, we have mounted the current working directory, which also allows files in that directory to be passed as arguments.4. Serving Static\u00a0FilesThe above Dockerfile assumed that you are running an API server with Python. Let\u2019s say you want to serve your React.js/Vue.js/Angular.js app using Python. Flask provides a quick way to render static files. Your html should be present inside the templates folder and your css, js, images should be present inside the static folder.Check out a sample hello world static app structure here.In your server.py,if __name__ == '__main__':\n    app.run(host='0.0.0.0')Note the host, 0.0.0.0\u200a\u2014\u200athis allows your container to be accessible from outside. By default, if you don\u2019t give a host, it binds only to the localhost interface.5. Single Stage Production BuildBuild and run the all-in-one image:$ cd python-docker\n$ docker build -t python-docker-prod .\n$ docker run --rm -it -p 8080:8080 python-docker-prodThe image built will be ~700MB (depending on your source code), due to the underlying Debian layer. Let\u2019s see how we can cut this down.6. Multi Stage Production BuildWith multi stage builds, you use multiple FROM statements in your Dockerfile but the final build stage will be the one used, which will ideally be a tiny production image with only the exact dependencies required for a production server.This will be really useful when you are using system dependent modules or ones that requires compiling etc. pycrypto, numpy are good examples of this type.With the above, the image built with Alpine comes to around ~90MB, a 8X reduction in size. The alpine variant is usually a very safe choice to reduce image sizes.Note: All of the above Dockerfiles were written with python 3\u200a\u2014\u200athe same can be replicated for python 2 with little to no changes. If you are looking to deploy your django app, you should be able to deploy a production ready app with minimal tweaking to the Dockerfiles above.Any suggestions to improve the ideas above? Any other use-cases that you\u2019d like to see? Do let me know in the comments.Join the discussion on Reddit and HackerNews\u00a0:)\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Getting started with HDFS on Kubernetes Description: \nMy experience running an HDFS proof-of-concept on Kubernetes\n Author: Tirumarai Selvan"}, {"role": "assistant", "content": "My experience running a proof-of-concept of HDFS on KubernetesContentsBasic architecture of HDFSArchitecture of HDFS on KubernetesWrap namenode in a ServiceIdentify datanodes through Stateful SetsRun fully distributed HDFS on single nodeNext: Apache Spark on HDFSIf you are a Kubernetes expert, then you can jump straight to the source code here.Basic architecture of\u00a0HDFSLet\u2019s begin by recapping the traditional architecture of a completely distributed HDFS. The block diagram below illustrates this architecture:Traditional HDFS architectureTypically, there are dedicated VMs for running each HDFS daemon viz. namenodes and datanodes (we will use daemons and the nodes running the daemons interchangeably). Namenodes store the metadata of each file and datanodes store the actual data. There is one namenode (sometimes two for high availability but we will ignore it for our PoC) and multiple datanodes.Each daemon communicates with the other using the host VM\u2019s network. This is an important point that will affect the way we run this on Kubernetes.Architecture of HDFS on KubernetesNow, lets understand how a typical application looks like on Kubernetes ( also known as k8s ).Kubernetes is a container manager for a cluster of nodes. This means it deploys containers and manages their lifecycle on a cluster. A specific resource kind in Kubernetes specifies how a container should behave: should it be a long-running or batch process, should there be a single instance or multiple replicas, etc. A \u2018pod\u2019 is the simplest resource kind which is basically a single instance of few tightly coupled long-running containers.So how does our application, a complex consortium of different processes interacting with each other, look like on Kubernetes?At first glance, it seems simple. The different daemon processes will run in different pods and use their pod IPs to interact with each other. This looks something like below:HDFS on Kubernetes architectureNot so fast! Do you hard-code the namenode pod IP into your datanodes? What if the namenode pod goes down and comes back up on a different machine (changing its IP)? Same issue with datanode. How can they interact with each other if they keep changing IPs, machines and whatever else happens under Kubernetes management?So we have the following issues:Namenode and datanodes can go down and come back up with a different pod IP. How do they keep interacting henceforth?Namenode and datanodes can go down and come back up in a different machine. What happens to the data they were storing?Wrap namenode in a\u00a0ServiceThe way k8s solves the problem of ephemeral nature of pods is to use a Service resource. A Kubernetes Service basically gives you a static IP/hostname in the cluster which load-balances incoming requests across selected pods. The pods are selected based on labels that are injected in the pod definition. So we can give our namenode pod a label say \u2018app: namenode\u2019 and create a service which selects pods with that label.This gives us a static way to reach our namenode. The Datanodes are now happy. For e.g. datanodes can refer to the namenodes with Service hostname in their spec:Datanodes can use K8s Service\u00a0hostnameA HDFS client in the cluster can also use this hostname:Use HDFS client inside the\u00a0clusterIdentify datanodes through Stateful\u00a0SetsNow that we have seen a way to communicate with namenode regardless of pod behaviour, can we do the same for datanodes?Unfortunately, no. A fundamental aspect of a Kubernetes Service is that it load-balances randomly across its selected pods. This means all the pods selected by the Service should be indistinguishable for the application to behave properly. This is not the case with our datanodes wherein each datanode has a different state (different data). Datanodes seem more like a case of \u201cpets\u201d not \u201ccattle\u201d.Stateful applications such as this are quite common and hence Kubernetes provides another resource kind called Stateful Sets to help such applications. In a Stateful Set, each pod gets a sticky identity to its name, its storage and its hostname. Voila! Just what we needed. If each of our datanode has its sticky identity always present (even across pod rescheduling) then we can still be assured of stable communication and consistent data across the application.In fact, we can make our namenode also stateful if we wished. Note that for all this to work, we need to configure our HDFS to use hostnames instead of IPs for communication. This is the default setting with the Docker images we are using.Run fully distributed HDFS on single\u00a0nodeAre we done? Almost. Why not make all this work in a single node? But isn\u2019t that contrary to the concept of \u201cdistributed file\u201d system? Well, in the Kubernetes world, the notion of distributed is at the container level. So, if we have multiple pods, managing their own dedicated disk, running on a single node; it\u2019s distributed!But for this PoC, we will do another trick. We will emulate multiple storage volumes on a single disk and attach them to our different pods (in more serious settings you will have separate disks backing each volume, but it\u2019s a matter of simple configuration). Kubernetes Persistent Volume (PV) resource kinds are perfect for this. Kubernetes PVs are a set of storage volumes available for consumption in your cluster. A pod can ask for a PV and it will be mounted in the pod.We use a PV of type `hostPath` to use the underlying VMs local directory as its storage. And, we can create arbitrary number of them with a single disk with different `hostPath` values.Persistent Volume on `hostPath`And that\u2019s it! We have a fully distributed HDFS running on a single node. This is what our final architecture looks like:Single node fully distributed architectureNext: Apache Spark on\u00a0HDFSA HDFS by itself is not of much use. In our follow-up blogpost, we will show how to deploy Apache Spark on Kubernetes to process data stored in our new k8s HDFS.Stay tuned!If you want to deep dive into any aspect of this post or want to give your suggestions, feel free to hit the comments section below.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: An Exhaustive Guide to Writing Dockerfiles for Node.js Web Apps Description: \nStep by step guide to writing highly optimized Dockerfiles for Node.js web app development and deployment.\n Author: Praveen Durairaju"}, {"role": "assistant", "content": "TL;DRThis post is filled with examples ranging from a simple Dockerfile to multistage production builds for Node.js web apps. Here\u2019s a quick summary of what this guide covers:Using an appropriate base image (carbon for dev, alpine for production).Using nodemon for hot reloading during development.Optimising for Docker cache layers\u200a\u2014\u200aplacing commands in the right order so that npm install is executed only when necessary.Serving static files (bundles generated via React/Vue/Angular) using serve package.Using multi-stage alpine build to reduce final image size for production.#ProTips\u200a\u2014\u200a1) Using COPY over ADD 2) Handling CTRL-C Kernel Signals using init flag.If you\u2019d like to jump right ahead to the code, check out the GitHub repo.ContentsSimple Dockerfile and\u00a0.dockerignoreHot Reloading with nodemonOptimisationsServing Static FilesSingle Stage Production BuildMulti Stage Production BuildLet\u2019s assume a simple directory structure. The application is called node-app. The top level directory has a Dockerfileand package.json The source code of your node app will be in src folder. For brevity, let\u2019s assume that server.js defines a node express server running on port 8080.node-app\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 server.js1. Simple Dockerfile ExampleFor the base image, we have used the latest LTS versionnode:carbonDuring image build, docker takes all files in the context directory. To increase the docker build\u2019s performance, exclude files and directories by adding a\u00a0.dockerignore file to the context directory.Typically, your\u00a0.dockerignore file should be:.git\nnode_modules\nnpm-debugBuild and run this image:$ cd node-docker\n$ docker build -t node-docker-dev .\n$ docker run --rm -it -p 8080:8080 node-docker-devThe app will be available at http://localhost:8080. Use Ctrl+C to quit.Now let\u2019s say you want this to work every time you change your code. i.e local development. Then you would mount the source code files into the container for starting and stopping the node server. $ docker run --rm -it -p 8080:8080 -v $(pwd):/app \\\n             node-docker-dev bash\nroot@id:/app# node src/server.js2. Hot Reloading with\u00a0Nodemonnodemon is a popular package which will watch the files in the directory in which it was started. If any files change, nodemon will automatically restart your node application.We\u2019ll build the image and run nodemon so that the code is rebuilt whenever there is any change inside the app directory.$ cd node-docker\n$ docker build -t node-hot-reload-docker .\n$ docker run --rm -it -p 8080:8080 -v $(pwd):/app \\\n             node-hot-reload-docker bash\nroot@id:/app# nodemon src/server.jsAll edits in theappdirectory will trigger a rebuild and changes will be available live at http://localhost:8080. Note that we have mounted the files into the container so that nodemon can actually work.3. OptimisationsIn your Dockerfile, prefer COPY over ADD unless you are trying to add auto-extracting tar files, according to Docker\u2019s best practices.Bypass package.json \u2018s start command and bake it directly into the image itself. So instead of$ CMD [\"npm\",\"start\"]you would use something like$ CMD [\"node\",\"server.js\"]in your Dockerfile CMD. This reduces the number of processes running inside the container and it also causes exit signals such as SIGTERM and SIGINT to be received by the Node.js process instead of npm swallowing them. (Reference\u200a\u2014\u200aNode.js Docker Best Practices)You can also use the --init flag to wrap your Node.js process with a lightweight init system, which will respond to Kernel Signals like SIGTERM (CTRL-C) etc. For example, you can do:$ docker run --rm -it --init -p 8080:8080 -v $(pwd):/app \\\n             node-docker-dev bash4. Serving Static\u00a0FilesThe above Dockerfile assumed that you are running an API server with Node.js. Let\u2019s say you want to serve your React.js/Vue.js/Angular.js app using Node.js.As you can see above, we are using the npm package serve to serve static files. Assuming you are building a UI app using React/Vue/Angular, you would ideally build your final bundle using npm run build which would generate a minified JS and CSS file.The other alternative is to either 1) build the files locally and use an nginx docker to serve these static files or 2) via a CI/CD pipleline.5. Single Stage Production BuildBuild and run the all-in-one image:$ cd node-docker\n$ docker build -t node-docker-prod .\n$ docker run --rm -it -p 8080:8080 node-docker-prodThe image built will be ~700MB (depending on your source code), due to the underlying Debian layer. Let\u2019s see how we can cut this down.6. Multi Stage Production BuildWith multi stage builds, you use multiple FROM statements in your Dockerfile but the final build stage will be the one used, which will ideally be a tiny production image with only the exact dependencies required for a production server.With the above, the image built with Alpine comes to around ~70MB, a 10X reduction in size. The alpine variant is usually a very safe choice to reduce image sizes.Any suggestions to improve the ideas above? Any other use-cases that you\u2019d like to see? Do let me know in the comments.Join the discussion on Reddit / HackerNews\u00a0:)\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: The Ultimate Guide to Writing Dockerfiles for Go Web-apps Description: \nStep by step guide to build highly optimized docker images for Golang web-app development and deployment\n Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "You probably want to use Docker with Go, because:Packaging as a container is required if you\u2019re running it on Kubernetes (like me!)You have to work with different versions of Go on the same machine.You need exact, reproducible, shareable and deterministic environments for development as well as production.You need a quick and easy way of building and deploying a final compiled binary.You might want to get started quickly (anyone with Docker installed can start coding right away without setting up any other dependencies or GOPATH variables).Well, you\u2019ve come to the right place.We\u2019ll incrementally build a basic Dockerfile for Go, with live reloading and package management, and then extend the same to create a highly optimized production ready image with ~100x reduction in size. If you use a CI/CD system, image size might not matter, but when docker push and docker pulls are involved, a leaner image will definitely help.If you\u2019d like to jump right ahead to the code, check out the GitHub repo:shahidhk/go-dockergo-docker - Sample code and dockerfiles accompanying the blog post The Ultimate Guide to Writing Dockerfiles for Go\u2026github.comDockerfileContentsThe Simplest OnePackage Management & LayeringLive ReloadingSingle Stage Production BuildMulti Stage Production BuildBonus: Binary Compression using UPX[Update] Dep instead of Glide[Update] Scratch instead of AlpineLet\u2019s assume a simple directory structure. The application is called go-docker and the directory structure is as shown below. All source code is inside src directory and there is a Dockerfile at the same level. main.go defines a web-app listening on port 8080.go-docker\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 main.go1. The Simplest\u00a0OneWe are using debian jessie here since some commands like go get require git etc. to be present. Also, all Debian packages are available in case we need them. For production version we\u2019ll use a smaller image like alpine.Build and run this image:$ cd go-docker\n$ docker build -t go-docker-dev .\n$ docker run --rm -it -p 8080:8080 go-docker-devThe app will be available at http://localhost:8080. Use Ctrl+C to quit.But this doesn\u2019t make much sense because we\u2019ll have to build and run the docker image every time any change is made to the code.A better version would be to mount the source code into a docker container so that the environment is contained and using a shell inside the container to stop and start go run as we wish.$ cd go-docker\n$ docker build -t go-docker-dev .\n$ docker run --rm -it -p 8080:8080 -v $(pwd):/go/src/app \\\n             go-docker-dev bash\nroot@id:/go/src/app# go run src/main.goThese commands will give us a shell, where we can execute go run src/main.go and run the server. We can edit main.go from host machine and run the code again to see changes, as the the files are mounted directly into the container.But, what about packages?2. Package Management &\u00a0LayeringPackage management in Go is still in an experimental stage. There are a couple of tools around, but my favorite is Glide. We\u2019ll install Glide inside the container and use it from within.Create two files called glide.yaml and glide.lock inside go-docker directory:$ cd go-docker\n$ touch glide.yaml\n$ touch glide.lockChange the Dockerfile to the one below and build a new image.If you look closely, you can see that glide.yaml and glide.lock are being added separately (instead of doing a ADD\u00a0.\u00a0.), resulting in separate layers. By separating out package management to a separate layer, Docker will cache the layer and will only rebuild it if the corresponding files change, i.e. when a new package is added or an existing one is removed. Hence, glide install won\u2019t be executed for every source code change.Let\u2019s install a package by getting into the container\u2019s shell:$ cd go-docker\n$ docker build -t go-docker-dev .\n$ docker run --rm -it -v $(pwd):/go/src/app go-docker-dev bash\n\nroot@id:/go/src/app# glide get github.com/golang/glogGlide will install all packages into a vendor directory, which can be gitignore-d and dockerignore-d. It uses glide.lock to lock packages to specific versions. To (re-)install all packages mentioned in glide.yaml, execute:$ cd go-docker\n$ docker run --rm -it -p 8080:8080 -v $(pwd):/go/src/app \\\n             go-docker-dev bash\n\nroot@id:/go/src/app# glide installThe go-docker directory has grown a little bit now:\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 glide.lock\n\u251c\u2500\u2500 glide.yaml\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 main.go\n\u2514\u2500\u2500 vendor/Don\u2019t forget to add vendor to\u00a0.gitignore and\u00a0.dockerignore.3. Live Reloadingcodegangsta/gin is my favorite among all the live-reloading tools. It is specifically built for Go web servers. We\u2019ll install gin using go get:We\u2019ll build the image and run gin so that the code is rebuilt whenever there is any change inside src directory.$ cd go-docker\n$ docker build -t go-docker-dev .\n$ docker run --rm -it -p 8080:8080 -v $(pwd):/go/src/app \\\n             go-docker-dev bash\n\nroot@id:/go/src/app# gin --path src --port 8080 run main.goNote that the web-server should take a PORT environment variable to listen to since gin will set a random PORT variable and proxy connections to it.All edits in src directory will trigger a rebuild and changes will be available live at http://localhost:8080.Once we are done with development, we can build the binary and run it, instead of using the go run command. The binary can be built and served using the same image or we can make use of Docker multi-stage builds to build using a golang image and serve using a bare minimum linux container like alpine.4. Single Stage Production BuildBuild and run the all-in-one image:$ cd go-docker\n$ docker build -t go-docker-prod .\n$ docker run --rm -it -p 8080:8080 go-docker-prod\nThe image built will be ~750MB (depending on your source code), due to the underlying Debian layer. Let\u2019s see how we can cut this down.5. Multi Stage Production BuildMulti stage builds lets you build programs in a full-fledged OS environment, but the final binary can be run from a very slim image which is only slightly larger than the binary itself.The binary here is ~14MB and the docker image is ~18MB. Thanks to alpine awesomeness.Want to cut down the binary size itself? Read ahead.6. Bonus: Binary Compression using\u00a0UPXAt Hasura, we have been using UPX everywhere, our CLI tool binary which is ~50MB comes down to ~8MB after compression, making it easy to download. UPX can do extremely fast in-place decompression, without any extra tools since it injects the decompressor into the binary itself.The UPX compressed binary is ~3MB and the docker image is ~6MB.~100x reduction in size from where we started from.7. Dep instead of\u00a0Glidedep is a prototype dependency management tool for Go. Glide is considered to be in a state of support rather than active feature development, in favour of dep. Executing dep init in a directory with glide.yaml and glide.lock will create Gopkg.toml and Gopkg.lock by reading the glide files.Adding a new package using dep is similar to glide:$ dep ensure -add github.com/sirupsen/logrusglide install equivalent is dep ensure.8. Scratch instead of\u00a0AlpineAlpine is useful when you have to quickly access the shell inside the container and do some debugging. For example, shell comes to the rescue while debugging DNS issues in a Kubernetes cluster. We can run ping/wget etc. Also, if your application makes API calls to external services over HTTPS, ca-certificates need to be present.But, if you don\u2019t need a shell or ca-certs, but just want to run the binary, you can use scratch as the base for the image in multi-stage build.The resulting image is just 1.3 MB, compared to the 6MB apline image.Any suggestions to improve the ideas above? Any other use-cases that you\u2019d like to see? Do let me know in the comments or join the discussion on HackerNews & Reddit.Update (24th Feb 2018): Added sections 7 and 8.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using minikube as a \u201cdocker-machine\u201d to avoid sharing a local-registry Description: \nHow do I get a locally built docker image into minikube with the least amount of pain, and the greatest amount of speed?\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "How do I get a locally built docker image into minikube with the least amount of pain, and the greatest amount of\u00a0speed?If you\u2019re developing with minikube, you\u2019d frequently need to get docker images you build locally inside minikube so that your kubernetes deployments etc. work well.TL;DR:Run: eval $(minikube docker-env) and use docker to build images. Docker images will be built inside (and will hence be available inside) minikube.In an earlier guide, https://hasura.io/blog/sharing-a-local-registry-for-minikube-37c7240d0615 I talked about how you can run a local registry on minikube and integrate your normal docker workflow with it.Pros of running docker-registry inside minikube: Docker images are built outside minikube (on your local filesystem, or inside docker-machine)Cons of running docker-registry inside minikube: You need to setup atleast one port-forward and keep it running, and 2 if you\u2019re on a non-linux systemThere is an alternative approach of getting your files inside minikube without having to run a local registry at all.Instead of building images outside minikube and then getting them inside minikube:docker build some-image          #this happens on your local docker\ndocker push minikube/some-image  #you push the image to minikubeyou can:docker build some-image         #this is happening inside minikube!Before:The docker build runs on the \u201clocal\u201d docker daemon and the kubernetes \u201cdocker pull\u201d on a different daemon.After:They can just be the same docker-daemon and we\u2019ll save many\u00a0kittens!Point your docker client to run against the docker-daemon inside minikube:Once you\u2019ve created a minikube cluster with minikube start. This is a one-liner:$> eval $(minikube docker-env)And that\u2019s it. Your local docker is now using the docker daemon inside minikube.$> docker build -t my-image .   #Build a docker image locally\n$> minikube ssh\n$> docker images my-image       #You'll see the docker image hereSummaryQuick and dirty: eval $(minikube docker-env)The longer way, safer if you\u2019re re-creating minikube clusters often: https://hasura.io/blog/sharing-a-local-registry-for-minikube-37c7240d0615#47b3\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using git commands on Windows Description: \nUsing git commands are easy for people having Linux or Mac. For a windows user though, you will need to install something like Git-bash.\n Author: Arpit Kushwaha"}, {"role": "assistant", "content": "Using git commands are easy for people having Linux or Mac. For a windows user though, you will need to install something like Git-bash.Git-bash is a Linux Command Line Interface (CLI) which works in Windows, allowing the use of Linux commands while working in Windows\u200a\u2014\u200ano need to format your PC and install Linux, etc.It\u2019s really easy and simple to install git-bash for windows, just follow the steps in the video below.\u00a0:)Git-bash can be used to create SSH keys which can be used later in various applications. All you need to do is simply execute the command-\u2018ssh-keygen -t rsa\u2019 and keep hitting Enter at each step when it asks for an input. Doing so will create SSH keys at \u2018~/.ssh/\u2019P.S\u200a\u2014\u200aThis video is for the 64-bit version of Windows 8.1.\n\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Sharing a local registry with minikube Description: \nHow do I get a locally built docker image into minikube with the least amount of pain, and the greatest amount of speed?\n Author: Tanmai Gopal"}, {"role": "assistant", "content": "How do I get a locally built docker image into minikube with the least amount of pain, and the greatest amount of\u00a0speed?If you\u2019re developing with minikube you need to use containers. Containers need images, and during development you may want to frequently modify images. To complete your development workflow you need to get the images you build into minikube so that kubernetes can run containers/pods built off the image.The problem on Linux\u00a0hostsThe problem on non-linux hosts (docker-machine is another\u00a0VM)For the rest of this post, we\u2019ll use the more complicated case of having docker-machine to get our system running. I mean we entered the rabbit hole when we decided to roll with docker anyway, so why start making things easy now.Workflow #1: Use docker-hubPush to docker hub, pull from docker hub. Kittens will\u00a0die.But:This will result in way too many image tags on your docker hub repoAlso, uploading and downloading hundreds of floppy disks worth of data through ze internetz to just get software from one part of your computer to another doesn\u2019t sound right.There was a time you know. There was a time when all this was\u00a0enough.Workflow #2: Use a local\u00a0registryThis will totally not work the first time. Or the second. Or the third in\u00a0fact.Both docker push and kubectl run will fail because the registry is insecure.First, let\u2019s look at: docker push 192.168.99.100:5000/my-imageWill fail because 192.168.99.100 is not a secure registryUse SSL certs and restart the docker daemon to get this to work, or set this as an insecure registry and restart the docker daemon to get this to work.The push command will change every time docker-machine\u2019s ip changes (potentially whenever docker-machine starts)Second, let\u2019s look at: kubectl run --image=192.168.99.100:5000/my-imageWill fail to pull because 192.168.99.100 is not a secure repositoryUse SSL certs, or set this as an insecure registry and restart minikube (this is a pain to get working on minikube)This command will also change every time the docker-machine ip changesBreak-through leading to the final solution:docker doesn\u2019t complain about pushing to or pulling fromlocalhost:5000 even if that\u2019s an insecure registry. Ooooh.So: docker push should push to a registry named localhost:5000 and inside minikube, docker pull should pull from a registry named localhost:5000.Let the port-mapping shenanigans begin!Final workflow: Use localhost:5000 as the\u00a0registryNot sphagetti\u2026just one long noodle\u00a0really.The idea here is for the docker daemon on minikube to be able to pull from a registry called localhost:5000. This is achieved by actually running a registry on minikube and then setting up a proxy so that the minikube VM port 5000 maps to the registry\u2019s 5000.Now, we need to make sure that the docker daemon on docker-machine thinks that localhost:5000 is legit. So, we map 5000 of the docker-machine \u21e2 5000 of the host (via a reverse SSH tunnel) and 5000 of the host \u21e2 5000 of the registry running on minikube (via kubectl port-forward). Ofcourse, if you\u2019re one of the enlightened, you\u2019re already running a linux host and of your 99 problems this ain\u2019t one.So here are all the steps finally:1. Create a registry on\u00a0minikubeCreate a registry (a replication-controller and a service) and create a proxy to make sure the minikube VM\u2019s 5000 is proxied to the registry service\u2019s 5000.kubectl create -f kube-registry.yaml(Grab kube-registry.yaml from this gist on github.)At this point, minikube ssh && curl localhost:5000 should work and give you a response from the docker registry.2. Map the host port 5000 to minikube registry\u00a0podkubectl port-forward --namespace kube-system \\ \n$(kubectl get po -n kube-system | grep kube-registry-v0 | \\awk '{print $1;}') 5000:5000\nAfter this, from the host curl localhost:5000 should return a valid response from the docker registry running on minikube3. Non-linux peoples: Map docker-machine\u2019s 5000 to the host\u2019s\u00a05000.ssh -i ~/.docker/machine/machines/default/id_rsa \\\n-R 5000:localhost:5000 \\docker@$(docker-machine ip)\nAfter this, running docker-machine ssh && curl localhost:5000 should return a valid response from the docker registry running on minikube.All of this put toghether, and finally:\u2026and it\u2019s truly amazing that it somehow\u00a0does.SummaryIt\u2019s not elegant, but it works without fiddling around and restarting docker client, docker daemon, minikube flags, minikube\u2019s config.json. All of which can change any time today, tomorrow or have probably already changed yesterday and you\u2019re on an older version. Hah.Also, just FYI, we\u2019ve spent a spectacular amount of effort in moving code a nanometer distance or something. 2017 y\u2019all.Containers livin\u2019 up to their namesakes.\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Using Kubernetes to power Asia\u2019s largest MOOC Description: \nKubernetes helps MOOC students practice web development and deployment using just a browser.\n Author: Shahidh K Muhammed"}, {"role": "assistant", "content": "Kubernetes helps MOOC students practice web development and deployment using just a\u00a0browser.Last year, we ran a MOOC on application development called IMAD (Introduction to Modern Application Development) which saw over 83,000 registrations.The application designTo begin with, we were sure that asking students to use their machines for programming would not be a good idea. The audience for an online course would have huge variations in terms of hardware, operating systems etc. Providing instructions or guidance for multiple platforms throughout the course would have been an operational nightmare.But, everybody has a web browser, and we decided to make use of it.As our main focus was on developing web applications, an immediate requirement of a platform for students to publish their applications also became apparent.At Hasura, we had been using Kubernetes for deploying production workloads for some time then. Since, we would have needed to provide isolated runtimes for all our students to build and deploy their applications, it became clear to us from the very beginning that Kubernetes would be the way to go.Our plan was to create a web application which lets users write code using a browser and deploy it in real-time. After comparing a few different models, we decided to create a Kubernetes deployment and service for each user, hosting their code on GitHub and running it inside a nodeJS container. Each user would be given a subdomain and all requests were to be reverse proxied to the corresponding Kubernetes service using nginx.The application workflowA user logs in using GitHub and, using GitHub API v3, we fork a template repository into the user\u2019s GitHub account. Files from this repository are then fetched for the user to edit using Ace Editor. All edits made to the files are committed to the user\u2019s repository. We eliminated the need to use our own storage backend for users\u2019 code as well as gave all students, mostly first time developers, a taste of GitHub by doing this. When a user triggers a deployment, our API server creates a deployment and a service on a dedicated Kubernetes cluster. The deployment has a gitRepo volume with the user\u2019s repository, thereby pulling the user\u2019s latest code on to a pre-built nodeJS docker image. An nginx daemonset, running on all nodes, directs incoming HTTP traffic to the user\u2019s containers based on the subdomain, thereby serving their web-app.Overall architecture of the\u00a0systemThe Kubernetes cluster was initially hosted on Microsoft Azure and then migrated to Google Container Engine (effortlessly).The ResultOver the duration of the course, we saw participation from a diverse set of students\u200a\u2014\u200acollege students, working professionals, stay-at-home mothers and educators, often with little or no background in computer science or application development experience. A mind boggling ~10000 apps were built!The students also had a lot of good things to say about their first experience with developing applications (you can read more about it here):None of this would have been easy without Kubernetes (and the Hasura platform, of course).\n"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: How and why we started Hasura Description: \nHasura started because building and deploying an application was more painful than it needed to be. Read on to find out more.\n Author: Hasura"}, {"role": "assistant", "content": "Hasura started because building and deploying an application was more painful than it needed to be. Read on to find out\u00a0more.Originally written on 1st Sep 2016. Last updated on 26th Dec 2019.The story of Hasura starts as all of my best stories do\u200a\u2014\u200aI was pining for some good food. Having just moved to Chennai, I was looking forward to sampling some of the local cuisine. However my schedule left me heavily dependent on existing food delivery networks, none of whom gave any weight to healthy/wholesome/local food; Instead I was lucky enough to discover that Chennai has more pizza/burger joints than New York. Uncompromising person that I am (especially when it comes to my hunger pangs), I decided to take matters into my own hands, and start a food delivery company that would focus exclusively on home cooked food.Tanmai and Rajoshi, co-founders, HasuraThings came to a boil when I met Tanmai (co-founder at Hasura; all-around nice guy), who had just finished his master\u2019s degree in computer science from one of India\u2019s best engineering colleges. Tanmai was on his own search for a business partner, having turned down lucrative offers from both corporates and academia. He was equally uncompromising when it came to food. Clearly, destiny also wants you to have healthy wholesome food.Tanmai expected that he would be able to dive straight into application development and that building the apps themselves would be the easiest part of the whole process\u200a\u2014\u200ahe did have two degrees after all, and had worked on the cutting edge of technology\u200a\u2014\u200acomputer vision, machine learning, the works.That was when Tanmai first met the app development ecosystem. As someone who had spent his entire life discerning patterns, he was baffled by the lack of common standards among the various frontend, backend and deployment frameworks. The only point of agreement was that the process of application development needs to be made easier\u200a\u2014\u200aa contention he shared, particularly in light of the excellent job that the million different frameworks had made in simplifying the same. Tanmai likens the encounter to that fateful day in his childhood when he made the connection between Santa being more svelte than he appeared in all the pictures he had seen, and his dad being a thin person\u200a\u2014\u200aat the time, he was left devastated.Wanting to make the process as simple as possible for his team, he dived headlong into the various frameworks and technology choices, and set about developing a combination of informed judgements and tooling that would streamline the entire effort. His favourite phrase became in my humble opinion, and his favourite acronym IMHO\u200a\u2014\u200ahe was 100% committed.The first outcome of this effort was a platform named procrust.es. In keeping with the elegant simplicity of its mythological namesake, procrust.es was a database platform that combined the ease of use of a noSQL API with the ability to handle transactions. It freed up developers from having to worry about consistency every time there was a database change, so they could instead focus on the more important customer side of the application.The penny finally dropped when several of Chennai\u2019s developers started dropping by our office to enquire about Tanmai\u2019s work, and to ask if it were possible for them to use procrust.es themselves. Realising the value of the work we had done on the technology front, we (very reluctantly I might add\u200a\u2014\u200amy love for food still persists) decided to abandon our food industry revolutionising dreams and switch to make technology the core focus of our company. Procrust.es itself underwent several iterations and eventually evolved into Hasura.Hasura is an open-source GraphQL engine that helps you instantly setup a scalable and realtime GraphQL backend. Hasura makes your team super productive by dynamically composing a schema backed by databases and services that you can securely query from frontend clients.We are now a 25 member strong team based out of Bangalore. Our mission to simplify the app development ecosystem continues apace\u200a\u2014\u200aeach new version of the platform gets us one step closer to that dream. (One Day!)If you are looking to build a new product, or migrate an existing product to a more modern stack, and want to know how Hasura can help do drop us a note at build@hasura.io.\u2014Rajoshi Ghosh,\u00a0Co-founder, Hasura"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: An introduction to containers and Kubernetes Description: A primer on the concepts underlying containers and Kubernetes, and how they enable new patterns for application development. Author: Akshaya Acharya"}, {"role": "assistant", "content": "A primer on the concepts underlying containers and Kubernetes, and how they enable new patterns for application development.The Container is the New\u00a0ProcessIn the beginning there was the computer. The computer, when it started, ran a program called init. The init program then ran every other program that was needed: servers, terminals, and window managers. init could do several interesting things, like starting a program on boot, running a program at regular time intervals, and making sure that programs that it had run had not failed and crashed, whereupon it would restart them. The programs running could see everything on the computer: the other running programs, all files, and the network.Multiple processes running together on a computer. All processes can interact with each other and with common resources freely.Because it gives programmers an easier model to reason about things by compartmentalizing them, tools to create namespaces were developed. Programs, or processes, running in a namespace can see only the other processes in the same namespace. If they look for files, they see only the portion of the hard disk allocated to that namespace. From a security perspective, a compromised process in a namespace can affect only that namespace.Tools like Docker and Rkt were developed to use these features systematically. These tools offer the advantage of packaging up a namespace as a container so that it can be easily moved to a different computer, knowing well that it will continue to run in exactly the same way because of its isolated nature. In fact, it\u2019s often easy to think about containers like tiny computers that run completely independently. Because these new tools are so easy to use, they have become a very popular way to build software.The container is the new process.Processes in containers. Here, a process can only interact with other processes and resources in the same container.Scale: A Good Problem to\u00a0HaveA computer has finite resources, and it can only process so much data and run so many programs at the same time. A simple way to deal with increased load such as more users, or larger data sets is to scale vertically or add more processing power and memory to the computer, but this gets prohibitively expensive very quickly and there is a limit to how far you can go. Another way is to horizontally scale by adding more computers. These computers now form a cluster.Applications need to be built differently to run well on a cluster. For example, if we make sure that two copies of the same program can operate without needing to access the other\u2019s data, then we can confidently run multiple copies of the same program on multiple computers and know nothing bad will happen.Horizontal scaling. In this cluster, three computers run two containers each. There are two instances of the app server container to handle larger\u00a0loads.Although containers themselves do not give us any extra tools to build distributed applications, thinking at this level of abstraction makes it easier to build applications for clusters. The container model encourages the assumptions thatthere may be multiple copies running (build for concurrency),containers may be started and stopped dynamically on any computer in the cluster (prefer statelessness, ephemeral-ism), and thatcomputers or processes may fail or be unavailable at any time but the whole system should still keep working (build for failure and recovery).With so many computers to manage in the cluster, we are presented with some additional challenges.Firstly, we need to manage resources on the computers like processing and storage. This means that we have to efficiently distribute and schedule the processes that consume them across different computers.We also need affinities or ways to run related processes together for efficient sharing of resources; and anti-affinities to make sure that processes that compete for the same resources are not run on the same computer.For instance, if we wanted to run two copies of the application server process to serve twice the number of requests, we would want these processes to run on different computers in the cluster.With many processes running all over the place, we need a way to discover and communicate with all of them. We can communicate with a process if we know the ip address of the computer on which it runs.With a single computer there is only one ip address. With multiple computers, we need to maintain a map of processes to ip addresses, such as in a distributed database like etcd. When a process starts on a computer, we add this information to the database. We also need to remove the entry from the database if the process fails, or the computer goes down.Programmers are already very good at writing applications that run on one computer. Ideally, what we\u2019d like to do is have a tool that takes all the computers in the cluster and presents it to the programmer like one big computer.A step in this direction is Fleet on CoreOS which is basically the idea of the init process on one computer extended to an init for the entire cluster.The Kubernetes project, introduced by Google, brings us closer to the model we want of having one giant computer.Kubernetes: The Pod is the New\u00a0ComputerThe first thing Kubernetes does is take away your computers and give you back a single giant computer as a Kubernetes cluster.A Kubernetes pod specifies a set of Docker or Rkt containers to run.Now, instead of interacting with different processes running on different computers in the cluster, we can see different processes running in different pods in the Kubernetes cluster.A Kubernetes cluster builds a model around pods, or groups of containers. These pods are dynamically assigned to the underlying nodes based on resource and affinity constraints.Before, we thought of what processes to run together on the same computer. Now, we can think what pods to build around what groups of processes; pods become a nice way to model a functional unit of an application. We can even just drop in pods built by the community and have it working straight away, for instance for logging and monitoring.All processes in a pod are run on the same computer, this solves sharing resources like a mounted disk. In the background, Kubernetes allocates pods to different computers or Kubernetes nodes. We can set criteria by which this occurs like resource constraints and affinities to other pods and to nodes.A computer is a collection of resources, some processing, memory, disk, and network interfaces. Just like a computer, a pod too can be assigned a quantity of resources from the underlying pool. It also receives its own network interfaces and ip address on a virtual network for pods.The pod is the new computer.If we need a particular function to scale, we just run more copies of the pod on the cluster. When hardware is limiting, we just add more compute or storage resources to the cluster. By decoupling the resource from the function that it was created to perform, the scheduler can ensure that all available compute resources are being used as efficiently as possible.The Kubernetes replication controller is responsible for making sure that a certain number of copies of a same pod is always running at any time. Like a distributed init, if a pod fails; because a process in it failed, because pod dependencies failed, or because the node it was on goes down; Kubernetes detects it and brings up another copy on an available node.A Kubernetes service keeps track of all pods of a certain type in the cluster. For instance, we could have a app server service that keeps track of all the app server pods in the cluster. Services form a very convenient abstraction; our application can quickly find all functional units of a particular type and distribute work over them.The full Kubernetes picture. Pods are dynamically assigned to nodes. Services for each kind of pod enables discovery and load balancing. Virtual networks for the pods and services.Kubernetes is as much a framework to manage and schedule processes over a cluster as it is a new mental model to build applications on top of enforced by its grouping of processes in pods and discovery enabled by services.Full Circle and the Road\u00a0AheadManaging one computer is already a hard problem. Managing a whole bunch that talk to each other is much more complex. Thanks to the good people who\u2019re making incredible tools like Docker and Kubernetes, we now have simple models like containers, and tools for making clusters of computers behave like a single computer. Building applications at scale has never been easier.Containers and cluster management tools have also impacted the way people are building applications. They create new patterns and abstractions, the implications of which are still being explored. For instance, it would be interesting to use containers to build reusable application components and libraries. At Hasura, we\u2019re building components for databases, search, user management, and file management amongst other things so that applications can be quickly built by assembling these components and tying them together.In general, we\u2019ve come further in our pursuit to create simpler models. The fact remains that all software today is about running code and executing functions. In that respect, everything we\u2019ve built is just about managing these functions: grouping them, running multiple copies of them, locating and speaking with them, and handling failure. Pushed to its logical conclusion, perhaps some day we will see a system where we specify only functions, and the system takes care of literally everything else. That would be nice.Add backend APIs to your apps in minutes with the Hasura platform. \nSign up now for Hasura Cloud to get started!"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Postgres Containment Operators : Part 1 \u2014 Introduction Description: \nAn unofficial guide to JSONB operators in Postgres.\n Author: Hasura"}, {"role": "assistant", "content": ")An unofficial guide to JSONB operators in Postgres.What is JSONB\u00a0?jsonb and json are datatypes in Postgres that can store arbitrary JSON documents. They differ primarily in the storage format - json stores data verbatim (as text) whereas jsonb stores it in a decomposed binary format, allowing the content to be indexed.jsonb has more interesting operators like containment and existence of key/element, in addition to the data extraction operators that json supports.The official Postgres documentation is lacking, especially in defining the containment operators. This article aims to correct that.ContainmentThe containment boolean operators, @> and <@, are used to check if one JSON document is contained inside another. They denote contains and contained-in respectively. We describe only the @> operator.The datatypes in jsonb fall into three broad categories - scalars, objects and arrays. The scalars are null, boolean, string or int.a @> b is false whenever a and b are in different categories, however with one exception - the case when ais an array and b is a scalar. This leaves us with four non-trivial cases.Case 1\u200a\u2014\u200a(object,\u00a0object)a @> b is true if all the keys in b are in a and moreover, for each key k in b, v_a @> v_b must hold, where v_a and v_b are values corresponding to the key k in objects a and b respectively. For example,select '{\"a\" : 1, \"b\" : 2}' :: jsonb @> '{\"a\" : 1}' as result;  \n result\n-------- \nt\n(1 row)Case 2\u200a\u2014\u200a(array,\u00a0array)a @> b is true if for each element y of b, there is an x in a such that x @> y. Note that this definition implies that ordering and repetition are irrelevant.If you think that\u2019s a reasonable definition you may be in for a nasty surprise! Suppose you want to find all arrays that contain the objects {\"a\"\u00a0: 1}, and {\"b\"\u00a0: 2}. More things contain [{\"a\"\u00a0: 1}, {\"b\"\u00a0: 2}] than you might think. For example,select '[{\"a\" : 2},  {\"b\" : 4}]' :: jsonb @> '[{\"a\" : 2}, {\"b\" : 4}]' as result;  \n result\n--------\n t\n(1 row)\nselect '[{\"a\" : 2, \"b\" : 4}]' :: jsonb @> '[{\"a\" : 2}, {\"b\" : 4}]' as result;  \n result\n--------\n t\n(1 row)In the second case, an array with one object contains an array with two objects! Wat!\u00b9Case 3\u200a\u2014\u200a(scalar,\u00a0scalar)a @> b is true if a is exactly equal to b.Case 4\u200a\u2014\u200a(array,\u00a0scalar)a @> b is true if a @> [b]. Note, this seems to work only at the top level as demonstrated by the example below.postgres=# SELECT '[1]'::jsonb @> '1'::jsonb as result;  \n result\n--------\n t\n(1 row)\npostgres=# SELECT '[[1]]'::jsonb @> '[1]'::jsonb as result;  \n result\n--------\n f\n(1 row)ExistenceThough this class of operators is well-defined in the official documentation we include them for the sake of completeness.This class of operators is used to query for the existence of keys in objects or elements in arrays. The examples below demonstrate their usage. All of them evaluate to true.OperatorDescriptionExample?Does the key/element string exist within the JSON value?'{\"a\":1, \"b\":2}'::jsonb\u00a0? 'b'?|Do any of these key/element strings exist?'{\"a\":1, \"b\":2, \"c\":3}'::jsonb\u00a0?| array['b', 'c']?&Do all of these key/element strings exist?'[\"a\", \"b\"]'::jsonb\u00a0?& array['a', 'b']These operators combined with indexing put Postgres in a good position in the field of document-oriented database systems. It will be interesting to compare this to other document-oriented databases. Comments are welcome!)https://www.destroyallsoftware.com/talks/wat"}]}
{"messages": [{"role": "system", "content": "You are a blogger for the Hasura Blog."}, {"role": "user", "content": "Title: Haskell In Production Description: \nWhy we chose Haskell to build Hasura, and why you should too\n Author: Hasura"}, {"role": "assistant", "content": "Why we chose Haskell to build the Hasura platform, and why you should try Haskell\u00a0tooOriginally published a long time ago in a galaxy far away, AKA so long ago we don\u2019t remember the date.(TL;DR)We\u2019re building a micro-service platform christened Hasura.io, and we used Haskell as the core programming language to build it.This is a post for people who\u2019re not very sure about using Haskell in production, to convince them otherwise. We\u2019re not proselytising for conversion of a huge enterprise codebase into complete Haskell. But a little bit of Haskell here and there is probably safe.The post is bereft of any actual code samples or quantitative benchmarks and focusses on qualitative comparisons.Our BackgroundWe\u2019re a team of developers with fairly diverse backgrounds. Most of our production experience had been with the mainstream languages like C, Java, Python, C++, C#, Javascript. When we set about the task of building a PaaS/BaaS, we started evaluating what sort of language and toolkit would be best suited for building the core platform.What Attracted Us To Haskell:To be really honest, we\u2019re a bunch of young people, and seeing something like this gets us really fired up:Haskell is an advanced purely-functional programming language. An open-source product of more than twenty years of cutting-edge research, it allows rapid development of robust, concise, correct software. With strong support for integration with other languages, built-in concurrency and parallelism, debuggers, profilers, rich libraries and an active community, Haskell makes it easier to produce flexible, maintainable, high-quality software.\u00a0Source: wiki.haskell.orgPurely functional (whatever that meant at the time)! Twenty years of cutting edge research! Robust, concise and correct software! We\u2019d also already been part of some conversations about how tight and helpful the Haskell community was supposed to be.This was the stuff of engineering dreams. But Haskell was notorious for having a steep learning curve. Was it worth putting business requirements at risk? Is it really as hard as people claim it to be? What is up with the category theory discussions that come up at the drop of a hat?And then, we came across this: Beating the averagesThe promise of something that\u2019s hard to use, and then works really well and increases productivity by a few orders of magnitude, is a pretty solid and sensible promise.We all do use vim after all.First steps with HaskellAfter slightly more rigorous evaluation with some sample programs, we thus started working on projects (webapps in production) with some parts entirely written in Haskell. (Yay, microservices!). Some of the first real-world things we wrote included a ZeroMQ broker, automatically generating a typesafe CRUD and query API from a database schema and a redis-caching layer\u00a0.Some of the libraries we used during development were written by stalwarts in the Haskell and programming languages community. Haskell library authors tend to be spectacularly experienced and qualified. Coupled with the nature of the language, this sort of means the code you write is on good foundations, safety and performance wise. Check the JSON parsing speed improvements out as the library version numbers go up!After each project, we felt we were getting better superlinearly. Our experiences carried forward elsewhere too. Our Python code became more functional, concise and easier to maintain.A Real World Problem: Expose a subset of SQL in JSON with fine grained permissionsWe needed performant libraries for setting up an HTTP server, JSON parsing and interfacing with Postgres. Easy AST manipulation, the ability to reason about the correctness, and easily adding new features were the prime requirements during development. Above all, we needed the final program to scale up well, and be efficient with the tons of IO it would have to do.Requirements: Flexible codebase, Scale up, Handle IO wellDecisions decisions.Objectively too, Haskell was the choice here.The three performant libraries that we ended up with were:Warp- http serverAeson\u200a\u2014\u200aJSON parsingHasql\u200a\u2014\u200aPostgres connectionsIt was a breeze to rapidly prototype and test individual components before composing them into a whole. Midway through the project, we had to switch to a different Postgres library. The changes that we had to make were so localised and easy to reason about, that the entire transition took hardly a few programmer days.Very surprisingly we got done really quickly. When we think back, we realise that the majority of the time we spent on the project was only on the problem we had set out to solve. The code in the project relates very closely to the actual JSON to SQL transformation rules we\u2019ve formulated on paper, and Haskell quickly got everything else out of the way.A Conclusion to the RambleTo conclude this slightly rambly post, let\u2019s revisit some of the initial claims made by the maintainers of the language, and how it was useful to us in practice. There are so many discussions about how and why haskell. In fact, here\u2019s a good place to see it being thrashed out: What is haskell actually useful for?Purely functional: We could reason about our entire program, and the code reflected our reasoning almost exactly.Concise code: Write code now, refactor soon. Smaller code lets you refactor larger portions easily (and hence frequently).Robust code: Static typing with type inference, and a smart compiler (The Glorious Glasgow Haskell Compilation System).Testing: As waxed poetic by everyone, the type system dramatically reduces a huge class of unit tests. Quickcheck makes another class of tests quite a breeze, by automatically generating test cases for assertions that you make about your functions. Most of what remains is best tested manually anyway (eg: IO related failures).Concurrency with IO: GHC\u2019s threading abstractions and IO manager, makes it easy to think about concurrently executing computations and gives you all the benefit of event-based asynchronous runtimes like node. GHC has non-blocking IO (like node), supports real multiple threads and multicores naturally (unlike node) and works using events underneath (like node). It\u2019s not even a fair war.Rich libraries: Haskell libraries are extremely easy to integrate and very very easy to just dig in and modify safely for your own use (and maintain forked versions).Deployment: Private cloud deployments for Haskell code becomes really easy, because all library dependencies can be statically-linked. This makes version control and deployment a one step process.Some common roadbumpsType errors could be bizarre for a beginner, and parse errors are not helpful at all.Documentation: Types are great for documentation when they are simple and concise. This is mostly the case, but in some libraries (ahem..lens), the type signature offers little to no documentation. In these cases, it takes a significant effort to use the library, but types finally make sense and are often what one returns to.Language extensions: More often than not, when you\u2019re looking at a library in Haskell, you\u2019ll see use of GHC\u2019s language extensions. Some are very intuitive (OverloadedStrings, ScopedTypeVariables). Some are not (RankNTypes). This makes it harder to learn from the source\u009d.The ClincherNo matter what you work on, there is a joy in programming with Haskell. You feel better after you write progressively harder code. And progressively harder code keeps coming your way. There\u2019s so much working Haskell code out there, that you can just stare at days for and not really understand (but always use!).The satisfaction we feel after a good day of Haskell is unparalleled, and at the end of the day, that\u2019s what it\u2019s really all about isn\u2019t it.The Hasura platform is now live. Take it for a spin here: https://hasura.io\n"}]}
